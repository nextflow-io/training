= Introduction

== Basic concepts

Nextflow is a workflow orchestration engine and domain specific language (DSL)
that makes it easy to write data-intensive computational pipelines.

It is designed around the idea that the Linux platform is the _lingua franca_ of data science.
Linux provides many simple but powerful command-line and scripting tools that, when chained together,
facilitate complex data manipulations.

Nextflow extends this approach, adding the ability to define complex program interactions and a
high-level parallel computational environment, based on the dataflow programming model. Nextflow's
core features are:

* Workflow portability and reproducibility
* Scalability of parallelization and deployment
* Integration of existing tools, systems, and industry standards

=== Processes and Channels

In practice, a Nextflow pipeline is made by joining together different processes.
Each `process` can be written in any scripting language that can be executed by the Linux platform (Bash, Perl, Ruby, Python, etc.).

Processes are executed independently and are isolated from each other, i.e. they do not share a common
(writable) state. The only way they can communicate is via asynchronous first-in, first-out (FIFO) queues, called
`channels` in Nextflow.

Any `process` can define one or more `channels` as an `input` and `output`. The interaction between these processes,
and ultimately the pipeline execution flow itself, is implicitly defined by these `input` and `output` declarations.

image::channel-process.png[]

=== Execution abstraction

While a `process` defines _what_ command or `script` has to be executed, the executor determines
_how_ that `script` is actually run in the target platform.

If not otherwise specified, processes are executed on the local computer. The local executor
is very useful for pipeline development and testing purposes, but for real world computational
pipelines, a High Performance Computing (HPC) or cloud platform is often required.

In other words, Nextflow provides an abstraction between the pipeline's functional logic and
the underlying execution system (or runtime). Thus, it is possible to write a pipeline which runs seamlessly on your computer, a cluster, or the cloud, without being modified. You simply define the target execution platform in the configuration file.

image::execution_abstraction.png[]

=== Scripting language

Nextflow implements a declarative DSL that simplifies the writing
of complex data analysis workflows as an extension of a general purpose programming language.

This approach makes Nextflow flexible — it provides the benefits of a concise DSL for the handling of recurrent use cases with ease, *and* the flexibility and power of a general purpose
programming language to handle corner cases, in the same
computing environment. This would be difficult to implement using a purely declarative approach.

In practical terms, Nextflow scripting is an extension of the https://groovy-lang.org/[Groovy programming language] which, in turn, is a super-set of the Java programming language. Groovy can be thought of as "Python for Java", in that it simplifies the writing of code and is more approachable.

== Your first script

Here you will execute your first Nextflow script (`hello.nf`), which we will go through line-by-line.

In this toy example, the script takes an input string (a parameter called `params.greeting`) and splits it into chunks of six characters in the first process. The second process then converts the characters to upper case. The result is finally displayed on-screen.

=== Nextflow code (hello.nf)

[source,nextflow,linenums]
----
#!/usr/bin/env nextflow     <1>

params.greeting  = 'Hello world!' <2>
greeting_ch = Channel.of(params.greeting) <3>

process SPLITLETTERS { <4>
    input: <5>
    val x <6>

    output: <7>
    path 'chunk_*' <8>

    """ <9>
    printf '$x' | split -b 6 - chunk_  <10>
    """ <11>
} <12>

process CONVERTTOUPPER { <13>
    input:  <14>
    path y <15>

    output: <16>
    stdout <17>

    """ <18>
    cat $y | tr '[a-z]' '[A-Z]'  <19>
    """ <20>
} <21>

workflow{ <22>
    letters_ch = SPLITLETTERS(greeting_ch) <23>
    results_ch = CONVERTTOUPPER(letters_ch.flatten()) <24>
    results_ch.view{ it } <25>
} <26>
----
<1> The code begins with a shebang, which declares Nextflow as the interpreter.
<2> Declares a parameter `greeting` that is initialized with the value 'Hello world!'.
<3> Initializes a `channel` labelled `greeting_ch`, which contains the value from `params.greeting`. Channels are the input type for processes in Nextflow.
<4> Begins the first process, defined as `SPLITLETTERS`.
<5> Input declaration for the `SPLITLETTERS` process. Inputs can be values (`val`), files or paths (`path`), or other qualifiers (https://www.nextflow.io/docs/latest/process.html#inputs[see here]).
<6> Tells the `process` to expect an input value (`val`), that we assign to the variable 'x'.
<7> Output declaration for the `SPLITLETTERS` process.
<8> Tells the process to expect an output file(s) (`path`), with a filename starting with 'chunk_*', as output from the script. The process sends the output as a channel.
<9> Three double quotes initiate the code block to execute in this `process`.
<10> Code to execute — printing the `input` value x (called using the dollar symbol [$] prefix), splitting the string into chunks with a length of 6 characters ("Hello " and "world!"), and saving each to a file (chunk_aa and chunk_ab).
<11> Three double quotes end the code block.
<12> End of the first process block.
<13> Begins the second process, defined as `CONVERTTOUPPER`.
<14> Input declaration for the `CONVERTTOUPPER` `process`.
<15> Tells the `process` to expect an `input` file(s) (`path`; i.e. chunk_aa and chunk_ab), that we assign to the variable 'y'.
<16> Output declaration for the `CONVERTTOUPPER` process.
<17> Tells the process to expect output as standard output (stdout) and send this output as a channel.
<18> Three double quotes initiate the code block to execute in this `process`.
<19> Script to read files (cat) using the '$y' input variable, then pipe to uppercase conversion, outputting to standard output.
<20> Three double quotes end the code block.
<21> End of first `process` block.
<22> Start of the workflow scope, where each process can be called.
<23> Execute the `process` `SPLITLETTERS` on the `greeting_ch` (aka greeting channel), and store the output in the channel `letters_ch`.
<24> Execute the `process` `CONVERTTOUPPER` on the letters channel `letters_ch`, which is flattened using the operator `.flatten()`. This transforms the input channel in such a way that every item is a separate element. We store the output in the channel `results_ch`.
<25> The final output (in the `results_ch` channel) is printed to screen using the `view` operator (appended onto the channel name).
<26> End of the workflow scope.

TIP: The use of the operator `.flatten()` here is to split the two files into two separate items to be put through the next process (else they would be treated as a single element).

=== In practice

Now copy the above example into your favourite text editor and save it to a file named `hello.nf`.

WARNING: For the Gitpod tutorial, make sure you are in the folder called `nf-training`

Execute the script by entering the following command in your terminal:

[source,cmd]
----
nextflow run hello.nf
----

The output will look similar to the text shown below:

[source,cmd,linenums]
----
N E X T F L O W  ~  version 22.04.2
Launching `hello.nf` [tiny_venter] DSL2 - revision: 6879fb9372
executor >  local (3)
[26/004297] process > SPLITLETTERS (1)   [100%] 1 of 1 ✔
[8a/537930] process > CONVERTTOUPPER (1) [100%] 2 of 2 ✔
HELLO
WORLD!
----

The standard output shows (line by line):

* *1*: The Nextflow version executed.

* *2*: The script and version names.

* *3*: The executor used (in the above case: local).

* *4*: The first `process` is executed once (1). The line starts with a unique hexadecimal value (see TIP below), and ends with the percentage and job completion information.

* *5*: The second process is executed twice (2) (once for chunk_aa, once for chunk_ab).

* *6-7*: The result string from stdout is printed.

TIP: The hexadecimal numbers, like `8a/537930`, identify the unique process
execution. These numbers are also the prefix of the directories where each
process is executed. You can inspect the files produced by changing to the directory
`$PWD/work` and using these numbers to find the process-specific execution path.

IMPORTANT: The second process runs twice, executing in two different work directories
for each input file. Therefore, in the previous example the work directory [9f/1dd42a]
represents just one of the two directories that were processed. To print all the
relevant paths to the screen, use the `-ansi-log` flag (e.g. `nextflow run hello.nf -ansi-log false`).

It's worth noting that the process `CONVERTTOUPPER` is executed in parallel, so there's no guarantee that the instance processing the first split (the chunk 'Hello ') will be executed before the one processing the second split (the chunk 'world!').

Thus, it is perfectly possible that your final result will be
printed out in a different order:

[source,cmd]
....
WORLD!
HELLO
....

== Modify and resume

Nextflow keeps track of all the processes executed in your pipeline. If
you modify some parts of your script, only the processes that are changed will be re-executed. The execution of the processes
that are not changed will be skipped and the cached result will be used instead.

This allows for testing or modifying part of your pipeline without
having to re-execute it from scratch.

For the sake of this tutorial, modify the `CONVERTTOUPPER` process in
the previous example, replacing the process script with the string
`rev $y`, so that the process looks like this:

[source,nextflow,linenums]
----
process CONVERTTOUPPER {
    input:
    path y

    output:
    stdout

    """
    rev $y
    """
}
----

Then save the file with the same name, and execute it by adding the
`-resume` option to the command line:

[source,cmd]
----
nextflow run hello.nf -resume
----

It will print output similar to this:

[source,cmd]
----
N E X T F L O W  ~  version 22.04.2
Launching `hello.nf` [nostalgic_franklin] DSL2 - revision: 0b20bd3365
executor >  local (2)
[bd/6aa32b] process > SPLITLETTERS (1)   [100%] 1 of 1, cached: 1 ✔
[a0/67846c] process > CONVERTTOUPPER (1) [100%] 2 of 2 ✔
!dlrow
 olleH
----

You will see that the execution of the process `SPLITLETTERS` is
actually skipped (the process ID is the same as in the first output) — its results are
retrieved from the cache. The second process is executed as expected,
printing the reversed strings.

TIP: The pipeline results are cached by default in the directory `$PWD/work`.
Depending on your script, this folder can take a lot of disk space.
If you are sure you won't need to resume your pipeline execution, clean this folder periodically.


== Pipeline parameters

Pipeline parameters are simply declared by prepending the prefix `params` to a
variable name, separated by a dot character. Their value can be
specified on the command line by prefixing the parameter name with a
double dash character, i.e. `--paramName`.

Now, let's try to execute the previous example specifying a different input string parameter, as shown below:

[source,cmd]
----
nextflow run hello.nf --greeting 'Bonjour le monde!'
----

The string specified on the command line will override the default value
of the parameter. The output will look like this:

[source,cmd]
----
N E X T F L O W  ~  version 22.04.2
Launching `hello.nf` [adoring_heyrovsky] DSL2 - revision: 0b20bd3365
executor >  local (4)
[e9/8ebd19] process > SPLITLETTERS (1)   [100%] 1 of 1 ✔
[18/bc88cb] process > CONVERTTOUPPER (3) [100%] 3 of 3 ✔
uojnoB
m el r
!edno
----

=== In DAG-like format

To better understand how Nextflow is dealing with the data in this pipeline, below is a DAG-like figure to visualise all the `inputs`, `outputs`, `channels` and `processes`:

.Click here:
[%collapsible]
====

image::helloworlddiagram.png[]

====

