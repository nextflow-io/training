= Channels 

Channels are a key data structure of Nextflow that allows the implementation
of reactive-functional oriented computational workflows based on the https://en.wikipedia.org/wiki/Dataflow_programming[Dataflow] programming paradigm.

They are used to logically connect tasks to each other or to implement functional style data transformations.

image::channel-files.png[]

== Channel types

Nextflow distinguishes two different kinds of channels: *queue* channels and *value* channels.

=== Queue channel

A _queue_ channel is an _asynchronous_ unidirectional _FIFO_ queue that connects two processes or operators.

* _asynchronous_ means that operations are non-blocking.

* _unidirectional_ means that data flows from a producer to a consumer.

* _FIFO_ means that the data is guaranteed to be delivered in the same order as it is produced. First In, First Out.

A queue channel is implicitly created by process output definitions or using channel factories 
such as https://www.nextflow.io/docs/latest/channel.html#from[Channel.from] or https://www.nextflow.io/docs/latest/channel.html#frompath[Channel.fromPath].

Try the following snippets:

[source,nextflow,linenums]
----
ch = Channel.from(1,2,3)
println(ch)     // <1>
ch.view()       // <2>
----


<1> Use the built-in print line function `println` to print the `ch` channel.
<2> Apply the `view` method to the `ch` channel prints each item emitted by the channels.


[discrete]
=== Exercise

Try to execute this snippet, it will produce an error message.

[source,nextflow,linenums]
----
ch = Channel.from(1,2,3)
ch.view()
ch.view()
----

IMPORTANT: A queue channel can only have one and exactly one producer and one and exactly one consumer.

=== Value channels

A *value* channel (a.k.a. singleton channel) by definition is bound to a single value and it can be read unlimited times without consuming its contents.

[source,nextflow,linenums]
----
ch = Channel.value('Hello')
ch.view()
ch.view()
ch.view()
----

It prints:

```
Hello
Hello
Hello
```

A `value` channel is created using the https://www.nextflow.io/docs/latest/channel.html#value[value] factory method or by operators returning a single value, such as https://www.nextflow.io/docs/latest/operator.html#first[first], https://www.nextflow.io/docs/latest/operator.html#last[last], https://www.nextflow.io/docs/latest/operator.html#operator-collect[collect], https://www.nextflow.io/docs/latest/operator.html#operator-count[count], https://www.nextflow.io/docs/latest/operator.html#operator-min[min], https://www.nextflow.io/docs/latest/operator.html#operator-max[max], https://www.nextflow.io/docs/latest/operator.html#operator-reduce[reduce], and https://www.nextflow.io/docs/latest/operator.html#operator-sum[sum].

== Channel factories

These are Nextflow commands for creating channels that have implicit expected inputs and functions.

=== value

The `value` factory method is used to create a _value_ channel. An optional not ``null`` argument
can be specified to bind the channel to a specific value. For example:

[source,nextflow,linenums]
----
ch1 = Channel.value()                 // <1>
ch2 = Channel.value( 'Hello there' )  // <2>
ch3 = Channel.value( [1,2,3,4,5] )    // <3>
----

<1> Creates an _empty_ value channel.
<2> Creates a value channel and binds a string to it.
<3> Creates a value channel and binds a list object to it that will be emitted as a sole emission.

=== from

The factory `Channel.from` allows the creation of a queue channel with the values specified as arguments.

[source,nextflow,linenums]
----
ch = Channel.from( 1, 3, 5, 7 )
ch.view{ "value: $it" }
----

The first line in this example creates a variable `ch` which holds a channel object. This channel emits the values specified as a parameter in the `from` method. Thus the second line will print the following:

----
value: 1
value: 3
value: 5
value: 7
----


IMPORTANT: Method `Channel.from` will be deprecated and replaced by `Channel.of` (see below). 

=== of 

The method `Channel.of` works in a similar manner to `Channel.from`, though it fixes
some inconsistent behavior of the latter and provides better handling when specifying a range of values. 
For example:

[source,nextflow,linenums]
----
Channel
    .of(1..23, 'X', 'Y')
    .view()
----

=== fromList

The method `Channel.fromList` creates a channel emitting the elements provided 
by a list object specified as an argument:

[source,nextflow,linenums]
----
list = ['hello', 'world']

Channel
    .fromList(list)
    .view()
----

=== fromPath

The `fromPath` factory method creates a queue channel emitting one or more files
matching the specified glob pattern.

[source,nextflow,linenums]
----
Channel.fromPath( './data/meta/*.csv' )
----

This example creates a channel and emits as many items as there are files with a `csv` extension in the `/data/meta` folder. Each element is a file object implementing the https://docs.oracle.com/javase/8/docs/api/java/nio/file/Paths.html[Path] interface.

TIP: Two asterisks, i.e. `\**`, works like `*` but cross directory boundaries. This syntax is generally used for matching complete paths. Curly brackets specify a collection of sub-patterns.


.Available options
[%header,cols="15%,85%"]
|===
|Name
|Description

|glob
|When ``true`` interprets characters ``*``, ``?``, ``[]`` and ``{}`` as glob wildcards, otherwise handles them as normal characters (default: ``true``)

|type
| Type of path returned, either ``file``, ``dir`` or ``any`` (default: ``file``)

|hidden
| When ``true`` includes hidden files in the resulting paths (default: ``false``)

|maxDepth
| Maximum number of directory levels to visit (default: `no limit`)

|followLinks
| When ``true`` symbolic links are followed during directory tree traversal, otherwise they are managed as files (default: ``true``)

|relative
| When ``true`` return paths are relative to the top-most common directory (default: ``false``)

|checkIfExists
| When ``true`` throws an exception when the specified path does not exist in the file system (default: ``false``)
|===

Learn more about the glob patterns syntax at https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob[this link].

[discrete]
=== Exercise

Use the `Channel.fromPath` method to create a channel emitting all files with the suffix `.fq` in the `data/ggal/` directory and any subdirectory, in addition to hidden files. Then print the file names.

.Click here for the answer:
[%collapsible]
====
[source,nextflow]
----
Channel.fromPath( './data/ggal/**.fq' , hidden:true)
       .view()
----
====


=== fromFilePairs

The `fromFilePairs` method creates a channel emitting the file pairs matching a glob pattern provided by the user. The matching files are emitted as tuples, in which the first element is the grouping key of the matching pair and the second element is the list of files (sorted in lexicographical order).

[source,nextflow,linenums]
----
Channel
    .fromFilePairs('./data/ggal/*_{1,2}.fq')
    .view()
----

It will produce an output similar to the following:

```
[liver, [/user/nf-training/data/ggal/liver_1.fq, /user/nf-training/data/ggal/liver_2.fq]]
[gut, [/user/nf-training/data/ggal/gut_1.fq, /user/nf-training/data/ggal/gut_2.fq]]
[lung, [/user/nf-training/data/ggal/lung_1.fq, /user/nf-training/data/ggal/lung_2.fq]]
```

IMPORTANT: The glob pattern must contain at least a star wildcard character.

.Available options
[%header,cols="15%,85%"]
|===
|Name
|Description

|type
|Type of paths returned, either ``file``, ``dir`` or ``any`` (default: ``file``)

|hidden
|When ``true`` includes hidden files in the resulting paths (default: ``false``)

|maxDepth
|Maximum number of directory levels to visit (default: `no limit`)

|followLinks
| When ``true`` symbolic links are followed during directory tree traversal, otherwise they are managed as files (default: ``true``)

|size
| Defines the number of files each emitted item is expected to hold (default: 2). Set to ``-1`` for any.

|flat
|When ``true`` the matching files are produced as sole elements in the emitted tuples (default: ``false``).

|checkIfExists
| When ``true``, it throws an exception of the specified path that does not exist in the file system (default: ``false``)
|===

[discrete]
=== Exercise

Use the `fromFilePairs` method to create a channel emitting all pairs of fastq read in the `data/ggal/`
directory and print them. Then use the `flat:true` option and compare the output with the previous execution.

.Click here for the answer:
[%collapsible]
====
Use the following, with or without 'flat:true':

[source,nextflow]
----
Channel.fromFilePairs( './data/ggal/*_{1,2}.fq', flat:true)
       .view()
----

Then check the square brackets around the file names, to see the difference with `flat`. 
====

=== fromSRA 

The `Channel.fromSRA` method makes it possible to query the https://www.ncbi.nlm.nih.gov/sra[NCBI SRA] archive and returns a channel emitting the FASTQ files matching the specified selection criteria.

The query can be project ID(s) or accession number(s) supported by the 
https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch[NCBI ESearch API]. 

IMPORTANT: This function now requires an API key you can only get by logging into your NCBI account. 

.For help with NCBI login and key acquisition, click here:
[%collapsible]
====
1. Go to: https://www.ncbi.nlm.nih.gov/
2. Click the top right button to "Sign into NCBI". Follow their instructions.
3. Once into your account, click the button at the top right, left of `My NCBI`, usually your ID.
4. Scroll down to API key section. Copy your key.
====

IMPORTANT: You also need to use the latest edge version of Nextflow. Check your `nextflow -version`, it should say `-edge`, if not: download the newest Nextflow version, following the instructions https://www.nextflow.io/docs/edge/getstarted.html#stable-edge-releases[linked here].

For example, the following snippet will print the contents of an NCBI project ID:

[source,nextflow,linenums]
----
params.ncbi_api_key = '<Your API key here>'

Channel
  .fromSRA(['SRP073307'], apiKey: params.ncbi_api_key)
  .view()
----

IMPORTANT: Replace `<Your API key here>` with your API key.

This should print: 

[source,text,linenums]
----
[SRR3383346, [/vol1/fastq/SRR338/006/SRR3383346/SRR3383346_1.fastq.gz, /vol1/fastq/SRR338/006/SRR3383346/SRR3383346_2.fastq.gz]]
[SRR3383347, [/vol1/fastq/SRR338/007/SRR3383347/SRR3383347_1.fastq.gz, /vol1/fastq/SRR338/007/SRR3383347/SRR3383347_2.fastq.gz]]
[SRR3383344, [/vol1/fastq/SRR338/004/SRR3383344/SRR3383344_1.fastq.gz, /vol1/fastq/SRR338/004/SRR3383344/SRR3383344_2.fastq.gz]]
[SRR3383345, [/vol1/fastq/SRR338/005/SRR3383345/SRR3383345_1.fastq.gz, /vol1/fastq/SRR338/005/SRR3383345/SRR3383345_2.fastq.gz]]
(remaining omitted)
----

Multiple accession IDs can be specified using a list object:

[source,nextflow,linenums]
----
ids = ['ERR908507', 'ERR908506', 'ERR908505']
Channel
    .fromSRA(ids, apiKey: params.ncbi_api_key)
    .view()
----

[source,text,linenums]
----
[ERR908507, [/vol1/fastq/ERR908/ERR908507/ERR908507_1.fastq.gz, /vol1/fastq/ERR908/ERR908507/ERR908507_2.fastq.gz]]
[ERR908506, [/vol1/fastq/ERR908/ERR908506/ERR908506_1.fastq.gz, /vol1/fastq/ERR908/ERR908506/ERR908506_2.fastq.gz]]
[ERR908505, [/vol1/fastq/ERR908/ERR908505/ERR908505_1.fastq.gz, /vol1/fastq/ERR908/ERR908505/ERR908505_2.fastq.gz]]
----

TIP: Read pairs are implicitly managed and are returned as a list of files.

It's straightforward to use this channel as an input using the usual Nextflow syntax. The code below creates a channel containing two samples from a public SRA study and runs FASTQC on the resulting files. See: 

[source,nextflow,linenums]
----
params.ncbi_api_key = '<Your API key here>'

params.accession = ['ERR908507', 'ERR908506']
reads = Channel.fromSRA(params.accession, apiKey: params.ncbi_api_key)

process fastqc {
    input:
    tuple sample_id, file(reads_file) from reads

    output:
    file("fastqc_${sample_id}_logs") into fastqc_ch

    script:
    """
    mkdir fastqc_${sample_id}_logs
    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads_file}
    """
}
----

=== Text files

The `splitText` operator allows you to split multi-line strings or text file items, emitted by a source channel into chunks containing n lines, which will be emitted by the resulting channel. See:

----
Channel
     .fromPath('data/meta/random.txt') // <1>
     .splitText()                      // <2>
     .view()                           // <3>
----

<1> Instructs Nextflow to make a channel from the path "data/meta/random.txt".
<2> The `splitText` operator splits each item into chunks of one line by default.
<3> View contents of the channel.


You can define the number of lines in each chunk by using the parameter `by`, as shown in the following example:

----
Channel
     .fromPath('data/meta/random.txt')
     .splitText( by: 2 )
     .subscribe {
         print it;
         print "--- end of the chunk ---\n"
     }
----

TIP: The `subscribe` operator permits executionf of user defined functions each time a new value is emitted by the source channel.

An optional closure can be specified in order to transform the text chunks produced by the operator. The following example shows how to split text files into chunks of 10 lines and transform them into capital letters:

----
Channel
   .fromPath('data/meta/random.txt')
   .splitText( by: 10 ) { it.toUpperCase() }
   .view()
----

You can also make counts for each line:

----
count=0

Channel
   .fromPath('data/meta/random.txt')
   .splitText()
   .view { "${count++}: ${it.toUpperCase().trim()}" }

----

Finally, you can also use the operator on plain files (outside of the channel context):

----
  def f = file('data/meta/random.txt')
  def lines = f.splitText()
  def count=0
  for( String row : lines ) {
    log.info "${count++} ${row.toUpperCase()}"
  }
----

=== Comma separate values (.csv)

The `splitCsv` operator allows you to parse text items emitted by a channel, that are CSV file formatted. 

It then splits them into records or groups them as a list of records with a specified length.

In the simplest case, just apply the `splitCsv` operator to a channel emitting a CSV formatted text files or text entries. For example, to view only the first and fourth columns:

----
  Channel
    .fromPath("data/meta/patients_1.csv")
    .splitCsv()
    // row is a list object 
    .view { row -> "${row[0]},${row[3]}" }
----

When the CSV begins with a header line defining the column names, you can specify the parameter `header: true` which allows you to reference each value by its column name, as shown in the following example:

----
  Channel
    .fromPath("data/meta/patients_1.csv")
    .splitCsv(header: true)
    // row is a list object 
    .view { row -> "${row.patient_id},${row.num_samples}" }
----

Alternatively, you can provide custom header names by specifying a list of strings in the header parameter as shown below:

----
  Channel
    .fromPath("data/meta/patients_1.csv")
    .splitCsv(header: ['col1', 'col2', 'col3', 'col4', 'col5'] )
    // row is a list object 
    .view { row -> "${row.col1},${row.col4}" }
----

You can also process multiple csv files at the same time:

----
    Channel
      .fromPath("data/meta/patients_*.csv") // <-- just use a pattern
      .splitCsv(header:true)
      .view { row -> "${row.patient_id}\t${row.num_samples}" }
----

TIP: Notice that you can change the output format simply by adding a different delimiter.

Finally, you can also operate on csv files outside the channel context:

----
def f = file('data/meta/patients_1.csv')
  def lines = f.splitCsv()
  for( List row : lines ) {
    log.info "${row[0]} -- ${row[2]}"
  }
----

[discrete]
=== Exercise

Try inputting fastq reads into the RNA-Seq workflow from earlier using `.splitCSV`.

.Click here for the answer:
[%collapsible]
====
Add a csv text file containing the following, as an example input with the name "fastq.csv":

[source,nextflow,linenums]
----
gut,/workspace/nf-training-public/nf-training/data/ggal/gut_1.fq,/workspace/nf-training-public/nf-training/data/ggal/gut_2.fq
----

Then replace the input channel for the reads in `script7.nf`. Changing the following lines:

[source,nextflow,linenums]
----
Channel 
    .fromFilePairs( params.reads, checkIfExists: true )
    .into { read_pairs_ch; read_pairs2_ch } 
----

To a splitCsv channel factory input:

[source,nextflow,linenums]
----
Channel 
    .fromPath("fastq.csv")
    .splitCsv()
    .view () { row -> "${row[0]},${row[1]},${row[2]}" }
    .into { read_pairs_ch; read_pairs2_ch } 
----

Finally, change the cardinality of the processes that use the input data. For example, for the quantification process, change it from:

[source,nextflow,linenums]
----
process quantification {
    tag "$sample_id"
         
    input:
    path salmon_index from index_ch
    tuple val(sample_id), path(reads) from read_pairs_ch
 
    output:
    path sample_id into quant_ch
 
    script:
    """
    salmon quant --threads $task.cpus --libType=U -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id
    """
}
----

To:

[source,nextflow,linenums]
----
process quantification {
    tag "$sample_id"
         
    input:
    path salmon_index from index_ch
    tuple val(sample_id), path(reads1), path(reads2) from read_pairs_ch
 
    output:
    path sample_id into quant_ch
 
    script:
    """
    salmon quant --threads $task.cpus --libType=U -i $salmon_index -1 ${reads1} -2 ${reads2} -o $sample_id
    """
}
----

Repeat the above for the fastqc step. Now the workflow should run from a CSV file.
====

=== Tab separated values (.tsv)

Parsing tsv files works in a similar way, simply add the `sep:'\t'` option in the `splitCsv` context:

----
 Channel
      .fromPath("data/meta/regions.tsv", checkIfExists:true)
      // use `sep` option to parse TAB separated files
      .splitCsv(sep:'\t')
      // row is a list object 
      .view()
----

[discrete]
=== Exercise

Try using the tab separation technique on the file "data/meta/regions.tsv", but print just the first column, and remove the header.

.Answer:
[%collapsible]
====
 Channel
      .fromPath("data/meta/regions.tsv", checkIfExists:true)
      // use `sep` option to parse TAB separated files
      .splitCsv(sep:'\t', header:true )
      // row is a list object 
      .view { row -> "${row.patient_id}" }
====

== More complex file formats

=== JSON

We can also easily parse the JSON file format using the following groovy schema:

----
import groovy.json.JsonSlurper

def f = file('data/meta/regions.json')
def records = new JsonSlurper().parse(f)


for( def entry : records ) {
  log.info "$entry.patient_id -- $entry.feature"
}
----

IMPORTANT: When using an older JSON version, you may need to replace `parse(f)` with `parseText(f.text)`

=== YAML

This can also be used as a way to parse YAML files: 

----
import org.yaml.snakeyaml.Yaml

def f = file('data/meta/regions.json')
def records = new Yaml().load(f)


for( def entry : records ) {
  log.info "$entry.patient_id -- $entry.feature"
}
----

=== Storage of parsers into modules

The best way to store parser scripts is to keep them in a Nextflow module file. 

See the following Nextflow script:

----
include{ parseJsonFile } from './modules/parsers.nf'

process foo {
  input:
  tuple val(meta), path(data_file)

  """
  echo your_command $meta.region_id $data_file
  """
}

workflow {
    Channel.fromPath('data/meta/regions*.json') \
      | flatMap { parseJsonFile(it) } \
      | map { entry -> tuple(entry,"/some/data/${entry.patient_id}.txt") } \
      | foo
}
----

For this script to work, a file called `parsers.nf` needs to be created and stored in the modules folder in the current directory.

This file should contain the `parseJsonFile` function.

Nextflow will use this as a custom function within the workflow scope.

