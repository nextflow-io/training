= DSL2 and modules

== Basic concepts

DSL2 is our new syntax extension, developed primarily to improve readability and allow the use of modules. 

However, DSL1 is still a valid way of writing pipelines (and indeed many public pipelines are written this way), so to ensure backward compatibility, the DSL2 update has to be defined with the following directive at the beginning of each workflow script: 

----
nextflow.enable.dsl=2
----

DSL2 core features include:

* Separation of processes from their invocation.
* Use of a `workflow` directive to execute specific processes.
* Archiving of processes into modules.
* Update of syntax (pipe operator, & operator and channel forking)


== Process

=== Process definition

The new DSL separates the definition of a process from its invocation. The process definition follows the usual syntax as described in the process https://www.seqera.io/training/#_processes[documentation]. The only difference is that the `from` and `into` channel declaration has to be omitted.

Then a process can be invoked as a function in the `workflow` scope, passing the expected input channels as parameters as it if were a custom function. For example :

[source,nextflow,linenums]
----
nextflow.enable.dsl=2

process foo {
    output:
      path 'foo.txt'
    script:
      """
      echo first_command > foo.txt
      """
}

workflow {
  foo()
}
----

At the first line we enable dsl2, then we define a process called 'foo', which puts into the output channel (path: foo.txt), without using "into out_ch". This is then executed in the `workflow` directive where the individual process is called. 

IMPORTANT: A process component can be invoked only once in the same workflow context.

Next, we can add additional processes to this script and add another call from the workflow directive, thereby separating the processes into 'submodules'. 

[source,nextflow,linenums]
----
nextflow.enable.dsl=2

process foo {
    output:
      path 'foo.txt'
    script:
      """
      echo first_command > foo.txt
      """
}

process bar {
    input:
      path x
    output:
      path 'bar.txt'
    script:
      """
      head -n 1 $x > bar.txt
      echo third_command >> bar.txt
      """
}

workflow {
  foo()
  data = channel.fromPath('data/test/baz.txt')
  bar(data)
}
----

In this example, we have added the second process 'bar', which again is called within the latter workflow directive, where its input comes from a specified `channel.fromPath`. 

[discrete]
=== Exercise

Guess what will be in the output files `foo.txt` and `bar.txt`? Given `data/test/baz.txt` contains the string `second_command`. Then run the script to find out.

.Answer:
[%collapsible]
====
foo.txt will have the following content:
----
first_command
----
bar.txt will have the following content:
----
second_command
third_command
----
====

=== Process composition

Processes having matching input-output declaration can be composed so that the output of the first process is passed as input to the following process. Taking in consideration the previous process definition, it’s possible to write the following workflow directive:

[source,nextflow,linenums]
----
workflow {
    bar(foo())
}
----

[discrete]
=== Exercise

Try to work out what will be in the output `path` `bar.txt` using the updated workflow ?

.Answer:
[%collapsible]
====
`bar.txt` should contain the following:
----
first_command
third_command
----
====


=== Process outputs
A process output can also be accessed using the `out` attribute for the respective process object. 

For example:

[source,nextflow,linenums]
----
workflow {
    foo()
    bar(foo.out)
    bar.out.view()
}
----

When a process defines two or more output channels, each of them can be accessed using the array element operator e.g. `out[0]`, `out[1]`, etc. 

For example:

[source,nextflow,linenums]
----
nextflow.enable.dsl=2

process foo {
    output:
      path 'foo.txt'
      path 'extra.txt'
    script:
      """
      echo first_command > foo.txt
      echo fourth_command > extra.txt
      """
}

process bar {
    input:
      path x
    output:
      path 'bar.txt'
    script:
      """
      head -n 1 $x > bar.txt
      echo third_command >> bar.txt
      """
}

data = channel.fromPath('./baz.txt')

workflow {
  foo()
  bar(foo.out[1])
  bar.out.view()
}
----


[discrete]
=== Exercise

What would you expect to find in `bar.txt`?:


.Answer:
[%collapsible]
====
`bar.txt` should contain the following:
----
fourth_command
third_command
----
====

Another option is using named outputs (see below).

=== Process named `output`

The process output definition allows the use of the `emit` option to define a name identifier that can be used to reference the channel in the external scope. For example:

[source,nextflow,linenums]
----
nextflow.enable.dsl=2

process foo {
  output:
    path '*.bam', emit: samples_bam

  '''
  echo result > output.bam
  '''
}

workflow {
    foo()
    foo.out.samples_bam.view()
}
----

=== Process named `stdout`

The process can name `stdout` using the `emit` option:

[source,nextflow,linenums]
----
nextflow.enable.dsl=2

process sayHello {
    input:
        val cheers
    output:
        stdout emit: verbiage
    script:
    """
    echo -n $cheers
    """
}

workflow {
    things = channel.of('Hello world!', 'Yo, dude!', 'Duck!')
    sayHello(things)
    sayHello.out.verbiage.view()
}
----

== Workflow

=== Workflow definition

The `workflow` keyword allows the definition of sub-workflow components that enclose the invocation of one or more processes and operators:

[source,nextflow,linenums]
----
workflow my_pipeline {
    foo()
    bar( foo.out.collect() )
}
----

For example, the above snippet defines a workflow component, named `my_pipeline`, that can be invoked from another workflow component definition as any other function or process i.e. `my_pipeline()`.

=== Workflow parameters

A workflow component can access any variable and parameter defined in the outer scope:

[source,nextflow,linenums]
----
params.data = '/some/data/file'

workflow my_pipeline {
    if( params.data )
        bar(params.data)
    else
        bar(foo())
}
----

=== Workflow inputs

A workflow component can declare one or more input channels using the `take` keyword. For example:

[source,nextflow,linenums]
----
workflow my_pipeline {
    take: data
    main:
    foo(data)
    bar(foo.out)
}
----

IMPORTANT: When the `take` keyword is used, the beginning of the workflow body needs to be identified with the `main` keyword.

Then, the input can be specified as an argument in the workflow invocation statement:

[source,nextflow,linenums]
----
workflow {
    my_pipeline( channel.from('/some/data') )
}
----

NOTE: Workflow inputs are by definition: channel data structures. If a basic data type is provided instead, i.e. number, string, list, etc., it’s implicitly converted to a channel value (ie. non-consumable).

=== Workflow outputs

A workflow component can declare one or more out channels using the emit keyword. For example:

[source,nextflow,linenums]
----
workflow my_pipeline {
    main:
      foo(data)
      bar(foo.out)
    emit:
      bar.out
}
----

Then, the result of the `my_pipeline` execution can be accessed using the out property i.e. `my_pipeline.out`. When there are multiple output channels declared, use the array bracket notation to access each output component as described for the Process outputs definition.

Alternatively, the output channel can be accessed using the identifier name it’s assigned to in the emit declaration:

[source,nextflow,linenums]
----
workflow my_pipeline {
   main:
     foo(data)
     bar(foo.out)
   emit:
     my_data = bar.out
}
----

Then, the result of the above snippet can accessed using `my_pipeline.out.my_data`.

=== Implicit workflow

A workflow definition which does not declare any name is assumed to be the main workflow and it’s implicitly executed. Therefore it’s the entry point of the workflow application.

NOTE: Implicit workflow definition is ignored when a script is included as module. This allows the writing of a workflow script that can be used either as a library module and as application script.

TIP: An alternative workflow entry can be specified using the `-entry` command line option.

=== Workflow composition

Workflows defined in your script or imported by a module inclusion can be invoked and composed as any other process in your application.

[source,nextflow,linenums]
----
workflow flow1 {
    take: data
    main:
        foo(data)
        bar(foo.out)
    emit:
        bar.out
}

workflow flow2 {
    take: data
    main:
        foo(data)
        baz(foo.out)
    emit:
        baz.out
}

workflow {
    take: data
    main:
      flow1(data)
      flow2(flow1.out)
}
----

NOTE: Nested workflow execution determines an implicit scope. Therefore the same process can be invoked in two different workflow scopes, like for example foo in the above snippet that is used in both flow1 and flow2. The workflow execution path along with the process names, defines the process (fully qualified) name that is used to distinguish the two different process invocations (i.e. flow1:foo and flow2:foo in the above example).

TIP : The process fully qualified name can be used as a valid process selector in the `nextflow.config` file and it has priority over the process simple name.

== Modules

The new DSL allows the definition of module scripts that can be included and shared across workflow applications.

A module can contain the definition of a function, `process` and `workflow` definitions as described in the above sections.

=== Modules include

A component defined in a module script can be imported into another Nextflow script using the `include` keyword.

For example:

[source,nextflow,linenums]
----
include { foo } from './path/to/modules.nf'

workflow {
    data = channel.fromPath('/some/data/*.txt')
    foo(data)
}
----

The above snippets include a process with name `foo` defined in the module script in the main execution context, as such it can be invoked in the `workflow` scope. "modules.nf" is a file that would contain multiple process code blocks (including `foo`).

Nextflow implicitly looks for the script file "./path/to/modules.nf", resolving the path within the included script location.

NOTE: Relative paths must begin with the `./` prefix.

=== Multiple inclusions

A Nextflow script allows the inclusion of any number of modules. When multiple components need to be included from the same module script, the component names can be specified in the same inclusion using the curly brackets notation as shown below:

[source,nextflow,linenums]
----
include { foo; bar } from './some/module'

workflow {
    data = channel.fromPath('/some/data/*.txt')
    foo(data)
    bar(data)
}
----

=== Module aliases

When including a module component it’s possible to specify a name alias. This allows the inclusion and the invocation of the same component multiple times in your script using different names. For example:

[source,nextflow,linenums]
----
include { foo } from './some/module'
include { foo as bar } from './other/module'

workflow {
    foo(some_data)
    bar(other_data)
}
----

The same is possible when including multiple components from the same module script as shown below:

[source,nextflow,linenums]
----
include { foo; foo as bar } from './some/module'

workflow {
    foo(some_data)
    bar(other_data)
}
----

=== Module parameters

A module script can define one or more parameters using the same syntax as Nextflow workflow scripts (as well as defining workflow or defined functions):

[source,nextflow,linenums]
----
params.foo = 'Hello'
params.bar = 'world!'

def sayHello() {
    println "$params.foo $params.bar"
}
----

Parameters are inherited from the including context. For example:

[source,nextflow,linenums]
----
params.foo = 'Hola'
params.bar = 'Mundo'

include {sayHello} from './some/module'

workflow {
    sayHello()
}
----

The above snippet should print:

[source,bash,linenums]
----
Hola Mundo
----

NOTE: The module inherits the parameters defined before the include statement, therefore any further parameters set later are ignored.

TIP: Define all pipeline parameters at the beginning of the script before any include declaration.

The option `addParams` can be used to extend the module parameters without affecting the external scope. For example:

[source,nextflow,linenums]
----
include {sayHello} from './some/module' addParams(foo: 'Ciao')

workflow {
    sayHello()
}
----

The above snippet should prints:

[source,bash,linenums]
----
Ciao world!
----

Finally the include option `params` allows the specification of one or more parameters without inheriting any value from the external environment.

[discrete]
=== Exercise

Try to run the above code. Replacing `./some/module` with the file name to a process called `sayHello()`, which expects `foo` and `bar` parameters. Remember to use ./ for current directory.

.Answer:
[%collapsible]
====
1. First save the following to `./modules/my_modules.nf`:
----
params.foo = 'Hello'
params.bar = 'world!'

def sayHello() {
    println "$params.foo $params.bar"
}
----

2. Then run `nextflow run myscript.nf`:

Where `myscript.nf` is the following:
----
nextflow.enable.dsl=2

params.foo = 'Hola'
params.bar = 'Mundo'

include {sayHello} from './modules/my_modules.nf'

workflow {
    sayHello()
}
----
====

== DSL2 migration notes

Some of the syntax has changed between DSL1 and DSL2. 

These are a few of the key changes:

- The declaration: `nextflow.enable.dsl=2` is used in place of `nextflow.preview.dsl=2`.

- Process inputs and outputs of type `set` have to be replaced with `tuple`.

- Process output option mode `flatten` is not available anymore. Replace it using the `flatten` operator on the corresponding output channel.

- Anonymous and unwrapped includes are not supported anymore. Replace it with a explicit module inclusion. For example:

[source,nextflow,linenums]
----
include './some/library'
include bar from './other/library'

workflow {
  foo()
  bar()
}
----

Should be replaced with:

[source,nextflow,linenums]
----
include { foo } from './some/library'
include { bar } from './other/library'

workflow {
  foo()
  bar()
}
----

- The use of unqualified value (`val`) and `file` elements into input tuples is not allowed anymore. Replace them with a corresponding `val` or `path` qualifiers:

[source,nextflow,linenums]
----
process foo {
input:
  tuple X, 'some-file.bam'
 script:
   '''
   your_command
   '''
}
----

Use:

[source,nextflow,linenums]
----
process foo {
input:
  tuple val(X), path('some-file.bam')
 script:
   '''
   your_command --in $X some-file.bam
   '''
}
----

- The use of unqualified value (`val`) and `file` elements into output tuples is not allowed anymore. Replace them with a corresponding `val` or `path` qualifiers:

[source,nextflow,linenums]
----
process foo {
output:
  tuple X, 'some-file.bam'

script:
   X = 'some value'
   '''
   your_command > some-file.bam
   '''
}
----
Use:

[source,nextflow,linenums]
----
process foo {
output:
  tuple val(X), path('some-file.bam')

script:
   X = 'some value'
   '''
   your_command > some-file.bam
   '''
}
----

- Operator `bind` has been deprecated by DSL2 syntax

- Operator `operator` << has been deprecated by DSL2 syntax.

- Operator `choice` has been deprecated by DSL2 syntax. Use `branch` instead.

- Operator `close` has been deprecated by DSL2 syntax.

- Operator `create` has been deprecated by DSL2 syntax.

- Operator `countBy` has been deprecated by DSL2 syntax.

- Operator `into` has been deprecated by DSL2 syntax since it’s not needed anymore.

- Operator `fork` has been renamed to `multiMap`.

- Operator `groupBy` has been deprecated by DSL2 syntax. Replace it with `groupTuple`

- Operator `print` and `println` have been deprecated by DSL2 syntax. Use `view` instead.

- Operator `merge` has been deprecated by DSL2 syntax. Use `join` instead.

- Operator `separate` has been deprecated by DSL2 syntax.

- Operator `spread` has been deprecated with DSL2 syntax. Replace it with `combine`.

- Operator `route` has been deprecated by DSL2 syntax.

