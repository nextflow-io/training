= AWS Configuration

https://aws.amazon.com/batch/[AWS Batch] is a managed computing service that allows the execution of containerized workloads in the Amazon cloud infrastructure.

Nextflow provides built-in support for AWS Batch and enables the seamless deployment of a Nextflow pipeline in the cloud by offloading the process executions as Batch jobs.


== Basic requirements

* All workflow process executions must be containerized, therefore one or more containers must
be defined either in the pipeline script or the workflow config file.

* Container images need to be published in a Docker registry such as Docker Hub, Quay or ECS Container Registry that can be reached by ECS Batch.

* An S3 bucket use as shared memory to transfer input-output data across tasks.

* AWS CLI tool installed either in the job container or the compute node AMI,
to transfer the input data from the S3 bucket to the container, and back from the
container to the shared bucket.

* Configure at least a Batch queue and associated Compute Environment.


== Minimal configuration

Once the Batch environment is configured specifying the instance types to be used and the max number
of CPUs to be allocated, you need to create a Nextflow configuration file like the one shown below:

[source,config,linenums]
----
process.executor = 'awsbatch'       // <1>
process.queue = 'nextflow-ci'       // <2>
process.container = 'nextflow/rnaseq-nf:latest'      // <3>
workDir = 's3://nf-course/work/'    // <4>
aws.region = 'eu-west-1'            // <5>
aws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws' // <6>
----

<1> Set AWS Batch as the executor to run the processes in the workflow
<2> The name of the computing queue defined in the Batch environment
<3> The Docker container image to be used to run each job
<4> The workflow work directory must be an AWS S3 bucket
<5> The AWS region to be used
<6> The path of the AWS CLI tool required to download/upload files to/from the container

TIP: The best practice is to keep this setting as a separate
https://www.nextflow.io/docs/latest/config.html#config-profiles[profile] in your
workflow config file. This allows the execution with a simple command.

[cmd]
----
nextflow run <my pipeline> -profile my-batch-profile
----

The complete details describing AWS Batch deployment are available at https://www.nextflow.io/docs/latest/awscloud.html#aws-batch[this link].

== Task directives

The following task directives are applied and converted to the corresponding Batch API
when using the AWS Batch executor:

[%header,cols="15%,85%"]
|=======================
|Name           |Description
|https://www.nextflow.io/docs/latest/process.html#accelerator[accelerator]  | The hardware accelerator i.e. GPU to be used.
|https://www.nextflow.io/docs/latest/process.html#container[container]      | The Docker container to be used for the job execution.
|https://www.nextflow.io/docs/latest/process.html#cpus[cpus]                | Number of CPUs (make sure the chosen instance types are able to fullfill the request).
|https://www.nextflow.io/docs/latest/process.html#errorstrategy[errorStrategy]  | Errors handling policy.
|https://www.nextflow.io/docs/latest/process.html#queue[queue]              | The (AWS) queue to be used to run the task.
|https://www.nextflow.io/docs/latest/process.html#memory[memory]            | Amount of memory to be reserved to run the job (make sure the chosen instance types are able to fullfill the request).
|https://www.nextflow.io/docs/latest/process.html#maxretries[maxRetries]    | Max number of times a failing job can be retried.
|https://www.nextflow.io/docs/latest/process.html#time[time]                | Max job execution time i.e. job timeout.
|=======================

== Note on retry

Automatic error fail-over is an essential feature when deploying cloud workloads,
in particular, when https://aws.amazon.com/ec2/spot/[spot-instances] are used.

Both Nextflow and AWS Batch provides their own error handling strategies to record faulty conditions.

The error strategy can be selected when deploying Nextflow:

[%header,cols="20%,80%"]
|=======================
| Mechanism                       | Behavior
|Nextflow native error handling   | When directive `errorStrategy` is set to `'retry'` *and* `maxRetries` is greater than `0` (default: `1`) then Nextflow built-in error handling policy is applied. If a job returns
a non-zero or some expected output file is missing, the job is retried according to the specified limits.
Each time a new job execution is retried, a new task work directory is created using the usual Nextflow
strategy.
|AWS Batch native error handling  | When directive `errorStrategy` is not specified or set to any strategy
different from `retry` *and* `maxRetries` greater than zero, then AWS built-in
https://docs.aws.amazon.com/batch/latest/userguide/job_retries.html[job retries] mechanism is used. The value of `maxRetries` +1 is used for the max number of expected job attempts. This approach is completely transparent to Nextflow i.e. Nextflow will not be aware if a job is re-executed. Additionally, for
this reason, the same task work directory will be used for all retries (this is still safe as the computation
happens in the container scratch storage). Nextflow will still report an error if an expected output is missing.
|=======================


== Volume mounts

EBS volumes (or other supported storage) can be mounted in the job container using the following configuration snippet:

[source,config,linenums]
----
aws {
  batch {
      volumes = '/some/path'
  }
}
----

Multiple volumes can be specified using comma-separated paths. The usual Docker volume mount syntax can be used to define complex volumes for which the container paths are different from the host paths or to specify a read-only option:

[source,config,linenums]
----
aws {
  region = 'eu-west-1'
  batch {
      volumes = ['/tmp', '/host/path:/mnt/path:ro']
  }
}
----

IMPORTANT:

* This is a global configuration that has to be specified in a Nextflow config file, as such it's applied to *all* process executions.
* Nextflow expects those paths to be available. It does not handle the provision of EBS volumes or
another kind of storage.


== Custom job definition

Nextflow automatically creates the Batch https://docs.aws.amazon.com/batch/latest/userguide/job_definitions.html[Job definitions] needed to execute your pipeline processes. Therefore, it's not required to define them before running your workflow.

However, you may still need to specify a custom Job Definition to provide fine-grained control of the configuration settings of a specific job e.g. to define custom mount paths or other special settings of a Batch Job.

To use your own job definition in a Nextflow workflow, use it in place of the container image name by
adding the `job-definition://` string as a prefix. For example:

[source,nextflow,linenums]
----
process {
    container = 'job-definition://your-job-definition-name'
}
----

== Custom image

Since Nextflow requires the AWS CLI tool to be accessible in the computing environment
a common solution consists of creating a custom AMI and install it in a self-contained manner
e.g. using Conda package manager.

IMPORTANT: When creating your custom AMI for AWS Batch, make sure to use the _Amazon ECS-Optimized Amazon Linux AMI_ as the base image.

The following snippet shows how to install AWS CLI with Miniconda:

[cmd,linenums]
----
sudo yum install -y bzip2 wget
wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh -b -f -p $HOME/miniconda
$HOME/miniconda/bin/conda install -c conda-forge -y awscli
rm Miniconda3-latest-Linux-x86_64.sh
----

NOTE: The `aws` tool will be placed in a directory named `bin` in the main installation folder. Modifying this directory structure, after the installation, will cause the tool to not work properly.

Finally, specify the `aws` full path in the Nextflow config file as shown below:

[source,config]
----
aws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws'
----


== Launch template

An alternative approach is to create a custom AMI using a
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-templates.html[Launch template] that
installs the AWS CLI tool during the instance boot using custom user data.

In the EC2 dashboard, create a Launch template specifying the following in the user data field:

[source,config,linenums]
----
MIME-Version: 1.0
Content-Type: multipart/mixed; boundary="//"

--//
Content-Type: text/x-shellscript; charset="us-ascii"

#!/bin/sh
## install required deps
set -x
export PATH=/usr/local/bin:$PATH
yum install -y jq python27-pip sed wget bzip2
pip install -U boto3

## install awscli
USER=/home/ec2-user
wget -q https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh -b -f -p $USER/miniconda
$USER/miniconda/bin/conda install -c conda-forge -y awscli
rm Miniconda3-latest-Linux-x86_64.sh
chown -R ec2-user:ec2-user $USER/miniconda

--//--
----

Then in the Batch dashboard create a new compute environment and specify the newly created
launch template in the corresponding field.

== Expandable EBS volume

A common issue when deploying genomics workload is related to estimating the amount of storage
that is allocated in the compute nodes.

One possible solution consists of using a background process running in the compute nodes that
periodically checks the amount of free space and automatically expands the avail storage
mounting new EBS volume(s).

To take advantage of this mechanism with AWS Batch, we also need to make sure the Docker storage driver is mounted over this expandable volume instead of the boot disk.

The aforementioned pattern can be implemented using the following launch template:

[source,config,linenums]
----
MIME-Version: 1.0
Content-Type: multipart/mixed; boundary="//"

--//
Content-Type: text/cloud-boothook; charset="us-ascii"

su - root << 'EOF'
(
set -x
uname -r
env | sort
export PATH=/usr/local/bin:$PATH
yum install -y jq btrfs-progs python27-pip sed wget bzip2
pip install -U boto3
cp -au /var/lib/docker /var/lib/docker.bk
rm -rf /var/lib/docker/*
cd /opt && curl -s https://nf-xpack.s3.amazonaws.com/v1/aws-ebs-autoscale.tgz | tar xz
sh /opt/ebs-autoscale/bin/init-ebs-autoscale.sh /var/lib/docker /dev/sdc  2>&1 > /var/log/init-ebs-autoscale.log
sed -i 's+^DOCKER_STORAGE_OPTIONS=.*+DOCKER_STORAGE_OPTIONS="--storage-driver btrfs"+g' /etc/sysconfig/docker-storage
cp -au /var/lib/docker.bk/* /var/lib/docker

) 2>&1 | grep -v LESS_TERMCAP >  ~/boot.log
EOF

--//
Content-Type: text/x-shellscript; charset="us-ascii"

#!/bin/sh
su - root << 'EOF'
(
set -x
## install awscli
USER=/home/ec2-user
wget -q https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh -b -f -p $USER/miniconda
$USER/miniconda/bin/conda install -c conda-forge -y awscli
rm Miniconda3-latest-Linux-x86_64.sh
chown -R ec2-user:ec2-user $USER/miniconda

) &>> ~/boot.log
EOF
cp ~/boot.log ~ec2-user/boot.log

--//--
----

Once created, the template can be specified when creating the AWS Batch
compute environment.

IMPORTANT: Make sure to use _Amazon ECS-Optimized Amazon Linux AMI_ (not Amazon Linux 2) when
using the launch template.


== FSx for Lustre with Nf-xpack

AWS S3 is a fast and cheap storage solution in the cloud; however, it's not a file storage solution designed for use
in HPC shared file systems.

The optional Enterprise Extension Pack for Nextflow provides an extended executor for AWS Batch that allows the usage
of https://aws.amazon.com/fsx/lustre/[Amazon FSx for Lustre] (or any other POSIX compliant file system)
as shared storage in place of an S3 bucket.

The Nextflow extended executor for Batch takes care of the mounting of the shared file system in the corresponding job containers. However, it also needs to be mounted in the computing nodes.

=== Launch template

The following launch template can be used to mount the Lustre shared file system:

[source,config,linenums]
----
MIME-Version: 1.0
Content-Type: multipart/mixed; boundary="//"

--//
Content-Type: text/cloud-boothook; charset="us-ascii"

su - root << 'EOF'
(
set -x
uname -r
env | sort
export PATH=/usr/local/bin:$PATH
yum install -y jq btrfs-progs python27-pip sed wget bzip2
pip install -U boto3
cp -au /var/lib/docker /var/lib/docker.bk
rm -rf /var/lib/docker/*
cd /opt && curl -s https://nf-xpack.s3.amazonaws.com/v1/aws-ebs-autoscale.tgz | tar xz
sh /opt/ebs-autoscale/bin/init-ebs-autoscale.sh /var/lib/docker /dev/sdc  2>&1 > /var/log/init-ebs-autoscale.log
sed -i 's+^DOCKER_STORAGE_OPTIONS=.*+DOCKER_STORAGE_OPTIONS="--storage-driver btrfs"+g' /etc/sysconfig/docker-storage
cp -au /var/lib/docker.bk/* /var/lib/docker
## install fsx
SCRATCH=/scratch
FSXNAME=fs-0269031ec160509c9.fsx.eu-west-1.amazonaws.com
yum -q install -y lustre-client
mkdir -p $SCRATCH
mount -t lustre -o noatime,flock $FSXNAME@tcp:/fsx $SCRATCH

) 2>&1 | grep -v LESS_TERMCAP >  ~/boot.log
EOF

--//
Content-Type: text/x-shellscript; charset="us-ascii"

#!/bin/sh
su - root << 'EOF'
(
set -x
## install awscli
USER=/home/ec2-user
wget -q https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh -b -f -p $USER/miniconda
$USER/miniconda/bin/conda install -c conda-forge -y awscli
rm Miniconda3-latest-Linux-x86_64.sh
chown -R ec2-user:ec2-user $USER/miniconda
## fix fsx ownership
SCRATCH=/scratch
chown ec2-user:ec2-user $SCRATCH
) &>> ~/boot.log
EOF
cp ~/boot.log ~ec2-user/boot.log

--//--
----

In the above snippet, replace the variables `FSXNAME` and `SCRATCH` with the appropriate values
corresponding to your environment.

NOTE: Nextflow has to be launched from an instance having access to the same FSx Lustre storage.

=== Launching instance configuration

Use the following snippet to install the Lustre client:

[cmd,linenums]
----
SCRATCH=/scratch
FSXNAME=fs-0269031ec160509c9.fsx.eu-west-1.amazonaws.com
sudo yum  install -y lustre-client
sudo mkdir -p $SCRATCH
sudo mount -t lustre -o noatime,flock $FSXNAME@tcp:/fsx $SCRATCH
sudo chown ec2-user:ec2-user $SCRATCH
----

NOTE: Make also sure the storage and the computing nodes use the same VPC and security groups.
For further details check https://docs.aws.amazon.com/fsx/latest/LustreGuide/limit-access-security-groups.html[here].

=== Nextflow configuration

Define the following env variable:

[cmd]
----
export NXF_GRAB=io.seqera:nf-xpack:0.2.0
----

Define basic Nextflow configuration parameters:

[source,config,linenums]
----
process.container = 'nextflow/rnaseq-nf:latest'
process.executor = 'awsbatch'
process.queue = 'nf-queue-with-fsx'
aws.region = 'eu-west-1'
workDir = '/scratch/work'
----

Then run Nextflow as usual:

[cmd]
----
nextflow run rnaseq-nf
----

NOTE: This requires an extra endpoint configuration to access the Nf-xpack distribution.


== Batch squared

Batch squared consists in submitting a Nextflow launcher application
as a Batch job itself.

A good tutorial with additional information about this deployment can be found https://docs.opendata.aws/genomics-workflows/orchestration/nextflow/nextflow-overview.html[here].

== Advanced tuning

When deploying data-intensive workloads using S3 as shared storage the large number
of parallel file uploads/downloads can create network congestion and stall the ECS
agent running in the compute node, making it irresponsive.

A simple solution is to try to avoid to big instances for jobs requiring few CPUs (in order to
avoid too many parallel jobs in the compute instance).

Also, the following parameters can be used to help to mitigate this issue:

[source,config,linenums]
----
aws {
    batch {
        maxTransferAttempts = 20
        delayBetweenAttempts = 1000
        maxParallelTransfers = 8
    }

    client {
        maxConnections = 8          // This may depends on num of avail CPUs
        uploadMaxThreads = 8        // This may depends on num of avail CPUs
        uploadChunkSize = '100MB'   // Larger chunk sizes may be more stable
        uploadMaxAttempts = 10
        uploadRetrySleep = '10 sec'
        maxErrorRetry = 20
    }
}
----

Advanced configuration settings are described at https://www.nextflow.io/docs/latest/config.html#scope-aws[this link].

== Hybrid deployments

Nextflow allows the use of multiple executors in the same workflow application. This feature enables the deployment of hybrid workloads in which some jobs are executed in the local computer or local computing cluster, and some jobs are offloaded to AWS Batch service.

To enable this feature, use one or more https://www.nextflow.io/docs/latest/config.html#config-process-selectors[process selectors] in your Nextflow configuration file to apply the https://www.nextflow.io/docs/latest/awscloud.html#awscloud-batch-config[AWS Batch configuration] for subsets of processes in your workflow. For example:

[source,config,linenums]
----
process {
    executor = 'slurm'  // <1>
    queue = 'short'     // <2>

    withLabel: bigTask {          // <3>
      executor = 'awsbatch'       // <4>
      queue = 'my-batch-queue'    // <5>
      container = 'my/image:tag'  // <6>
  }
}

aws {
    region = 'eu-west-1'    // <7>
}
----

<1> Set `slurm` as the default executor
<2> Set the queue for the SLURM cluster
<3> Setting of for the process named `bigTask`
<4> Set `awsbatch` as executor for the `bigTask` process
<5> Set the queue for the `bigTask` process
<6> set the container image to deploy the `bigTask` process
<7> Defines the region for Batch execution

