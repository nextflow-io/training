= AWS Configuration 

https://aws.amazon.com/batch/[AWS Batch] is a managed computing service that allows the execution of containerized workloads in the Amazon cloud infrastructure.

Nextflow provides built-in support for AWS Batch which allows the seamless deployment of a Nextflow pipeline in the cloud offloading the process executions as Batch jobs.


== Basic requirements 

* All workflow process executions must be containerized, therefore one or more containers must 
be defined either in the pipeline script or the workflow config file.

* Container images need to be published in a Docker registry such as Docker Hub, Quay or ECS Container Registry that can be reached by ECS Batch

* An S3 bucket used as shared memory to transfer input-output data across tasks. 

* AWS CLI tool installed either in the job container or the compute node AMI,
to transfer the input data from the S3 bucket to the container, and back from the 
container to the shared bucket.

* Configure at least a Batch queue and associated Compute Environment.


== Minimal configuration 

Once the Batch environment is configured specifying the instance types to be used and the max number 
of CPUs to be allocated, you need to created a Nextflow configuration file like the one showed below:

[source,config,linenums]
----
process.executor = 'awsbatch'       // <1>
process.queue = 'nextflow-ci'       // <2>
process.container = 'nextflow/rnaseq-nf:latest'      // <3>
workDir = 's3://nf-course/work/'    // <4>
aws.region = 'eu-west-1'            // <5>
aws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws' // <6>
----

<1> Set AWS Batch as the executor to run the processes in the workflow
<2> The name of the computing queue defined in the Batch environment
<3> The Docker container image to be used to run each job
<4> The workflow work directory must be an AWS S3 bucket
<5> The AWS region to be used
<6> The path of the AWS CLI tool required to download/upload files to/from the container 

TIP: The best practices is to keep this setting as a separate 
https://www.nextflow.io/docs/latest/config.html#config-profiles[profile] in your 
workflow config file. This allows the execution with a simple command. 

```
nextflow run <my pipeline> -profile my-batch-profile
```

The complete details about AWS Batch deployment are available at https://www.nextflow.io/docs/latest/awscloud.html#aws-batch[this link].

== Task directives 

The following tasks directives are applied and converted to the corresponding Batch API 
when using the AWS Batch executor: 

[%header,cols="15%,85%"]
|=======================
|Name           |Description 
|https://www.nextflow.io/docs/latest/process.html#accelerator[accelerator]  | The hardware accelerator i.e. GPU to be used.
|https://www.nextflow.io/docs/latest/process.html#container[container]      | The Docker container to be used for the job execution.
|https://www.nextflow.io/docs/latest/process.html#cpus[cpus]                | Number of CPUs (make sure the chosen instance types are able to fullfil the request).
|https://www.nextflow.io/docs/latest/process.html#errorstrategy[errorStrategy]  | Errors handling policy.
|https://www.nextflow.io/docs/latest/process.html#queue[queue]              | The (AWS) queue to be used to run the task.
|https://www.nextflow.io/docs/latest/process.html#memory[memory]            | Amount of memory to be reserved to run the job (make sure the chosen instance types are able to fullfil the request). 
|https://www.nextflow.io/docs/latest/process.html#maxretries[maxRetries]    | Max number of times a failing job can be retried.
|https://www.nextflow.io/docs/latest/process.html#time[time]                | Max job execution time i.e. job timeout.
|=======================

== Note on retry 

Automatic error fail-over is an essential feature when deploying cloud workloads,
in particular, when https://aws.amazon.com/ec2/spot/[spot-instances] are used.

Both Nextflow and AWS Batch provides their own error handling strategies to record faulty condition. 

When deploying Nextflow can choose which one to use: 

[%header,cols="20%,80%"]
|=======================
| Mechanism                       | Behavior
|Nextflow native error handling   | When directive `errorStrategy` is set to `'retry'` *and* `maxRetries` is greater than `0` (default: `1`) then Nextflow built-in error handling policy is applied. If a job returns
a non-zero or some expected output file is missing, the job is retied according the specified limits.
Each time a new job execution is retried, a new task work directory is created using the usual Nextflow 
strategy.
|AWS Batch native error handling  | When directive `errorStrategy` is not specified or set to any strategy 
different from `retry` *and* `maxRetries` greater than zero, then AWS built-in 
https://docs.aws.amazon.com/batch/latest/userguide/job_retries.html[job retries] mechanism is used. The value of `maxRetries` +1 is used is used the max number of expected job attempts. This approach is completely transparent to Nextflow i.e. if job is re-executed Nextflow will not be aware of that, for 
the same reason when same task work directory will be used for all retries (still safe since computation 
happens in the container scratch storage). Still Nextflow will report an error if a expected output is missing.
|=======================


== Volume mounts 

EBS volumes (or other supported storage) can be mounted in the job container using the following configuration snippet: 

```
aws {
  batch {
      volumes = '/some/path'
  }
}
```

Multiple volumes can be specified using comma-separated paths. The usual Docker volume mount syntax can be used to define complex volumes for which the container paths is different from the host paths or to specify a read-only option: 

```
aws {
  region = 'eu-west-1'
  batch {
      volumes = ['/tmp', '/host/path:/mnt/path:ro']
  }
}
```

IMPORTANT: 

* This a global configuration that has to be specified in a Nextflow config file, as such it's applied to *all* process executions.
* Nextflow expects those paths to be available. It does not handle the provision of EBS volumes or 
other kind of storage. 


== Custom job definition

Nextflow automatically creates the Batch https://docs.aws.amazon.com/batch/latest/userguide/job_definitions.html[Job definitions] needed to execute your pipeline processes. Therefore it's not required to define them before run your workflow.

However, you may still need to specify a custom Job Definition to provide fine-grained control of the configuration settings of a specific job e.g. to define custom mount paths or other special settings of a Batch Job.

To use your own job definition in a Nextflow workflow, use it in place of the container image name,
prefixing it with the `job-definition://` string. For example: 

```
process {
    container = 'job-definition://your-job-definition-name'
}
```

== Custom image 

Since Nextflow requires the AWS CLI tool to be accessible in the computing environment 
a common solution consists of creating a custom AMI and install it in a self-contained manner 
e.g. using Conda package manager.

IMPORTANT: When creating your custom AMI for AWS Batch, make sure to use the _Amazon ECS-Optimized Amazon Linux AMI_ as the base image.

The following snippet shows how to install AWS CLI with Miniconda:

```
sudo yum install -y bzip2 wget
wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh -b -f -p $HOME/miniconda
$HOME/miniconda/bin/conda install -c conda-forge -y awscli
rm Miniconda3-latest-Linux-x86_64.sh
```

NOTE: The `aws` tool will be placed in a directory named `bin` in the main installation folder. Modifying this directory structure, after the installation, this will cause the tool not to work properly.

Finally specify the `aws` full path in the Nextflow config file as show below: 

```
aws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws'
```


== Launch template 

An alternative to is to create a custom AMI using a
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-templates.html[Launch template] that 
installs the AWS CLI tool during the instance boot via a custom user-data. 

In the EC2 dashboard create a Launch template specifying in the user data field:

```
MIME-Version: 1.0
Content-Type: multipart/mixed; boundary="//"

--//
Content-Type: text/x-shellscript; charset="us-ascii"

#!/bin/sh
## install required deps
set -x
export PATH=/usr/local/bin:$PATH
yum install -y jq python27-pip sed wget bzip2
pip install -U boto3

## install awscli 
USER=/home/ec2-user
wget -q https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh -b -f -p $USER/miniconda
$USER/miniconda/bin/conda install -c conda-forge -y awscli
rm Miniconda3-latest-Linux-x86_64.sh
chown -R ec2-user:ec2-user $USER/miniconda

--//--
```

Then in the Batch dashboard create a new compute environment and specify the newly created
launch template in the corresponding field.

== Expandable EBS volume

A common problem issue when deploying genomics workload is related to the amount of storage
to be allocated in the compute nodes which is challenging to estimate.

A possible solution consists of using a background process running in the compute nodes which 
periodically checks the amount of free space and automatically expands the avail storage
mounting new EBS volume(s).

To take advantage of this mechanism with AWS Batch, we also need to make sure the Docker storage driver is mounted over this expandable volume instead of the boot disk. 

The above pattern can be implemented using the following launch template: 

```
MIME-Version: 1.0
Content-Type: multipart/mixed; boundary="//"

--//
Content-Type: text/cloud-boothook; charset="us-ascii"

su - root << 'EOF'
(
set -x
uname -r
env | sort
export PATH=/usr/local/bin:$PATH
yum install -y jq btrfs-progs python27-pip sed wget bzip2
pip install -U boto3
cp -au /var/lib/docker /var/lib/docker.bk
rm -rf /var/lib/docker/*
cd /opt && curl -s https://nf-xpack.s3.amazonaws.com/v1/aws-ebs-autoscale.tgz | tar xz
sh /opt/ebs-autoscale/bin/init-ebs-autoscale.sh /var/lib/docker /dev/sdc  2>&1 > /var/log/init-ebs-autoscale.log
sed -i 's+^DOCKER_STORAGE_OPTIONS=.*+DOCKER_STORAGE_OPTIONS="--storage-driver btrfs"+g' /etc/sysconfig/docker-storage
cp -au /var/lib/docker.bk/* /var/lib/docker

) 2>&1 | grep -v LESS_TERMCAP >  ~/boot.log
EOF

--//
Content-Type: text/x-shellscript; charset="us-ascii"

#!/bin/sh
su - root << 'EOF'
(
set -x
## install awscli 
USER=/home/ec2-user
wget -q https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh -b -f -p $USER/miniconda
$USER/miniconda/bin/conda install -c conda-forge -y awscli
rm Miniconda3-latest-Linux-x86_64.sh
chown -R ec2-user:ec2-user $USER/miniconda

) &>> ~/boot.log
EOF
cp ~/boot.log ~ec2-user/boot.log

--//--
```

Once created the above launch template, specify it when creating the required AWS Batch 
compute environment. 

IMPORTANT: Make sure to use _Amazon ECS-Optimized Amazon Linux AMI_ (not Amazon Linux 2) when 
using the above launch template.


== FSx for Lustre with Nf-xpack

AWS S3 is a fast and cheap storage solution in the cloud; however it's not a file storage solution designed for use 
in HPC shared file systems. 

The optional Enterprise Extension Pack for Nextflow provides an extended executor for AWS Batch that allows the usage 
of https://aws.amazon.com/fsx/lustre/[Amazon FSx for Lustre] (or any other POSIX compliant file system)
as shared storage in place of an S3 bucket.

The Nextflow extended executor for Batch takes care of the mounting of the shared file system in the corresponding job containers. However, it also needs to be mounted in the computing nodes. 

=== Launch template

The following launch template can be used to mount the Lustre shared file system: 

```
MIME-Version: 1.0
Content-Type: multipart/mixed; boundary="//"

--//
Content-Type: text/cloud-boothook; charset="us-ascii"

su - root << 'EOF'
(
set -x
uname -r
env | sort
export PATH=/usr/local/bin:$PATH
yum install -y jq btrfs-progs python27-pip sed wget bzip2
pip install -U boto3
cp -au /var/lib/docker /var/lib/docker.bk
rm -rf /var/lib/docker/*
cd /opt && curl -s https://nf-xpack.s3.amazonaws.com/v1/aws-ebs-autoscale.tgz | tar xz
sh /opt/ebs-autoscale/bin/init-ebs-autoscale.sh /var/lib/docker /dev/sdc  2>&1 > /var/log/init-ebs-autoscale.log
sed -i 's+^DOCKER_STORAGE_OPTIONS=.*+DOCKER_STORAGE_OPTIONS="--storage-driver btrfs"+g' /etc/sysconfig/docker-storage
cp -au /var/lib/docker.bk/* /var/lib/docker
## install fsx
SCRATCH=/scratch
FSXNAME=fs-0269031ec160509c9.fsx.eu-west-1.amazonaws.com
yum -q install -y lustre-client
mkdir -p $SCRATCH
mount -t lustre -o noatime,flock $FSXNAME@tcp:/fsx $SCRATCH

) 2>&1 | grep -v LESS_TERMCAP >  ~/boot.log
EOF

--//
Content-Type: text/x-shellscript; charset="us-ascii"

#!/bin/sh
su - root << 'EOF'
(
set -x
## install awscli 
USER=/home/ec2-user
wget -q https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh -b -f -p $USER/miniconda
$USER/miniconda/bin/conda install -c conda-forge -y awscli
rm Miniconda3-latest-Linux-x86_64.sh
chown -R ec2-user:ec2-user $USER/miniconda
## fix fsx ownership 
SCRATCH=/scratch
chown ec2-user:ec2-user $SCRATCH
) &>> ~/boot.log
EOF
cp ~/boot.log ~ec2-user/boot.log

--//--
```

In the above snippet replace the variables `FSXNAME` and `SCRATCH` with the appropriate values 
corresponding to your environment.

NOTE: Nextflow has to be launched from an instance having access to the same FSx Lustre storage. 

=== Launching instance configuration 

Use the following snippet to install the Lustre client: 

```
SCRATCH=/scratch
FSXNAME=fs-0269031ec160509c9.fsx.eu-west-1.amazonaws.com
sudo yum  install -y lustre-client
sudo mkdir -p $SCRATCH
sudo mount -t lustre -o noatime,flock $FSXNAME@tcp:/fsx $SCRATCH
sudo chown ec2-user:ec2-user $SCRATCH
```

NOTE: Make also sure the storage and the computing nodes uses the same VPC and security groups.
For details check https://docs.aws.amazon.com/fsx/latest/LustreGuide/limit-access-security-groups.html[here].

=== Nextflow configuration 

Define the following env variable: 

```
export NXF_GRAB=io.seqera:nf-xpack:0.2.0
```

A basic Nextflow config looks like the following:

```
process.container = 'nextflow/rnaseq-nf:latest' 
process.executor = 'awsbatch'
process.queue = 'nf-queue-with-fsx'
aws.region = 'eu-west-1'
workDir = '/scratch/work'
```

Then run NF as usual: 

```
nextflow run rnaseq-nf
```

NOTE: This requires an extra endpoint configuration to access Nf-xpack distribution.


== Batch squared 

Batch squared consists in submitting a Nextflow launcher application 
as a Batch job itself. 

There's a good tutorial about that in the https://docs.opendata.aws/genomics-workflows/orchestration/nextflow/nextflow-overview/[AWS documentation].

== Advanced tuning

When deploying data intensive workloads using S3 as shared storage the large number 
of parallel file uploads/downloads can create network congestions and stall the ECS
agent running in the compute node, making it irresponsive. 

A simple tip is to try to avoid to big instances for jobs requiring few CPUs (in order to 
avoid too many parallel jobs in the compute instance). 

Also the following parameters can help to mitigate this issue: 

```
aws {
    batch {
        maxTransferAttempts = 20
        delayBetweenAttempts = 1000
        maxParallelTransfers = 8
    }

    client {
        maxConnections = 8          // This may depends on num of avail CPUs
        uploadMaxThreads = 8        // This may depends on num of avail CPUs
        uploadChunkSize = '100MB'   // Larger chunk sizes may be more stable
        uploadMaxAttempts = 10
        uploadRetrySleep = '10 sec'
        maxErrorRetry = 20
    }
}
```

Advanced configuration settings are described at https://www.nextflow.io/docs/latest/config.html#scope-aws[this link].

== Hybrid deployments 

Nextflow allows the use of multiple executors in the same workflow application. This feature enables the deployment of hybrid workloads in which some jobs are executed in the local computer or local computing cluster, and some jobs are offloaded to AWS Batch service.

To enable this feature, use one or more https://www.nextflow.io/docs/latest/config.html#config-process-selectors[process selectors] in your Nextflow configuration file to apply the https://www.nextflow.io/docs/latest/awscloud.html#awscloud-batch-config[AWS Batch configuration] only to a subset of processes in your workflow. For example:

[source,config,linenums]
----
process {
    executor = 'slurm'  // <1>
    queue = 'short'     // <2>

    withLabel: bigTask {          // <3> 
      executor = 'awsbatch'       // <4>
      queue = 'my-batch-queue'    // <5>
      container = 'my/image:tag'  // <6>
  }
}

aws {
    region = 'eu-west-1'    // <7>
}
----

<1> Set `slurm` as the default executor 
<2> Set the queue for the SLURM cluster 
<3> Setting of for the process named `bigTask` 
<4> Set `awsbatch` as executor for the `bigTask` process
<5> Set the queue for the for the `bigTask` process
<6> set the container image to deploy the `bigTask` process
<7> Defines the region for Batch execution

