= Processes 

In Nextflow, a `process` is the basic computing primitive to execute foreign functions (i.e. custom scripts or tools).

The `process` definition starts with the keyword `process`, followed by the process name and finally the process body delimited by curly brackets. 

A basic `process`, only using the `script` definition block, looks like the following:

[source,nextflow,linenums]
----
process sayHello {
  script:
  """
  echo 'Hello world!'
  """
}
----

In more complex examples, the process body can contain up to *five* definition blocks:

1. *Directives* are initial declarations that define optional settings.

2. *Input* defines the expected input file/s and the channel from where to find them.

3. *Output* defines the expected output file/s and the channel to send the data to.

4. *When* is a optional clause statement to allow conditional processes.

5. *Script* is a string statement that defines the command to be executed by the process

The full process syntax is defined as follows:

[source,nextflow,linenums]
----
process < name > {

  [ directives ]        // <1>

  input:                // <2>
  < process inputs >
  
  output:               // <3>
  < process outputs >
  
  when:                 // <4>
  < condition >
  
  [script|shell|exec]:  // <5>
  """
  < user script to be executed >
  """
}
----

<1> Zero, one or more process directives.
<2> Zero, one or more process inputs.
<3> Zero, one or more process outputs.
<4> An optional boolean conditional to trigger the process execution.
<5> The command to be executed.

== Script

The `script` block is a string statement that defines the command to be executed by the process.

A process contains one and only one `script` block, and it must be the last statement when the process contains input and output declarations.

The `script` block can be a single or multi-line string. The latter simplifies the writing of non-trivial scripts
composed by multiple commands spanning over multiple lines. For example:

[source,nextflow,linenums]
----
process example {
  script:
  """
  echo 'Hello world!\nHola mundo!\nCiao mondo!\nHallo Welt!' > file
  cat file | head -n 1 | head -c 5 > chunk_1.txt
  gzip -c chunk_1.txt  > chunk_archive.gz
  """
}
----

By default the `process` command is interpreted as a *Bash* script. However any other scripting language can be used by simply starting the script with the corresponding https://en.wikipedia.org/wiki/Shebang_(Unix)[Shebang] declaration. For example:

[source,nextflow,linenums]
----
process pyStuff {
  script:
  """
  #!/usr/bin/env python

  x = 'Hello'
  y = 'world!'
  print ("%s - %s" % (x,y))
  """
}
----

NOTE: Multiple programming languages can be used within the same workflow script. However, for large chunks of code is suggested to save them into separate files and invoke them from the process script. One can store the specific scripts in the ./bin folder.

=== Script parameters

Script parameters (`params`) can be defined dynamically using variable values, like this:

[source,nextflow,linenums]
----
params.data = 'World'

process foo {
  script:
  """
  echo Hello $params.data
  """
}
----

NOTE: A process script can contain any string format supported by the Groovy programming language.
This allows us to use string interpolation as in the script above or multiline strings. 
Refer to <<groovy.adoc#_string_interpolation,String interpolation>> for more information.

IMPORTANT: Since Nextflow uses the same Bash syntax for variable substitutions in strings, Bash environment variables need to be escaped using `\` character.

[source,nextflow,linenums]
----
process foo {
  script:
  """
  echo "The current directory is \$PWD"
  """
}
----

TIP: Try to modify the above script using `$PWD` instead of `\$PWD` and  check the difference.

This can be tricky when the script uses many Bash variables. A possible alternative
is to use a script string delimited by single-quote characters

[source,nextflow,linenums]
----
process bar {
  script:
  '''
  echo $PATH | tr : '\\n'
  '''
}
----

However, this blocks the usage of Nextflow variables in the command script.

Another alternative is to use a `shell` statement instead of `script` which uses a different
syntax for Nextflow variables: `!{..}`. This allows the use of both Nextflow and Bash variables in the same script.

[source,nextflow,linenums]
----
params.data = 'le monde'

process baz {
  shell:
  '''
  X='Bonjour'
  echo $X !{params.data}
  '''
}
----

=== Conditional script

The process script can also be defined in a completely dynamic manner using an `if` statement or any other expression evaluating to a string value. For example:

[source,nextflow,linenums]
----
params.compress = 'gzip'
params.file2compress = "$baseDir/data/ggal/transcriptome.fa"

process foo {

  input:
  path file from params.file2compress

  script:
  if( params.compress == 'gzip' )
    """
    gzip -c $file > ${file}.gz
    """
  else if( params.compress == 'bzip2' )
    """
    bzip2 -c $file > ${file}.bz2
    """
  else
    throw new IllegalArgumentException("Unknown aligner $params.compress")
}   
----

== Inputs

Nextflow processes are isolated from each other but can communicate between themselves sending values through channels.

Inputs implicitly determine the dependency and the parallel execution of the process. 
The process execution is fired each time _new_ data is ready to be consumed from the input channel: 

image::channel-process.png[]

The `input` block defines which channels the `process` is expecting to receive data from. You can only define one `input` block at a time, and it must contain one or more input declarations.

The `input` block follows the syntax shown below:

```nextflow
input:
  <input qualifier> <input name> from <source channel>
```

=== Input values

The `val` qualifier allows you to receive data of any type as input. It can be accessed in the process script by using the specified input name, as shown in the following example:

[source,nextflow,linenums]
----
num = Channel.from( 1, 2, 3 )

process basicExample {
  input:
  val x from num

  """
  echo process job $x
  """
}
----

In the above example the process is executed three times, each time a value is received from the channel num and used to process the script. Thus, it results in an output similar to the one shown below:

```
process job 3
process job 1
process job 2
```

IMPORTANT: The channel guarantees that items are delivered in the same order as they have been sent - but - since the process is executed in a parallel manner, there is no guarantee that they are processed in the same order as they are received.

=== Input files

The `file` qualifier allows the handling of file values in the process execution context. This means that
Nextflow will stage it in the process execution directory, and it can be access in the script by using the name specified in the input declaration.


[source,nextflow,linenums]
----
reads = Channel.fromPath( 'data/ggal/*.fq' )

process foo {
    input:
    file 'sample.fastq' from reads
    script:
    """
    echo your_command --reads sample.fastq
    """
}
----

The input file name can also be defined using a variable reference as shown below:

[source,nextflow,linenums]
----
reads = Channel.fromPath( 'data/ggal/*.fq' )

process foo {
    input:
    file sample from reads
    script:
    """
    echo your_command --reads $sample
    """
}
----

The same syntax it's also able to handle more than one input file in the same execution.
Only requiring a change in the channel composition.

[source,nextflow,linenums]
----
reads = Channel.fromPath( 'data/ggal/*.fq' )

process foo {
    input:
    file sample from reads.collect()
    script:
    """
    echo your_command --reads $sample
    """
}
----

WARNING: When a process declares an input `file`, the corresponding channel elements 
must be *file* objects, i.e. created with the `file` helper function from the file specific 
channel factories e.g. `Channel.fromPath` or `Channel.fromFilePairs`. 

Consider the following snippet: 

[source,nextflow,linenums]
----
params.genome = 'data/ggal/transcriptome.fa'

process foo {
    input:
    file genome from params.genome
    script:
    """
    echo your_command --reads $genome
    """
}
----

The above code creates a temporary file named `input.1` with the string `data/ggal/transcriptome.fa` as content. That likely is not what you wanted to do. 


=== Input path

As of version 19.10.0, Nextflow introduced a new `path` input qualifier that simplifies 
the handling of cases such as the one shown above. In a nutshell, the input `path` automatically 
handles string values as file objects. The following example works as expected:

[source,nextflow,linenums]
----
params.genome = "$baseDir/data/ggal/transcriptome.fa"

process foo {
    input:
    path genome from params.genome
    script:
    """
    echo your_command --reads $genome
    """
}
----

NOTE: The path qualifier should be preferred over file to handle process input files when using Nextflow 19.10.0 or later.


[discrete]
=== Exercise

Write a script that creates a channel containing all read files matching the pattern `data/ggal/*_1.fq`
followed by a process that concatenates them into a single file and prints the first 20 lines.

.Click here for the answer:
[%collapsible]
====
[source,nextflow,linenums]
----
params.reads = "$baseDir/data/ggal/*_1.fq"

Channel 
    .fromPath( params.reads )
    .set { read_ch } 

process concat {
  tag "Concat all files"

  input:
  path '*' from read_ch.collect()

  output:
  path 'top_10_lines' into concat_ch
 
  script:
  """
  cat * > concatenated.txt
  head -n 20 concatenated.txt > top_10_lines
  """
  }

concat_ch.view()
----
====

=== Combine input channels

A key feature of processes is the ability to handle inputs from multiple channels. However it's
important to understands how channel content and their semantics affect the execution
of a process.

Consider the following example:

[source,nextflow,linenums]
----
process foo {
  echo true
  input:
  val x from Channel.from(1,2,3)
  val y from Channel.from('a','b','c')
  script:
   """
   echo $x and $y
   """
}
----

Both channels emit three values, therefore the process is executed three times, each time with a different pair:

* (1, a)
* (2, b)
* (3, c)

What is happening is that the process waits until there's a complete input configuration i.e. it receives an input value from all the channels declared as input.

When this condition is verified, it consumes the input values coming from the respective channels, and spawns a task execution, then repeat the same logic until one or more channels have no more content.

This means channel values are consumed serially one after another and the first empty channel cause the process execution to stop even if there are other values in other channels.

*So what happens when channels do not have the same cardinality (i.e. they emit a different number of elements)?*

For example:

[source,nextflow,linenums]
----
process foo {
  echo true
  input:
  val x from Channel.from(1,2)
  val y from Channel.from('a','b','c')
  script:
   """
   echo $x and $y
   """
}
----

In the above example the process is executed only two times, because when a channel has no more data to be processed it stops the process execution.

However, what happens if you replace value x with a `value` channel?

Compare the previous example with the following one :

[source,nextflow,linenums]
----
process bar {
  echo true
  input:
  val x from Channel.value(1)
  val y from Channel.from('a','b','c')
  script:
   """
   echo $x and $y
   """
}
----

.The output should look something like:
[%collapsible]
====
[source,nextflow,linenums]
----
1 and b
1 and a
1 and c
----
====

This is because _value_ channels can be consumed multiple times, so it doesn't affect process termination.

[discrete]
=== Exercise

Write a process that is executed for each read file matching the pattern `data/ggal/*_1.fq` and
use the same `data/ggal/transcriptome.fa` in each execution.


.Click here for the answer:
[%collapsible]
====
[source,nextflow,linenums]
----
params.reads = "$baseDir/data/ggal/*_1.fq"
params.transcriptome_file = "$baseDir/data/ggal/transcriptome.fa"

Channel 
    .fromPath( params.reads )
    .set { read_ch } 

process command {
  tag "Run_command"

  input:
  path reads from read_ch
  path transcriptome from params.transcriptome_file

  output:
  path result into concat_ch
 
  script:
  """
  echo your_command $reads $transcriptome > result
  """
  }

concat_ch.view()
----
====

=== Input repeaters

The `each` qualifier allows you to repeat the execution of a process for each item in a collection, every time new data is received. For example:

[source,nextflow,linenums]
----
sequences = Channel.fromPath('data/prots/*.tfa')
methods = ['regular', 'expresso', 'psicoffee']

process alignSequences {
  input:
  path seq from sequences
  each mode from methods

  """
  echo t_coffee -in $seq -mode $mode
  """
}
----

In the above example every time a file of sequences is received as input by the process, it executes three tasks, each running a different alignment method, set as a `mode` variable. This is useful when you need to repeat the same task for a given set of parameters.

[discrete]
=== Exercise

Extend the previous example so a task is executed for each read file matching the pattern `data/ggal/*_1.fq` and repeat the same task both with `salmon` and `kallisto`.

.Click here for the answer:
[%collapsible]
====
[source,nextflow,linenums]
----
params.reads = "$baseDir/data/ggal/*_1.fq"
params.transcriptome_file = "$baseDir/data/ggal/transcriptome.fa"
methods= ['salmon', 'kallisto']

Channel 
    .fromPath( params.reads )
    .set { read_ch } 

process command {
  tag "Run_command"

  input:
  path reads from read_ch
  path transcriptome from params.transcriptome_file
  each mode from methods

  output:
  path result into concat_ch
 
  script:
  """
  echo $mode $reads $transcriptome > result
  """
  }

concat_ch
    .view { "To run : ${it.text}" }
----
====

== Outputs

The _output_ declaration block defines the channels used by the process to send out the results produced.

Only one output block can be defined containing one or more output declarations. The output block follows the syntax shown below:

----
output:
  <output qualifier> <output name> into <target channel>[,channel,..]
----

=== Output values

The `val` qualifier specifies a defined _value_ output in the script context. In a common usage scenario,
this is a value, which has been defined in the _input_ declaration block, as shown in the following example:

[source,nextflow,linenums]
----
methods = ['prot','dna', 'rna']

process foo {
  input:
  val x from methods

  output:
  val x into receiver

  """
  echo $x > file
  """
}

receiver.view { "Received: $it" }
----

=== Output files

The `file` qualifier specifies one or more files as output, produced by the process, into the specified channel.

[source,nextflow,linenums]
----
process randomNum {

    output:
    file 'result.txt' into numbers

    '''
    echo $RANDOM > result.txt
    '''
}

numbers.view { "Received: " + it.text }
----

In the above example the process `randomNum` creates a file named `result.txt` containing a random number.

Since a file parameter using the same name is declared in the output block, when the task is completed that
file is sent over the `numbers` channel. A downstream `process` declaring the same channel as _input_ will
be able to receive it.


=== Multiple output files

When an output file name contains a `*` or `?` wildcard character it is interpreted as a
http://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob[glob] path matcher.
This allows us to _capture_ multiple files into a list object and output them as a sole emission. For example:

[source,nextflow,linenums]
----
process splitLetters {

    output:
    file 'chunk_*' into letters

    '''
    printf 'Hola' | split -b 1 - chunk_
    '''
}

letters
    .flatMap()
    .view { "File: ${it.name} => ${it.text}" }
----

it prints:

----
File: chunk_aa => H
File: chunk_ab => o
File: chunk_ac => l
File: chunk_ad => a
----

Some caveats on glob pattern behavior:

* Input files are not included in the list of possible matches.
* Glob pattern matches against both files and directory paths.
* When a two stars pattern ``**`` is used to recourse across directories, only file paths are matched
  i.e. directories are not included in the result list.

[discrete]
=== Exercise

Remove the `flatMap` operator and see out the output change. The documentation
for the `flatMap` operator is available at https://www.nextflow.io/docs/latest/operator.html#flatmap[this link].

.Click here for the answer:
[%collapsible]
====
[source,nextflow,linenums]
----
File: [chunk_aa, chunk_ab, chunk_ac, chunk_ad] => [H, o, l, a]
----
====

=== Dynamic output file names

When an output file name needs to be expressed dynamically, it is possible to define it using a dynamic evaluated
string, which references values defined in the input declaration block or in the script global context.
For example:

[source,nextflow,linenums]
----
species = ['cat','dog', 'sloth']
sequences = ['AGATAG','ATGCTCT', 'ATCCCAA']

process align {
  input:
  val x from species
  val seq from sequences

  output:
  file "${x}.aln" into genomes

  """
  echo align -in $seq > ${x}.aln
  """
}

genomes.view()
----

In the above example, each time the process is executed an alignment file is produced whose name depends
on the actual value of the `x` input.

=== Composite inputs and outputs

So far we have seen how to declare multiple input and output channels, but each channel was handling
only one value at time. However Nextflow can handle a _tuple_ of values.

When using a channel emitting a _tuple_ of values, the corresponding input declaration must be declared with a `tuple` qualifier followed by definition of each element in the tuple.

In the same manner, output channels emitting a tuple of values can be declared using the `tuple` qualifier
following by the definition of each tuple element.

[source,nextflow,linenums]
----
reads_ch = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')

process foo {
  input:
    tuple val(sample_id), file(sample_files) from reads_ch
  output:
    tuple val(sample_id), file('sample.bam') into bam_ch
  script:
  """
    echo your_command_here --reads $sample_id > sample.bam
  """
}

bam_ch.view()
----

TIP: In previous versions of Nextflow `tuple` was called `set` but it was used exactly with the 
  same semantic. It can still be used for backward compatibility. 

[discrete]
=== Exercise

Modify the script of the previous exercise so that the _bam_ file is named as the given `sample_id`.

.Click here for the answer:
[%collapsible]
====
[source,nextflow,linenums]
----
reads_ch = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')

process foo {
  input:
    tuple val(sample_id), file(sample_files) from reads_ch
  output:
    tuple val(sample_id), file("${sample_id}.bam") into bam_ch
  script:
  """
    echo your_command_here --reads $sample_id > ${sample_id}.bam
  """
}

bam_ch.view()
----
====

== When

The `when` declaration allows you to define a condition that must be verified in order to execute the process. This can be any expression that evaluates a boolean value.

It is useful to enable/disable the process execution depending on the state of various inputs and parameters. For example:

[source,nextflow,linenums]
----
params.dbtype = 'nr'
params.prot = 'data/prots/*.tfa'
proteins = Channel.fromPath(params.prot)

process find {
  input:
  file fasta from proteins
  val type from params.dbtype

  when:
  fasta.name =~ /^BB11.*/ && type == 'nr'

  script:
  """
  echo blastp -query $fasta -db nr
  """
}
----

== Directives

Directive declarations allow the definition of optional settings that affect the execution of the current process without affecting the _semantic_ of the task itself.

They must be entered at the top of the process body, before any other declaration blocks (i.e. `input`, `output`, etc.).

Directives are commonly used to define the amount of computing resources to be used or
other meta directives that allow the definition of extra information for configuration or
logging purpose. For example:

[source,nextflow,linenums]
----
process foo {
  cpus 2
  memory 1.GB
  container 'image/name'

  script:
  """
  echo your_command --this --that
  """
}
----

The complete list of directives is available https://www.nextflow.io/docs/latest/process.html#directives[at this link].

.Commonly used directives
[%header,cols="15%,85%"]
|===
|Name
|Description

|https://www.nextflow.io/docs/latest/process.html#cpus[cpus] 
|Allows you to define the number of (logical) CPU required by the process’ task. 

|https://www.nextflow.io/docs/latest/process.html#time[time] 
|Allows you to define how long a process is allowed to run. e.g. time '1h': 1 hour, '1s' 1 second, '1m' 1 minute, '1d' 1 day.

|https://www.nextflow.io/docs/latest/process.html#memory[memory]
|Allows you to define how much memory the process is allowed to use. e.g. '2 GB' is 2 GB. Can use B, KB,MB,GB and TB.

|https://www.nextflow.io/docs/latest/process.html#disk[disk] 
|Allows you to define how much local disk storage the process is allowed to use.

|https://www.nextflow.io/docs/latest/process.html#tag[tag]
|Allows you to associate each process execution with a custom label, so that it will be easier to identify them in the log file or in the trace execution report.
|===

== Organise outputs

=== PublishDir directive

Given each process is being executed in separate temporary work/ folders (e.g. work/f1/850698...; work/g3/239712...; etc.), we may want to save important, non-intermediary, final files into a results folder. 

TIP: Remember to delete the work folder from time to time, else all your intermediate files will fill up your computer!

To store our workflow result files, we need to be explicitly mark them using the directive
https://www.nextflow.io/docs/latest/process.html#publishdir[publishDir] in
the process that's creating these file. For example:

[source,nextflow,linenums,options="nowrap"]
----
params.outdir = 'my-results'
params.prot = 'data/prots/*.tfa'
proteins = Channel.fromPath(params.prot)


process blastSeq {
    publishDir "$params.outdir/bam_files", mode: 'copy'

    input:
    file fasta from proteins

    output:
    file ('*.txt') into blast_ch

    """
    echo blastp $fasta > ${fasta}_result.txt
    """
}

blast_ch.view()
----

The above example will copy all blast script files created by the `blastSeq` task in the
directory path `my-results`. 

TIP: The publish directory can be local or remote. For example output files
could be stored to a https://aws.amazon.com/s3/[AWS S3 bucket] just using the `s3://` prefix in the target path.

=== Manage semantic sub-directories

You can use more then one `publishDir` to keep different outputs in separate directory. For example:

[source,nextflow,linenums,options="nowrap"]
----
params.reads = 'data/reads/*_{1,2}.fq.gz'
params.outdir = 'my-results'

Channel
    .fromFilePairs(params.reads, flat: true)
    .set{ samples_ch }

process foo {
  publishDir "$params.outdir/$sampleId/", pattern: '*.fq'
  publishDir "$params.outdir/$sampleId/counts", pattern: "*_counts.txt"
  publishDir "$params.outdir/$sampleId/outlooks", pattern: '*_outlook.txt'

  input:
    tuple val(sampleId), file('sample1.fq.gz'), file('sample2.fq.gz') from samples_ch
  output:
    file "*"
  script:
  """
    < sample1.fq.gz zcat > sample1.fq
    < sample2.fq.gz zcat > sample2.fq

    awk '{s++}END{print s/4}' sample1.fq > sample1_counts.txt
    awk '{s++}END{print s/4}' sample2.fq > sample2_counts.txt

    head -n 50 sample1.fq > sample1_outlook.txt
    head -n 50 sample2.fq > sample2_outlook.txt
  """
}
----

The above example will create an output structure in the directory `my-results`,
which contains a separate sub-directory for each given sample ID each of which
contain the folders `counts` and `outlooks`.

