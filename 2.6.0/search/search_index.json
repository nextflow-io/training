{"config":{"lang":["en","pt","es","fr","it","ko"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Nextflow Training","text":"<p>Welcome to the Nextflow community training portal!</p> <p>We have several distinct training courses available on this website. Scroll down to find the one that's right for you!</p> <p>The training courses listed below are designed to be usable as a self-service resource; you can work through them on your own at any time (see Environment Setup for practical details). However, you may get even more out of them by joining a group training event.</p> <ul> <li>Free online events are run regularly by the nf-core community, see the nf-core events page for more.</li> <li>Seqera (the company that develops Nextflow) runs a variety of training events, see the Seqera Events page and look for 'Seqera Sessions' and 'Nextflow Summit'.</li> <li>Our Community team also regularly teaches trainings hosted by third party organizations; announcements and signups for those are typically managed by the third-party hosts.</li> </ul> <p>When you're ready to get down to work, click on the 'Open in GitHub Codespaces' button, either on this page or on the index page of the course you chose, to open a web-based training environment (requires a free GitHub account).</p> <p></p>"},{"location":"#training-environment-setup","title":"Training Environment Setup","text":"<p>Environment Setup</p> <p> Set up your environment for the first time.</p> <p>Instructions for setting up your environment to work through training materials (all courses). Provides an orientation to GitHub Codespaces as well as alternate installation instructions for working on your own local machine.</p> <p>Start the Environment Setup training </p>"},{"location":"#nextflow-for-newcomers","title":"Nextflow for Newcomers","text":"<p>These are foundational, domain-agnostic courses intended for those who are completely new to Nextflow. Each course consists of a series of training modules that are designed to help learners build up their skills progressively.</p> <p>Hello Nextflow</p> <p> Learn to develop pipelines in Nextflow.</p> <p> Video material available.</p> <p>This is a course for newcomers who wish to learn how to develop their own pipelines. The course covers the core components of the Nextflow language in enough detail to enable developing simple but fully functional pipelines. It also covers key elements of pipeline design, development and configuration practices.</p> <p>The course is calibrated to take a full day to cover in group trainings.</p> <p>Start the Hello Nextflow training </p> <p>Nextflow Run</p> <p> Learn to run pipelines in Nextflow.</p> <p>This is an abridged version of the Hello Nextflow course intended for newcomers who wish to learn how to run their own pipelines but do not necessarily plan to develop pipelines themselves. The course covers the core components of the Nextflow language in enough detail to understand the basic structure of Nextflow pipelines and relate that to the experience of running and monitoring pipeline executions on the command-line. It also covers the basics of tool management and pipeline configuration practices.</p> <p>The course is calibrated to take a half day to cover in group trainings.</p> <p>Start the Nextflow Run training </p> <p>Hello nf-core</p> <p> Learn to develop nf-core compliant pipelines.</p> <p>This is a course for newcomers who wish to learn run and develop nf-core compliant pipelines. The course covers the structure of nf-core pipelines in enough detail to enable developing simple but fully functional pipelines that follow the nf-core template and development best practices.</p> <p>The course is calibrated to take a half day to cover in group trainings.</p> <p>Start the Hello nf-core training </p>"},{"location":"#nextflow-for-science","title":"Nextflow for Science","text":"<p>These are courses that demonstrate how to apply the concepts and components presented in 'Hello Nextflow' (see above) to specific scientific use cases. Each course consists of a series of training modules that are designed to help learners build up their skills progressively.</p> <p>Nextflow for Genomics</p> <p> Learn to develop a pipeline for genomics in Nextflow.</p> <p>This is a course for researchers who wish to learn how to develop their own genomics pipelines. The course uses a variant calling use case to demonstrate how to develop a simple but functional genomics pipeline.</p> <p>Start the Nextflow for Genomics training </p> <p>Nextflow for RNAseq</p> <p> Learn to develop a pipeline for RNAseq data processing in Nextflow.</p> <p>This is a course for researchers who wish to learn how to develop their own RNAseq pipelines. The course uses a bulk RNAseq processing use case to demonstrate how to develop a simple but functional RNAseq pipeline.</p> <p>Start the Nextflow for RNAseq training </p> <p>Let us know what other domains and use cases you'd like to see covered here by posting in the Training section of the community forum.</p>"},{"location":"#advanced-nextflow-training","title":"Advanced Nextflow Training","text":"<p>These are materials that cover advanced concepts and mechanisms for developing and deploying Nextflow pipelines to address real-world use cases. These materials are organized into Side Quests that cover individual topics, and Training Collections that combine multiple Side Quests in order to provide a comprehensive learning experience around a particular around a common theme or use case.</p> <p>Side Quests</p> <p> Training modules for a variety of topics of interest.</p> <p>Side Quests are individual training modules intended for Nextflow developers who wish to widen their range and/or deepen their skills on particular topics. Although the modules are presented linearly, learners are welcome to pick and choose topics in any order. Any dependencies on components/skills that go beyond the scope of the 'Hello Nextflow' course are indicated in the corresponding module overview. For structured learning paths combining multiple Side Quests, see Training Collections below.</p> <p>Start the Side Quests training </p> <p>Training Collections</p> <p> Collections of Side Questions grouped around a particular theme or use case.</p> <p>Training Collections combine multiple Side Quests in order to provide a comprehensive learning experience around a particular theme or use case.</p> <p>Browse the Training Collections </p>"},{"location":"#archived-materials","title":"Archived materials","text":"<p>These are the original Nextflow training materials that were developed at the start of the project. We are in the process of deprecating them in favor of the newer materials listed above. However, some topics covered in the original trainings are not yet represented in the newer material, so we are keeping these around for reference, with the caveat that they are no longer maintained and some exercises may no longer work.</p> <p>Fundamentals Training</p> <p> Comprehensive training material for exploring the full scope of Nextflow's capabilities.</p> <p>The fundamentals training material covers all things Nextflow. Intended as a reference material for anyone looking to build complex workflows with Nextflow.</p> <p>Start the Fundamentals Training </p> <p>Advanced Training</p> <p> Advanced training material for mastering Nextflow.</p> <p>Advanced material exploring the more advanced features of the Nextflow language and runtime, and how to use them to write efficient and scalable data-intensive workflows.</p> <p>Start the Advanced Training </p>"},{"location":"#otherexperimental","title":"Other/Experimental","text":"<p>These are other training courses that are not being actively taught/maintained and that we may repurpose elsewhere or delete in the near future. The corresponding materials are not available within the training environment. You can still find the materials in the GitHub repository and download them for local use.</p> <ul> <li> <p>nf-customize \u2014 Configuring nf-core pipelines (docs / code)</p> </li> <li> <p>troubleshoot \u2014 Troubleshooting exercises (docs / code)</p> </li> <li> <p>hands-on (rnaseq) \u2014 Developing a pipeline for bulk RNAseq (deprecated) (docs / code)</p> </li> </ul>"},{"location":"#resources","title":"Resources","text":"<p>Quick reference to some handy links:</p> Reference \u00a0Community Nextflow Docs Nextflow Slack Nextflow Homepage nf-core Seqera Seqera Community Forum <p>Not sure where to go? Check out the Getting help page.</p>"},{"location":"#credits-and-contributions","title":"Credits and contributions","text":"<p>This training material is developed and maintained by Seqera and released under an open-source license (CC BY-NC-ND) for the benefit of the community. You are welcome to reuse these materials according to the terms of the license. If you are an instructor running your own trainings, we'd love to hear about how it goes and what we could do to make it easier.</p> <p>We welcome fixes and improvements from the community. Every page has a  icon in the top right of the page, which will take you to GitHub where you can propose changes to the training source material via a pull request.</p> <p></p> <p></p>"},{"location":"help/","title":"Getting Help","text":""},{"location":"help/#nextflow-documentation","title":"Nextflow Documentation","text":"<p>Nextflow has an excellent set of docs, which should be your first destination whenever you run into something you don't understand or want to learn more about. You can browse topics or search for specific terms.</p>"},{"location":"help/#community-forum","title":"Community forum","text":"<p>If you're struggling with the training, please don't hesitate to reach out for help. Our amazing community is one of the great strengths of Nextflow!</p> <p>The Seqera community forum is a great place to ask questions about Nextflow. It's also a great place to find answers to questions that have already been asked!</p> <p>If you can't find a solution to your problem, just log in and click the \"New Topic\" button to post your question in the Ask for Help category. Feel free to include any tags you think might be relevant, since those can help our team and your peers find and answer your question.</p>"},{"location":"help/#professional-support","title":"Professional support","text":"<p>Nextflow is a free and open-source software developed by Seqera, a company headquartered in Spain with satellite offices in the UK and the US.</p> <p>Seqera offers professional support services for Nextflow and associated products, including bespoke training sessions. If this sounds like something that might be of interest to you, please Get in touch.</p>"},{"location":"archive/advanced/","title":"Advanced Training","text":"<p>Welcome to our Nextflow workshop for intermediate and advanced users!</p> <p>In this workshop, we will explore the advanced features of the Nextflow language and runtime, and learn how to use them to write efficient and scalable data-intensive workflows. We will cover topics such as parallel execution, error handling, and workflow customization.</p> <p>Please note that this is not an introductory workshop, and we will assume extensive familiarity with Nextflow.</p> <p>By the end of this workshop, you will have the skills and knowledge to create complex and powerful Nextflow pipelines for your own data analysis projects.</p> <p>Let's get started!</p> <p></p>"},{"location":"archive/advanced/#learning-objectives","title":"Learning objectives","text":"<p>By the end of this course you should:</p> <ul> <li>Describe commonly used and well understood operators</li> <li>Apply good practices for the propagation of metadata</li> <li>Group and split channels</li> <li>Apply Groovy helper classes to Nextflow scripts</li> <li>Sensibly structure workflows</li> <li>Apply layers of configuration to a workflow</li> </ul>"},{"location":"archive/advanced/#audience-prerequisites","title":"Audience &amp; prerequisites","text":"<p>Please note that this is not a beginner's workshop and familiarity with Nextflow, the command line, and common file formats is assumed.</p> <p>Prerequisites</p> <ul> <li>A GitHub account</li> <li>Experience with command line</li> <li>Familiarity with Nextflow and Groovy</li> <li>An understanding of common file formats</li> </ul>"},{"location":"archive/advanced/#follow-the-training-videos-and-get-help","title":"Follow the training videos and get help","text":"<p>Video recordings are available for this course. You can ask questions in the Seqera community forum.</p> <p>You can watch the recording of the most recent training (September, 2023) below:</p>"},{"location":"archive/advanced/configuration/","title":"Configuration","text":"<p>This is an aspect of Nextflow that can be confusing. There are multiple ways of loading configuration and parameters into a Nextflow.</p> <p>This gives us two complications:</p> <ul> <li>At which location should I be loading a configuration value?</li> <li>Given a particular parameter, how do I know where it was set?</li> </ul>"},{"location":"archive/advanced/configuration/#precedence","title":"Precedence","text":"<ol> <li>Parameters specified on the command line (<code>--something value</code>)</li> <li>Parameters provided using the <code>-params-file</code> option</li> <li>Config file specified using the <code>-c my_config option</code></li> <li>The config file named <code>nextflow.config</code> in the current directory</li> <li>The config file named <code>nextflow.config</code> in the workflow project directory</li> <li>The config file <code>$HOME/.nextflow/config</code></li> <li>Values defined within the pipeline script itself (e.g. <code>main.nf</code>)</li> </ol> <p>Precedence is in order of 'distance'</p> <p>A handy guide to understand configuration precedence is in order of 'distance from the command-line invocation'. Parameters specified directly on the CLI <code>--example foo</code> are \"closer\" to the run than configuration specified in the remote repository.</p>"},{"location":"archive/advanced/configuration/#system-wide-configuration-homenextflowconfig","title":"System-wide configuration - <code>$HOME/.nextflow/config</code>","text":"<p>There may be some configuration values that you will want applied on all runs for a given system. These configuration values should be written to <code>~/.nextflow/config</code>.</p> <p>For example - you may have an account on an HPC system and you know that you will always want to submit jobs using the SLURM scheduler when using that machine and always use the Singularity container engine. In this case, your <code>~/.nextflow/config</code> file may include:</p> <pre><code>process.executor = 'slurm'\nsingularity.enable = true\n</code></pre> <p>These configuration values would be inherited by every run on that system without you needing to remember to specify them each time.</p>"},{"location":"archive/advanced/configuration/#overriding-for-a-run-pwdnextflowconfig","title":"Overriding for a run - <code>$PWD/nextflow.config</code>","text":"<p>Create a chapter example directory:</p> <pre><code>mkdir configuration &amp;&amp; cd configuration\n</code></pre>"},{"location":"archive/advanced/configuration/#overriding-process-directives","title":"Overriding Process Directives","text":"<p>Process directives (listed here) can be overridden using the <code>process</code> block. For example, if we wanted to specify that all tasks for a given run should use 2 cpus. In the <code>nextflow.config</code> file in the current working directory:</p> <pre><code>process {\n    cpus = 2\n}\n</code></pre> <p>... and then run:</p> <pre><code>nextflow run rnaseq-nf\n</code></pre> <p>We can make the configuration more specific by using process selectors. We can use process names and/or labels to apply process-level directives to specific tasks:</p> <pre><code>process {\n    withName: 'RNASEQ:INDEX' {\n        cpus = 2\n    }\n}\n</code></pre> <p>Glob pattern matching can also be used:</p> <pre><code>process {\n    withName: '.*:INDEX' {\n        cpus = 2\n    }\n}\n</code></pre>"},{"location":"archive/advanced/configuration/#dynamic-directives","title":"Dynamic Directives","text":"<p>We can specify dynamic directives using closures that are computed as the task is submitted. This allows us to (for example) scale the number of CPUs used by a task by the number of input files.</p> <p>Give the <code>FASTQC</code> process in the <code>rnaseq-nf</code> workflow</p> <pre><code>process FASTQC {\n    tag \"FASTQC on $sample_id\"\n    conda 'fastqc=0.12.1'\n    publishDir params.outdir, mode:'copy'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"fastqc_${sample_id}_logs\"\n\n    script:\n    \"\"\"\n    fastqc.sh \"$sample_id\" \"$reads\"\n    \"\"\"\n}\n</code></pre> <p>we might choose to scale the number of CPUs for the process by the number of files in <code>reads</code>:</p> <pre><code>process {\n    withName: 'FASTQC' {\n        cpus = { reads.size() }\n    }\n}\n</code></pre> <p>we can even use the size of the input files. Here we simply sum together the file sizes (in bytes) and use it in the <code>tag</code> block:</p> <pre><code>process {\n    withName: 'FASTQC' {\n        cpus = { reads.size() }\n        tag = { \"Total size: ${reads*.size().sum() as MemoryUnit}\" }\n    }\n}\n</code></pre> <p>When we run this:</p> <pre><code>N E X T F L O W  ~  version 23.04.3\nLaunching `https://github.com/nextflow-io/rnaseq-nf` [fabulous_bartik] DSL2 - revision: d910312506 [master]\n R N A S E Q - N F   P I P E L I N E\n ===================================\n transcriptome: /home/gitpod/.nextflow/assets/nextflow-io/rnaseq-nf/data/ggal/ggal_1_48850000_49020000.Ggal71.500bpflank.fa\n reads        : /home/gitpod/.nextflow/assets/nextflow-io/rnaseq-nf/data/ggal/ggal_gut_{1,2}.fq\n outdir       : results\n\nexecutor &gt;  local (4)\n[1d/3c5cfc] process &gt; RNASEQ:INDEX (ggal_1_48850000_49020000) [100%] 1 of 1 \u2714\n[38/a6b717] process &gt; RNASEQ:FASTQC (Total size: 1.3 MB)      [100%] 1 of 1 \u2714\n[39/5f1cc4] process &gt; RNASEQ:QUANT (ggal_gut)                 [100%] 1 of 1 \u2714\n[f4/351d02] process &gt; MULTIQC                                 [100%] 1 of 1 \u2714\n\nDone! Open the following report in your browser --&gt; results/multiqc_report.html\n</code></pre> <p>Note that dynamic directives need to be supplied as closures encases in curly braces.</p>"},{"location":"archive/advanced/configuration/#retry-strategies","title":"Retry Strategies","text":"<p>The most common use for dynamic process directives is to enable tasks that fail due to insufficient memory to be resubmitted for a second attempt with more memory.</p> <p>To enable this, two directives are needed:</p> <ul> <li><code>maxRetries</code></li> <li><code>errorStrategy</code></li> </ul> <p>The <code>errorStrategy</code> directive determines what action Nextflow should take in the event of a task failure (a non-zero exit code). The available options are:</p> <ul> <li><code>terminate</code>: Nextflow terminates the execution as soon as an error condition is reported. Pending jobs are killed (default)</li> <li><code>finish</code>: Initiates an orderly pipeline shutdown when an error condition is raised, waiting the completion of any submitted job.</li> <li><code>ignore</code>: Ignores processes execution errors.</li> <li><code>retry</code>: Re-submit for execution a process returning an error condition.</li> </ul> <p>If the <code>errorStrategy</code> is \"retry\", then it will retry up to the value of <code>maxRetries</code> times.</p> <p>If using a closure to specify a directive in configuration, you have access to the <code>task</code> variable, which includes the <code>task.attempt</code> value - an integer specifying how many times the task has been retried. We can use this to dynamically set values such as <code>memory</code> and <code>cpus</code></p> <pre><code>process {\n    withName: 'RNASEQ:QUANT' {\n        errorStrategy = 'retry'\n        maxRetries = 3\n        memory = { 2.GB * task.attempt }\n        time = { 1.hour * task.attempt }\n    }\n}\n</code></pre> <p>Configuration vs process</p> <p>When defining values inside configuration, an equals sign <code>=</code> is required as shown above.</p> <p>When specifying process directives inside the process (in a <code>.nf</code> file), no <code>=</code> is required:</p> <pre><code>process MULTIQC {\n    cpus 2\n    // ...\n</code></pre>"},{"location":"archive/advanced/groovy/","title":"Groovy Imports","text":"<p>There exists in Groovy a wealth of helper classes that can be imported into Nextflow scripts. In this chapter, we create a very small Workflow using the FastP tool to investigate importing the Groovy JSONSlurper class.</p> <p>First, let's move into the chapter 4 directory:</p> <pre><code>cd groovy\n</code></pre> <p>Let's assume that we would like to pull in a samplesheet, parse the entries and run them through the FastP tool. So far, we have been concerned with local files, but Nextflow will handle remote files transparently:</p> <pre><code>params.input = \"https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/samplesheet/v3.10/samplesheet_test.csv\"\n\nworkflow {\n\n    Channel.fromPath(params.input)\n        .splitCsv(header: true)\n        .view()\n}\n</code></pre> <p>Let's write a small closure to parse each row into the now-familiar map + files shape. We might start by constructing the meta-map:</p> <pre><code>workflow {\n\n    samples = Channel.fromPath(params.input)\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def meta = row.subMap('sample', 'strandedness')\n            meta\n        }\n        .view()\n}\n</code></pre> <p>... but this precludes the possibility of adding additional columns to the samplesheet. We might to ensure the parsing will capture any extra metadata columns should they be added. Instead, let's partition the column names into those that begin with \"fastq\" and those that don't. Within the map closure, let's add an additional line to partition the column names:</p> <pre><code>def (readKeys, metaKeys) = row.keySet().split { key -&gt; key =~ /^fastq/ }\n</code></pre> <p>New methods</p> <p>We've introduced a new keySet method here. This is a method on Java's LinkedHashMap class (docs here)</p> <p>We're also using the <code>.split()</code> method, which divides collection based on the return value of the closure. The mrhaki blog provides a succinct summary.</p> <p>From here, let's add another line collect the values of the read keys into a list of file objects:</p> <pre><code>def reads = row.subMap(readKeys).values().collect { value -&gt; file(value) }\n</code></pre> <p>... but we run into an error:</p> <pre><code>Argument of `file` function cannot be empty\n</code></pre> <p>If we have a closer look at the samplesheet, we notice that not all rows have two read pairs. Let's add a condition to the collect method to only include the values that are not empty:</p> <pre><code>def reads = row.subMap(readKeys).values()\n    .findAll { value -&gt; value != \"\" } // Single-end reads will have an empty string\n    .collect { path -&gt; file(path) }\n</code></pre> <p>Now we need to construct the meta map. Let's have a quick look at the FASTP module that I've already pre-defined:</p> <pre><code>process FASTP {\n    container 'quay.io/biocontainers/fastp:0.23.2--h79da9fb_0'\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.fastp.fastq.gz') , optional:true, emit: reads\n    tuple val(meta), path('*.json')           , emit: json\n\n    script:\n    def prefix = task.ext.prefix ?: meta.id\n    if (meta.single_end) {\n        // SNIP\n    } else {\n        // SNIP\n    }\n</code></pre> <p>I can see that we require two extra keys, <code>id</code> and <code>single_end</code>:</p> <pre><code>def meta = row.subMap(metaKeys)\nmeta = meta + [ id: meta.sample, single_end: reads.size == 1 ]\n[meta, reads]\n</code></pre> <p>This is now able to be passed through to our FASTP process:</p> <pre><code>workflow {\n\n    samples = Channel.fromPath(params.input)\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def (readKeys, metaKeys) = row.keySet().split { key -&gt; key =~ /^fastq/ }\n            def reads = row.subMap(readKeys).values()\n                .findAll { value -&gt; value != \"\" } // Single-end reads will have an empty string\n                .collect { path -&gt; file(path) }\n            def meta = row.subMap(metaKeys)\n            meta = meta + [ id: meta.sample, single_end: reads.size == 1 ]\n            [meta, reads]\n        }\n\n    FASTP(samples)\n\n    FASTP.out.json.view()\n}\n</code></pre> <p>Let's assume that we want to pull some information out of these JSON files. To make our lives a little more convenient, let's \"publish\" these json files so that they are more convenient. We're going to discuss configuration more completely in a later chapter, but that's no reason not to dabble a bit here.</p> <p>We'd like to add a <code>publishDir</code> directive to our FASTP process.</p> <pre><code>process {\n    withName: 'FASTP' {\n        publishDir = [\n            path: { \"results/fastp/json\" },\n            saveAs: { filename -&gt; filename.endsWith('.json') ? filename : null },\n        ]\n    }\n}\n</code></pre> <p>Groovy Tip: Elvis Operator</p> <p>This pattern of returning something if it is true and <code>somethingElse</code> if not:</p> <pre><code>somethingThatMightBeFalsey ? somethingThatMightBeFalsey : somethingElse\n</code></pre> <p>has a shortcut in Groovy - the \"Elvis\" operator:</p> <pre><code>somethingThatMightBeFalsey ?: somethingElse\n</code></pre> <p>This enables us to iterate quickly to test out our JSON parsing without waiting on the FASTP caching to calculate on these slow virtual machines.</p> <pre><code>nextflow run . -resume\n</code></pre> <p>Let's consider the possibility that we'd like to capture some of these metrics so that they can be used downstream. First, we'll have a quick peek at the Groovy docs and I see that I need to use <code>JsonSlurper</code>.</p> <p>Now let's create a second entrypoint to quickly pass these JSON files through some tests:</p> <p>Entrypoint developing</p> <p>Using a second Entrypoint allows us to do quick debugging or development using a small section of the workflow without disturbing the main flow.</p> <pre><code>workflow Jsontest {\n    Channel.fromPath(\"results/fastp/json/*.json\")\n        .view()\n}\n</code></pre> <p>which we run with</p> <pre><code>nextflow run . -resume -entry Jsontest\n</code></pre> <p>Let's create a small function inside the workflow to take the JSON path and pull out some basic metrics:</p> <pre><code>def getFilteringResult(json_file) {\n    return new groovy.json.JsonSlurper().parseText(json_file.text)\n}\n\nworkflow Jsontest {\n    Channel.fromPath(\"results/fastp/json/*.json\")\n        .view()\n}\n</code></pre> <p>The <code>fastpResult</code> returned from the <code>parseText</code> method is a large Map - a class which we're already familiar with. Modify the <code>getFilteringResult</code> function to return just the <code>after_filtering</code> section of the report.</p> <p>In the interest of brevity, here is the solution to return just the <code>after_filtering</code> section of the report:</p> <pre><code>def getFilteringResult(json_file) {\n    return new groovy.json.JsonSlurper().parseText(json_file.text)\n        ?.summary\n        ?.after_filtering\n}\n</code></pre> <p>Note</p> <p><code>?.</code> is new notation is a null-safe access operator. The <code>?.summary</code> will access the summary property if the property exists.</p> <p>We can then join this new map back to the original reads using the <code>join</code> operator:</p> <pre><code>    FASTP.out.json\n        .map { meta, json -&gt; [meta, getFilteringResult(json)] }\n        .join( FASTP.out.reads )\n        .view()\n}\n</code></pre> <p>Exercise</p> <p>Can you amend this pipeline to create two channels that filter the reads to exclude any samples where the Q30 rate is less than 93.5%?</p> Solution <pre><code>    reads = FASTP.out.json\n        .map { meta, json -&gt; [meta, getFilteringResult(json)] }\n        .join( FASTP.out.reads )\n        .map { meta, fastpMap, reads -&gt; [meta + fastpMap, reads] }\n        .branch { meta, reads -&gt;\n            pass: meta.q30_rate &gt;= 0.935\n            fail: true\n        }\n\n    reads.fail.view { meta, _reads -&gt; \"Failed: ${meta.id}\" }\n    reads.pass.view { meta, _reads -&gt; \"Passed: ${meta.id}\" }\n}\n</code></pre>"},{"location":"archive/advanced/grouping/","title":"Grouping and Splitting","text":""},{"location":"archive/advanced/grouping/#grouping-using-submap","title":"Grouping using subMap","text":"<p>Now that we have a channel that conforms to the <code>tuple val(meta) path(&lt;something&gt;)</code> pattern, we can investigate splitting and grouping patterns.</p> <p>We'll start with a simple main.nf in the <code>grouping</code> directory</p> <pre><code>cd grouping\n</code></pre> <pre><code>workflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n        .splitCsv( header:true )\n        .map { row -&gt;\n            def meta = [id:row.id, repeat:row.repeat, type:row.type]\n            [\n                meta,\n                [\n                    file(row.fastq1, checkIfExists: true),\n                    file(row.fastq2, checkIfExists: true)\n                ]\n            ]\n        }\n        .view()\n}\n</code></pre> <p>The first change we're going to make is to correct some repetitive code that we've seen quite a lot already in this workshop. The construction of the meta map from this row stutters quite a lot. We can make use of the <code>subMap</code> method available Maps to quickly return a new map constructed from the subset of an existing map:</p> <pre><code>workflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n        .splitCsv( header:true )\n        .map { row -&gt;\n            def meta = row.subMap('id', 'repeat', 'type')\n            [\n                meta,\n                [\n                    file(row.fastq1, checkIfExists: true),\n                    file(row.fastq2, checkIfExists: true)\n                ]\n            ]\n        }\n        .view()\n}\n</code></pre> <p>Complete meta map safety</p> <p>The <code>subMap</code> method will take a collection of keys and construct a new map with just the keys listed in the collection. This method, in combination with the <code>plus</code> or <code>+</code> method for combining maps and resetting values should allow all contraction, expansion and modification of maps safely.</p> <p>Exercise</p> <p>Can you extend our workflow in an unsafe manner? Use the <code>set</code> operator to name the channel in our workflow above, and then <code>map</code> (the operator) over that without modification. In a separate map operation, try modifying the meta map in a way that is reflected in the first map.</p> <p>Note that we're trying to do the wrong thing in this example to clarify what the correct approach might be.</p> Solution <p>To ensure that the modification of the map happens first, we introduce a <code>sleep</code> into the first map operation. This <code>sleep</code> emulates a long-running Nextflow process.</p> <pre><code>workflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n        .splitCsv( header:true )\n        .map { row -&gt;\n            def meta = row.subMap('id', 'repeat', 'type')\n            [\n                meta,\n                [\n                    file(row.fastq1, checkIfExists: true),\n                    file(row.fastq2, checkIfExists: true)\n                ]\n            ]\n        }\n        .set { samples }\n\n\n    samples\n        .map { element -&gt; sleep 10; element }\n        .view { meta, reads -&gt; \"Should be unmodified: $meta\" }\n\n    samples\n        .map { meta, reads -&gt;\n            meta.type = meta.type == \"tumor\" ? \"abnormal\" : \"normal\"\n            [meta, reads]\n        }\n        .view { meta, reads -&gt; \"Should be modified: $meta\" }\n}\n</code></pre> <p>Exercise</p> <p>How would you fix the example above to use the safe operators <code>plus</code> and <code>subMap</code> to ensure that the original map remains unmodified?</p> Solution <pre><code>workflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n        .splitCsv( header:true )\n        .map { row -&gt;\n            def meta = row.subMap('id', 'repeat', 'type')\n            [\n                meta,\n                [\n                    file(row.fastq1, checkIfExists: true),\n                    file(row.fastq2, checkIfExists: true)\n                ]\n            ]\n        }\n        .set { samples }\n\n\n    samples\n        .map { element -&gt; sleep 10; element }\n        .view { meta, reads -&gt; \"Should be unmodified: $meta\" }\n\n    samples\n        .map { meta, reads -&gt;\n            def newmap = [type: meta.type == \"tumor\" ? \"abnormal\" : \"normal\"]\n            [meta + newmap, reads]\n        }\n        .view { meta, reads -&gt; \"Should be modified: $meta\" }\n}\n</code></pre>"},{"location":"archive/advanced/grouping/#passing-maps-through-processes","title":"Passing maps through processes","text":"<p>Let's construct a dummy read mapping process. This is not a bioinformatics workshop, so we can 'cheat' in the interests of time.</p> <pre><code>process MapReads {\n    input:\n    tuple val(meta), path(reads)\n    path(genome)\n\n    output:\n    tuple val(meta), path(\"*.bam\")\n\n    script:\n    \"touch out.bam\"\n}\n\nworkflow {\n    reference = Channel.fromPath(\"data/genome.fasta\").first()\n\n    samples = Channel.fromPath(\"data/samplesheet.csv\")\n        .splitCsv( header:true )\n        .map { row -&gt;\n            def meta = row.subMap('id', 'repeat', 'type')\n            [\n              meta,\n              [\n                file(row.fastq1, checkIfExists: true),\n                file(row.fastq2, checkIfExists: true)\n              ]\n            ]\n        }\n\n    mapped_reads = MapReads( samples, reference )\n    mapped_reads.view()\n}\n</code></pre> <p>Let's consider that we might now want to merge the repeats. We'll need to group bams that share the <code>id</code> and <code>type</code> attributes.</p> <pre><code>mapped_reads = MapReads( samples, reference )\n    .map { meta, bam -&gt; [meta.subMap('id', 'type'), bam]}\n    .groupTuple()\nmapped_reads.view()\n</code></pre> <p>This is easy enough, but the <code>groupTuple</code> operator has to wait until all items are emitted from the incoming queue before it is able to reassemble the output queue. If even one read mapping job takes a long time, the processing of all other samples is held up. We need a way of signalling to Nextflow how many items are in a given group so that items can be emitted as early as possible.</p> <p>By default, the <code>groupTuple</code> operator groups on the first item in the element, which at the moment is a <code>Map</code>. We can turn this map into a special class using the <code>groupKey</code> method, which takes our grouping object as a first parameter and the number of expected elements in the second parameter.</p> <pre><code>mapped_reads = MapReads( samples, reference )\n    .map { meta, bam -&gt;\n        def key = groupKey(meta.subMap('id', 'type'), NUMBER_OF_ITEMS_IN_GROUP)\n        [key, bam]\n    }\n    .groupTuple()\nmapped_reads.view()\n</code></pre> <p>Exercise</p> <p>How might we modify the upstream channels to the number of repeats into the metamap?</p> Solution <pre><code>workflow {\n    reference = Channel.fromPath(\"data/genome.fasta\").first()\n\n    samples = Channel.fromPath(\"data/samplesheet.csv\")\n        .splitCsv( header:true )\n        .map { row -&gt;\n            def meta = row.subMap('id', 'repeat', 'type')\n            [\n              meta,\n              [\n                file(row.fastq1, checkIfExists: true),\n                file(row.fastq2, checkIfExists: true)\n              ]\n            ]\n        }\n        .map { meta, reads -&gt; [meta.subMap('id', 'type'), meta.repeat, reads] }\n        .groupTuple()\n        .map { meta, repeats, reads -&gt; [meta + [repeatcount:repeats.size()], repeats, reads] }\n        .transpose()\n        .map { meta, repeat, reads -&gt; [meta + [repeat:repeat], reads]}\n\n    mapped_reads = MapReads( samples, reference )\n        .map { meta, bam -&gt;\n            def key = groupKey(meta.subMap('id', 'type'), meta.repeatcount)\n            [key, bam]\n        }\n        .groupTuple()\n    mapped_reads.view()\n}\n</code></pre> <p>Now that we have our repeats together in an output channel, we can combine them using \"advanced bioinformatics\":</p> <pre><code>process CombineBams {\n    input:\n    tuple val(meta), path(\"input/in_*_.bam\")\n\n    output:\n    tuple val(meta), path(\"combined.bam\")\n\n    script:\n    \"cat input/*.bam &gt; combined.bam\"\n}\n</code></pre> <p>In our workflow:</p> <pre><code>mapped_reads = MapReads( samples, reference )\n    .map { meta, bam -&gt;\n        def key = groupKey(meta.subMap('id', 'type'), meta.repeatcount)\n        [key, bam]\n    }\n    .groupTuple()\n\nCombineBams(mapped_reads)\n  .view()\n</code></pre>"},{"location":"archive/advanced/grouping/#fanning-out-over-intervals","title":"Fanning out over intervals","text":"<p>The previous exercise demonstrated the fan-in approach using <code>groupTuple</code> and <code>groupKey</code>, but we might want to fan out our processes. An example might be computing over some intervals - genotyping over intervals, for example.</p> <p>We can take an existing bed file, for example and turn it into a channel of Maps.</p> <pre><code>intervals = Channel.fromPath(\"data/intervals.bed\")\n    .splitCsv(header: ['chr', 'start', 'stop', 'name'], sep: '\\t')\n    .collectFile { entry -&gt; [\"${entry.name}.bed\", entry*.value.join(\"\\t\")] }\n    .view()\n</code></pre> <p>Given a dummy genotyping process:</p> <pre><code>process GenotypeOnInterval {\n    input:\n    tuple val(meta), path(bam), path(bed)\n\n    output:\n    tuple val(meta), path(\"genotyped.vcf\")\n\n    script:\n    \"cat $bam $bed &gt; genotyped.vcf\"\n}\n</code></pre> <p>We can use the <code>combine</code> operator to emit a new channel where each combined bam is attached to each bed file. These can then be piped into the genotyping process:</p> <pre><code>mapped_reads = MapReads( samples, reference )\n    .map { meta, bam -&gt;\n        def key = groupKey(meta.subMap('id', 'type'), meta.repeatcount)\n        [key, bam]\n    }\n    .groupTuple()\n\ncombined_bams = CombineBams(mapped_reads)\n    .combine( intervals )\n\ngenotyped_bams = GenotypeOnInterval(combined_bams)\n    .view()\n</code></pre> <p>Finally, we can combine these genotyped bams back using <code>groupTuple</code> and another bam merge process. We construct our \"merge\" process that will combine the bam files from multiple intervals:</p> <pre><code>process MergeGenotyped {\n    input:\n    tuple val(meta), path(\"input/in_*_.vcf\")\n\n    output:\n    tuple val(meta), path(\"merged.genotyped.vcf\")\n\n    script:\n    \"cat input/*.vcf &gt; merged.genotyped.vcf\"\n}\n</code></pre> <p>We might be tempted to pipe the output of <code>GenotypeOnInterval</code> directly into groupTuple, but the <code>meta</code> object we are passing down is still the <code>groupKey</code> we created earlier:</p> <pre><code>mapped_reads = MapReads( samples, reference )\n    .map { meta, bam -&gt;\n        def key = groupKey(meta.subMap('id', 'type'), meta.repeatcount)\n        [key, bam]\n    }\n    .groupTuple()\n\ncombined_bams = CombineBams(mapped_reads)\n    .map { meta, bam -&gt; [meta.subMap('id', 'type'), bam] }\n    .combine( intervals )\n\ngenotyped_bams = GenotypeOnInterval(combined_bams)\n    .view { meta, bamfile -&gt; \"Meta is of ${meta.getClass()}\" }\n</code></pre> <p>To ensure that grouping is performed only on the relevant elements, we can unwrap the <code>groupKey</code> to return the underlying <code>Map</code> using the <code>getGroupTarget()</code> method available on groupKeys. This allows the <code>groupTuple</code> operator to group by just the keys present in the map, similar to how <code>subMap</code> works. This approach ensures that downstream grouping and merging steps operate on the intended sample attributes.</p> <pre><code>mapped_reads = MapReads( samples, reference )\n    .map { meta, bam -&gt;\n        def key = groupKey(meta.subMap('id', 'type'), meta.repeatcount)\n        [key, bam]\n    }\n    .groupTuple()\n\ncombined_bams = CombineBams(mapped_reads)\n    .map { meta, bam -&gt; [meta.subMap('id', 'type'), bam] }\n    .combine( intervals )\n\ngenotyped_bams = GenotypeOnInterval(combined_bams)\n    .map { groupKey, bamfile -&gt; [groupKey.getGroupTarget(), bamfile] }\n    .groupTuple()\n\nmerged_bams = MergeGenotyped(genotyped_bams)\nmerged_bams.view()\n</code></pre>"},{"location":"archive/advanced/grouping/#publishing-the-bams","title":"Publishing the bams","text":"<p>This will return us six bam files - a tumor and normal pair for each of the three samples:</p> Final channel output<pre><code>[[id:sampleB, type:normal], merged.genotyped.vcf]\n[[id:sampleB, type:tumor], merged.genotyped.vcf]\n[[id:sampleA, type:normal], merged.genotyped.vcf]\n[[id:sampleA, type:tumor], merged.genotyped.vcf]\n[[id:sampleC, type:normal], merged.genotyped.vcf]\n[[id:sampleC, type:tumor], merged.genotyped.vcf]\n</code></pre> <p>If we would like to save the output of our <code>MergeGenotyped</code> process, we can \"publish\" the outputs of a process using the <code>publishDir</code> directive. Try modifying the <code>MergeGenotyped</code> process to include the directive:</p> <pre><code>process MergeGenotyped {\n    publishDir 'results/genotyped'\n\n    input:\n    tuple val(meta), path(\"input/in_*_.vcf\")\n\n    output:\n    tuple val(meta), path(\"merged.genotyped.vcf\")\n\n    script:\n    \"cat input/*.vcf &gt; merged.genotyped.vcf\"\n}\n</code></pre> <p>This will publish all of the files in the <code>output</code> block of this process to the <code>results/genotyped</code> directory.</p> <p>Workflow outputs</p> <p>As of Nextflow 24.04 you can also manage result publication at the workflow level using the new <code>output { }</code> block. This allows you to publish files by pushing them from a channel instead of a process, similar to the channel operations we have been exploring in this workshop.</p> <p>See the Nextflow documentation for a full description.</p> <pre><code>nextflow.preview.output = true\n\nworkflow {\n    main:\n    merged_bams = MergeGenotyped(genotyped_bams)\n\n    publish:\n    results = merged_bams\n}\n\noutput {\n    results {\n        path 'genotyped'\n    }\n}\n</code></pre> <p>Exercise</p> <p>Inspect the contents of the <code>results</code> directory. Does this match what you were expecting? What is missing here?</p> <p>Can you modify the <code>MergeGenotyped</code> process to ensure we are capturing all of the expected output files?</p> Solution <p>One solution might be to modify the <code>script</code> block to ensure that each file has a unique name:</p> <pre><code>process MergeGenotyped {\n    publishDir 'results/genotyped'\n\n    input:\n    tuple val(meta), path(\"input/in_*_.vcf\")\n\n    output:\n    tuple val(meta), path(\"*.vcf\")\n\n    script:\n    \"cat input/*.vcf &gt; ${meta.id}.${meta.type}.genotyped.vcf\"\n}\n</code></pre> <p>Another option might be to use the <code>saveAs</code> argument to the <code>publishDir</code> directive:</p> <pre><code>process MergeGenotyped {\n    publishDir 'results/genotyped', saveAs: { \"${meta.id}.${meta.type}.genotyped.vcf\" }\n\n    input:\n    tuple val(meta), path(\"input/in_*_.vcf\")\n\n    output:\n    tuple val(meta), path(\"merged.genotyped.vcf\")\n\n    script:\n    \"cat input/*.vcf &gt; merged.genotyped.vcf\"\n}\n</code></pre> <p>If you are using the new output syntax described above, you can edit the filename using the <code>path</code> directive:</p> <pre><code>output {\n    results {\n        path { meta, vcf -&gt;\n            \"genotyped/${meta.id}.${meta.type}.genotyped.vcf\"\n        }\n    }\n}\n</code></pre>"},{"location":"archive/advanced/introduction/","title":"Introduction","text":"<p>Welcome to our Nextflow workshop for intermediate and advanced users!</p> <p>In this workshop, we will explore the advanced features of the Nextflow language and runtime, and learn how to use them to write efficient and scalable data-intensive workflows. We will cover topics such as parallel execution, error handling, and workflow customization.</p> <p>Please note that this is not an introductory workshop, and we will assume some basic familiarity with Nextflow.</p> <p>By the end of this workshop, you will have the skills and knowledge to create complex and powerful Nextflow pipelines for your own data analysis projects.</p> <p>Let's get started!</p>"},{"location":"archive/advanced/introduction/#learning-objectives","title":"Learning objectives","text":"<p>By the end of this course you should:</p> <ul> <li>Describe commonly used and well understood operators</li> <li>Apply good practices for the propagation of metadata</li> <li>Group and split channels</li> <li>Apply Groovy helper classes to Nextflow scripts</li> <li>Sensibly structure workflows</li> <li>Apply layers of configuration to a workflow</li> </ul>"},{"location":"archive/advanced/introduction/#audience-prerequisites","title":"Audience &amp; prerequisites","text":"<p>Please note that this is not a beginner's workshop and familiarity with Nextflow, the command line, and common file formats is assumed.</p> <p>Prerequisites</p> <ul> <li>A GitHub account</li> <li>Experience with command line</li> <li>Familiarity with Nextflow and Groovy</li> <li>An understanding of common file formats</li> </ul>"},{"location":"archive/advanced/introduction/#follow-the-training-video","title":"Follow the training video","text":"<p>We run a free online training event for this course approximately every six months. Videos are streamed to YouTube and questions are handled in the nf-core Slack community. You can watch the recording of the most recent training (September, 2023) below:</p>"},{"location":"archive/advanced/metadata/","title":"Metadata Propagation","text":"<p>A central challenge in a lot of batch-style computation is how to ensure the metadata describing a file remains with the file. Two good rules for handling metadata in Nextflow are:</p> <ul> <li>Metadata should be explicit - be extremely wary of metadata encoded in filenames</li> <li>Metadata should travel through channels with the data in a tuple element.</li> </ul>"},{"location":"archive/advanced/metadata/#metadata-import","title":"Metadata Import","text":"<p>Ideally, you have sample information stored in a simple and structured samplesheet. These are often files in CSV/TSV format or similar. I'd like to start with a worst-case scenario where you've been handed a bag of files that you need to make sense of. We'll use this example to introduce some helpful Groovy syntactic sugar and features that will be helpful in other Nextflow contexts.</p> <p>Given a bag of fastq reads:</p> <pre><code>cd metadata\ntree data/reads\n</code></pre> Output<pre><code>data/reads/\n|-- treatmentA\n|   |-- sampleA_rep1_normal_R1.fastq.gz\n|   |-- sampleA_rep1_normal_R2.fastq.gz\n|   |-- sampleA_rep1_tumor_R1.fastq.gz\n|   |-- sampleA_rep1_tumor_R2.fastq.gz\n|   |-- sampleA_rep2_normal_R1.fastq.gz\n|   |-- sampleA_rep2_normal_R2.fastq.gz\n|   |-- sampleA_rep2_tumor_R1.fastq.gz\n|   |-- sampleA_rep2_tumor_R2.fastq.gz\n|   |-- sampleB_rep1_normal_R1.fastq.gz\n|   |-- sampleB_rep1_normal_R2.fastq.gz\n|   |-- sampleB_rep1_tumor_R1.fastq.gz\n|   |-- sampleB_rep1_tumor_R2.fastq.gz\n|   |-- sampleC_rep1_normal_R1.fastq.gz\n|   |-- sampleC_rep1_normal_R2.fastq.gz\n|   |-- sampleC_rep1_tumor_R1.fastq.gz\n|   `-- sampleC_rep1_tumor_R2.fastq.gz\n`-- treatmentB\n    |-- sampleA_rep1_normal_R1.fastq.gz\n    |-- sampleA_rep1_normal_R2.fastq.gz\n    |-- sampleA_rep1_tumor_R1.fastq.gz\n    `-- sampleA_rep1_tumor_R2.fastq.gz\n</code></pre> <p>Warning</p> <p>Whomever has handed us these files has encoded metadata in both the filename, but also the name of the parent directories <code>treatmentA</code> and <code>treatmentB</code>.</p>"},{"location":"archive/advanced/metadata/#first-pass","title":"First Pass","text":"<p>A first pass attempt at pulling these files into Nextflow might use the <code>fromFilePairs</code> method:</p> <pre><code>workflow {\n    Channel.fromFilePairs(\"data/reads/*/*_R{1,2}.fastq.gz\")\n        .view()\n}\n</code></pre> <p>Nextflow will pull out the first part of the fastq filename and returned us a channel of tuple elements where the first element is the filename-derived ID and the second element is a list of two fastq files.</p> <p>The id is stored as a simple string. We'd like to move to using a map of key-value pairs because we have more than one piece of metadata to track. In this example, we have sample, replicate, tumor/normal, and treatment. We could add extra elements to the tuple, but this changes the 'cardinality' of the elements in the channel and adding extra elements would require updating all downstream processes. A map is a single object and is passed through Nextflow channels as one value, so adding extra metadata fields will not require us to change the cardinality of the downstream processes.</p> <p>There are a couple of different ways we can pull out the metadata</p> <p>We can use the <code>tokenize</code> method to split our id. To sanity-check, I just pipe the result directly into the <code>view</code> operator.</p> <pre><code>workflow {\n    Channel.fromFilePairs(\"data/reads/*/*_R{1,2}.fastq.gz\")\n        .map { id, reads -&gt;\n            id.tokenize(\"_\")\n        }\n        .view()\n}\n</code></pre> <p>If we are confident about the stability of the naming scheme, we can destructure the list returned by <code>tokenize</code> and assign them to variables directly:</p> <pre><code>.map { id, reads -&gt;\n    def (sample, replicate, type) = id.tokenize(\"_\")\n    def meta = [sample:sample, replicate:replicate, type:type]\n    [meta, reads]\n}\n</code></pre> <p>Destructuring requires parentheses</p> <p>Make sure that you're using a tuple with parentheses e.g. <code>(one, two)</code> rather than a List e.g. <code>[one, two]</code></p> <p>Another option is to use the <code>transpose</code> method with the <code>collectEntries()</code> to produce the same map. I'd warn that this method is bordering on a little 'too clever' and is more difficult to read. It also assumes that the order of the filename-encoded metadata is consistent.</p> <pre><code>.map { id, reads -&gt;\n    def meta = [['sample', 'replicate', 'type'], id.tokenize(\"_\")]\n        .transpose()\n        .collectEntries()\n    [meta, reads]\n}\n</code></pre> <p>If we move back to the previous method, but decided that the 'rep' prefix on the replicate should be removed, we can use regular expressions to simply \"subtract\" pieces of a string. Here we remove a 'rep' prefix from the <code>replicate</code> variable if the prefix is present:</p> <pre><code>.map { id, reads -&gt;\n    def (sample, replicate, type) = id.tokenize(\"_\")\n    replicate -= ~/^rep/\n    def meta = [sample:sample, replicate:replicate, type:type]\n    [meta, reads]\n}\n</code></pre> <p>Trim strings</p> <p>Groovy has a lot of very helpful syntactic sugar for string manipulation. You can trim parts of a string by simply subtracting another string:</p> <pre><code>demo = \"one two three\"\nassertEquals(demo - \"two \", \"one three\")\n</code></pre> <p>... or by subtracting a regular expression:</p> <pre><code>demo = \"one two three\"\nassertEquals(demo - ~/t.o ?/, \"one three\")\n</code></pre> <p>To quickly sanity-check a groovy expression, try the Groovy web console</p> <p>We are almost there, but we still don't have the \"treatment\" metadata captured in our meta map. The treatment is encoded in this example in the name of the parent directory relative to the reads. Inside the map object, the reads are a list of two UnixPath objects. These objects implement the <code>java.nio.Path</code> interface, which provides us many useful methods, including <code>getParent()</code>.</p> <p>We can call the <code>getParent()</code> method on each of the paths like so:</p> <pre><code>.map { id, reads -&gt;\n    reads.collect { read -&gt; read.getParent() }\n}\n</code></pre> <p>If we want to call a set method on every item in a Collection, Groovy provides this convenient \"spread dot\" notation:</p> <pre><code>.map { id, reads -&gt;\n    reads*.getParent()\n}\n</code></pre> <p>This returns another Path object, but we only want the name of the last directory, so we need to call <code>.getName()</code> method on each of these Paths. We can use the spread-dot notation again:</p> <pre><code>.map { id, reads -&gt;\n    reads*.getParent()*.getName()\n}\n</code></pre> <p>The last piece of Groovy sugar is to note that methods with <code>get</code> and <code>set</code> prefixes can be called with a property-style notation, converting <code>getParent()</code> to <code>parent</code> and <code>getName()</code> to <code>name</code>:</p> <pre><code>.map { id, reads -&gt;\n    reads*.parent*.name\n}\n</code></pre> <p>If we wanted to remove the \"treatment\" prefix, we can combine this new notation with the \"minus\" method which we used earlier in the aliased <code>-</code> form.</p> <pre><code>.map { id, reads -&gt;\n    reads*.parent*.name*.minus(~/treatment/)\n}\n</code></pre> <p>In this particular example, we know ahead of time that the treatments must be the same because of the way the <code>fromFilePairs</code> method gathers pairs, but we'll continue for the sake of the demonstration. Our final <code>map</code> closure might look like:</p> <pre><code>workflow {\n    Channel.fromFilePairs(\"data/reads/*/*_R{1,2}.fastq.gz\")\n        .map { id, reads -&gt;\n            def (sample, replicate, type) = id.tokenize(\"_\")\n            def (treatmentFwd, treatmentRev) = reads*.parent*.name*.minus(~/treatment/)\n            def meta = [\n                sample:sample,\n                replicate:replicate,\n                type:type,\n                treatmentFwd:treatmentFwd,\n                treatmentRev:treatmentRev,\n            ]\n            [meta, reads]\n        }\n        .view()\n}\n</code></pre> <p>This metadata map can be passed through the workflow with the reads and used to split, join and recombine the data. The resulting channel would be suitable for any Nextflow process with inputs of the form</p> <pre><code>process ExampleProcess {\n    input:\n    tuple val(meta), path(reads)\n\n    // ...\n</code></pre> <p>This channel \"shape\" or cardinality is extremely common in nf-core modules and subworkflows and is critical to enabling reusability of these modules.</p>"},{"location":"archive/advanced/operators/","title":"Operator Tour","text":"<p>In this chapter, we take a curated tour of the Nextflow operators. Commonly used and well understood operators are not covered here - only those that we've seen could use more attention or those where the usage could be more elaborate. These set of operators have been chosen to illustrate tangential concepts and Nextflow features.</p>"},{"location":"archive/advanced/operators/#map","title":"<code>map</code>","text":""},{"location":"archive/advanced/operators/#basics","title":"Basics","text":"<p>Map is certainly the most commonly used of the operators covered here. It's a way to supply a closure through which each element in the channel is passed. The return value of the closure is emitted as an element in a new output channel. A canonical example is a closure that multiplies two numbers:</p> <pre><code>workflow {\n    Channel.of( 1, 2, 3, 4, 5 )\n        .map { num -&gt; num * num }\n        .view()\n}\n</code></pre> <p>The code above is available in a starter <code>main.nf</code> file available at <code>advanced/operators/main.nf</code>. It is recommended to open and edit this file to follow along with the examples given in the rest of this chapter. The workflow can be executed with:</p> <pre><code>cd operators\nnextflow run .\n</code></pre> <p>The code creates a workflow that emits the numbers 1 through 5 into a channel, applies the <code>map</code> operator to square each number, and then prints the results. This demonstrates how <code>map</code> transforms each element in a channel by applying a closure, producing a new channel with the squared values.</p> <p>The value passed into <code>map</code> is assigned to a variable using the <code>-&gt;</code> syntax (e.g., <code>num</code> in <code>{ num -&gt; ... }</code>). This variable is local to the closure and used for any transformations inside it.</p> <p>Groovy is an optionally typed language, and it is possible to specify the type of the argument passed to the closure.</p> <pre><code>workflow {\n    Channel.of( 1, 2, 3, 4, 5 )\n        .map { Integer num -&gt; num * num }\n        .view()\n}\n</code></pre>"},{"location":"archive/advanced/operators/#named-closures","title":"Named Closures","text":"<p>If you find yourself re-using the same closure multiple times in your pipeline, the closure can be named and referenced:</p> <pre><code>workflow {\n    def squareIt = { Integer num -&gt; num * num }\n\n    Channel.of( 1, 2, 3, 4, 5 )\n        .map( squareIt )\n        .view()\n}\n</code></pre> <p>If you have these re-usable closures defined, you can compose them together.</p> <pre><code>workflow {\n    def squareIt = { num -&gt; num * num }\n    def addTwo = { num -&gt; num + 2 }\n\n    Channel.of( 1, 2, 3, 4, 5 )\n        .map( squareIt &gt;&gt; addTwo )\n        .view()\n}\n</code></pre> Output<pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `./main.nf` [focused_borg] DSL2 - revision: f3c3e751fe\n3\n6\n11\n18\n27\n</code></pre> <p>The above is the same as writing:</p> <pre><code>workflow {\n    def squareIt = { num -&gt; num * num }\n    def addTwo = { num -&gt; num + 2 }\n\n    Channel.of( 1, 2, 3, 4, 5 )\n        .map( squareIt )\n        .map( addTwo )\n        .view()\n}\n</code></pre> <p>For those inclined towards functional programming, you'll be happy to know that closures can be curried:</p> <pre><code>workflow {\n    def timesN = { multiplier, num -&gt; num * multiplier }\n    def timesTen = timesN.curry(10)\n\n    Channel.of( 1, 2, 3, 4, 5 )\n        .map( timesTen )\n        .view()\n}\n</code></pre>"},{"location":"archive/advanced/operators/#view","title":"<code>view</code>","text":"<p>In addition to the argument-less usage of <code>view</code> as shown above, this operator can also take a closure to customize the stdout message. We can create a closure to print the value of the elements in a channel as well as their type, for example:</p> <pre><code>workflow {\n    def timesN = { multiplier, num -&gt; num * multiplier }\n    def timesTen = timesN.curry(10)\n\n    Channel.of( 1, 2, 3, 4, 5 )\n        .map( timesTen )\n        .view { value -&gt; \"Found '$value' (${value.getClass()})\"}\n}\n</code></pre> <p>Most closures will remain anonymous</p> <p>In many cases, it is simply cleaner to keep the closure anonymous, defined inline. Giving closures a name is only recommended when you find yourself defining the same or similar closures repeatedly in a given workflow.</p>"},{"location":"archive/advanced/operators/#splitcsv","title":"<code>splitCsv</code>","text":"<p>A common Nextflow pattern is for a simple samplesheet to be passed as primary input into a workflow. We'll see some more complicated ways to manage these inputs later on in the workshop, but the <code>splitCsv</code> (docs) is an excellent tool to have in a pinch. This operator will parse a csv/tsv and return a channel where each item is a row in the csv/tsv:</p> <pre><code>workflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n        .splitCsv( header: true )\n        .view()\n}\n</code></pre> <p>Exercise</p> <p>From the directory <code>advanced/operators</code>, use the <code>splitCsv</code> and <code>map</code> operators to read the file <code>data/samplesheet.csv</code> and return a channel that would be suitable input to the process below. Feel free to consult the splitCsv documentation for tips.</p> <pre><code>process FastQC {\n    input:\n    tuple val(id), path(fastqs)\n    // ... rest of the process\n</code></pre> Solution <p>Specifying the <code>header</code> argument in the <code>splitCsv</code> operator, we have convenient named access to csv elements. The closure returns a list of two elements where the second element a list of paths.</p> <pre><code>workflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n        .splitCsv( header: true )\n        .map { row -&gt;\n            [row.id, [file(row.fastq1), file(row.fastq2)]]\n        }\n        .view()\n}\n</code></pre> <p>Convert Strings to Paths</p> <p>The fastq paths are simple strings in the context of a csv row. In order to pass them as paths to a Nextflow process, they need to be converted into objects that adjere to the <code>Path</code> interface. This is accomplished by wrapping them in <code>file</code>.</p> <p>In the sample above, we've lost an important piece of metadata - the tumor/normal classification, choosing only the sample id as the first element in the output list.</p> <p>In the next chapter, we'll discuss the \"meta map\" pattern in more detail, but we can preview that here.</p> <pre><code>workflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n        .splitCsv( header: true )\n        .map { row -&gt;\n            def metaMap = [id: row.id, type: row.type, repeat: row.repeat]\n            [metaMap, [file(row.fastq1), file(row.fastq2)]]\n        }\n        .view()\n}\n</code></pre> <p>The construction of this map is very repetitive, and in the next chapter, we'll discuss some Groovy methods available on the <code>Map</code> class that can make this pattern more concise and less error-prone.</p>"},{"location":"archive/advanced/operators/#multimap","title":"<code>multiMap</code>","text":"<p>The <code>multiMap</code> (documentation) operator is a way of taking a single input channel and emitting into multiple channels for each input element.</p> <p>Let's assume we've been given a samplesheet that has tumor/normal pairs bundled together on the same row. View the example samplesheet with:</p> <pre><code>cd operators\ncat data/samplesheet.ugly.csv\n</code></pre> <p>Using the <code>splitCsv</code> operator would give us one entry that would contain all four fastq files. Let's consider that we wanted to split these fastqs into separate channels for tumor and normal. In other words, for every row in the samplesheet, we would like to emit an entry into two new channels. To do this, we can use the <code>multiMap</code> operator:</p> <pre><code>workflow {\n    Channel.fromPath(\"data/samplesheet.ugly.csv\")\n        .splitCsv( header: true )\n        .multiMap { row -&gt;\n            tumor:\n                def tumor_meta = [id: row.id, type:'tumor', repeat:row.repeat]\n                [tumor_meta, file(row.tumor_fastq_1), file(row.tumor_fastq_2)]\n            normal:\n                def normal_meta = [id: row.id, type:'normal', repeat:row.repeat]\n                [normal_meta, file(row.normal_fastq_1), file(row.normal_fastq_2)]\n        }\n        .set { samples }\n\n    samples.tumor.view { sample -&gt; \"Tumor: $sample\"}\n    samples.normal.view { sample -&gt; \"Normal: $sample\"}\n}\n</code></pre> <p>multiMapCriteria</p> <p>The closure supplied to <code>multiMap</code> needs to return multiple channels, so using named closures as described in the <code>map</code> section above will not work. Fortunately, Nextflow provides the convenience <code>multiMapCriteria</code> method to allow you to define named <code>multiMap</code> closures should you need them. See the <code>multiMap</code> documentation for more info.</p>"},{"location":"archive/advanced/operators/#branch","title":"<code>branch</code>","text":"<p>The <code>branch</code> operator (documentation) is a way of taking a single input channel and emitting a new element into one (and only one) of a selection of output channels.</p> <p>In the example above, the <code>multiMap</code> operator was necessary because we were supplied with a samplesheet that combined two pairs of fastq per row and we wanted to turn each row into new elements in multiple channels. If we were to use the neater samplesheet that had tumor/normal pairs on separate rows, we could use the <code>branch</code> operator to achieve the same result as we are routing each input element into a single output channel.</p> <pre><code>workflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n        .splitCsv( header: true )\n        .map { row -&gt; [[id: row.id, repeat: row.repeat, type: row.type], [file(row.fastq1), file(row.fastq2)]] }\n        .branch { meta, _reads -&gt;\n            tumor: meta.type == \"tumor\"\n            normal: meta.type == \"normal\"\n        }\n        .set { samples }\n\n    samples.tumor.view { sample -&gt; \"Tumor: $sample\"}\n    samples.normal.view { sample -&gt; \"Normal: $sample\"}\n}\n</code></pre> <p>An element is only emitted to the first channel were the test condition is met. If an element does not meet any of the tests, it is not emitted to any of the output channels. You can 'catch' any such samples by specifying <code>true</code> as a condition. If we knew that all samples would be either tumor or normal and no third 'type', we could write</p> <pre><code>.branch { meta, reads -&gt;\n    tumor: meta.type == \"tumor\"\n    normal: true\n}\n</code></pre> <p>We may want to emit a slightly different element than the one passed as input. The <code>branch</code> operator can (optionally) return a new element to a channel. For example, to add an extra key in the meta map of the tumor samples, we add a new line under the condition and return our new element. In this example, we modify the first element of the <code>List</code> to be a new list that is the result of merging the existing meta map with a new map containing a single key:</p> <pre><code>.branch { meta, reads -&gt;\n    tumor: meta.type == \"tumor\"\n        return [meta + [newKey: 'myValue'], reads]\n    normal: true\n}\n</code></pre> <p>Exercise</p> <p>How would you modify the element returned in the <code>tumor</code> channel to have the key:value pair <code>type:'abnormal'</code> instead of <code>type:'tumor'</code>?</p> Solution <p>There are many ways to accomplish this, but the map merging pattern introduced above can also be used to safely and concisely rename values in a map.</p> <pre><code>.branch { meta, reads -&gt;\n    tumor: meta.type == \"tumor\"\n        return [meta + [type: 'abnormal'], reads]\n    normal: true\n}\n</code></pre> <p>Merging maps is safe</p> <p>Using the <code>+</code> operator to merge two or more Maps returns a new Map. There are rare edge cases where modification of map rather than returning a new map can affect other channels. We discuss this further in the next chapter, but just be aware that this <code>+</code> operator is safer and often more convenient than modifying the <code>meta</code> object directly.</p> <p>See the Groovy Map documentation for details.</p>"},{"location":"archive/advanced/operators/#multi-channel-objects","title":"Multi-channel Objects","text":"<p>Certain Nextflow operators, such as <code>multiMap</code> and <code>branch</code>, return special objects containing multiple named channels. These multi-channel objects allow you to split data into separate streams while maintaining the ability to reference each stream independently.</p> <pre><code>workflow {\n    numbers = Channel.of( 1, 2, 3, 4, 5 )\n        .multiMap { num -&gt;\n            small: num\n            large: num * 10\n        }\n    numbers.small.view { num -&gt; \"Small: $num\"}\n    numbers.large.view { num -&gt; \"Large: $num\"}\n}\n</code></pre> <p>This creates two channels accessible through the <code>numbers</code> object: <code>small</code> containing the original values (1, 2, 3, 4, 5) and <code>large</code> containing the values multiplied by 10 (10, 20, 30, 40, 50).</p> <p>When a process requires multiple input channels, Nextflow automatically synchronizes the values from these channels. Each channel provides values as separate input parameters, and Nextflow pairs them together for each process execution:</p> <pre><code>process MultiInput {\n    debug true\n    input:\n    val(smallNum)\n    val(bigNum)\n\n    script:\n    \"echo -n small is $smallNum and big is $bigNum\"\n}\n\nworkflow {\n</code></pre> <p>The following will be kept synchronous, allowing you to supply multiple channel inputs to a process and keeping the order. This is the only place where order is guaranteed in Nextflow!</p> <pre><code>workflow {\n    numbers = Channel.of( 1, 2, 3, 4, 5 )\n        .multiMap { num -&gt;\n            small: num\n            large: num * 10\n        }\n\n    MultiInput(numbers.small, numbers.large)\n}\n</code></pre> <p>This workflow produces synchronized output:</p> Multi-channel Input Output<pre><code>small is 1 and big is 10\nsmall is 2 and big is 20\nsmall is 3 and big is 30\nsmall is 4 and big is 40\nsmall is 5 and big is 50\n</code></pre> <p>Multi-channel objects, created with <code>multiMap</code> or <code>branch</code>, let you split data streams while keeping named references. They allow related data to be processed in sync\u2014Nextflow guarantees ordering only when multiple channels are used as process inputs.</p>"},{"location":"archive/advanced/operators/#grouptuple","title":"<code>groupTuple</code>","text":"<p>A common operation is to group elements from a single channel where those elements share a common key. Take this example samplesheet as an example:</p> <pre><code>workflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def meta = [id: row.id, type: row.type]\n            [meta, row.repeat, [row.fastq1, row.fastq2]]\n        }\n        .view()\n}\n</code></pre> <p>We see that there are multiple rows where the first element in the item emitted by the channel is the Map <code>[id:sampleA, type:normal]</code> and items in the channel where the first element is the Map <code>[id:sampleA, type:tumor]</code>.</p> <p>The <code>groupTuple</code> operator allows us to combine elements that share a common key:</p> <pre><code>workflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def meta = [id: row.id, type: row.type]\n            [meta, row.repeat, [row.fastq1, row.fastq2]]\n        }\n        .groupTuple()\n        .view()\n}\n</code></pre>"},{"location":"archive/advanced/operators/#transpose","title":"<code>transpose</code>","text":"<p>The transpose operator is often misunderstood. It can be thought of as the inverse of the <code>groupTuple</code> operator. Give the following workflow, the <code>groupTuple</code> and <code>transpose</code> operators cancel each other out. Removing lines 8 and 9 returns the same result.</p> <p>Given a workflow that returns one element per sample, where we have grouped the samplesheet lines on a meta containing only id and type:</p> <pre><code>workflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def meta = [id: row.id, type: row.type]\n            [meta, row.repeat, [row.fastq1, row.fastq2]]\n        }\n        .groupTuple()\n        .view()\n}\n</code></pre> Output<pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `./main.nf` [spontaneous_rutherford] DSL2 - revision: 7dc1cc0039\n[[id:sampleA, type:normal], [1, 2], [[data/reads/sampleA_rep1_normal_R1.fastq.gz, data/reads/sampleA_rep1_normal_R2.fastq.gz], [data/reads/sampleA_rep2_normal_R1.fastq.gz, data/reads/sampleA_rep2_normal_R2.fastq.gz]]]\n[[id:sampleA, type:tumor], [1, 2], [[data/reads/sampleA_rep1_tumor_R1.fastq.gz, data/reads/sampleA_rep1_tumor_R2.fastq.gz], [data/reads/sampleA_rep2_tumor_R1.fastq.gz, data/reads/sampleA_rep2_tumor_R2.fastq.gz]]]\n[[id:sampleB, type:normal], [1], [[data/reads/sampleB_rep1_normal_R1.fastq.gz, data/reads/sampleB_rep1_normal_R2.fastq.gz]]]\n[[id:sampleB, type:tumor], [1], [[data/reads/sampleB_rep1_tumor_R1.fastq.gz, data/reads/sampleB_rep1_tumor_R2.fastq.gz]]]\n[[id:sampleC, type:normal], [1], [[data/reads/sampleC_rep1_normal_R1.fastq.gz, data/reads/sampleC_rep1_normal_R2.fastq.gz]]]\n[[id:sampleC, type:tumor], [1], [[data/reads/sampleC_rep1_tumor_R1.fastq.gz, data/reads/sampleC_rep1_tumor_R2.fastq.gz]]]\n</code></pre> <p>If we add in a <code>transpose</code>, each repeat number is matched back to the appropriate list of reads:</p> <pre><code>workflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def meta = [id: row.id, type: row.type]\n            [meta, row.repeat, [row.fastq1, row.fastq2]]\n        }\n        .groupTuple()\n        .transpose()\n        .view()\n}\n</code></pre> Output<pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `./main.nf` [elegant_rutherford] DSL2 - revision: 2c5476b133\n[[id:sampleA, type:normal], 1, [data/reads/sampleA_rep1_normal_R1.fastq.gz, data/reads/sampleA_rep1_normal_R2.fastq.gz]]\n[[id:sampleA, type:normal], 2, [data/reads/sampleA_rep2_normal_R1.fastq.gz, data/reads/sampleA_rep2_normal_R2.fastq.gz]]\n[[id:sampleA, type:tumor], 1, [data/reads/sampleA_rep1_tumor_R1.fastq.gz, data/reads/sampleA_rep1_tumor_R2.fastq.gz]]\n[[id:sampleA, type:tumor], 2, [data/reads/sampleA_rep2_tumor_R1.fastq.gz, data/reads/sampleA_rep2_tumor_R2.fastq.gz]]\n[[id:sampleB, type:normal], 1, [data/reads/sampleB_rep1_normal_R1.fastq.gz, data/reads/sampleB_rep1_normal_R2.fastq.gz]]\n[[id:sampleB, type:tumor], 1, [data/reads/sampleB_rep1_tumor_R1.fastq.gz, data/reads/sampleB_rep1_tumor_R2.fastq.gz]]\n[[id:sampleC, type:normal], 1, [data/reads/sampleC_rep1_normal_R1.fastq.gz, data/reads/sampleC_rep1_normal_R2.fastq.gz]]\n[[id:sampleC, type:tumor], 1, [data/reads/sampleC_rep1_tumor_R1.fastq.gz, data/reads/sampleC_rep1_tumor_R2.fastq.gz]]\n</code></pre>"},{"location":"archive/advanced/operators/#flatmap","title":"<code>flatMap</code>","text":"<p>As the name suggests, the <code>flatMap</code> operator allows you to modify the elements in a channel and then flatten the resulting collection. This is useful if you need to \"expand\" elements in a channel an incoming element can turn into zero or more elements in the output channel. For example:</p> <pre><code>workflow {\n    numbers = Channel.of(1, 2)\n\n    numbers\n        .flatMap { n -&gt; [ n, n*10, n*100 ] }\n        .view()\n}\n</code></pre> <p>The input channel has two elements. For each element in the input channel, we return a List of length three. The List is flattened and each element in our returned list is emitted independently into the output channel:</p> <pre><code>1\n10\n100\n2\n20\n200\n</code></pre> <p>Exercise</p> <p>The <code>flatten</code> operation only \"unfolds\" one layer from the returned collection. Given this information, what do you expect the following workflow to return?</p> <pre><code>workflow {\n    numbers = Channel.of(1, 2)\n\n    numbers\n        .flatMap { n -&gt; [ n, [n*10, n*100] ] }\n        .view()\n}\n</code></pre> <p>Exercise</p> <p>Let's say we have some collection of data for two samples:</p> <pre><code>mkdir -p data/datfiles/sample{1,2}\ntouch data/datfiles/sample1/data.{1,2,3,4,5,6,7}.dat\ntouch data/datfiles/sample2/data.{1,2,3,4}.dat\ntree data/datfiles\n</code></pre> <p>You would like to process these datfiles in batches - up to three at a time. The catch is that your batch process requires that the samples are processed independently. You have begun your workflow and grouped the samples together:</p> <pre><code>workflow {\n    Channel.fromPath(\"data/datfiles/sample*/*.dat\", checkIfExists: true)\n        .map { myfile -&gt; [myfile.getParent().name, myfile] }\n        .groupTuple()\n        .view()\n}\n</code></pre> <p>Which returns a channel with two elements corresponding to each sample:</p> <pre><code>[sample2, [sample2/data.1.dat, sample2/data.3.dat, sample2/data.2.dat, sample2/data.4.dat]]\n[sample1, [sample1/data.1.dat, sample1/data.3.dat, sample1/data.2.dat, sample1/data.6.dat, sample1/data.5.dat, sample1/data.4.dat]]\n</code></pre> <p>You would like to turn this channel into something that has at most three datfiles in each element. Something like:</p> <pre><code>[sample1, [sample1/data.1.dat, sample1/data.3.dat, sample1/data.2.dat]]\n[sample1, [sample1/data.6.dat, sample1/data.5.dat, sample1/data.4.dat]]\n[sample2, [sample2/data.1.dat, sample2/data.3.dat, sample2/data.2.dat]]\n[sample2, [sample2/data.4.dat]]\n</code></pre> <p>This is a challenging problem that pushes slightly beyond what we have covered.</p> <p>Tip</p> <p>You may find it helpful to use Groovy's <code>collate()</code> method (docs, tutorial). This method segments a list into sub-lists of specified size.</p> <p>You may also need the <code>collect()</code> method, which will perform an operation on every element in the collection (array, tuple, etc) and return a new collection of the outputs. This is similar to the <code>map</code> operator for but works on collections instead of channels (docs, tutorial).</p> Solution <p>This is a sensible approach. For each item in the channel, the <code>flatmap</code> will collate the files into groups of 3 using <code>.collate(3)</code>, then collect them into a single tuple using the id as the first value and the collection of 3 files as the second value.</p> <pre><code>workflow {\n    Channel.fromPath(\"data/datfiles/sample*/*.dat\", checkIfExists: true)\n        .map { myfile -&gt; [myfile.getParent().name, myfile] }\n        .groupTuple()\n        .flatMap { id, files -&gt;\n            files\n                .collate(3)\n                .collect { chunk -&gt; [ id, chunk ]\n            }\n        }\n        .view()\n}\n</code></pre>"},{"location":"archive/advanced/operators/#collectfile","title":"<code>collectFile</code>","text":"<p>The <code>collectFile</code> operator allows you to write one or more new files based on the contents of a channel.</p>"},{"location":"archive/advanced/operators/#writing-strings-to-a-single-file","title":"Writing strings to a single file","text":"<p>At its most basic, this operator writes the contents of the elements of a channel directly into a file. If the objecting being passed into the <code>collectFile</code> operator is a string, the strings are written to the collected file:</p> <pre><code>workflow {\n    characters = Channel.of(\n        ['name': 'Jake', 'title': 'Detective'],\n        ['name': 'Rosa', 'title': 'Detective'],\n        ['name': 'Terry', 'title': 'Sergeant'],\n        ['name': 'Amy', 'title': 'Detective'],\n        ['name': 'Charles', 'title': 'Detective'],\n        ['name': 'Gina', 'title': 'Administrator'],\n        ['name': 'Raymond', 'title': 'Captain'],\n        ['name': 'Michael', 'title': 'Detective'],\n        ['name': 'Norm', 'title': 'Detective']\n    )\n\n    characters\n        .map { character -&gt; character.name }\n        .collectFile()\n        .view()\n}\n</code></pre> <p>The operator returns a channel containing a new file <code>collect-file.data</code>:</p> Output<pre><code>N E X T F L O W  ~  version 23.10.1\nLaunching `./main.nf` [distracted_ritchie] DSL2 - revision: 1e5061406d\nwork/tmp/57/8052c566f657c59679f07958031231/collect-file.data\n</code></pre> <p>If we view the contents of this file, we see the strings written (without delimiter) to a single file:</p> collect-file.data<pre><code>RaymondTerryNormRosaCharlesJakeGinaMichaelAmy\n</code></pre> <p>We can supply arguments <code>name</code> and <code>newLine</code> to the <code>collectFile</code> operator to return a file with a more informative name and newlines separating each entry:</p> <pre><code>characters\n    .map { it.name }\n    .collectFile(name: 'people.txt', newLine: true)\n    .view()\n</code></pre> people.txt<pre><code>Raymond\nTerry\nNorm\nRosa\nCharles\nJake\nGina\nMichael\nAmy\n</code></pre>"},{"location":"archive/advanced/operators/#collecting-to-a-new-location","title":"Collecting to a new location","text":"<p>By default, the collected file is written into the work directory, which makes it suitable for input into a downstream process. If the collected file is an output of the workflow instead of an intermediate, it can be written to a directory of your choosing using the <code>storeDir</code> argument:</p> <pre><code>characters\n    .map { it.name }\n    .collectFile(name: 'characters.txt', newLine: true, storeDir: 'results')\n    .view()\n</code></pre>"},{"location":"archive/advanced/operators/#collecting-file-contents","title":"Collecting file contents","text":"<p>If the contents of the input channel is a file, its contents are appended to the collected file. Here we make a small process <code>WriteBio</code> that generates a CSV from the Map object supplied as input:</p> <p>Groovy in the script block</p> <p>In the example below, we include a line of groovy to define a variable <code>article</code> which is used in the interpolated script string. This is a convenient way to avoid crowding the final string block with too much logic.</p> <p>This line includes two Groovy syntax features:</p> <ol> <li>The ternary operator - a terse if/else block</li> <li>The find operator <code>=~</code></li> </ol> <pre><code>process WriteBio {\n    input: val(character)\n    output: path('bio.txt')\n    script:\n    def article = character.title.toLowerCase() =~ ~/^[aeiou]/ ? 'an' : 'a'\n    \"\"\"\n    echo ${character.name} is ${article} ${character.title} &gt; bio.txt\n    \"\"\"\n}\n\nworkflow {\n    characters = Channel.of(\n        ['name': 'Jake', 'title': 'Detective'],\n        ['name': 'Rosa', 'title': 'Detective'],\n        ['name': 'Terry', 'title': 'Sergeant'],\n        ['name': 'Amy', 'title': 'Detective'],\n        ['name': 'Charles', 'title': 'Detective'],\n        ['name': 'Gina', 'title': 'Administrator'],\n        ['name': 'Raymond', 'title': 'Captain'],\n        ['name': 'Michael', 'title': 'Detective'],\n        ['name': 'Norm', 'title': 'Detective']\n    )\n\n    WriteBio(characters)\n      .collectFile()\n      .view()\n}\n</code></pre> <p>... to produce the collected file</p> bio.txt<pre><code>Charles is a Detective\nAmy is a Detective\nNorm is a Detective\nJake is a Detective\nRosa is a Detective\nRaymond is a Captain\nGina is an Administrator\nTerry is a Sergeant\nMichael is a Detective\n</code></pre>"},{"location":"archive/advanced/operators/#collecting-into-multiple-files","title":"Collecting into multiple files","text":"<p>Instead of writing all entries to a single file, you can direct entries from the input channel to different files by supplying a closure to the <code>collectFile</code> operator. The closure must return a <code>List</code> of two entries where the two elements in the <code>List</code> are</p> <ol> <li>the name of the file into which the data should be written, and</li> <li>the data to write.</li> </ol> <p>For example:</p> <pre><code>characters\n    .collectFile(newLine: true, storeDir: 'results') { character -&gt;\n      def filename = \"${character.title}s.txt\"\n      def article = character.title.toLowerCase() =~ ~/^[aeiou]/ ? 'an' : 'a'\n      def text = \"${character.name} is ${article} ${character.title}\"\n        [filename, text]\n    }\n    .view()\n</code></pre> <p>The <code>collectFile</code> operator now returns a channel containing four files where we have the outputs grouped by the filename we specified.</p> <pre><code>Launching `./main.nf` [marvelous_legentil] DSL2 - revision: a571cfd449\nresults/Administrators.txt\nresults/Captains.txt\nresults/Detectives.txt\nresults/Sergeants.txt\n</code></pre>"},{"location":"archive/advanced/operators/#dealing-with-headers","title":"Dealing with headers","text":"<p>Let's say we have a process that returns a CSV. In this example, we're going to create very small two-line CVSs, but the same approach is applicable to larger files as well.</p> <pre><code>process WriteBio {\n    input: val(character)\n    output: path('bio.csv')\n    script:\n    \"\"\"\n    echo \"precinct,name,title\" &gt; bio.csv\n    echo 99th,${character.name},${character.title} &gt;&gt; bio.csv\n    \"\"\"\n}\n</code></pre> <p>If we run this with the same workflow as before:</p> <pre><code>WriteBio(characters)\n    .collectFile(name: 'characters.csv', storeDir: 'results')\n    .view()\n</code></pre> <p>... the CSVs are simply concatenated with the header included each time:</p> results/characters.csv<pre><code>precinct,name,title\n99th,Jake,Detective\nprecinct,name,title\n99th,Terry,Sergeant\nprecinct,name,title\n99th,Rosa,Detective\n...\n</code></pre> <p>To keep the header from only the first entry, we can use the <code>keepHeader</code> argument to <code>collectFile</code>:</p> <pre><code>WriteBio(characters)\n    .collectFile(name: 'characters.csv', storeDir: 'results', keepHeader: true)\n    .view()\n</code></pre> <p>Exercise</p> <p>If we modify the <code>WriteBio</code> to also emit the <code>character</code> Map into the output channel:</p> <pre><code>process WriteBio {\n    input: val(character)\n    output: tuple val(character), path('bio.csv')\n    script:\n    \"\"\"\n    echo \"precinct,name,title\" &gt; bio.csv\n    echo 99th,${character.name},${character.title} &gt;&gt; bio.csv\n    \"\"\"\n}\n</code></pre> <p>How might we produce a <code>results</code> directory that has one csv for each character title/rank where the csv includes the appropriate header?</p> <pre><code>results\n\u251c\u2500\u2500 Administrators.csv\n\u251c\u2500\u2500 Captains.csv\n\u251c\u2500\u2500 Detectives.csv\n\u2514\u2500\u2500 Sergeants.csv\n</code></pre> Solution <p>A good solution would be to pass a closure to the <code>collectFile</code> operator. The closure will return the filename and the file in a List:</p> <pre><code>WriteBio(characters)\n    .collectFile(storeDir: 'results', keepHeader: true) { character, file -&gt;\n        [\"${character.title}s.csv\", file]\n    }\n    .view()\n</code></pre> <p>Another viable option would be to <code>map</code> over the channel before <code>collectFile</code>:</p> <pre><code>WriteBio(characters)\n    .map { character, file -&gt; [\"${character.title}s.csv\", file] }\n    .collectFile(storeDir: 'results', keepHeader: true)\n    .view()\n</code></pre>"},{"location":"archive/advanced/orientation/","title":"Orientation","text":"<p>The GitHub Codespaces environment contains some test data that will be used in this workshop.</p> <p>Note</p> <p>Follow this link if you have not yet setup your GitHub Codespaces environment.</p>"},{"location":"archive/advanced/orientation/#getting-started","title":"Getting started","text":"<p>You will complete this module in the <code>nf-training-advanced/</code> folder.</p> <p>In this folder you will find a series of folders that will be used during different sections of this training.</p> <pre><code>nf-training-advanced\n\u251c\u2500\u2500 groovy\n\u2502   \u251c\u2500\u2500 main.nf\n\u2502   \u251c\u2500\u2500 modules\n\u2502   \u2502   \u2514\u2500\u2500 local\n\u2502   \u2502       \u2514\u2500\u2500 fastp\n\u2502   \u2502           \u2514\u2500\u2500 main.nf\n\u2502   \u2514\u2500\u2500 nextflow.config\n\u251c\u2500\u2500 grouping\n\u2502   \u251c\u2500\u2500 data\n\u2502   \u2502   \u251c\u2500\u2500 genome.fasta\n\u2502   \u2502   \u251c\u2500\u2500 genome.fasta.fai\n\u2502   \u2502   \u251c\u2500\u2500 intervals.bed\n\u2502   \u2502   \u251c\u2500\u2500 reads\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 treatmentA\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 &lt;data files&gt;\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 treatmentB\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 &lt;data files&gt;\n\u2502   \u2502   \u251c\u2500\u2500 samplesheet.csv\n\u2502   \u2502   \u2514\u2500\u2500 samplesheet.ugly.csv\n\u2502   \u2514\u2500\u2500 main.nf\n\u251c\u2500\u2500 metadata\n\u2502   \u251c\u2500\u2500 data\n\u2502   \u2502   \u251c\u2500\u2500 reads\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 treatmentA\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 &lt;data files&gt;\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 treatmentB\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 &lt;data files&gt;\n\u2502   \u2502   \u251c\u2500\u2500 samplesheet.csv\n\u2502   \u2502   \u2514\u2500\u2500 samplesheet.ugly.csv\n\u2502   \u2514\u2500\u2500 main.nf\n\u251c\u2500\u2500 operators\n\u2502   \u251c\u2500\u2500 data\n\u2502   \u2502   \u251c\u2500\u2500 reads\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 &lt;data files&gt;\n\u2502   \u2502   \u251c\u2500\u2500 samplesheet.csv\n\u2502   \u2502   \u2514\u2500\u2500 samplesheet.ugly.csv\n\u2502   \u2514\u2500\u2500 main.nf\n\u2514\u2500\u2500 structure\n    \u251c\u2500\u2500 lib\n    \u2502   \u2514\u2500\u2500 Food.groovy\n    \u251c\u2500\u2500 main.nf\n    \u2514\u2500\u2500 templates\n        \u251c\u2500\u2500 adder.py\n        \u2514\u2500\u2500 demo_script.sh\n</code></pre>"},{"location":"archive/advanced/orientation/#selecting-a-nextflow-version","title":"Selecting a Nextflow version","text":"<p>By default, Nextflow will pull the latest stable version into your environment.</p> <p>However, Nextflow is constantly evolving as we make improvements and fix bugs.</p> <p>The latest releases can be viewed on GitHub here.</p> <p>If you want to use a specific version of Nextflow, you can set the <code>NXF_VER</code> variable as shown below:</p> <pre><code>export NXF_VER=25.04.6\n</code></pre> <p>Note</p> <p>This tutorial workshop requires <code>NXF_VER=23.10.0</code>, or later.</p> <p>Run <code>nextflow -version</code> again to confirm that the change has taken effect.</p>"},{"location":"archive/advanced/references/","title":"References","text":""},{"location":"archive/advanced/structure/","title":"Workflow Structure","text":"<p>Nextflow includes a specific directory structure for workflows which can provide some features that can facilitate or enhance your code. In this section we will explore them.</p> <p>First, let's move into the right directory:</p> <pre><code>cd structure\n</code></pre> <p>There are three directories in a Nextflow workflow repository that have a special purpose:</p>"},{"location":"archive/advanced/structure/#bin","title":"<code>./bin</code>","text":"<p>The <code>bin</code> directory (if it exists) is always added to the <code>$PATH</code> for all tasks. If the tasks are performed on a remote machine, the directory is copied across to the new machine before the task begins. This Nextflow feature is designed to make it easy to include accessory scripts directly in the workflow without having to commit those scripts into the container. This feature also ensures that the scripts used inside the workflow move on the same revision schedule as the workflow itself.</p> <p>It is important to know that Nextflow will take care of updating <code>$PATH</code> and ensuring the files are available wherever the task is running, but will not change the permissions of any files in that directory. If a file is called by a task as an executable, the workflow developer must ensure that the file has the correct permissions to be executed.</p> <p>For example, let's say we have a small R script that produces a csv and a tsv:</p> <pre><code>#!/usr/bin/env Rscript\nlibrary(tidyverse)\n\nplot &lt;- ggplot(mpg, aes(displ, hwy, colour = class)) + geom_point()\nmtcars |&gt; write_tsv(\"cars.tsv\")\nggsave(\"cars.png\", plot = plot)\n</code></pre> <p>We'd like to use this script in a simple workflow:</p> <pre><code>process PlotCars {\n    container 'rocker/tidyverse:latest'\n\n    output:\n    path(\"*.png\"), emit: plot\n    path(\"*.tsv\"), emit: table\n\n    script:\n    \"\"\"\n    cars.R\n    \"\"\"\n}\n\nworkflow {\n    PlotCars()\n\n    PlotCars.out.table.view { myfile -&gt; \"Found a tsv: $myfile\" }\n    PlotCars.out.plot.view { myfile -&gt; \"Found a png: $myfile\" }\n}\n</code></pre> <p>To do this, we can create the bin directory, write our R script into the directory. Finally, and crucially, we make the script executable. This is the code we used to create the <code>cars.R</code> script, no need to run it:</p> <pre><code>mkdir -p bin\ncat &lt;&lt; EOF &gt; bin/cars.R\n#!/usr/bin/env Rscript\nlibrary(tidyverse)\n\nplot &lt;- ggplot(mpg, aes(displ, hwy, colour = class)) + geom_point()\nmtcars |&gt; write_tsv(\"cars.tsv\")\nggsave(\"cars.png\", plot = plot)\nEOF\nchmod +x bin/cars.R\n</code></pre> <p>Warning</p> <p>Always ensure that your scripts are executable. The scripts will not be available to your Nextflow processes without this step.</p> <p>Let's run the script and see what Nextflow is doing for us behind the scenes:</p> <pre><code>nextflow run .\n</code></pre> <p>and then inspect the <code>.command.run</code> file that Nextflow has generated</p> <pre><code>code work/*/*/.command.run\n</code></pre> <p>You'll notice a <code>nxf_container_env</code> bash function that appends our bin directory to <code>$PATH</code>:</p> <pre><code>nxf_container_env() {\ncat &lt;&lt; EOF\nexport PATH=\"\\$PATH:/workspaces/training/nf-training-advanced/structure/bin\"\nEOF\n}\n</code></pre> <p>When working on the cloud, Nextflow will also ensure that the bin directory is copied onto the virtual machine running your task in addition to the modification of <code>$PATH</code>.</p> <p>Warning</p> <p>Always use a portable shebang line in your bin directory scripts.</p> <p>In the R script example shown above, I may have the <code>Rscript</code> program installed at (for example) <code>/opt/homebrew/bin/Rscript</code>. If I hard-code this path into my <code>cars.R</code>, everything will work when I'm testing locally outside of the docker container, but will fail when running with docker/singularity or in the cloud as the <code>Rscript</code> program may be installed in a different location in those contexts.</p> <p>It is strongly recommended to use <code>#!/usr/bin/env</code> when setting the shebang for scripts in the <code>bin</code> directory to ensure maximum portability.</p>"},{"location":"archive/advanced/structure/#templates","title":"<code>./templates</code>","text":"<p>If a process script block is becoming too long, it can be moved to a template file. The template file can then be imported into the process script block using the <code>template</code> method. This is useful for keeping the process block tidy and readable. Nextflow's use of <code>$</code> to indicate variables also allows for directly testing the template file by running it as a script.</p> <p>The structure directory already contains an example template - a very simple python script. We can add a new process that uses this template:</p> <pre><code>process SayHiTemplate {\n    debug true\n    input: val(name)\n    script: template 'adder.py'\n}\n</code></pre>"},{"location":"archive/advanced/structure/#lib","title":"<code>./lib</code>","text":"<p>In the previous chapter, we saw the addition of small helper Groovy functions to the <code>main.nf</code> file. It may at times be helpful to bundle functionality into a new Groovy class. Any classes defined in the <code>lib</code> directory are available for use in the workflow - both <code>main.nf</code> and any imported modules.</p>"},{"location":"archive/advanced/structure/#making-a-metadata-class","title":"Making a Metadata Class","text":"<p>Note</p> <p>Using custom Groovy is considered a very advanced use case and you should not need it for the majority of workflows. The language server will complain about this but you can safely ignore it.</p> <p>Exercise</p> <p>Create a new class in <code>./lib/Metadata.groovy</code> that extends the <code>HashMap</code> class and adds a <code>hi</code> method.</p> <p>Let's consider an example where we create our own custom class to handle metadata. We can create a new class in <code>./lib/Metadata.groovy</code>. We'll extend the built-in <code>HashMap</code> class, and add a simple method to return a value:</p> <pre><code>class Metadata extends HashMap {\n    def hi() {\n        return \"Hello, workshop participants!\"\n    }\n}\n</code></pre> <p>We can then use this class in our workflow:</p> <pre><code>workflow {\n    Channel.of(\"Montreal\")\n        .map { new Metadata() }\n        .view()\n}\n</code></pre> <p>We can use the new <code>hi</code> method in the workflow:</p> <pre><code>workflow {\n    Channel.of(\"Montreal\")\n        .map { new Metadata() }\n        .view { metadata -&gt; metadata.hi() }\n}\n</code></pre> <p>At the moment, the <code>Metadata</code> class is not making use of the \"Montreal\" being passed into the closure. Let's change that by adding a constructor to the class:</p> <pre><code>class Metadata extends HashMap {\n    Metadata(String location) {\n        this.location = location\n    }\n\n    def hi() {\n        return this.location ? \"Hello, from ${this.location}!\" : \"Hello, workshop participants!\"\n    }\n}\n</code></pre> <p>Which we can use like so:</p> <pre><code>workflow {\n    Channel.of(\"Montreal\")\n        .map { place -&gt; new Metadata(place) }\n        .view { metadata -&gt; metadata.hi() }\n}\n</code></pre> <p>We can also use this method when passing the object to a process:</p> <pre><code>process UseMeta {\n    input: val(meta)\n    output: path(\"out.txt\")\n    script: \"echo '${meta.hi()}' | tee out.txt\"\n}\n\nworkflow {\n    place_ch = Channel.of(\"Montreal\")\n        .map { place -&gt; new Metadata(place) }\n\n    UseMeta(place_ch)\n        .view()\n}\n</code></pre> <p>Why might this be helpful? You can add extra classes to the metadata which can be computed from the existing metadata. For example, we might want to add a method to get the adapter prefix into our Metadata class:</p> <pre><code>def getAdapterStart() {\n    this.adapter?.substring(0, 3)\n}\n</code></pre> <p>Which we might use like so:</p> <pre><code>process UseMeta {\n    input: val(meta)\n    output: path(\"out.txt\")\n    script: \"echo '${meta.adapter} prefix is ${meta.getAdapterStart()}' | tee out.txt\"\n}\n\nworkflow {\n    place_ch = Channel.of(\"Montreal\")\n        .map { place -&gt; new Metadata(place) }\n        .map { metadata -&gt; metadata + [adapter:\"AACGTAGCTTGAC\"] }\n\n    UseMeta(place_ch)\n        .view()\n}\n</code></pre> <p>You might even want to reach out to external services such as a LIMS or the E-utilities API. Here we add a dummy \"getSampleName()\" method that reaches out to a public API:</p> <pre><code>def getSampleName() {\n    def get = new URL('https://postman-echo.com/get?sampleName=Fido').openConnection()\n    def getRC = get.getResponseCode();\n    if (getRC.equals(200)) {\n        JsonSlurper jsonSlurper = new JsonSlurper()\n        def json = jsonSlurper.parseText(get.getInputStream().getText())\n        return json.args.sampleName\n    }\n}\n</code></pre> <p>This relies on jsonSlurper which isn't included by default. Import this by adding the following to the top of the file:</p> <pre><code>import groovy.json.JsonSlurper\n</code></pre> <p>Which we can use like so:</p> <pre><code>process UseMeta {\n    input: val(meta)\n    output: path(\"out.txt\")\n    script:\n    \"echo '${meta.adapter} prefix is ${meta.getAdapterStart()} with sampleName ${meta.getSampleName()}' | tee out.txt\"\n}\n</code></pre> <p>Nextflow caching</p> <p>When we start passing custom classes through the workflow, it's important to understand a little about the Nextflow caching mechanism. When a task is run, a unique hash is calculated based on the task name, the input files/values, and the input parameters. Our class extends from <code>HashMap</code>, which means that the hash will be calculated based on the contents of the <code>HashMap</code>. If we add a new method to the class, or amend a class method, this does not change the value of the objects in the hash, which means that the hash will not change.</p> <p>Exercise</p> <p>Can you show changing a method in our <code>Metadata</code> class does not change the hash?</p> Solution <p>We might increase the length of the adapter prefix to 5 characters:</p> <pre><code>    def getAdapterStart() {\n        this.adapter?.substring(0, 5)\n    }\n</code></pre> <p>Changing this method and resuming the workflow will not change the hash, and the existing method will be used.</p> <p>We are not limited to using or extending the built-in Groovy classes. Let's start by creating a <code>Dog</code> class in <code>./lib/Dog.groovy</code>:</p> <pre><code>class Dog {\n    String name\n    Boolean isHungry = true\n}\n</code></pre> <p>We can create a new dog at the beginning of the workflow:</p> <pre><code>workflow {\n    def dog = new Dog(name: \"fido\")\n    log.info \"Found a new dog: ${dog.name}\"\n}\n</code></pre> <p>We can pass objects of our class through channels. Here we take a channel of dog names and create a channel of dogs:</p> <pre><code>workflow {\n    Channel.of(\"Argente\", \"Absolon\", \"Chowne\")\n        .map { name -&gt; new Dog(name: name) }\n        .view { dog -&gt; \"Found a new dog: ${dog.name}\" }\n}\n</code></pre> <p>If we try to use this new class in a resumed process, no caches will be used.</p> <p>Exercise</p> <p>Show that the <code>Dog</code> class is not cached when resuming a workflow.</p>"},{"location":"archive/advanced/structure/#making-a-valueobject","title":"Making a ValueObject","text":"<p>Nextflow has provided a decorator to help serialize your custom classes. By adding <code>@ValueObject</code> to the class definition, Nextflow will automatically serialize the class and cache it. This is useful if you want to pass a custom class through a channel, or if you want to use the class in a resumed workflow.</p> <p>Let's add the decorator to our <code>Dog</code> class:</p> <pre><code>import nextflow.io.ValueObject\n\n@ValueObject\nclass Dog {\n    String name\n    Boolean isHungry = true\n}\n</code></pre> <p>Lastly, we will need to register the class with Kryo, the Java serialization framework. Again, Nextflow provides a helper method to do this. We can add the following to the <code>main.nf</code> file:</p> <pre><code>workflow {\n    nextflow.util.KryoHelper.register(Dog)\n</code></pre> <p>Exercise</p> <p>Show that the <code>Dog</code> class can now be used in processes and cached correctly.</p>"},{"location":"archive/advanced/summary/","title":"Summary","text":"<p>Summary available on day #2</p>"},{"location":"archive/advanced/support/","title":"Support","text":"<ul> <li>Nextflow official documentation is available at www.nextflow.io/docs/latest</li> <li>If you have questions, ask on the Seqera Community Forum at http://community.seqera.io/</li> <li>If you want to chat about Nextflow or plugin development, check the Nextflow Slack at https://www.nextflow.io/slack-invite.html</li> </ul>"},{"location":"archive/advanced/testing/","title":"Troubleshooting","text":"<p>To be introduced on day #2</p>"},{"location":"archive/basic_training/","title":"Fundamentals Training","text":"<p>You are now on the path to writing reproducible and scalable scientific workflows using Nextflow. This guide complements the full Nextflow documentation - if you ever have any doubts, please refer to that.</p> <p>Let's get started!</p> <p></p>"},{"location":"archive/basic_training/#learning-objectives","title":"Learning objectives","text":"<p>By the end of this course you should:</p> <ul> <li>Write simple Nextflow workflows</li> <li>Describe the Nextflow concepts of Channels, Processes and Operators</li> <li>Have an understanding of containerized workflows</li> <li>Understand the different execution platforms supported by Nextflow</li> <li>Describe the Nextflow community and ecosystem</li> </ul>"},{"location":"archive/basic_training/#audience-prerequisites","title":"Audience &amp; prerequisites","text":"<p>Please note that this is not a beginner's workshop. Familiarity with Nextflow, the command line, and common file formats is assumed.</p> <p>For a beginner's introduction to Nextflow, please see the Hello Nextflow course.</p> <p>Prerequisites</p> <ul> <li>A GitHub account</li> <li>Experience with command line</li> <li>An understanding of common file formats</li> </ul>"},{"location":"archive/basic_training/#follow-the-training-videos-and-get-help","title":"Follow the training videos and get help","text":"<p>Video recordings are available for this course. You can ask questions in the Seqera community forum.</p> <p>You can watch the recording of the most recent training (March, 2024) in the YouTube playlist below:</p> <p>Warning</p> <p>Please note that the training material is updated regularly and that the videos linked below may be out of date.</p> <p>If English is not your preferred language, you may find it useful to follow the training from the March 2023 event, which is available in multiple languages.</p> <ul> <li> English recording</li> <li> Hindi recording</li> <li> Spanish recording</li> <li> Portuguese recording</li> <li> French recording</li> </ul>"},{"location":"archive/basic_training/cache_and_resume/","title":"Execution cache and resume","text":"<p>The Nextflow caching mechanism works by assigning a unique ID to each task which is used to create a separate execution directory where the tasks are executed and the results stored.</p> <p>The task unique ID is generated as a 128-bit hash value composing the task input values, files and command string.</p> <p>The workflow work directory is organized as shown below:</p> <pre><code>work/\n\u251c\u2500\u2500 12\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 1adacb582d2198cd32db0e6f808bce\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 genome.fa -&gt; /data/../genome.fa\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 index\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 hash.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 header.json\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 indexing.log\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 quasi_index.log\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 refInfo.json\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 rsd.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 sa.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 txpInfo.bin\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 versionInfo.json\n\u251c\u2500\u2500 19\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 663679d1d87bfeafacf30c1deaf81b\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 aux_info\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ambig_info.tsv\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 expected_bias.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fld.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta_info.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 observed_bias.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 observed_bias_3p.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 cmd_info.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 libParams\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 flenDist.txt\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 lib_format_counts.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 salmon_quant.log\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 quant.sf\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut_1.fq -&gt; /data/../ggal_gut_1.fq\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut_2.fq -&gt; /data/../ggal_gut_2.fq\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 index -&gt; /data/../asciidocs/day2/work/12/1adacb582d2198cd32db0e6f808bce/index\n</code></pre> <p>Info</p> <p>You can create these plots using the <code>tree</code> function if you have it installed.</p>"},{"location":"archive/basic_training/cache_and_resume/#how-resume-works","title":"How resume works","text":"<p>The <code>-resume</code> command-line option allows the continuation of a workflow execution from the last step that was completed successfully:</p> <pre><code>nextflow run &lt;script&gt; -resume\n</code></pre> <p>In practical terms, the workflow is executed from the beginning. However, before launching the execution of a process, Nextflow uses the task unique ID to check if the work directory already exists and that it contains a valid command exit state with the expected output files.</p> <p>If this condition is satisfied the task execution is skipped and previously computed results are used as the process results.</p> <p>The first task for which a new output is computed invalidates all downstream executions in the remaining DAG.</p>"},{"location":"archive/basic_training/cache_and_resume/#work-directory","title":"Work directory","text":"<p>The task work directories are created in the folder <code>work</code> in the launching path by default. It is recommended that this is a scratch storage area that can be cleaned up once the computation is completed.</p> <p>Note</p> <p>It is recommended that you store the final output(s) in a different location using one or more publishDir directives.</p> <p>Warning</p> <p>Make sure to delete your work directory occasionally, else your machine/environment may be filled with unused files.</p> <p>A different location for the execution work directory can be specified using the command line option <code>-w</code>:</p> <pre><code>nextflow run &lt;script&gt; -w /some/scratch/dir\n</code></pre> <p>Warning</p> <p>If you delete or move the workflow work directory, it will prevent the use of the resume feature in subsequent runs.</p> <p>The hash code for input files is computed using:</p> <ul> <li>The complete file path</li> <li>The file size</li> <li>The last modified timestamp</li> </ul> <p>Therefore, just touching a file will invalidate the related task execution.</p>"},{"location":"archive/basic_training/cache_and_resume/#how-to-organize-in-silico-experiments","title":"How to organize in-silico experiments","text":"<p>It\u2019s good practice to organize each experiment in its own folder. The main experiment input parameters should be specified using a Nextflow config file. This makes it simple to track and replicate an experiment over time.</p> <p>Note</p> <p>In the same experiment, the same workflow can be executed multiple times, however, launching two (or more) Nextflow instances in the same directory concurrently should be avoided.</p> <p>The <code>nextflow log</code> command lists the executions run in the current folder:</p> <pre><code>nextflow log\n</code></pre> Output<pre><code>TIMESTAMP            DURATION  RUN NAME          STATUS  REVISION ID  SESSION ID                            COMMAND\n2019-05-06 12:07:32  1.2s      focused_carson    ERR     a9012339ce   7363b3f0-09ac-495b-a947-28cf430d0b85  nextflow run hello\n2019-05-06 12:08:33  21.1s     mighty_boyd       OK      a9012339ce   7363b3f0-09ac-495b-a947-28cf430d0b85  nextflow run rnaseq-nf -with-docker\n2019-05-06 12:31:15  1.2s      insane_celsius    ERR     b9aefc67b4   4dc656d2-c410-44c8-bc32-7dd0ea87bebf  nextflow run rnaseq-nf\n2019-05-06 12:31:24  17s       stupefied_euclid  OK      b9aefc67b4   4dc656d2-c410-44c8-bc32-7dd0ea87bebf  nextflow run rnaseq-nf -resume -with-docker\n</code></pre> <p>You can use either the session ID or the run name to recover a specific execution:</p> <pre><code>nextflow run rnaseq-nf -resume mighty_boyd\n</code></pre>"},{"location":"archive/basic_training/cache_and_resume/#execution-provenance","title":"Execution provenance","text":"<p>The <code>log</code> command, when provided with a run name or session ID, can return many useful bits of information about a workflow execution that can be used to create a provenance report.</p> <p>By default, it will list the work directories used to compute each task:</p> <pre><code>nextflow log tiny_fermat\n</code></pre> Output<pre><code>/data/.../work/7b/3753ff13b1fa5348d2d9b6f512153a\n/data/.../work/c1/56a36d8f498c99ac6cba31e85b3e0c\n/data/.../work/f7/659c65ef60582d9713252bcfbcc310\n/data/.../work/82/ba67e3175bd9e6479d4310e5a92f99\n/data/.../work/e5/2816b9d4e7b402bfdd6597c2c2403d\n/data/.../work/3b/3485d00b0115f89e4c202eacf82eba\n</code></pre> <p>The <code>-f</code> (fields) option can be used to specify which metadata should be printed by the <code>log</code> command:</p> <pre><code>nextflow log tiny_fermat -f 'process,exit,hash,duration'\n</code></pre> Output<pre><code>index    0   7b/3753ff  2.0s\nfastqc   0   c1/56a36d  9.3s\nfastqc   0   f7/659c65  9.1s\nquant    0   82/ba67e3  2.7s\nquant    0   e5/2816b9  3.2s\nmultiqc  0   3b/3485d0  6.3s\n</code></pre> <p>The complete list of available fields can be retrieved with the command:</p> <pre><code>nextflow log -l\n</code></pre> <p>The <code>-F</code> option allows the specification of filtering criteria to print only a subset of tasks:</p> <pre><code>nextflow log tiny_fermat -F 'process =~ /fastqc/'\n</code></pre> Output<pre><code>/data/.../work/c1/56a36d8f498c99ac6cba31e85b3e0c\n/data/.../work/f7/659c65ef60582d9713252bcfbcc310\n</code></pre> <p>This can be useful to locate specific task work directories.</p> <p>Finally, the <code>-t</code> option enables the creation of a basic custom provenance report, showing a template file in any format of your choice:</p> <pre><code>&lt;div&gt;\n  &lt;h2&gt;${name}&lt;/h2&gt;\n  &lt;div&gt;\n    Script:\n    &lt;pre&gt;${script}&lt;/pre&gt;\n  &lt;/div&gt;\n\n  &lt;ul&gt;\n    &lt;li&gt;Exit: ${exit}&lt;/li&gt;\n    &lt;li&gt;Status: ${status}&lt;/li&gt;\n    &lt;li&gt;Work dir: ${workdir}&lt;/li&gt;\n    &lt;li&gt;Container: ${container}&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n</code></pre> <p>Exercise</p> <p>Save the above snippet in a file named <code>template.html</code>. Then run this command (using the correct id for your run, e.g., <code>tiny_fermat</code>):</p> <pre><code>nextflow log tiny_fermat -t template.html &gt; prov.html\n</code></pre> <p>Finally, open the <code>prov.html</code> file with a browser.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How the workflow execution cache works</li> <li>How to use the <code>-resume</code> command line option</li> <li>How to organize in-silico experiments in different folders</li> <li>How to create and customize a basic provenance report</li> </ol>"},{"location":"archive/basic_training/cache_and_resume/#troubleshooting-resume","title":"Troubleshooting resume","text":"<p>Being able to resume workflows is a key feature of Nextflow, but it doesn't always work as you expect. In this section you will learn common reasons why Nextflow may be ignoring your cached results.</p> <p>Tip</p> <p>To learn more details about the resume mechanism and how to troubleshoot please refer to the following three blog posts:</p> <ol> <li>Demystifying Nextflow resume</li> <li>Troubleshooting Nextflow resume</li> <li>Analyzing caching behavior of pipelines</li> </ol>"},{"location":"archive/basic_training/cache_and_resume/#input-file-changed","title":"Input file changed","text":"<p>Make sure that there\u2019s no change in your input file(s). Don\u2019t forget the task unique hash is computed by taking into account the complete file path, the last modified timestamp and the file size. If any of this information has changed, the workflow will be re-executed even if the input content is the same.</p>"},{"location":"archive/basic_training/cache_and_resume/#a-process-modifies-an-input","title":"A process modifies an input","text":"<p>A process should never alter input files, otherwise the <code>resume</code> for future executions will be invalidated for the same reason explained in the previous point.</p>"},{"location":"archive/basic_training/cache_and_resume/#inconsistent-file-attributes","title":"Inconsistent file attributes","text":"<p>Some shared file systems, such as NFS, may report an inconsistent file timestamp (i.e. a different timestamp for the same file) even if it has not been modified. To prevent this problem use the lenient cache strategy.</p>"},{"location":"archive/basic_training/cache_and_resume/#race-condition-in-global-variable","title":"Race condition in global variable","text":"<p>Nextflow is designed to simplify parallel programming without taking care about race conditions and the access to shared resources. One of the few cases in which a race condition can arise is when using a global variable with two (or more) operators</p> snippet.nf<pre><code>Channel\n    .of(1, 2, 3)\n    .map { it -&gt; X = it; X += 2 }\n    .view { \"ch1 = $it\" }\n\nChannel\n    .of(1, 2, 3)\n    .map { it -&gt; X = it; X *= 2 }\n    .view { \"ch2 = $it\" }\n</code></pre> <p>The problem in this snippet is that the <code>X</code> variable in the closure definition is defined in the global scope. Therefore, since operators are executed in parallel, the <code>X</code> value can be overwritten by the other <code>map</code> invocation.</p> <p>The correct implementation requires the use of the <code>def</code> keyword to declare the variable local.</p> snippet.nf<pre><code>Channel\n    .of(1, 2, 3)\n    .map { it -&gt; def X = it; X += 2 }\n    .println { \"ch1 = $it\" }\n\nChannel\n    .of(1, 2, 3)\n    .map { it -&gt; def X = it; X *= 2 }\n    .println { \"ch2 = $it\" }\n</code></pre>"},{"location":"archive/basic_training/cache_and_resume/#non-deterministic-input-channels","title":"Non-deterministic input channels","text":"<p>While dataflow channel ordering is guaranteed \u2013 data is read in the same order in which it\u2019s written in the channel \u2013 be aware that there is no guarantee that the elements will maintain their order in the process output channel. This is because a process may spawn multiple tasks, which can run in parallel. For example, the operation on the second element may end sooner than the operation on the first element, changing the output channel order.</p> <p>In practical terms, consider the following snippet:</p> snippet.nf<pre><code>process FOO {\n    input:\n    val x\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    sleep \\$((RANDOM % 3))\n    echo -n \"$x\"\n    \"\"\"\n}\n\nprocess BAR {\n    input:\n    val x\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo -n \"$x\" | tr '[:upper:]' '[:lower:]'\n    \"\"\"\n}\n\nprocess FOOBAR {\n    input:\n    val foo\n    val bar\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo $foo - $bar\n    \"\"\"\n}\n\nworkflow {\n    ch_letters = channel.of('A', 'B', 'C', 'D')\n    FOO(ch_letters)\n    BAR(ch_letters)\n    FOOBAR(FOO.out, BAR.out).view()\n\n}\n</code></pre> <p>Processes FOO and BAR receive the same inputs, and return something on standard output. Somebody wrote process FOOBAR to receive those processed outputs and generate combined output, from matched inputs. But even though we set the order of the input values, the FOO and BAR processes output whenever their tasks are complete, not respecting input order. So FOOBAR receiving the outputs of those channels, will not receive them in the same order, as you can see from the output:</p> Output<pre><code>...\nB - c\n\nD - a\n\nC - d\n\nA - b\n</code></pre> <p>So D is matched with 'a' here, which was not the intention. That order will likely be different every time the workflow is run, meaning that the processing will not be deterministic, and caching will also not work, since the inputs to FOOBAR will vary constantly.</p> <p>Exercise</p> <p>Re-run the above code a couple of times using <code>-resume</code>, and determine if the FOOBAR process reruns, or uses cached results.</p> Solution <p>You should see that while FOO and BAR reliably re-use their cache, FOOBAR will re-run at least a subset of its tasks due to differences in the combinations of inputs it receives.</p> <p>The output will look like this:</p> Output<pre><code> [58/f117ed] FOO (4)    [100%] 4 of 4, cached: 4 \u2714\n [84/e88fd9] BAR (4)    [100%] 4 of 4, cached: 4 \u2714\n [6f/d3f672] FOOBAR (1) [100%] 4 of 4, cached: 2 \u2714\n D - c\n\n A - d\n\n C - a\n\n B - b\n</code></pre> <p>A common solution for this is to use what is commonly referred to as a meta map. A groovy object with sample information is passed out together with the file results within an output channel as a tuple. This can then be used to pair samples from separate channels together for downstream use.</p> <p>To illustrate, here is a change to the above workflow, with meta maps added:</p> snippet.nf<pre><code>process FOO {\n    input:\n    tuple val(meta), val(x)\n\n    output:\n    tuple val(meta), stdout\n\n    script:\n    \"\"\"\n    sleep \\$((RANDOM % 3))\n    echo -n \"$x\"\n    \"\"\"\n}\n\nprocess BAR {\n    input:\n    tuple val(meta), val(x)\n\n    output:\n    tuple val(meta), stdout\n\n    script:\n    \"\"\"\n    echo -n \"$x\" | tr '[:upper:]' '[:lower:]'\n    \"\"\"\n}\n\nprocess FOOBAR {\n    input:\n    tuple val(meta), val(foo), val(bar)\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo $foo - $bar\n    \"\"\"\n}\n\nworkflow {\n    ch_letters = channel.of(\n        [[id: 'A'], 'A'],\n        [[id: 'B'], 'B'],\n        [[id: 'C'], 'C'],\n        [[id: 'D'], 'D']\n    )\n    FOO(ch_letters)\n    BAR(ch_letters)\n    FOOBAR(FOO.out.join(BAR.out)).view()\n\n}\n</code></pre> <p>Now, we define <code>ch_letters</code> with a meta map (e.g. <code>[id: 'A']</code>). Both FOO and BAR pass the <code>meta</code> through and attach it to their outputs. Then, in our call to FOOBAR we can use a <code>join</code> operation to ensure that only matched values are passed. Running this code provides us with matched processes, as we'd expect:</p> Output<pre><code>...\nD - d\n\nB - b\n\nA - a\n\nC - c\n</code></pre> <p>If meta maps are not possible, an alternative is to use the <code>fair</code> process directive. When this directive is specified, Nextflow will guarantee that the order of outputs will match the order of inputs (not the order in which the tasks run, only the order of the output channel).</p> <p>Warning</p> <p>Depending on your situation, using the <code>fair</code> directive will lead to a decrease in performance.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>Common reasons why Nextflow may be ignoring your cached results</li> </ol>"},{"location":"archive/basic_training/channels/","title":"Channels","text":"<p>Channels are a key data structure of Nextflow that allows the implementation of reactive-functional oriented computational workflows based on the Dataflow programming paradigm.</p> <p>They are used to logically connect tasks to each other or to implement functional style data transformations.</p>"},{"location":"archive/basic_training/channels/#channel-types","title":"Channel types","text":"<p>Nextflow distinguishes two different kinds of channels: queue channels and value channels.</p>"},{"location":"archive/basic_training/channels/#queue-channel","title":"Queue channel","text":"<p>A queue channel is an asynchronous unidirectional FIFO queue that connects two processes or operators.</p> <ul> <li>asynchronous means that operations are non-blocking.</li> <li>unidirectional means that data flows from a producer to a consumer.</li> <li>FIFO means that the data is guaranteed to be delivered in the same order as it is produced. First In, First Out.</li> </ul> <p>A queue channel is implicitly created by process output definitions or using channel factories such as Channel.of or Channel.fromPath.</p> <p>Try the following snippets:</p> <p>Click the  icons in the code for explanations.</p> snippet.nf<pre><code>ch = Channel.of(1, 2, 3)\nch.view() // (1)!\n</code></pre> <ol> <li>Applying the <code>view</code> channel operator to the <code>ch</code> channel prints each item emitted by the channels</li> </ol> <p>Exercise</p> <p>The script <code>snippet.nf</code> contains the code from above. Execute it with Nextflow and view the output.</p> Solution <p>Run the following command:</p> <pre><code>nextflow run snippet.nf\n</code></pre> <p>The output should be:</p> Output<pre><code>1\n2\n3\n</code></pre>"},{"location":"archive/basic_training/channels/#value-channels","title":"Value channels","text":"<p>A value channel (a.k.a. a singleton channel) is bound to a single value and it can be read unlimited times without consuming its contents. A <code>value</code> channel is created using the value channel factory or by operators returning a single value, such as first, last, collect, count, min, max, reduce, and sum.</p> <p>To see the difference between value and queue channels, you can modify <code>snippet.nf</code> to the following:</p> snippet.nf<pre><code>ch1 = Channel.of(1, 2, 3)\nch2 = Channel.of(1)\n\nprocess SUM {\n    input:\n    val x\n    val y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo \\$(($x+$y))\n    \"\"\"\n}\n\nworkflow {\n    SUM(ch1, ch2).view()\n}\n</code></pre> <p>This workflow creates two channels, <code>ch1</code> and <code>ch2</code>, and then uses them as inputs to the <code>SUM</code> process. The <code>SUM</code> process sums the two inputs and prints the result to the standard output.</p> <p>When you run this script, it only prints <code>2</code>, as you can see below:</p> Output<pre><code>2\n</code></pre> <p>A process will only instantiate a task when there are elements to be consumed from all the channels provided as input to it. Because <code>ch1</code> and <code>ch2</code> are queue channels, and the single element of <code>ch2</code> has been consumed, no new process instances will be launched, even if there are other elements to be consumed in <code>ch1</code>.</p> <p>To use the single element in <code>ch2</code> multiple times, you can either use the <code>Channel.value</code> channel factory, or use a channel operator that returns a single element, such as <code>first()</code>:</p> snippet.nf<pre><code>ch1 = Channel.of(1, 2, 3)\nch2 = Channel.value(1)\n\nprocess SUM {\n    input:\n    val x\n    val y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo \\$(($x+$y))\n    \"\"\"\n}\n\nworkflow {\n    SUM(ch1, ch2).view()\n}\n</code></pre> Output<pre><code>2\n3\n4\n</code></pre> <p>In many situations, Nextflow will implicitly convert variables to value channels when they are used in a process invocation.</p> <p>For example, when you invoke a process with a workflow parameter (<code>params.ch2</code>) which has a string value, it is automatically cast into a value channel:</p> snippet.nf<pre><code>ch1 = Channel.of(1, 2, 3)\nparams.ch2 = \"1\"\n\nprocess SUM {\n    input:\n    val x\n    val y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo \\$(($x+$y))\n    \"\"\"\n}\n\nworkflow {\n    SUM(ch1, params.ch2).view()\n}\n</code></pre> <p>As you can see, the output is the same as the previous example when the <code>first()</code> operator was used:</p> Output<pre><code>2\n3\n4\n</code></pre> <p>Exercise</p> <p>Use the <code>.first()</code> operator to create a value channel from <code>ch2</code> so that all 3 elements of <code>ch1</code> are consumed.</p> snippet.nf<pre><code>ch1 = Channel.of(1, 2, 3)\nch2 = Channel.of(1)\n\nprocess SUM {\n    input:\n    val x\n    val y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo \\$(($x+$y))\n    \"\"\"\n}\n\nworkflow {\n    SUM(ch1, ch2).view()\n}\n</code></pre> Solution <p>Modify the <code>workflow</code> section to the following:</p> snippet.nf<pre><code>workflow {\n    SUM(ch1, ch2.first()).view()\n}\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>The features of a value and queue channels</li> <li>Strategies to change channel types</li> </ol>"},{"location":"archive/basic_training/channels/#channel-factories","title":"Channel factories","text":"<p>Channel factories are Nextflow commands for creating channels that have implicit expected inputs and functions. There are several different Channel factories which are useful for different situations. The following sections will cover the most common channel factories.</p> <p>Tip</p> <p>New in version 20.07.0: channel was introduced as an alias of Channel, allowing factory methods to be specified as <code>channel.of()</code> or <code>Channel.of()</code>, and so on.</p>"},{"location":"archive/basic_training/channels/#value","title":"<code>value()</code>","text":"<p>The <code>value</code> channel factory is used to create a value channel. An optional not <code>null</code> argument can be specified to bind the channel to a specific value. For example:</p> snippet.nf<pre><code>ch1 = Channel.value() // (1)!\nch2 = Channel.value('Hello there') // (2)!\nch3 = Channel.value([1, 2, 3, 4, 5]) // (3)!\n</code></pre> <ol> <li>Creates an empty value channel</li> <li>Creates a value channel and binds a string to it</li> <li>Creates a value channel and binds a list object to it that will be emitted as a sole emission</li> </ol>"},{"location":"archive/basic_training/channels/#of","title":"<code>of()</code>","text":"<p>The factory <code>Channel.of</code> allows the creation of a queue channel with the values specified as arguments.</p> snippet.nf<pre><code>Channel\n    .of(1, 3, 5, 7)\n    .view()\n</code></pre> <p>This example creates a channel that emits the values specified as a parameter in the <code>of</code> channel factory. It will print the following:</p> Output<pre><code>1\n3\n5\n7\n</code></pre> <p>The <code>Channel.of</code> channel factory works in a similar manner to <code>Channel.from</code> (which is now deprecated), fixing some inconsistent behaviors of the latter and providing better handling when specifying a range of values. For example, the following works with a range from 1 to 23:</p> snippet.nf<pre><code>Channel\n    .of(1..23, 'X', 'Y')\n    .view()\n</code></pre>"},{"location":"archive/basic_training/channels/#fromlist","title":"<code>fromList()</code>","text":"<p>The <code>Channel.fromList</code> channel factory creates a channel emitting the elements provided by a list object specified as an argument:</p> snippet.nf<pre><code>list = ['hello', 'world']\n\nChannel\n    .fromList(list)\n    .view()\n</code></pre>"},{"location":"archive/basic_training/channels/#frompath","title":"<code>fromPath()</code>","text":"<p>The <code>fromPath</code> channel factory creates a queue channel emitting one or more files matching the specified glob pattern.</p> snippet.nf<pre><code>Channel\n    .fromPath('./data/meta/*.csv')\n</code></pre> <p>This example creates a channel and emits as many items as there are files with a <code>csv</code> extension in the <code>./data/meta</code> folder. Each element is a file object implementing the Path interface.</p> <p>Tip</p> <p>Two asterisks, i.e. <code>**</code>, works like <code>*</code> but cross directory boundaries. This syntax is generally used for matching complete paths. Curly brackets specify a collection of sub-patterns.</p> <p>Some channel factories also have options to help you control their behaviour. For example, the <code>fromPath</code> channel factory has the following options:</p> Name Description glob When <code>true</code> interprets characters <code>*</code>, <code>?</code>, <code>[]</code> and <code>{}</code> as glob wildcards, otherwise handles them as normal characters (default: <code>true</code>) type Type of path returned, either <code>file</code>, <code>dir</code> or <code>any</code> (default: <code>file</code>) hidden When <code>true</code> includes hidden files in the resulting paths (default: <code>false</code>) maxDepth Maximum number of directory levels to visit (default: <code>no limit</code>) followLinks When <code>true</code> symbolic links are followed during directory tree traversal, otherwise they are managed as files (default: <code>true</code>) relative When <code>true</code> return paths are relative to the top-most common directory (default: <code>false</code>) checkIfExists When <code>true</code> throws an exception when the specified path does not exist in the file system (default: <code>false</code>) <p>Learn more about the glob patterns syntax at this link.</p> <p>Exercise</p> <p>Use the <code>Channel.fromPath</code> channel factory to create a channel emitting all files with the suffix <code>.fq</code> in the <code>data/ggal/</code> directory and any subdirectory. Include any hidden files and print the file names with the <code>view</code> operator.</p> Solution snippet.nf<pre><code>Channel\n    .fromPath('./data/ggal/**.fq', hidden: true)\n    .view()\n</code></pre>"},{"location":"archive/basic_training/channels/#fromfilepairs","title":"<code>fromFilePairs()</code>","text":"<p>The <code>fromFilePairs</code> channel factory creates a channel emitting the file pairs matching a glob pattern provided by the user. The matching files are emitted as tuples, in which the first element is the grouping key of the matching pair and the second element is the list of files (sorted in lexicographical order).</p> snippet.nf<pre><code>Channel\n    .fromFilePairs('./data/ggal/*_{1,2}.fq')\n    .view()\n</code></pre> <p>It will produce an output similar to the following:</p> Output<pre><code>[liver, [/workspaces/training/nf-training/data/ggal/liver_1.fq, /workspaces/training/nf-training/data/ggal/liver_2.fq]]\n[gut, [/workspaces/training/nf-training/data/ggal/gut_1.fq, /workspaces/training/nf-training/data/ggal/gut_2.fq]]\n[lung, [/workspaces/training/nf-training/data/ggal/lung_1.fq, /workspaces/training/nf-training/data/ggal/lung_2.fq]]\n</code></pre> <p>Warning</p> <p>The glob pattern must contain at least an asterisk wildcard character (<code>*</code>).</p> <p>The <code>fromFilePairs</code> channel factory also has options to help you control its behaviour:</p> Name Description type Type of paths returned, either <code>file</code>, <code>dir</code> or <code>any</code> (default: <code>file</code>) hidden When <code>true</code> includes hidden files in the resulting paths (default: <code>false</code>) maxDepth Maximum number of directory levels to visit (default: <code>no limit</code>) followLinks When <code>true</code> symbolic links are followed during directory tree traversal, otherwise they are managed as files (default: <code>true</code>) size Defines the number of files each emitted item is expected to hold (default: <code>2</code>). Set to <code>-1</code> for any flat When <code>true</code> the matching files are produced as sole elements in the emitted tuples (default: <code>false</code>) checkIfExists When <code>true</code>, it throws an exception of the specified path that does not exist in the file system (default: <code>false</code>) <p>Exercise</p> <p>Use the <code>fromFilePairs</code> channel factory to create a channel emitting all pairs of fastq reads in the <code>data/ggal/</code> directory. Execute this script twice, once with the option <code>flat: true</code> and once with <code>flat: false</code>. What is the difference?</p> Solution <p>Use the following with the <code>flat</code> option equaling true:</p> snippet.nf<pre><code>Channel\n    .fromFilePairs('./data/ggal/*_{1,2}.fq', flat: true)\n    .view()\n</code></pre> <p>And false:</p> <p>snippet.nf<pre><code>Channel\n    .fromFilePairs('./data/ggal/*_{1,2}.fq', flat: false)\n    .view()\n</code></pre> Check the square brackets around the file names, to see the difference with <code>flat</code>.</p>"},{"location":"archive/basic_training/channels/#fromsra","title":"<code>fromSRA()</code>","text":"<p>The <code>Channel.fromSRA</code> channel factory makes it possible to query the NCBI SRA archive and returns a channel emitting the FASTQ files matching the specified selection criteria.</p> <p>The query can be project ID(s) or accession number(s) supported by the NCBI ESearch API.</p> <p>Info</p> <p>This function now requires an API key you can only get by logging into your NCBI account.</p> Instructions for NCBI login and key acquisition <ol> <li>Go to: https://www.ncbi.nlm.nih.gov/</li> <li>Click the top right \"Log in\" button to sign into NCBI. Follow their instructions.</li> <li>Once into your account, click the button at the top right, usually your ID.</li> <li>Go to Account settings</li> <li>Scroll down to the API Key Management section.</li> <li>Click on \"Create an API Key\".</li> <li>The page will refresh and the key will be displayed where the button was. Copy your key.</li> </ol> <p>The following snippet will print the contents of an NCBI project ID:</p> snippet.nf<pre><code>params.ncbi_api_key = '&lt;Your API key here&gt;'\n\nChannel\n    .fromSRA(['SRP073307'], apiKey: params.ncbi_api_key)\n    .view()\n</code></pre> <p> Replace <code>&lt;Your API key here&gt;</code> with your API key.</p> <p>This should print:</p> Output<pre><code>[SRR3383346, [/vol1/fastq/SRR338/006/SRR3383346/SRR3383346_1.fastq.gz, /vol1/fastq/SRR338/006/SRR3383346/SRR3383346_2.fastq.gz]]\n[SRR3383347, [/vol1/fastq/SRR338/007/SRR3383347/SRR3383347_1.fastq.gz, /vol1/fastq/SRR338/007/SRR3383347/SRR3383347_2.fastq.gz]]\n[SRR3383344, [/vol1/fastq/SRR338/004/SRR3383344/SRR3383344_1.fastq.gz, /vol1/fastq/SRR338/004/SRR3383344/SRR3383344_2.fastq.gz]]\n[SRR3383345, [/vol1/fastq/SRR338/005/SRR3383345/SRR3383345_1.fastq.gz, /vol1/fastq/SRR338/005/SRR3383345/SRR3383345_2.fastq.gz]]\n// (remaining omitted)\n</code></pre> <p>Multiple accession IDs can be specified using a list object:</p> snippet.nf<pre><code>ids = ['ERR908507', 'ERR908506', 'ERR908505']\nChannel\n    .fromSRA(ids, apiKey: params.ncbi_api_key)\n    .view()\n</code></pre> Output<pre><code>[ERR908507, [/vol1/fastq/ERR908/ERR908507/ERR908507_1.fastq.gz, /vol1/fastq/ERR908/ERR908507/ERR908507_2.fastq.gz]]\n[ERR908506, [/vol1/fastq/ERR908/ERR908506/ERR908506_1.fastq.gz, /vol1/fastq/ERR908/ERR908506/ERR908506_2.fastq.gz]]\n[ERR908505, [/vol1/fastq/ERR908/ERR908505/ERR908505_1.fastq.gz, /vol1/fastq/ERR908/ERR908505/ERR908505_2.fastq.gz]]\n</code></pre> <p>Info</p> <p>Read pairs are implicitly managed and are returned as a list of files.</p> <p>It\u2019s straightforward to use this channel as an input using the usual Nextflow syntax.</p> <p>The code below creates a channel containing two samples from a public SRA study and runs <code>FASTQC</code> on the resulting files. See:</p> snippet.nf<pre><code>params.ncbi_api_key = '&lt;Your API key here&gt;'\n\nparams.accession = ['ERR908507', 'ERR908506']\n\nprocess FASTQC {\n    input:\n    tuple val(sample_id), path(reads_file)\n\n    output:\n    path(\"fastqc_${sample_id}_logs\")\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads_file}\n    \"\"\"\n}\n\nworkflow {\n    reads = Channel.fromSRA(params.accession, apiKey: params.ncbi_api_key)\n    FASTQC(reads)\n}\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to use common channel factories</li> <li>How to use the <code>fromSRA</code> channel factory to query the NCBI SRA archive</li> </ol>"},{"location":"archive/basic_training/config/","title":"Nextflow configuration","text":"<p>A key Nextflow feature is the ability to decouple the workflow implementation by the configuration setting required by the underlying execution platform. This enables portable deployment without the need to modify the application code.</p> <p>When you launch Nextflow, it will look for configuration files in several locations. As each source can contain conflicting settings, the sources are ranked to decide which settings to apply. Configuration sources are reported below and listed in order of priority:</p> <ol> <li>Parameters specified on the command line (<code>--parameter</code>)</li> <li>Parameters that are provided using the <code>-params-file</code> option</li> <li>Config file that are provided using the <code>-c</code> option</li> <li>The config file named <code>nextflow.config</code> in the current directory</li> <li>The config file named <code>nextflow.config</code> in the pipeline project directory</li> <li>The config file <code>$HOME/.nextflow/config</code></li> <li>Values defined within the pipeline script itself (e.g., <code>main.nf</code>)</li> </ol>"},{"location":"archive/basic_training/config/#parameters","title":"Parameters","text":"<p>Parameters are pipeline specific settings. Parameters can be defined in the workflow script using the <code>params</code> keyword followed by the parameter name. For example:</p> hello.nf<pre><code>#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\n...\n</code></pre> <p>At the highest level, parameters can be customised using the command line. Any parameter can be configured on the command line by prefixing the parameter name with a double dash (--). For example, the <code>greeting</code> parameter in the <code>hello.nf</code> script can be configured using the command line as follows:</p> <pre><code>nextflow run hello.nf --greeting 'Bonjour le monde!'\n</code></pre> <p>Instead of including each parameter on the command line, parameters can also be configured using the <code>-params-file</code> and a JSON or YML file:</p> params.json<pre><code>{\n  \"greeting\": \"Bonjour le monde!\"\n}\n</code></pre> <p>Multiple parameters can be included in one params file and added to the execution command using the <code>-params-file</code> option:</p> <pre><code>nextflow run hello.nf -params-file params.json\n</code></pre> <p>Exercise</p> <p>Run the <code>hello.nf</code> script with the <code>greeting</code> parameter set to <code>Hallo Welt!</code> using a params file.</p> Solution params.json<pre><code>{\n    \"greeting\": \"Hallo Welt!\"\n}\n</code></pre> <pre><code>nextflow run hello.nf -params-file params.json\n</code></pre> <p>Tip</p> <p>Parameters files are useful to consolidate large number of parameters in a single file.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to define parameters in a workflow script</li> <li>How to configure parameters on the command line</li> <li>How to configure parameters using <code>-params-file</code></li> </ol>"},{"location":"archive/basic_training/config/#configuration-files","title":"Configuration files","text":"<p>When a workflow script is launched, Nextflow looks for a file named <code>nextflow.config</code> in the current directory and in the script base directory (if it is not the same as the current directory). Finally, it checks for the file: <code>$HOME/.nextflow/config</code>.</p> <p>When more than one of the above files exists, they are merged, so that the settings in the first override the same settings that may appear in the second, and so on.</p> <p>The default config file search mechanism can be extended by providing an extra configuration file by using the command line option: <code>-c &lt;config file&gt;</code>. For example:</p> <pre><code>nextflow run hello.nf -c custom.config\n</code></pre>"},{"location":"archive/basic_training/config/#config-syntax","title":"Config syntax","text":"<p>A Nextflow configuration file is a simple text file containing a set of properties defined using the syntax:</p> nextflow.config<pre><code>name = value\n</code></pre> <p>Info</p> <p>String values need to be wrapped in quotation characters while numbers and boolean values (<code>true</code>, <code>false</code>) do not. Also, note that values are typed, meaning for example that, <code>1</code> is different from <code>'1'</code>, since the first is interpreted as the number one, while the latter is interpreted as a string value.</p>"},{"location":"archive/basic_training/config/#config-variables","title":"Config variables","text":"<p>Configuration properties can be used as variables in the configuration file itself, by using the usual <code>$propertyName</code> or <code>${expression}</code> syntax.</p> nextflow.config<pre><code>propertyOne = 'world'\nanotherProp = \"Hello $propertyOne\"\ncustomPath = \"$PATH:/my/app/folder\"\n</code></pre> <p>Tip</p> <p>In the configuration file it\u2019s possible to access any variable defined in the host environment such as <code>$PATH</code>, <code>$HOME</code>, <code>$PWD</code>, etc.</p>"},{"location":"archive/basic_training/config/#config-comments","title":"Config comments","text":"<p>Configuration files use the same conventions for comments used in the Nextflow script:</p> <pre><code>// comment a single line\n\n/*\n    a comment spanning\n    multiple lines\n*/\n</code></pre>"},{"location":"archive/basic_training/config/#config-scopes","title":"Config scopes","text":"<p>Configuration settings can be organized in different scopes by dot prefixing the property names with a scope identifier or grouping the properties in the same scope using the curly brackets notation:</p> nextflow.config<pre><code>alpha.x  = 1\nalpha.y  = 'string value..'\n\nbeta {\n    p = 2\n    q = 'another string ..'\n}\n</code></pre>"},{"location":"archive/basic_training/config/#config-params","title":"Config params","text":"<p>The scope <code>params</code> allows the definition of workflow parameters that override the values defined in the main workflow script.</p> <p>This is useful to consolidate one or more execution parameters in a separate file.</p> nextflow.config<pre><code>params.foo = 'Bonjour'\nparams.bar = 'le monde!'\n</code></pre> snippet.nf<pre><code>params.foo = 'Hello'\nparams.bar = 'world!'\n\n// print both params\nprintln \"$params.foo $params.bar\"\n</code></pre> <p>Exercise</p> <p>Using the code blocks above, run <code>snippet.nf</code> without specifying any parameters. Then, run it again specifying the <code>foo</code> parameter on the command line.</p> <pre><code>nextflow run snippet.nf\n</code></pre> Solution <p>Run the script without any modification:</p> <pre><code>nextflow run snippet.nf\n</code></pre> Output<pre><code>Bonjour le monde!\n</code></pre> <p>Execute the snippit again specifying the <code>foo</code> parameter on the command line:</p> <pre><code>nextflow run snippet.nf --foo Hola\n</code></pre> Output<pre><code>Hola le monde!\n</code></pre> <p>Note how the <code>foo</code> parameter is overridden by the value specified on the command line and the <code>bar</code> parameter is taken from the configuration file.</p>"},{"location":"archive/basic_training/config/#config-env","title":"Config env","text":"<p>The <code>env</code> scope allows the definition of one or more variables that will be exported into the environment where the workflow tasks will be executed.</p> my-env.config<pre><code>env.ALPHA = 'some value'\nenv.BETA = \"$HOME/some/path\"\n</code></pre> snippet.nf<pre><code>process FOO {\n    debug true\n\n    script:\n    '''\n    env | egrep 'ALPHA|BETA'\n    '''\n}\n\nworkflow {\n    FOO()\n}\n</code></pre> <p>Executing the snippets above will produce the following output:</p> <pre><code>nextflow run snippet.nf -c my-env.config\n</code></pre> Output<pre><code>BETA=/home/user/some/path\nALPHA=some value\n</code></pre>"},{"location":"archive/basic_training/config/#config-process","title":"Config process","text":"<p>Process directives allow the specification of settings for the task execution such as <code>cpus</code>, <code>memory</code>, <code>container</code>, and other resources in the workflow script.</p> <p>This is useful when prototyping a small workflow script.</p> <p>However, it\u2019s always a good practice to decouple the workflow execution logic from the process configuration settings, i.e. it\u2019s strongly suggested to define the process settings in the workflow configuration file instead of the workflow script.</p> <p>The <code>process</code> configuration scope allows the setting of any <code>process</code> directives in the Nextflow configuration file:</p> nextflow.config<pre><code>process {\n    cpus = 10\n    memory = 8.GB\n    container = 'biocontainers/bamtools:v2.4.0_cv3'\n}\n</code></pre> <p>The above config snippet defines the <code>cpus</code>, <code>memory</code> and <code>container</code> directives for all processes in your workflow script.</p> <p>The process selector can be used to apply the configuration to a specific process or group of processes (discussed later).</p> <p>Info</p> <p>Memory and time duration units can be specified either using a string-based notation in which the digit(s) and the unit can be separated by a blank or by using the numeric notation in which the digit(s) and the unit are separated by a dot character and are not enclosed by quote characters.</p> String syntax Numeric syntax Value <code>'10 KB'</code> <code>10.KB</code> 10240 bytes <code>'500 MB'</code> <code>500.MB</code> 524288000 bytes <code>'1 min'</code> <code>1.min</code> 60 seconds <code>'1 hour 25 sec'</code> - 1 hour and 25 seconds <p>The syntax for setting <code>process</code> directives in the configuration file requires <code>=</code> (i.e. assignment operator), whereas it should not be used when setting the process directives within the workflow script.</p> Example snippet.nf<pre><code>process FOO {\n    cpus 4\n    memory 2.GB\n    time 1.hour\n    maxRetries 3\n\n    script:\n    \"\"\"\n    your_command --cpus $task.cpus --mem $task.memory\n    \"\"\"\n}\n</code></pre> <p>This is especially important when you want to define a config setting using a dynamic expression using a closure. For example, in a workflow script:</p> snippet.nf<pre><code>process FOO {\n    memory { 4.GB * task.cpus }\n}\n</code></pre> <p>You can also define the same setting in the configuration file using a similar syntax:</p> snippet.nf<pre><code>process {\n    withName: FOO {\n        memory = { 4.GB * task.cpus }\n    }\n}\n</code></pre> <p>Directives that require more than one value, e.g. pod, in the configuration file need to be expressed as a map object.</p> nextflow.config<pre><code>process {\n    pod = [env: 'FOO', value: '123']\n}\n</code></pre> <p>Finally, directives that are to be repeated in the process definition, in the configuration files need to be defined as a list object:</p> nextflow.config<pre><code>process {\n    pod = [[env: 'FOO', value: '123'],\n           [env: 'BAR', value: '456']]\n}\n</code></pre>"},{"location":"archive/basic_training/config/#config-docker-execution","title":"Config Docker execution","text":"<p>The container image to be used for the process execution can be specified in the <code>nextflow.config</code> file:</p> nextflow.config<pre><code>process.container = 'nextflow/rnaseq-nf'\ndocker.enabled = true\n</code></pre> <p>The use of unique \"SHA256\" Docker image IDs guarantees that the image content does not change over time, for example:</p> nextflow.config<pre><code>process.container = 'nextflow/rnaseq-nf@sha256:aeacbd7ea1154f263cda972a96920fb228b2033544c2641476350b9317dab266'\ndocker.enabled = true\n</code></pre>"},{"location":"archive/basic_training/config/#config-singularity-execution","title":"Config Singularity execution","text":"<p>To run a workflow execution with Singularity, a container image file path is required in the Nextflow config file using the container directive:</p> nextflow.config<pre><code>process.container = '/some/singularity/image.sif'\nsingularity.enabled = true\n</code></pre> <p>Info</p> <p>The container image file must be an absolute path: it must start with a <code>/</code>.</p> <p>The following protocols are supported:</p> <ul> <li><code>library://</code> download the container image from the Singularity Library service.</li> <li><code>shub://</code> download the container image from the Singularity Hub.</li> <li><code>docker://</code> download the container image from the Docker Hub and convert it to the Singularity format.</li> <li><code>docker-daemon://</code> pull the container image from a local Docker installation and convert it to a Singularity image file.</li> </ul> <p>Warning</p> <p>Singularity hub <code>shub://</code> is no longer available as a builder service. Though existing images from before 19th April 2021 will still work.</p> <p>Tip</p> <p>By specifying a plain Docker container image name, Nextflow implicitly downloads and converts it to a Singularity image when the Singularity execution is enabled.</p> nextflow.config<pre><code>process.container = 'nextflow/rnaseq-nf'\nsingularity.enabled = true\n</code></pre> <p>The above configuration instructs Nextflow to use the Singularity engine to run your script processes. The container is pulled from the Docker registry and cached in the current directory to be used for further runs.</p> <p>Alternatively, if you have a Singularity image file, its absolute path location can be specified as the container name either using the <code>-with-singularity</code> option or the <code>process.container</code> setting in the config file.</p>"},{"location":"archive/basic_training/config/#config-conda-execution","title":"Config Conda execution","text":"<p>The use of a Conda environment can also be provided in the configuration file by adding the following setting in the <code>nextflow.config</code> file:</p> nextflow.config<pre><code>process.conda = \"/home/ubuntu/miniconda2/envs/nf-tutorial\"\n</code></pre> <p>You can specify the path of an existing Conda environment as either directory or the path of Conda environment YAML file.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to write a Nextflow configuration file.</li> <li>How to use configuration files to define parameters, environment variables, and process directives</li> <li>How to use configuration files to define Docker, Singularity, and Conda execution</li> <li>How to use configuration files to define process directives</li> </ol>"},{"location":"archive/basic_training/containers/","title":"Manage dependencies and containers","text":"<p>Computational workflows are rarely composed of a single script or tool. More often, they depend on dozens of software components or libraries.</p> <p>Installing and maintaining such dependencies is a challenging task and a common source of irreproducibility in scientific applications.</p> <p>To overcome these issues, you can use a container technology that enables software dependencies, i.e. tools and libraries required by a data analysis application, to be encapsulated in one or more self-contained, ready-to-run, immutable container images. These container images can be easily deployed in any platform that supports the container runtime.</p> <p>Containers can be executed in an isolated manner from the hosting system. Having its own copy of the file system, processing space, and memory management.</p> <p>Info</p> <p>Containers were first introduced with kernel 2.6 as a Linux feature known as Control Groups or Cgroups.</p>"},{"location":"archive/basic_training/containers/#docker-basics","title":"Docker basics","text":"<p>Docker is a handy management tool to build, run and share container images.</p> <p>These container images can be uploaded and published in a centralized repository known as Docker Hub, or hosted by other parties, such as Quay.</p>"},{"location":"archive/basic_training/containers/#run-a-container","title":"Run a container","text":"<p>A container can be <code>run</code> using the following command:</p> <pre><code>docker run &lt;container-name&gt;\n</code></pre> <p>Exercise</p> <p>Run the publicly available <code>hello-world</code> container:</p> <pre><code>docker run hello-world\n</code></pre>"},{"location":"archive/basic_training/containers/#pull-a-container","title":"Pull a container","text":"<p>The <code>pull</code> command allows you to download a Docker image without running it. For example:</p> <pre><code>docker pull &lt;container-name&gt;\n</code></pre> <p>You can check if a container has been pulled using the <code>images</code> command. For example:</p> <pre><code>docker images\n</code></pre> <p>Exercise</p> <p>Pull the publicly available <code>debian:bullseye-slim</code> container and check that it has been downloaded:</p> Solution <p>Pull the container:</p> <pre><code>docker pull debian:bullseye-slim\n</code></pre> <p>Check the container has been downloaded:</p> <pre><code>docker images\n</code></pre>"},{"location":"archive/basic_training/containers/#run-a-container-in-interactive-mode","title":"Run a container in interactive mode","text":"<p>Launching a BASH shell in the container allows you to operate in an interactive mode in the containerized operating system. For example:</p> <pre><code>docker run -it debian:bullseye-slim bash\n</code></pre> <p>Once launched, you will notice that it is running as root (!). Use the usual commands to navigate the file system. This is useful to check if the expected programs are present within a container.</p> <p>To exit from the container, stop the BASH session with the <code>exit</code> command.</p>"},{"location":"archive/basic_training/containers/#your-first-dockerfile","title":"Your first Dockerfile","text":"<p>Docker images are created by using a so-called <code>Dockerfile</code>, a simple text file containing a list of commands to assemble and configure the image with the software packages required. For example, a Dockerfile to create a container with <code>curl</code> installed could be as simple as this:</p> Dockerfile<pre><code>FROM debian:bullseye-slim\n\nLABEL image.author.name \"Your Name Here\"\nLABEL image.author.email \"your@email.here\"\n\nRUN apt-get update &amp;&amp; apt-get install -y curl\n\nENV PATH=$PATH:/usr/games/\n</code></pre> <p>Once your Dockerfile is ready, you can build the image by using the <code>build</code> command. For example:</p> <pre><code>docker build -t my-image .\n</code></pre> <p>Where <code>my-image</code> is the user-specified name for the container image you plan to build.</p> <p>Tip</p> <p>Don\u2019t miss the dot in the above command.</p> <p>Warning</p> <p>The Docker build process automatically copies all files that are located in the current directory to the Docker daemon in order to create the image. This can take a lot of time when big/many files exist. For this reason, it\u2019s important to always work in a directory containing only the files you really need to include in your Docker image. Alternatively, you can use the <code>.dockerignore</code> file to select paths to exclude from the build.</p> <p>When it completes, you can verify that the image has been created by listing all available images:</p> <pre><code>docker images\n</code></pre> <p>Exercise</p> <p>Create a Docker image containing <code>cowsay</code>.</p> Solution <p>Use your favorite editor (e.g., <code>vim</code> or <code>nano</code>) to create a file named <code>Dockerfile</code>. Alternatively, run <code>code Dockerfile</code> to create a a file named <code>Dockerfile</code>. Copy the following content:</p> Dockerfile<pre><code>FROM debian:bullseye-slim\n\nLABEL image.author.name \"Your Name Here\"\nLABEL image.author.email \"your@email.here\"\n\nRUN apt-get update &amp;&amp; apt-get install -y curl cowsay\n\nENV PATH=$PATH:/usr/games/\n</code></pre> <p>Build the Docker image based on the Dockerfile by using the following command:</p> <pre><code>docker build -t my-image .\n</code></pre> <p>Try your new container by running this command:</p> <pre><code>docker run my-image cowsay Hello Docker!\n</code></pre>"},{"location":"archive/basic_training/containers/#adding-additional-software-package-to-the-image","title":"Adding additional software package to the image","text":"<p>Additional tools can be added to the image by adding the appropriate <code>RUN</code> command to the Dockerfile.</p> <p>For example, to add the <code>salmon</code> tool to the image, you would add the following line to the bottom of your Dockerfile:</p> Dockerfile<pre><code>RUN curl -sSL https://github.com/COMBINE-lab/salmon/releases/download/v1.5.2/salmon-1.5.2_linux_x86_64.tar.gz | tar xz \\\n&amp;&amp; mv /salmon-*/bin/* /usr/bin/ \\\n&amp;&amp; mv /salmon-*/lib/* /usr/lib/\n</code></pre> <p>You will then need to save the file and build the image again with the same command as before:</p> <pre><code>docker build -t my-image .\n</code></pre> <p>Exercise</p> <p>Add the Salmon tool to your Docker file and rebuild the image.</p> Solution <p>Open your Dockerfile and add the following lines to the bottom of the file:</p> Dockerfile<pre><code>RUN curl -sSL https://github.com/COMBINE-lab/salmon/releases/download/v1.5.2/salmon-1.5.2_linux_x86_64.tar.gz | tar xz \\\n&amp;&amp; mv /salmon-*/bin/* /usr/bin/ \\\n&amp;&amp; mv /salmon-*/lib/* /usr/lib/\n</code></pre> <p>Save the file and build the image again with the same command as before:</p> <pre><code>docker build -t my-image .\n</code></pre> <p>You will notice that it creates a new Docker image with the same name but with a different image ID.</p>"},{"location":"archive/basic_training/containers/#run-salmon-in-the-container","title":"Run Salmon in the container","text":"<p>Tip</p> <p>If you didn't complete the steps above, use the 'rnaseq-nf' image used elsewhere in these materials by specifying <code>nextflow/rnaseq-nf</code> in place of <code>my-image</code> in the following examples.</p> <p>You can run the software installed in the container by using the <code>run</code> command. For example, you can check that Salmon is running correctly in the container generated above by using the following command:</p> <pre><code>docker run my-image salmon --version\n</code></pre> <p>You can even launch a container in an interactive mode by using the following command:</p> <pre><code>docker run -it my-image bash\n</code></pre> <p>Use the <code>exit</code> command to terminate the interactive session.</p>"},{"location":"archive/basic_training/containers/#file-system-mounts","title":"File system mounts","text":"<p>Containers run in a completely separate file system and it cannot access the hosting file system by default.</p> <p>For example, running the following command that is attempting to generate a genome index using the Salmon tool will fail because Salmon cannot access the input file:</p> <pre><code>docker run my-image \\\n    salmon index -t $PWD/data/ggal/transcriptome.fa -i transcript-index\n</code></pre> <p>To mount a filesystem within a Docker container, you can use the <code>--volume</code> command-line option when running the container. Its argument consists of two fields separated by a colon (:):</p> <ul> <li>Host source directory path</li> <li>Container target directory path</li> </ul> <p>For example:</p> <pre><code>docker run --volume $PWD/data/ggal/transcriptome.fa:/transcriptome.fa my-image \\\n    salmon index -t /transcriptome.fa -i transcript-index\n</code></pre> <p>Warning</p> <p>The generated <code>transcript-index</code> directory is still not accessible in the host file system.</p> <p>An easier way to mount file systems is to mount a parent directory to an identical directory in the container. This allows you to use the same path when running it in the container. For example:</p> <pre><code>docker run --volume $PWD:$PWD --workdir $PWD my-image \\\n    salmon index -t $PWD/data/ggal/transcriptome.fa -i transcript-index\n</code></pre> <p>Or set a folder you want to mount as an environmental variable, called <code>DATA</code>:</p> <pre><code>DATA=/workspaces/training/nf-training/data\ndocker run --volume $DATA:$DATA --workdir $PWD my-image \\\n    salmon index -t $PWD/data/ggal/transcriptome.fa -i transcript-index\n</code></pre> <p>You can check the content of the <code>transcript-index</code> folder by entering the command:</p> <pre><code>ls -la transcript-index\n</code></pre> <p>Note</p> <p>Note that the permissions for files created by the Docker execution is <code>root</code>.</p>"},{"location":"archive/basic_training/containers/#upload-the-container-in-the-docker-hub-optional","title":"Upload the container in the Docker Hub (optional)","text":"<p>You can also publish your container in the Docker Hub to share it with other people.</p> <p>Create an account on the https://hub.docker.com website. Then from your shell terminal run the following command, entering the user name and password you specified when registering in the Hub:</p> <pre><code>docker login\n</code></pre> <p>Rename the image to include your Docker user name account:</p> <pre><code>docker tag my-image &lt;user-name&gt;/my-image\n</code></pre> <p>Finally push it to the Docker Hub:</p> <pre><code>docker push &lt;user-name&gt;/my-image\n</code></pre> <p>After that anyone will be able to download it by using the command:</p> <pre><code>docker pull &lt;user-name&gt;/my-image\n</code></pre> <p>Note how after a pull and push operation, Docker prints the container digest number e.g.</p> Output<pre><code>Digest: sha256:aeacbd7ea1154f263cda972a96920fb228b2033544c2641476350b9317dab266\nStatus: Downloaded newer image for nextflow/rnaseq-nf:latest\n</code></pre> <p>This is a unique and immutable identifier that can be used to reference a container image in a univocally manner. For example:</p> <pre><code>docker pull nextflow/rnaseq-nf@sha256:aeacbd7ea1154f263cda972a96920fb228b2033544c2641476350b9317dab266\n</code></pre>"},{"location":"archive/basic_training/containers/#run-a-nextflow-script-using-a-docker-container","title":"Run a Nextflow script using a Docker container","text":"<p>The simplest way to run a Nextflow script with a Docker image is using the <code>-with-docker</code> command-line option:</p> <pre><code>nextflow run script2.nf -with-docker my-image\n</code></pre> <p>As seen in the last section, you can also configure the Nextflow config file (<code>nextflow.config</code>) to select which container image to use instead of having to specify it as a command-line argument every time.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to <code>run</code> a docker container</li> <li>How to <code>pull</code> a docker container</li> <li>How to create your own docker container using a <code>Dockerfile</code></li> <li>How to add additional software packages to your container</li> <li>How to use your own container when running a nextflow script</li> </ol>"},{"location":"archive/basic_training/containers/#singularity","title":"Singularity","text":"<p>Singularity is a container runtime designed to work in high-performance computing data centers, where the usage of Docker is generally not allowed due to security constraints.</p> <p>Singularity implements a container execution model similar to Docker. However, it uses a completely different implementation design.</p> <p>A Singularity container image is archived as a plain file that can be stored in a shared file system and accessed by many computing nodes managed using a batch scheduler.</p>"},{"location":"archive/basic_training/containers/#create-a-singularity-images","title":"Create a Singularity images","text":"<p>Singularity images are created using a <code>Singularity</code> file in a similar manner to Docker but using a different syntax.</p> Singularity<pre><code>Bootstrap: docker\nFrom: debian:bullseye-slim\n\n%environment\nexport PATH=$PATH:/usr/games/\n\n%labels\nAUTHOR &lt;your name&gt;\n\n%post\n\napt-get update &amp;&amp; apt-get install -y locales-all curl cowsay\ncurl -sSL https://github.com/COMBINE-lab/salmon/releases/download/v1.0.0/salmon-1.0.0_linux_x86_64.tar.gz | tar xz \\\n&amp;&amp; mv /salmon-*/bin/* /usr/bin/ \\\n&amp;&amp; mv /salmon-*/lib/* /usr/lib/\n</code></pre> <p>Once you have saved the <code>Singularity</code> file, you can create the image with the build command:</p> <pre><code>sudo singularity build my-image.sif Singularity\n</code></pre> <p>Warning</p> <p>The <code>build</code> command requires <code>sudo</code> permissions. A common workaround consists of building the image on a local workstation and then deploying it in the cluster by copying the image file.</p>"},{"location":"archive/basic_training/containers/#running-a-container","title":"Running a container","text":"<p>You can run your container using the <code>exec</code> command:</p> <pre><code>singularity exec my-image.sif cowsay 'Hello Singularity'\n</code></pre> <p>By using the <code>shell</code> command you can enter in the container in interactive mode:</p> <pre><code>singularity shell my-image.sif\n</code></pre> <p>Once in the container instance run the following command:</p> <pre><code>touch hello.txt\nls -la\n</code></pre> <p>Info</p> <p>Note how the files on the host environment are shown. Singularity automatically mounts the host <code>$HOME</code> directory and uses the current work directory.</p>"},{"location":"archive/basic_training/containers/#import-a-docker-image","title":"Import a Docker image","text":"<p>An easier way to create a Singularity container without requiring <code>sudo</code> permission and boosting the containers interoperability is to import a Docker container image by pulling it directly from a Docker registry. For example:</p> <pre><code>singularity pull docker://debian:bullseye-slim\n</code></pre> <p>The above command automatically downloads the Debian Docker image and converts it to a Singularity image in the current directory with the name <code>debian-jessie.simg</code>.</p>"},{"location":"archive/basic_training/containers/#run-a-nextflow-script-using-a-singularity-container","title":"Run a Nextflow script using a Singularity container","text":"<p>As with Docker, Nextflow allows the transparent usage of Singularity containers.</p> <p>Simply enable the use of the Singularity engine in place of Docker in the Nextflow command line by using the <code>-with-singularity</code> command-line option:</p> <pre><code>nextflow run script7.nf -with-singularity  nextflow/rnaseq-nf\n</code></pre> <p>As before, the Singularity container can also be provided in the Nextflow config file. We\u2019ll see how to do this later.</p>"},{"location":"archive/basic_training/containers/#the-singularity-container-library","title":"The Singularity Container Library","text":"<p>The authors of Singularity, SyLabs have their own repository of Singularity containers.</p> <p>In the same way that you can push Docker images to Docker Hub, you can upload Singularity images to the Singularity Library.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to create and run a Singularity container</li> <li>How to Run a Nextflow script using a Singularity container</li> <li>How to access the the Singularity container library</li> </ol>"},{"location":"archive/basic_training/containers/#conda-packages","title":"Conda packages","text":"<p>Conda is a popular package and environment manager. The built-in support for Conda allows Nextflow workflows to automatically create and activate the Conda environment(s), given the dependencies specified by each process.</p> <p>In this GitHub Codespaces environment, conda is already installed.</p>"},{"location":"archive/basic_training/containers/#using-conda","title":"Using conda","text":"<p>A Conda environment is defined using a YAML file, which lists the required software packages.</p> <p>To use conda, you need to initiate conda and open a new terminal by running bash:</p> <pre><code>conda init\nbash\n</code></pre> <p>There is already a file named <code>env.yml</code> in the <code>nf-training</code> folder as an example. Its content is shown below:</p> nf-training/env.yml<pre><code>name: nf-tutorial\nchannels:\n  - conda-forge\n  - defaults\n  - bioconda\ndependencies:\n  - bioconda::salmon=1.5.1\n  - bioconda::fastqc=0.11.9\n  - bioconda::multiqc=1.12\n  - conda-forge::tbb=2020.2\n</code></pre> <p>Given the <code>env.yml</code> recipe file, the environment can be created using the command shown below.</p> <pre><code>conda env create --file env.yml\n</code></pre> <p>The <code>conda env create</code> command may take several minutes, as conda tries to resolve dependencies of the desired packages at runtime, and then downloads everything that is required.</p> <p>You can check if the environment was created successfully with the command shown below:</p> <pre><code>conda env list\n</code></pre> <p>This output will look something like this:</p> Output<pre><code># conda environments:\n#\nbase                  *  /opt/conda\nnf-tutorial              /opt/conda/envs/nf-tutorial\n</code></pre> <p>To enable the environment, you can use the <code>activate</code> command:</p> <pre><code>conda activate nf-tutorial\n</code></pre> <p>Nextflow is able to manage the activation of a Conda environment when its directory is specified using the <code>-with-conda</code> option (using the same path shown in the <code>list</code> function. For example:</p> <pre><code>nextflow run script7.nf -with-conda /opt/conda/envs/nf-tutorial\n</code></pre> <p>Info</p> <p>When creating a Conda environment with a YAML recipe file, Nextflow automatically downloads the required dependencies, builds the environment and activates it.</p> <p>This makes easier to manage different environments for the processes in the workflow script.</p> <p>See the docs for further details.</p>"},{"location":"archive/basic_training/containers/#create-and-use-conda-like-environments-using-micromamba","title":"Create and use conda-like environments using micromamba","text":"<p>Another way to build conda-like environments is through a <code>Dockerfile</code> and <code>micromamba</code>.</p> <p><code>micromamba</code> is a fast and robust package for building small conda-based environments.</p> <p>This saves having to build a conda environment each time you want to use it (as outlined in previous sections).</p> <p>To do this, you simply require a <code>Dockerfile</code> and you use micromamba to install the packages. However, a good practice is to have a YAML recipe file.</p> <p>Using the same <code>env.yml</code> from above, you can write a Dockerfile with <code>micromamba</code> installing the packages from the recipe file. For example:</p> Dockerfile<pre><code>FROM mambaorg/micromamba:0.25.1\n\nLABEL image.author.name \"Your Name Here\"\nLABEL image.author.email \"your@email.here\"\n\nCOPY --chown=$MAMBA_USER:$MAMBA_USER env.yml /tmp/env.yml\n\nRUN micromamba create -n nf-tutorial\n\nRUN micromamba install -y -n nf-tutorial -f /tmp/env.yml &amp;&amp; \\\n    micromamba clean --all --yes\n\nENV PATH /opt/conda/envs/nf-tutorial/bin:$PATH\n</code></pre> <p>The above <code>Dockerfile</code> takes the parent image mambaorg/micromamba, installs a <code>conda</code> environment using <code>micromamba</code>, and then installs <code>salmon</code>, <code>fastqc</code>, and <code>multiqc</code>.</p> <p>Exercise</p> <p>Execute <code>script7.nf</code> using your own micromamba <code>Dockerfile</code> that you have pushed to your Docker hub repo.</p> <p>Warning</p> <p>Building a Docker container and pushing to your personal repo can take &gt;10 minutes.</p> Solution <p>Make a file called <code>Dockerfile</code> in the current directory.</p> Dockerfile<pre><code>FROM mambaorg/micromamba:0.25.1\n\nLABEL image.author.name \"Your Name Here\"\nLABEL image.author.email \"your@email.here\"\n\nCOPY --chown=$MAMBA_USER:$MAMBA_USER env.yml /tmp/env.yml\n\nRUN micromamba create -n nf-tutorial\n\nRUN micromamba install -y -n nf-tutorial -f /tmp/env.yml &amp;&amp; \\\nmicromamba clean --all --yes\n\nENV PATH /opt/conda/envs/nf-tutorial/bin:$PATH\n</code></pre> <p>Build the image:</p> <pre><code>docker build -t my-image .\n</code></pre> <p>Publish the Docker image to your online Docker account.</p> <pre><code>docker login\ndocker tag my-image &lt;myrepo&gt;/my-image\ndocker push &lt;myrepo&gt;/my-image\n</code></pre> <p><code>&lt;myrepo&gt;</code> needs to be replaced with your own Docker ID, without the &lt; and &gt; characters.</p> <p><code>my-image</code> can be replaced with any name you choose. As good practice, choose something memorable and ensure the name matches the name you used in the previous command.</p> <p>Add the container image name to the <code>nextflow.config</code> file.</p> <p>For example, remove the following from the <code>nextflow.config</code>:</p> nextflow.config<pre><code>process.container = 'nextflow/rnaseq-nf'\n</code></pre> <p>Replace it with:</p> nextflow.config<pre><code>process.container = '&lt;myrepo&gt;/my-image'\n</code></pre> <p>Trying running Nextflow, For example:</p> <pre><code>nextflow run script7.nf -with-docker\n</code></pre> <p>Nextflow should now be able to find <code>salmon</code> to run the process.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to create conda-like environments using micromamba</li> <li>How to use use conda-like environments using micromamba</li> </ol>"},{"location":"archive/basic_training/containers/#biocontainers","title":"BioContainers","text":"<p>Another useful resource linking together Bioconda and containers is the BioContainers project. BioContainers is a community initiative that provides a registry of container images for every Bioconda recipe.</p> <p>With BioContainers, you don\u2019t need to create your own container image for the tools you want, and you don\u2019t need to use conda or micromamba to install the packages. Instead, BioContainers provides you with a Docker image containing the program you need. For example, you can pull the container image of fastqc using BioContainers with:</p> <pre><code>docker pull biocontainers/fastqc:v0.11.5\n</code></pre> <p>You can check the registry for the packages you want at BioContainers official website. For finding multi-tools container images, check their Multi-package images.</p> <p>Contrary to other registries that will pull the latest image when no tag (version) is provided, you must specify a tag when pulling BioContainers (after a colon <code>:</code>, e.g., <code>fastqc:v0.11.5</code>). Check the tags within the registry and pick the tag that best suits your needs.</p> <p>You can also install <code>galaxy-util-tools</code> and search for mulled containers in your CLI. You'll find instructions below, using conda to install the tool.</p> <pre><code>conda create -n galaxy-tool-util -y galaxy-tool-util # Create a new environment with 'galaxy-tool-util' installed\nconda activate galaxy-tool-util\nmulled-search --destination quay singularity --channel bioconda --search bowtie samtools | grep mulled\n</code></pre> <p>Tip</p> <p>You can have more complex definitions within your process block by letting the appropriate container image or conda package be used depending on if the user selected singularity, Docker or conda to be used. You can click here for more information and here for an example.</p> <p>Exercise</p> <p>During the earlier RNA-Seq tutorial (script2.nf), you created an index with the salmon tool. Given you do not have salmon installed locally in the machine provided by GitHub Codespaces, you had to either run it with <code>-with-conda</code> or <code>-with-docker</code>. Your task now is to run it again <code>-with-docker</code>, but without creating your own container image. Instead, use the BioContainers image for salmon 1.7.0.</p> Solution <pre><code>nextflow run script2.nf -with-docker quay.io/biocontainers/salmon:1.7.0--h84f40af_0\n</code></pre> <p>Instead of supplying one big container for a complete workflow, separate containers can be used for each process in a workflow. This way, you can quickly add, update, or remove a tool from your workflow without having to rebuild the entire workflow container.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>What Biocontainers are and where to find them</li> <li>How to use Biocontainers in your Nextflow workflow</li> </ol>"},{"location":"archive/basic_training/containers/#software-directives","title":"Software directives","text":"<p>Directives are optional settings that affect the execution of the current process.</p> <p>They must be entered at the top of the process body, before any other declaration blocks</p>"},{"location":"archive/basic_training/containers/#container-directives","title":"Container directives","text":"<p>The <code>container</code> directive allows you to execute the process script in a Docker container.</p> example.nf<pre><code>process FASTQC {\n    container 'biocontainers/fastqc:v0.11.5'\n    tag \"FASTQC on $sample_id\"\n    ...\n</code></pre>"},{"location":"archive/basic_training/containers/#conda-directives","title":"Conda directives","text":"<p>Similarly, the <code>conda</code> directive allows for the definition of the process dependencies using the Conda package manager.</p> example.nf<pre><code>process FASTQC {\n    conda 'fastqc=0.11.5'\n    tag \"FASTQC on $sample_id\"\n    ...\n</code></pre> <p>Nextflow automatically sets up an environment for the given package names listed by in the conda directive.</p> <p>Exercise</p> <p>The tools <code>fastqc</code> and <code>salmon</code> are both available in Biocontainers (<code>biocontainers/fastqc:v0.11.5</code> and <code>quay.io/biocontainers/salmon:1.7.0--h84f40af_0</code>, respectively). Add the appropriate <code>container</code> directives to the <code>FASTQC</code> and <code>QUANTIFICATION</code> processes in <code>script5.nf</code> to use Seqera Containers instead of the container image you have been using in this training.</p> <p>Hint</p> <p>Temporarily comment out the line <code>process.container = 'nextflow/rnaseq-nf'</code> in the <code>nextflow.config</code> file to make sure the processes are using the BioContainers that you set, and not the container image you have been using in this training.</p> Solution <p>Add the container directive with the appropriate BioContainers to the <code>FASTQC</code> and <code>QUANTIFICATION</code> processes in <code>script5.nf</code>:</p> script5.nf<pre><code>process FASTQC {\n    container 'biocontainers/fastqc:v0.11.5'\n    tag \"FASTQC on $sample_id\"\n...\n</code></pre> <p>and</p> script5.nf<pre><code>process QUANTIFICATION {\n    tag \"Salmon on $sample_id\"\n    container 'quay.io/biocontainers/salmon:1.7.0--h84f40af_0'\n    publishDir params.outdir, mode: 'copy'\n...\n</code></pre> <p>With these changes, you should be able to run the workflow with BioContainers by running the following in the command line:</p> <pre><code>nextflow run script5.nf\n</code></pre> <p>You can check the <code>.command.run</code> file in the work directory and ensure that the run line contains the correct Biocontainers.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to add software with directives</li> </ol>"},{"location":"archive/basic_training/debugging/","title":"Error handling and troubleshooting","text":""},{"location":"archive/basic_training/debugging/#execution-error-debugging","title":"Execution error debugging","text":"<p>When a process execution exits with a non-zero exit status, Nextflow stops the workflow execution and reports the failing task:</p> <p>Click the  icons in the code for explanations.</p> <pre><code>ERROR ~ Error executing process &gt; 'INDEX'\n\nCaused by: # (1)!\n  Process `INDEX` terminated with an error exit status (127)\n\nCommand executed: # (2)!\n\n  salmon index --threads 1 -t transcriptome.fa -i index\n\nCommand exit status: # (3)!\n  127\n\nCommand output: # (4)!\n  (empty)\n\nCommand error: # (5)!\n  .command.sh: line 2: salmon: command not found\n\nWork dir: # (6)!\n  /Users/pditommaso/work/0b/b59f362980defd7376ee0a75b41f62\n</code></pre> <ol> <li>A description of the error cause</li> <li>The command executed</li> <li>The command exit status</li> <li>The command standard output, when available</li> <li>The command standard error</li> <li>The command work directory</li> </ol> <p>Carefully review all error data as it can provide information valuable for debugging.</p> <p>If this is not enough, <code>cd</code> into the task work directory. It contains all the files to replicate the issue in an isolated manner.</p> <p>The task execution directory contains these files:</p> <ul> <li><code>.command.sh</code>: The command script.</li> <li><code>.command.run</code>: The command wrapped used to run the task.</li> <li><code>.command.out</code>: The complete task standard output.</li> <li><code>.command.err</code>: The complete task standard error.</li> <li><code>.command.log</code>: The wrapper execution output.</li> <li><code>.command.begin</code>: Sentinel file created as soon as the task is launched.</li> <li><code>.exitcode</code>: A file containing the task exit code.</li> <li>Task input files (symlinks)</li> <li>Task output files</li> </ul> <p>Verify that the <code>.command.sh</code> file contains the expected command and all variables are correctly resolved.</p> <p>Also verify the existence of the <code>.exitcode</code> and <code>.command.begin</code> files, which if absent, suggest the task was never executed by the subsystem (e.g. the batch scheduler). If the <code>.command.begin</code> file exists, the task was launched but was likely killed abruptly.</p> <p>You can replicate the failing execution using the command <code>bash .command.run</code> to verify the cause of the error.</p>"},{"location":"archive/basic_training/debugging/#ignore-errors","title":"Ignore errors","text":"<p>There are cases in which a process error may be expected and it should not stop the overall workflow execution.</p> <p>To handle this use case, set the process <code>errorStrategy</code> to <code>ignore</code>:</p> snippet.nf<pre><code>process FOO {\n    errorStrategy 'ignore'\n\n    script:\n    \"\"\"\n    your_command --this --that\n    \"\"\"\n}\n</code></pre> <p>If you want to ignore any error you can set the same directive in the config file as a default setting:</p> <pre><code>process.errorStrategy = 'ignore'\n</code></pre>"},{"location":"archive/basic_training/debugging/#automatic-error-fail-over","title":"Automatic error fail-over","text":"<p>In rare cases, errors may be caused by transient conditions. In this situation, an effective strategy is re-executing the failing task.</p> snippet.nf<pre><code>process FOO {\n    errorStrategy 'retry'\n\n    script:\n    \"\"\"\n    your_command --this --that\n    \"\"\"\n}\n</code></pre> <p>Using the <code>retry</code> error strategy the task is re-executed a second time if it returns a non-zero exit status before stopping the complete workflow execution.</p> <p>The directive maxRetries can be used to set the number of attempts the task can be re-executed before declaring it failed with an error condition.</p>"},{"location":"archive/basic_training/debugging/#retry-with-backoff","title":"Retry with backoff","text":"<p>There are cases in which the required execution resources may be temporarily unavailable (e.g. network congestion). In these cases simply re-executing the same task will likely result in an identical error. A retry with an exponential backoff delay can better recover these error conditions.</p> snippet.nf<pre><code>process FOO {\n    errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n    maxRetries 5\n\n    script:\n    '''\n    your_command --here\n    '''\n}\n</code></pre>"},{"location":"archive/basic_training/debugging/#dynamic-resources-allocation","title":"Dynamic resources allocation","text":"<p>It\u2019s a very common scenario that different instances of the same process may have very different needs in terms of computing resources. In such situations requesting, for example, an amount of memory too low will cause some tasks to fail. Instead, using a higher limit that fits all the tasks in your execution could significantly decrease the execution priority of your job in a scheduling system.</p> <p>To handle this use case, you can use a <code>retry</code> error strategy and increase the computing resources allocated by the task at each successive attempt.</p> snippet.nf<pre><code>process FOO {\n    cpus 4\n    memory { 2.GB * task.attempt } // (1)!\n    time { 1.hour * task.attempt } // (2)!\n    errorStrategy { task.exitStatus == 140 ? 'retry' : 'terminate' } // (3)!\n    maxRetries 3 // (4)!\n\n    script:\n    \"\"\"\n    your_command --cpus $task.cpus --mem $task.memory\n    \"\"\"\n}\n</code></pre> <ol> <li>The memory is defined in a dynamic manner, the first attempt is 2 GB, the second 4 GB, and so on.</li> <li>The wall execution time is set dynamically as well, the first execution attempt is set to 1 hour, the second 2 hours, and so on.</li> <li>If the task returns an exit status equal to <code>140</code> it will set the error strategy to <code>retry</code> otherwise it will terminate the execution.</li> <li>It will retry the process execution up to three times.</li> </ol>"},{"location":"archive/basic_training/executors/","title":"Deployment scenarios","text":"<p>Real-world genomic applications can spawn the execution of thousands of tasks. In this scenario a batch scheduler is commonly used to deploy a workflow in a computing cluster, allowing the execution of many jobs in parallel across many compute nodes.</p> <p>Nextflow has built-in support for the most commonly used batch schedulers, such as Univa Grid Engine, SLURM, and IBM LSF.</p> <p>You can view the Nextflow documentation for the complete list of supported execution platforms.</p>"},{"location":"archive/basic_training/executors/#cluster-deployment","title":"Cluster deployment","text":"<p>A key Nextflow feature is the ability to decouple the workflow implementation from the actual execution platform. The implementation of an abstraction layer allows the deployment of the resulting workflow on any executing platform supported by the framework.</p> <p></p> <p>To run your workflow with a batch scheduler, modify the <code>nextflow.config</code> file specifying the target executor and the required computing resources if needed. For example:</p> nextflow.config<pre><code>process.executor = 'slurm'\n</code></pre>"},{"location":"archive/basic_training/executors/#managing-cluster-resources","title":"Managing cluster resources","text":"<p>When using a batch scheduler, it is often needed to specify the number of resources (i.e. cpus, memory, execution time, etc.) required by each task.</p> <p>This can be done using the following process directives:</p> queue the cluster queue to be used for the computation cpus the number of cpus to be allocated for a task execution memory the amount of memory to be allocated for a task execution time the max amount of time to be allocated for a task execution disk the amount of disk storage required for a task execution"},{"location":"archive/basic_training/executors/#workflow-wide-resources","title":"Workflow wide resources","text":"<p>Use the scope <code>process</code> to define the resource requirements for all processes in your workflow applications. For example:</p> nextflow.config<pre><code>process {\n    executor = 'slurm'\n    queue = 'short'\n    memory = '10 GB'\n    time = '30 min'\n    cpus = 4\n}\n</code></pre>"},{"location":"archive/basic_training/executors/#submit-nextflow-as-a-job","title":"Submit Nextflow as a job","text":"<p>Whilst the main Nextflow command can be launched on the login / head node of a cluster, be aware that the node must be set up for commands that run for a long time, even if the compute resources used are negligible. Another option is to submit the main Nextflow process as a job on the cluster instead.</p> <p>Note</p> <p>This requires your cluster configuration to allow jobs be launched from worker nodes, as Nextflow will submit new tasks and manage them from here.</p> <p>For example, if your cluster uses Slurm as a job scheduler, you could create a file similar to the one below:</p> launch_nf.sh<pre><code>#!/bin/bash\n#SBATCH --partition WORK\n#SBATCH --mem 5G\n#SBATCH -c 1\n#SBATCH -t 12:00:00\n\nWORKFLOW=$1\nCONFIG=$2\n\n# Use a conda environment where you have installed Nextflow\n# (may not be needed if you have installed it in a different way)\nconda activate nextflow\n\nnextflow -C ${CONFIG} run ${WORKFLOW}\n</code></pre> <p>And then submit it with:</p> <pre><code>sbatch launch_nf.sh /home/my_user/path/my_workflow.nf /home/my_user/path/my_config_file.conf\n</code></pre> <p>You can find more details about the example above here. You can find more tips for running Nextflow on HPC in the following blog posts:</p> <ul> <li>5 Nextflow Tips for HPC Users</li> <li>Five more tips for Nextflow user on HPC</li> </ul>"},{"location":"archive/basic_training/executors/#configure-process-by-name","title":"Configure process by name","text":"<p>In real-world applications, different tasks need different amounts of computing resources. It is possible to define the resources for a specific task using the select <code>withName:</code> followed by the process name:</p> nextflow.config<pre><code>process {\n    executor = 'slurm'\n    queue = 'short'\n    memory = '10 GB'\n    time = '30 min'\n    cpus = 4\n\n    withName: FOO {\n        cpus = 2\n        memory = '20 GB'\n        queue = 'short'\n    }\n\n    withName: BAR {\n        cpus = 4\n        memory = '32 GB'\n        queue = 'long'\n    }\n}\n</code></pre>"},{"location":"archive/basic_training/executors/#configure-process-by-labels","title":"Configure process by labels","text":"<p>When a workflow application is composed of many processes, listing all of the process names and choosing resources for each of them in the configuration file can be difficult.</p> <p>A better strategy consists of annotating the processes with a label directive. Then specify the resources in the configuration file used for all processes having the same label.</p> <p>The workflow script:</p> snippet.nf<pre><code>process TASK1 {\n    label 'long'\n\n    script:\n    \"\"\"\n    first_command --here\n    \"\"\"\n}\n\nprocess TASK2 {\n    label 'short'\n\n    script:\n    \"\"\"\n    second_command --here\n    \"\"\"\n}\n</code></pre> <p>The configuration file:</p> nextflow.config<pre><code>process {\n    executor = 'slurm'\n\n    withLabel: 'short' {\n        cpus = 4\n        memory = '20 GB'\n        queue = 'alpha'\n    }\n\n    withLabel: 'long' {\n        cpus = 8\n        memory = '32 GB'\n        queue = 'omega'\n    }\n}\n</code></pre>"},{"location":"archive/basic_training/executors/#configure-multiple-containers","title":"Configure multiple containers","text":"<p>Containers can be set for each process in your workflow. You can define their containers in a config file as shown below:</p> nextflow.config<pre><code>process {\n    withName: FOO {\n        container = 'some/image:x'\n    }\n\n    withName: BAR {\n        container = 'other/image:y'\n    }\n}\n\ndocker.enabled = true\n</code></pre> <p>Tip</p> <p>Should I use a single fat container or many slim containers? Both approaches have pros &amp; cons. A single container is simpler to build and maintain, however when using many tools the image can become very big and tools can create conflicts with each other. Using a container for each process can result in many different images to build and maintain, especially when processes in your workflow use different tools for each task.</p> <p>Read more about config process selectors at this link.</p>"},{"location":"archive/basic_training/executors/#configuration-profiles","title":"Configuration profiles","text":"<p>Configuration files can contain the definition of one or more profiles. A profile is a set of configuration attributes that can be activated/chosen when launching a workflow execution by using the <code>-profile</code> command- line option.</p> <p>Configuration profiles are defined by using the special scope <code>profiles</code> which group the attributes that belong to the same profile using a common prefix. For example:</p> nextflow.config<pre><code>profiles {\n    standard {\n        params.genome = '/local/path/ref.fasta'\n        process.executor = 'local'\n    }\n\n    cluster {\n        params.genome = '/data/stared/ref.fasta'\n        process.executor = 'sge'\n        process.queue = 'long'\n        process.memory = '10 GB'\n        process.conda = '/some/path/env.yml'\n    }\n\n    cloud {\n        params.genome = '/data/stared/ref.fasta'\n        process.executor = 'awsbatch'\n        process.container = 'cbcrg/imagex'\n        docker.enabled = true\n    }\n}\n</code></pre> <p>This configuration defines three different profiles: <code>standard</code>, <code>cluster</code> and <code>cloud</code> that set different process configuration strategies depending on the target runtime platform. By convention, the <code>standard</code> profile is implicitly used when no other profile is specified by the user.</p> <p>To enable a specific profile use <code>-profile</code> option followed by the profile name:</p> <pre><code>nextflow run &lt;your script&gt; -profile cluster\n</code></pre> <p>Tip</p> <p>Two or more configuration profiles can be specified by separating the profile names with a comma character:</p> <pre><code>nextflow run &lt;your script&gt; -profile standard,cloud\n</code></pre>"},{"location":"archive/basic_training/executors/#cloud-deployment","title":"Cloud deployment","text":"<p>Nextflow supports deployment on your favourite cloud providers. The following sections describe how to deploy Nextflow workflows on AWS.</p>"},{"location":"archive/basic_training/executors/#aws-batch","title":"AWS Batch","text":"<p>AWS Batch is a managed computing service that allows the execution of containerized workloads in the Amazon cloud infrastructure.</p> <p>Nextflow provides built-in support for AWS Batch which allows the seamless deployment of a Nextflow workflow in the cloud, offloading the process executions as Batch jobs.</p> <p>Once the Batch environment is configured, specify the instance types to be used and the max number of CPUs to be allocated, you need to create a Nextflow configuration file like the one shown below:</p> <p>Click the  icons in the code for explanations.</p> nextflow.config<pre><code>process.executor = 'awsbatch' // (1)!\nprocess.queue = 'nextflow-ci' // (2)!\nprocess.container = 'nextflow/rnaseq-nf:latest' // (3)!\nworkDir = 's3://nextflow-ci/work/' // (4)!\naws.region = 'eu-west-1' // (5)!\naws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws' // (6)!\n</code></pre> <ol> <li>Set AWS Batch as the executor to run the processes in the workflow</li> <li>The name of the computing queue defined in the Batch environment</li> <li>The Docker container image to be used to run each job</li> <li>The workflow work directory must be a AWS S3 bucket</li> <li>The AWS region to be used</li> <li>The path of the AWS cli tool required to download/upload files to/from the container</li> </ol> <p>Tip</p> <p>The best practice is to keep this setting as a separate profile in your workflow config file. This allows the execution with a simple command.</p> <pre><code>nextflow run script7.nf -profile amazon\n</code></pre> <p>The complete details about AWS Batch deployment are available at this link.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to configure a cluster deployment</li> <li>How to manage cluster resources</li> <li>How to submit Nextflow as a job</li> <li>How to configure process by name</li> <li>How to configure process by labels</li> </ol>"},{"location":"archive/basic_training/executors/#volume-mounts","title":"Volume mounts","text":"<p>Elastic Block Storage (EBS) volumes (or other supported storage) can be mounted in the job container using the following configuration snippet:</p> nextflow.config<pre><code>aws {\n    batch {\n        volumes = '/some/path'\n    }\n}\n</code></pre> <p>Multiple volumes can be specified using comma-separated paths. The usual Docker volume mount syntax can be used to define complex volumes for which the container path is different from the host path or to specify a read-only option:</p> nextflow.config<pre><code>aws {\n    region = 'eu-west-1'\n    batch {\n        volumes = ['/tmp', '/host/path:/mnt/path:ro']\n    }\n}\n</code></pre> <p>Tip</p> <p>This is a global configuration that has to be specified in a Nextflow config file and will be applied to all process executions.</p> <p>Tip</p> <p>Additional documentation for AWS, GCP, and Azure are available on the Nextflow documentation site.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to configure AWS Batch</li> <li>How to configure volume mounts</li> </ol>"},{"location":"archive/basic_training/executors/#additional-configuration-options","title":"Additional configuration options","text":"<p>There are many different ways to deploy Nextflow workflows. The following sections describe additional configuration options for deployments.</p>"},{"location":"archive/basic_training/executors/#custom-job-definition","title":"Custom job definition","text":"<p>Nextflow automatically creates the Batch Job definitions needed to execute your workflow processes. Therefore it\u2019s not required to define them before you run your workflow.</p> <p>However, you may still need to specify a custom Job Definition to provide fine-grained control of the configuration settings of a specific job (e.g. to define custom mount paths or other special settings of a Batch Job).</p> <p>To use your own job definition in a Nextflow workflow, use it in place of the container image name, prefixing it with the <code>job-definition://</code> string. For example:</p> nextflow.config<pre><code>process {\n    container = 'job-definition://your-job-definition-name'\n}\n</code></pre>"},{"location":"archive/basic_training/executors/#custom-image","title":"Custom image","text":"<p>Since Nextflow requires the AWS CLI tool to be accessible in the computing environment, a common solution consists of creating a custom Amazon Machine Image (AMI) and installing it in a self-contained manner (e.g. using Conda package manager).</p> <p>Warning</p> <p>When creating your custom AMI for AWS Batch, make sure to use the Amazon ECS-Optimized Amazon Linux AMI as the base image.</p> <p>The following snippet shows how to install AWS CLI with Miniconda:</p> <pre><code>sudo yum install -y bzip2 wget\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -f -p $HOME/miniconda\n$HOME/miniconda/bin/conda install -c conda-forge -y awscli\nrm Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Note</p> <p>The <code>aws</code> tool will be placed in a directory named <code>bin</code> in the main installation folder. The tools will not work properly if you modify this directory     structure after the installation.</p> <p>Finally, specify the <code>aws</code> full path in the Nextflow config file as shown below:</p> <pre><code>aws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws'\n</code></pre>"},{"location":"archive/basic_training/executors/#launch-template","title":"Launch template","text":"<p>An alternative approach to is to create a custom AMI using a Launch template that installs the AWS CLI tool during the instance boot via custom user data.</p> <p>In the EC2 dashboard, create a Launch template specifying the user data field:</p> <pre><code>MIME-Version: 1.0\nContent-Type: multipart/mixed; boundary=\"//\"\n\n--//\nContent-Type: text/x-shellscript; charset=\"us-ascii\"\n\n##!/bin/sh\n### install required deps\nset -x\nexport PATH=/usr/local/bin:$PATH\nyum install -y jq python27-pip sed wget bzip2\npip install -U boto3\n\n### install awscli\nUSER=/home/ec2-user\nwget -q https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -f -p $USER/miniconda\n$USER/miniconda/bin/conda install -c conda-forge -y awscli\nrm Miniconda3-latest-Linux-x86_64.sh\nchown -R ec2-user:ec2-user $USER/miniconda\n\n--//--\n</code></pre> <p>Then create a new compute environment in the Batch dashboard and specify the newly created launch template in the corresponding field.</p>"},{"location":"archive/basic_training/executors/#hybrid-deployments","title":"Hybrid deployments","text":"<p>Nextflow allows the use of multiple executors in the same workflow application. This feature enables the deployment of hybrid workloads in which some jobs are executed on the local computer or local computing cluster, and some jobs are offloaded to the AWS Batch service.</p> <p>To enable this feature use one or more process selectors in your Nextflow configuration file.</p> <p>When running a hybrid workflow, <code>-bucket-dir</code> and <code>-work-dir</code> should be used to define separate work directories for remote tasks and local tasks, respectively.</p> <p>For example, apply the AWS Batch configuration only to a subset of processes in your workflow. You can try the following:</p> nextflow.config<pre><code>process {\n    executor = 'slurm' // (1)!\n    queue = 'short' // (2)!\n\n    withLabel: bigTask {  // (3)!\n        executor = 'awsbatch' // (4)!\n        queue = 'my-batch-queue' // (5)!\n        container = 'my/image:tag' // (6)!\n    }\n}\n\naws {\n    region = 'eu-west-1' // (7)!\n}\n</code></pre> <ol> <li>Set <code>slurm</code> as the default executor</li> <li>Set the queue for the SLURM cluster</li> <li>Setting of process(es) with the label <code>bigTask</code></li> <li>Set <code>awsbatch</code> as the executor for the process(es) with the <code>bigTask</code> label</li> <li>Set the queue for the process(es) with the <code>bigTask</code> label</li> <li>Set the container image to deploy for the process(es) with the <code>bigTask</code> label</li> <li>Define the region for Batch execution</li> </ol> <p>The workflow can then be executed with:</p> <pre><code>nextflow run &lt;script&gt; -bucket-dir 's3://my-bucket' -work-dir /path/to/scratch/dir\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to use a custom job definition</li> <li>How to use a custom image</li> <li>How to use a launch template</li> <li>How to use hybrid deployments</li> </ol>"},{"location":"archive/basic_training/groovy/","title":"Groovy basic structures and idioms","text":"<p>Nextflow is a domain specific language (DSL) implemented on top of the Groovy programming language, which in turn is a super-set of the Java programming language. This means that Nextflow can run any Groovy or Java code.</p> <p>You have already been using some Groovy code in the previous sections, but now it's time to learn more about it.</p>"},{"location":"archive/basic_training/groovy/#printing-values","title":"Printing values","text":"<p>To print something is as easy as using one of the <code>print</code> or <code>println</code> methods.</p> snippet.nf<pre><code>println(\"Hello, World!\")\n</code></pre> <p>The only difference between the two is that the <code>println</code> method implicitly appends a new line character to the printed string.</p> <p>Tip</p> <p>Parentheses for function invocations are optional. Therefore, the following syntax is also valid:</p> snippet.nf<pre><code>println \"Hello, World!\"\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to print a string to the console</li> </ol>"},{"location":"archive/basic_training/groovy/#comments","title":"Comments","text":"<p>Comments use the same syntax as C-family programming languages:</p> snippet.nf<pre><code>// comment a single line\n\n/*\n    a comment spanning\n    multiple lines\n*/\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to write comments in Groovy</li> </ol>"},{"location":"archive/basic_training/groovy/#variables","title":"Variables","text":"<p>To define a variable, simply assign a value to it:</p> snippet.nf<pre><code>x = 1\nprintln x\n\nx = new java.util.Date()\nprintln x\n\nx = -3.1499392\nprintln x\n\nx = false\nprintln x\n\nx = \"Hi\"\nprintln x\n</code></pre> <p>Local variables are defined using the <code>def</code> keyword:</p> snippet.nf<pre><code>def x = 'foo'\n</code></pre> <p>The <code>def</code> should be always used when defining variables local to a function or a closure.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to define variables in Groovy</li> </ol>"},{"location":"archive/basic_training/groovy/#lists","title":"Lists","text":"<p>A List object can be defined by placing the list items in square brackets:</p> snippet.nf<pre><code>list = [10, 20, 30, 40]\n\nprintln list\n</code></pre> <p>You can access a given item in the list with square-bracket notation (indexes start at <code>0</code>) or using the <code>get</code> method:</p> snippet.nf<pre><code>list = [10, 20, 30, 40]\n\nprintln list[0]\nprintln list.get(0)\n</code></pre> <p>In order to get the length of a list you can use the <code>size</code> method:</p> snippet.nf<pre><code>list = [10, 20, 30, 40]\n\nprintln list.size()\n</code></pre> <p>You can use the <code>assert</code> keyword to test if a condition is true (similar to an <code>if</code> function).</p> <p>Here, Groovy will print nothing if it is correct, else it will raise an AssertionError message.</p> snippet.nf<pre><code>list = [10, 20, 30, 40]\n\nassert list[0] == 10\n</code></pre> <p>Exercise</p> <p>This assertion should be correct, try changing it to an incorrect one.</p> snippet.nf<pre><code>list = [10, 20, 30, 40]\n\nassert list[0] == 10\n</code></pre> Solution <p>Your solution could look something similar to this:</p> snippet.nf<pre><code>list = [10, 20, 30, 40]\n\nassert list[0] == 11\n</code></pre> <p>You should see an error message similar to this:</p> Output<pre><code>ERROR ~ assert list[0] == 11\n   |   |\n   |   10\n   [10, 20, 30, 40]\n</code></pre> <p>Lists can also be indexed with negative indexes and reversed ranges.</p> snippet.nf<pre><code>list = [0, 1, 2]\nassert list[-1] == 2\nassert list[-1..0] == list.reverse()\n</code></pre> <p>Info</p> <p>In the last assert line you are referencing the initial list and converting this with a \"shorthand\" range (<code>..</code>), to run from the -1th element (2) to the 0th element (0).</p> <p>List objects implement all methods provided by the java.util.List interface, plus the extension methods provided by Groovy.</p> snippet.nf<pre><code>assert [1, 2, 3] &lt;&lt; 1 == [1, 2, 3, 1]\nassert [1, 2, 3] + [1] == [1, 2, 3, 1]\nassert [1, 2, 3, 1] - [1] == [2, 3]\nassert [1, 2, 3] * 2 == [1, 2, 3, 1, 2, 3]\nassert [1, [2, 3]].flatten() == [1, 2, 3]\nassert [1, 2, 3].reverse() == [3, 2, 1]\nassert [1, 2, 3].collect { it + 3 } == [4, 5, 6]\nassert [1, 2, 3, 1].unique().size() == 3\nassert [1, 2, 3, 1].count(1) == 2\nassert [1, 2, 3, 4].min() == 1\nassert [1, 2, 3, 4].max() == 4\nassert [1, 2, 3, 4].sum() == 10\nassert [4, 2, 1, 3].sort() == [1, 2, 3, 4]\nassert [4, 2, 1, 3].find { it % 2 == 0 } == 4\nassert [4, 2, 1, 3].findAll { it % 2 == 0 } == [4, 2]\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to define a list in Groovy</li> <li>How to access a list item</li> <li>How to apply methods</li> <li>How to use the <code>assert</code> keyword</li> </ol>"},{"location":"archive/basic_training/groovy/#maps","title":"Maps","text":"<p>Maps are like lists that have an arbitrary key instead of an integer. Therefore, the syntax is very much aligned.</p> snippet.nf<pre><code>map = [a: 0, b: 1, c: 2]\n</code></pre> <p>Maps can be accessed in a conventional square-bracket syntax or as if the key was a property of the map.</p> <p>Click the  icons in the code for explanations.</p> snippet.nf<pre><code>map = [a: 0, b: 1, c: 2]\n\nassert map['a'] == 0 // (1)!\nassert map.b == 1 // (2)!\nassert map.get('c') == 2 // (3)!\n</code></pre> <ol> <li>Using square brackets.</li> <li>Using dot notation.</li> <li>Using the <code>get</code> method.</li> </ol> <p>To add data or to modify a map, the syntax is similar to adding values to a list:</p> snippet.nf<pre><code>map = [a: 0, b: 1, c: 2]\n\nmap['a'] = 'x' // (1)!\nmap.b = 'y' // (2)!\nmap.put('c', 'z') // (3)!\nassert map == [a: 'x', b: 'y', c: 'z']\n</code></pre> <ol> <li>Using square brackets.</li> <li>Using dot notation.</li> <li>Using the put method.</li> </ol> <p>Map objects implement all methods provided by the java.util.Map interface, plus the extension methods provided by Groovy.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to define a map in Groovy</li> <li>How to access and update maps</li> </ol>"},{"location":"archive/basic_training/groovy/#string-interpolation","title":"String interpolation","text":"<p>String literals can be defined by enclosing them with either single- ('') or double- (\"\") quotation marks.</p> snippet.nf<pre><code>foxtype = 'quick'\nfoxcolor = ['b', 'r', 'o', 'w', 'n']\nprintln \"The $foxtype ${foxcolor.join()} fox\"\n\nx = 'Hello'\ny = 'World'\nprintln '$x $y'\n</code></pre> Output<pre><code>The quick brown fox\n$x $y\n</code></pre> <p>Info</p> <p>Note the different use of <code>$</code> and <code>${..}</code> syntax to interpolate value expressions in a string literal. The <code>$x</code> variable was not expanded, as it was enclosed by single quotes.</p> <p>Exercise</p> <p>Modify the script above to print <code>Hello World</code> instead of <code>$x $y</code>.</p> Solution <p>Modify <code>println '$x $y'</code> to <code>println \"$x $y\"</code>.</p> snippet.nf<pre><code>foxtype = 'quick'\nfoxcolor = ['b', 'r', 'o', 'w', 'n']\nprintln \"The $foxtype ${foxcolor.join()} fox\"\n\nx = 'Hello'\ny = 'World'\nprintln \"$x $y\"\n</code></pre> <p>Finally, string literals can also be defined using the <code>/</code> character as a delimiter. They are known as slashy strings and are useful for defining regular expressions and patterns, as there is no need to escape backslashes. As with double-quote strings they allow to interpolate variables prefixed with a <code>$</code> character.</p> <p>See the difference below:</p> snippet.nf<pre><code>x = /tic\\tac\\toe/\ny = 'tic\\tac\\toe'\nz = \"tic\\tac\\toe\"\n\nprintln x\nprintln y\n</code></pre> Output<pre><code>tic\\tac\\toe\ntic    ac    oe\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to define string literals in Groovy</li> <li>How to interpolate variables in string literals</li> </ol>"},{"location":"archive/basic_training/groovy/#multi-line-strings","title":"Multi-line strings","text":"<p>A block of text that spans multiple lines can be defined by delimiting it with triple single or double quotes:</p> snippet.nf<pre><code>text = \"\"\"\n    Hello there James.\n    How are you today?\n    \"\"\"\nprintln text\n</code></pre> Output<pre><code>Hello there James.\nHow are you today?\n</code></pre> <p>Finally, multi-line strings can also be defined with slashy strings. For example:</p> snippet.nf<pre><code>text = /\n    This is a multi-line\n    slashy string!\n    It's cool, isn't it?!\n    /\nprintln text\n</code></pre> Output<pre><code>This is a multi-line\nslashy string!\nIt's cool, isn't it?!\n</code></pre> <p>Info</p> <p>Like before, multi-line strings inside double quotes and slash characters support variable interpolation, while single-quoted multi-line strings do not.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to define multi-line strings in Groovy</li> </ol>"},{"location":"archive/basic_training/groovy/#if-statement","title":"If statement","text":"<p>The <code>if</code> statement uses the same syntax common in other programming languages, such as Java, C, and JavaScript.</p> <pre><code>if (&lt; boolean expression &gt;) {\n    // true branch\n}\nelse {\n    // false branch\n}\n</code></pre> <p>The <code>else</code> branch is optional. Also, the curly brackets are optional when the branch defines just a single statement.</p> snippet.nf<pre><code>x = 11\nif (x &gt; 10)\n    println 'Hello'\n</code></pre> Output<pre><code>Hello\n</code></pre> <p>Tip</p> <p><code>null</code>, empty strings, and empty collections are evaluated to <code>false</code>.</p> <p>Therefore a statement like:</p> snippet.nf<pre><code>list = [1, 2, 3]\nif (list != null &amp;&amp; list.size() &gt; 0) {\n    println list\n}\nelse {\n    println 'The list is empty'\n}\n</code></pre> <p>Can be written as:</p> snippet.nf<pre><code>list = [1, 2, 3]\nif (list)\n    println list\nelse\n    println 'The list is empty'\n</code></pre> <p>See the Groovy-Truth for further details.</p> <p>Tip</p> <p>In some cases it can be useful to replace the <code>if</code> statement with a ternary expression (aka a conditional expression):</p> snippet.nf<pre><code>println list ? list : 'The list is empty'\n</code></pre> <p>The previous statement can be further simplified using the Elvis operator:</p> snippet.nf<pre><code>println list ?: 'The list is empty'\n</code></pre> <p>Exercise</p> <p>Write an if statement that prints <code>Hello</code> if the variable <code>x</code> is greater than 10 and <code>Goodbye</code> if it is less than 10.</p> Solution <p>Your solution could look something similar to this:</p> snippet.nf<pre><code>x = 11\n\nif (x &gt; 10)\n    println 'Hello'\nelse\n    println 'Goodbye'\n</code></pre> <p>Or this:</p> snippet.nf<pre><code>x = 11\n\nprintln x &gt; 10 ? 'Hello' : 'Goodbye'\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to define an <code>if</code> statement in Groovy</li> <li>How to use the ternary operator</li> <li>How to use the Elvis operator</li> </ol>"},{"location":"archive/basic_training/groovy/#for-statement","title":"For statement","text":"<p>The classical <code>for</code> loop syntax is supported:</p> snippet.nf<pre><code>for (int i = 0; i &lt; 3; i++) {\n    println(\"Hello World $i\")\n}\n</code></pre> <p>Iteration over list objects is also possible using the syntax below:</p> snippet.nf<pre><code>list = ['a', 'b', 'c']\n\nfor (String elem : list) {\n    println elem\n}\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to define a <code>for</code> loop in Groovy</li> </ol>"},{"location":"archive/basic_training/groovy/#functions","title":"Functions","text":"<p>It is possible to define a custom function into a script:</p> snippet.nf<pre><code>def fib(int n) {\n    return n &lt; 2 ? 1 : fib(n - 1) + fib(n - 2)\n}\n\nassert fib(10)==89\n</code></pre> <p>A function can take multiple arguments separating them with a comma.</p> <p>The <code>return</code> keyword can be omitted and the function implicitly returns the value of the last evaluated expression. Also, explicit types can be omitted, though not recommended:</p> snippet.nf<pre><code>def fact(n) {\n    n &gt; 1 ? n * fact(n - 1) : 1\n}\n\nassert fact(5) == 120\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to define a function in Groovy</li> </ol>"},{"location":"archive/basic_training/groovy/#closures","title":"Closures","text":"<p>Closures are the Swiss army knife of Nextflow/Groovy programming. In a nutshell, a closure is a block of code that can be passed as an argument to a function.</p> <p>A closure can also be used to define an anonymous function.</p> <p>More formally, a closure allows the definition of functions as first-class objects.</p> snippet.nf<pre><code>square = { it * it }\n</code></pre> <p>The curly brackets around the expression <code>it * it</code> tells the script interpreter to treat this expression as code. The <code>it</code> identifier is an implicit variable that represents the value that is passed to the function when it is invoked.</p> <p>Once compiled, the function object is assigned to the variable <code>square</code> as any other variable assignment shown previously.</p> <p>To invoke the closure execution use the special method <code>call</code> or just use the round parentheses to specify the closure parameter(s):</p> snippet.nf<pre><code>assert square.call(5) == 25\nassert square(9) == 81\n</code></pre> <p>As is, this may not seem interesting, but you can now pass the <code>square</code> function as an argument to other functions or methods. Some built-in functions take a function like this as an argument. One example is the <code>collect</code> method on lists:</p> snippet.nf<pre><code>x = [1, 2, 3, 4].collect(square)\nprintln x\n</code></pre> Output<pre><code>[1, 4, 9, 16]\n</code></pre> <p>By default, closures take a single parameter called <code>it</code>.</p> <p>To give it a different name use the <code>-&gt;</code> syntax. For example:</p> snippet.nf<pre><code>square = { num -&gt; num * num }\n</code></pre> <p>It\u2019s also possible to define closures with multiple, custom-named parameters.</p> <p>For example, when the method <code>each()</code> is applied to a map it can take a closure with two arguments, to which it passes the key-value pair for each entry in the <code>map</code> object. For example:</p> snippet.nf<pre><code>printMap = { a, b -&gt; println \"$a with value $b\" } // (1)!\nvalues = [\"Yue\": \"Wu\", \"Mark\": \"Williams\", \"Sudha\": \"Kumari\"] // (2)!\nvalues.each(printMap) // (3)!\n</code></pre> <ol> <li>Closure object that prints the key-value pair.</li> <li>Defines a map object with three entries.</li> <li>Invokes the <code>each</code> method passing the closure object.</li> </ol> Output<pre><code>Yue with value Wu\nMark with value Williams\nSudha with value Kumari\n</code></pre> <p>A closure has two other important features.</p> <p>First, it can access and modify variables in the scope where it is defined.</p> <p>Second, a closure can be defined in an anonymous manner, meaning that it is not given a name, and is only defined in the place where it needs to be used.</p> <p>As an example showing both these features, see the following code fragment:</p> snippet.nf<pre><code>result = 0 // (1)!\nvalues = [\"China\": 1, \"India\": 2, \"USA\": 3] // (2)!\nvalues.keySet().each { result += values[it] } // (3)!\nprintln result\n</code></pre> <ol> <li>Defines a global variable.</li> <li>Defines a map object.</li> <li>Invokes the <code>each</code> method passing the closure object which modifies the <code>result</code> variable.</li> </ol> Output<pre><code>6\n</code></pre> <p>Learn more about closures in the Groovy documentation.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to define a closure in Groovy</li> <li>How to invoke a closure</li> </ol>"},{"location":"archive/basic_training/groovy/#more-resources","title":"More resources","text":"<p>The complete Groovy language documentation is available at this link.</p> <p>A great resource to master Apache Groovy syntax is the book: Groovy in Action.</p>"},{"location":"archive/basic_training/intro/","title":"Basic concepts","text":"<p>Nextflow is a workflow orchestration engine and domain-specific language (DSL) that makes it easy to write data-intensive computational workflows.</p> <p>It is designed around the idea that the Linux platform is the lingua franca of data science. Linux provides many simple but powerful command-line and scripting tools that, when chained together, facilitate complex data manipulations.</p> <p>Nextflow extends this approach, adding the ability to define complex program interactions and a high-level parallel computational environment, based on the dataflow programming model. Nextflow\u2019s core features are:</p> <ul> <li>Workflow portability and reproducibility</li> <li>Scalability of parallelization and deployment</li> <li>Integration of existing tools, systems, and industry standards</li> </ul>"},{"location":"archive/basic_training/intro/#processes-and-channels","title":"Processes and Channels","text":"<p>In practice, a Nextflow workflow is made by joining together different processes. Each process can be written in any scripting language that can be executed by the Linux platform (Bash, Perl, Ruby, Python, etc.).</p> <p>Processes are executed independently and are isolated from each other, i.e., they do not share a common (writable) state. The only way they can communicate is via asynchronous first-in, first-out (FIFO) queues, called <code>channels</code>. In other words, every input and output of a process is represented as a channel. The interaction between these processes, and ultimately the workflow execution flow itself, is implicitly defined by these <code>input</code> and <code>output</code> declarations.</p>"},{"location":"archive/basic_training/intro/#execution-abstraction","title":"Execution abstraction","text":"<p>While a process defines what command or <code>script</code> has to be executed, the executor determines how that <code>script</code> is run in the target platform.</p> <p>If not otherwise specified, processes are executed on the local computer. The local executor is very useful for workflow development and testing purposes, however, for real-world computational workflows a high-performance computing (HPC) or cloud platform is often required.</p> <p>In other words, Nextflow provides an abstraction between the workflow\u2019s functional logic and the underlying execution system (or runtime). Thus, it is possible to write a workflow that runs seamlessly on your computer, a cluster, or the cloud, without being modified. You simply define the target execution platform in the configuration file.</p> <p></p>"},{"location":"archive/basic_training/intro/#scripting-language","title":"Scripting language","text":"<p>Nextflow implements a declarative DSL that simplifies the writing of complex data analysis workflows as an extension of a general-purpose programming language.</p> <p>This approach makes Nextflow flexible \u2014 it provides the benefits of a concise DSL for the handling of recurrent use cases with ease and the flexibility and power of a general-purpose programming language to handle corner cases in the same computing environment. This would be difficult to implement using a purely declarative approach.</p> <p>In practical terms, Nextflow scripting is an extension of the Groovy programming language which, in turn, is a super-set of the Java programming language. Groovy can be thought of as \"Python for Java\", in that it simplifies the writing of code and is more approachable.</p>"},{"location":"archive/basic_training/intro/#your-first-script","title":"Your first script","text":"<p>Here you will execute your first Nextflow script (<code>hello.nf</code>), which we will go through line-by-line.</p> <p>In this toy example, the script takes an input string (provided with a parameter called <code>params.greeting</code>) and splits it into chunks of six characters in the first process. The second process then converts the characters to upper case. The result is finally displayed on-screen.</p>"},{"location":"archive/basic_training/intro/#nextflow-code","title":"Nextflow code","text":"<p>Info</p> <p>Click the  icons in the code for explanations.</p> nf-training/hello.nf<pre><code>#!/usr/bin/env nextflow\n// (1)!\nparams.greeting = 'Hello world!' // (2)!\ngreeting_ch = Channel.of(params.greeting) // (3)!\n\nprocess SPLITLETTERS { // (4)!\n    input: // (5)!\n    val x // (6)!\n\n    output: // (7)!\n    path 'chunk_*' // (8)!\n\n    script: // (9)!\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n} // (10)!\n\nprocess CONVERTTOUPPER { // (11)!\n    input: // (12)!\n    path y // (13)!\n\n    output: // (14)!\n    stdout // (15)!\n\n    script: // (16)!\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]'\n    \"\"\"\n} // (17)!\n\nworkflow { // (18)!\n    letters_ch = SPLITLETTERS(greeting_ch) // (19)!\n    results_ch = CONVERTTOUPPER(letters_ch.flatten()) // (20)!\n    results_ch.view { it } // (21)!\n} // (22)!\n</code></pre> <ol> <li>The code begins with a shebang, which declares Nextflow as the interpreter. This is optional, but recommended.</li> <li>Declares a parameter <code>greeting</code> that is initialized with the value 'Hello world!'.</li> <li>Initializes a <code>channel</code> labeled <code>greeting_ch</code>, which contains the value from <code>params.greeting</code>. Channels are the input type for processes in Nextflow.</li> <li>Begins the first process block, defined as <code>SPLITLETTERS</code>.</li> <li>Input declaration for the <code>SPLITLETTERS</code> process. Inputs can be values (<code>val</code>), files or paths (<code>path</code>), or other qualifiers (see here).</li> <li>Tells the process to expect an input value (<code>val</code>), that we assign to the variable 'x'.</li> <li>Output declaration for the <code>SPLITLETTERS</code> process.</li> <li>Tells the process to expect an output file(s) (<code>path</code>), with a filename starting with 'chunk_', as output from the script. The process sends the output as a channel.</li> <li>Three double quotes start and end the code block to execute this process.    Inside is the code to execute \u2014 printing the <code>input</code> value 'x' (called using the dollar symbol [$] prefix), splitting the string into chunks with a length of 6 characters (\"Hello \" and \"world!\"), and saving each to a separate file (chunk_aa and chunk_ab).</li> <li>End of the first process block.</li> <li>Beginning of the second process block, defined as <code>CONVERTTOUPPER</code>.</li> <li>Input declaration for the <code>CONVERTTOUPPER</code> process.</li> <li>Tells the process to expect an <code>input</code> file (<code>path</code>; e.g. chunk_aa), that we assign to the variable 'y'.</li> <li>Output declaration for the <code>CONVERTTOUPPER</code> process.</li> <li>Tells the process to expect output as standard output (<code>stdout</code>) and sends this output as a channel.</li> <li>Three double quotes start and end the code block to execute this process.     Within the block there is a script to read files (cat) using the '$y' input variable, then pipe to uppercase conversion, outputting to standard output.</li> <li>End of second process block.</li> <li>Start of the workflow scope where each process can be called.</li> <li>Execute the process <code>SPLITLETTERS</code> on the <code>greeting_ch</code> (aka greeting channel), and store the output in the channel <code>letters_ch</code>.</li> <li>Execute the process <code>CONVERTTOUPPER</code> on the letters channel <code>letters_ch</code>, which is flattened using the operator <code>.flatten()</code>. This transforms the input channel in such a way that every item is a separate element. We store the output in the channel <code>results_ch</code>.</li> <li>The final output (in the <code>results_ch</code> channel) is printed to screen using the <code>view</code> operator (appended onto the channel name).</li> <li>End of the workflow scope.</li> </ol> <p>This pipeline takes <code>params.greeting</code>, which defaults to the string <code>Hello world!</code>, and splits it into individual words in the <code>SPLITLETTERS</code> process. Each word is written to a separate file, named <code>chunk_aa</code>, <code>chunk_ab</code>, <code>chunk_ac</code>and so on. These files are picked up as the process <code>output</code>.</p> <p>The second process <code>CONVERTTOUPPER</code> takes the output channel from the first process as its input. The use of the operator <code>.flatten()</code> here is to split the <code>SPLITLETTERS</code> output channel element that contains two files into two separate elements to be put through the <code>CONVERTTOUPPER</code>process, else they would be treated as a single element. The <code>CONVERTTOUPPER</code> process thus launches two tasks, one for each element. The bash script uses <code>cat</code> to print the file contents and <code>tr</code> to convert to upper-case. It takes the resulting standard-out as the process output channel.</p>"},{"location":"archive/basic_training/intro/#python-instead-of-bash","title":"Python instead of bash","text":"<p>If you're not completely comfortable with the bash code used in the example, don't worry! You can use whatever programming language you like within Nextflow <code>script</code> blocks. For example, the <code>hello_py.nf</code> file contains the same example but using Python code:</p> nf-training/hello_py.nf<pre><code>    \"\"\"\n    #!/usr/bin/env python\n    x=\"$x\"\n    for i, word in enumerate(x.split()):\n        with open(f\"chunk_{i}\", \"w\") as f:\n</code></pre> nf-training/hello_py.nf<pre><code>    script:\n    \"\"\"\n    #!/usr/bin/env python\n</code></pre> <p>Note that the <code>$x</code> and <code>$y</code> variables are interpolated by Nextflow, so the resulting Python scripts will have fixed strings here (<code>x=\"Hello world!\"</code>). Check the <code>hello_py.nf</code> file for the full workflow script code.</p>"},{"location":"archive/basic_training/intro/#in-practice","title":"In practice","text":"<p>Now copy the above example into your favorite text editor and save it to a file named <code>hello.nf</code>.</p> <p>Warning</p> <p>For the GitHub Codespaces tutorial, make sure you are in the folder called <code>nf-training</code></p> <p>Execute the script by entering the following command in your terminal:</p> <pre><code>nextflow run hello.nf\n</code></pre> <p>The output will look similar to the text shown below:</p> Output<pre><code>N E X T F L O W  ~  version 23.10.1\nLaunching hello.nf [cheeky_keller] DSL2 - revision: 197a0e289a\nexecutor &gt;  local (3)\n[31/52c31e] process &gt; SPLITLETTERS (1)   [100%] 1 of 1 \u2714\n[37/b9332f] process &gt; CONVERTTOUPPER (2) [100%] 2 of 2 \u2714\nHELLO\nWORLD!\n</code></pre> <ol> <li>The version of Nextflow that was executed.</li> <li>The script and version names.</li> <li>The executor used (in the above case: local).</li> <li>The first process is executed once, which means there is one task. The line starts with a unique hexadecimal value (see TIP below), and ends with the percentage and other task completion information.</li> <li>The second process is executed twice (once for <code>chunk_aa</code> and once for <code>chunk_ab</code>), which means two tasks.</li> <li>The result string from <code>stdout</code> is printed.</li> </ol> <p>Info</p> <p>The hexadecimal numbers, like <code>31/52c31e</code>, identify the unique process execution, that we call a task. These numbers are also the prefix of the directories where each task is executed. You can inspect the files produced by changing to the directory <code>$PWD/work</code> and using these numbers to find the task-specific execution path.</p> <p>Tip</p> <p>The second process runs twice, executing in two different work directories for each input file. The ANSI log output from Nextflow dynamically refreshes as the workflow runs; in the previous example the work directory <code>[37/b9332f]</code> is the second of the two directories that were processed (overwriting the log with the first). To print all the relevant paths to the screen, disable the ANSI log output using the <code>-ansi-log</code> flag (e.g., <code>nextflow run hello.nf -ansi-log false</code>).</p> <p>It\u2019s worth noting that the process <code>CONVERTTOUPPER</code> is executed in parallel, so there\u2019s no guarantee that the instance processing the first split (the chunk Hello ) will be executed before the one processing the second split (the chunk world!).</p> <p>Thus, it could be that your final result will be printed out in a different order:</p> Output<pre><code>WORLD!\nHELLO\n</code></pre>"},{"location":"archive/basic_training/intro/#modify-and-resume","title":"Modify and resume","text":"<p>Nextflow keeps track of all the processes executed in your workflow. If you modify some parts of your script, only the processes that are changed will be re-executed. The execution of the processes that are not changed will be skipped and the cached result will be used instead.</p> <p>This allows for testing or modifying part of your workflow without having to re-execute it from scratch.</p> <p>For the sake of this tutorial, modify the <code>CONVERTTOUPPER</code> process in the previous example, replacing the process script with the string <code>rev $y</code>, so that the process looks like this:</p> nf-training/hello.nf<pre><code>process CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    rev $y\n    \"\"\"\n}\n</code></pre> <p>Then save the file with the same name, and execute it by adding the <code>-resume</code> option to the command line:</p> <pre><code>$ nextflow run hello.nf -resume\n</code></pre> Output<pre><code>N E X T F L O W  ~  version 23.10.1\nLaunching `hello.nf` [zen_colden] DSL2 - revision: 0676c711e8\nexecutor &gt;  local (2)\n[31/52c31e] process &gt; SPLITLETTERS (1)   [100%] 1 of 1, cached: 1 \u2714\n[0f/8175a7] process &gt; CONVERTTOUPPER (1) [100%] 2 of 2 \u2714\n!dlrow\n olleH\n</code></pre> <p>You will see that the execution of the process <code>SPLITLETTERS</code> is skipped (the task ID is the same as in the first output) \u2014 its results are retrieved from the cache. The second process is executed as expected, printing the reversed strings.</p> <p>Info</p> <p>The workflow results are cached by default in the directory <code>$PWD/work</code>. Depending on your script, this folder can take up a lot of disk space. If you are sure you won\u2019t need to resume your workflow execution, clean this folder periodically.</p>"},{"location":"archive/basic_training/intro/#workflow-parameters","title":"Workflow parameters","text":"<p>Workflow parameters are simply declared by prepending the prefix <code>params</code> to a variable name, separated by a dot character. Their value can be specified on the command line by prefixing the parameter name with a double dash character, i.e. <code>--paramName</code>.</p> <p>Now, let\u2019s try to execute the previous example specifying a different input string parameter, as shown below:</p> <pre><code>nextflow run hello.nf --greeting 'Bonjour le monde!'\n</code></pre> <p>The string specified on the command line will override the default value of the parameter. The output will look like this:</p> Output<pre><code>N E X T F L O W  ~  version 23.10.1\nLaunching `hello.nf` [goofy_kare] DSL2 - revision: 0676c711e8\nexecutor &gt;  local (4)\n[8b/7c7d13] process &gt; SPLITLETTERS (1)   [100%] 1 of 1 \u2714\n[58/3b2df0] process &gt; CONVERTTOUPPER (3) [100%] 3 of 3 \u2714\nuojnoB\nm el r\n!edno\n</code></pre>"},{"location":"archive/basic_training/intro/#in-dag-like-format","title":"In DAG-like format","text":"<p>To better understand how Nextflow is dealing with the data in this workflow, below is a DAG-like figure to visualize all the inputs, outputs, channels and processes:</p> <p></p>"},{"location":"archive/basic_training/modules/","title":"Modularization","text":"<p>The definition of module libraries simplifies the writing of complex data analysis workflows and makes re-use of processes much easier.</p> <p>Using the <code>hello.nf</code> example from earlier, you can convert the workflow\u2019s processes into modules, then call them within the workflow scope.</p>"},{"location":"archive/basic_training/modules/#modules","title":"Modules","text":"<p>Nextflow DSL2 allows for the definition of stand-alone module scripts that can be included and shared across multiple workflows. Each module can contain its own <code>process</code> or <code>workflow</code> definition.</p>"},{"location":"archive/basic_training/modules/#importing-modules","title":"Importing modules","text":"<p>Components defined in the module script can be imported into other Nextflow scripts using the <code>include</code> statement. This allows you to store these components in one or more file(s) that they can be re-used in multiple workflows.</p> <p>Using the <code>hello.nf</code> example, you can achieve this by:</p> <ul> <li>Creating a file called <code>modules.nf</code> in the top-level directory.</li> <li>Copying and pasting the two process definitions for <code>SPLITLETTERS</code> and <code>CONVERTTOUPPER</code> into <code>modules.nf</code>.</li> <li>Removing the <code>process</code> definitions in the <code>hello.nf</code> script.</li> <li>Importing the processes from <code>modules.nf</code> within the <code>hello.nf</code> script anywhere above the <code>workflow</code> definition:</li> </ul> hello.nf<pre><code>include { SPLITLETTERS   } from './modules.nf'\ninclude { CONVERTTOUPPER } from './modules.nf'\n</code></pre> <p>Note</p> <p>In general, you would use relative paths to define the location of the module scripts using the <code>./</code> prefix.</p> <p>Exercise</p> <p>Create a <code>modules.nf</code> file with the <code>SPLITLETTERS</code> and <code>CONVERTTOUPPER</code> processes from <code>hello.nf</code>. Then remove these processes from <code>hello.nf</code> and include them in the workflow using the <code>include</code> definitions shown above.</p> Solution <p>The <code>hello.nf</code> script should look similar like this:</p> hello.nf<pre><code>#!/usr/bin/env nextflow\n\nparams.greeting  = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\n\ninclude { SPLITLETTERS   } from './modules.nf'\ninclude { CONVERTTOUPPER } from './modules.nf'\n\nworkflow {\n    letters_ch = SPLITLETTERS(greeting_ch)\n    results_ch = CONVERTTOUPPER(letters_ch.flatten())\n    results_ch.view { it }\n}\n</code></pre> <p>Your <code>./modules.nf</code> file should look similar to this:</p> modules.nf<pre><code>process SPLITLETTERS {\n    input:\n    val x\n\n    output:\n    path 'chunk_*'\n\n    script:\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]'\n    \"\"\"\n}\n</code></pre>"},{"location":"archive/basic_training/modules/#multiple-imports","title":"Multiple imports","text":"<p>If a Nextflow module script contains multiple <code>process</code> definitions they can also be imported using a single <code>include</code> statement as shown in the example below:</p> hello.nf<pre><code>#!/usr/bin/env nextflow\n\nparams.greeting  = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\n\ninclude { SPLITLETTERS; CONVERTTOUPPER } from './modules.nf'\n\nworkflow {\n    letters_ch = SPLITLETTERS(greeting_ch)\n    results_ch = CONVERTTOUPPER(letters_ch.flatten())\n    results_ch.view { it }\n}\n</code></pre>"},{"location":"archive/basic_training/modules/#module-aliases","title":"Module aliases","text":"<p>When including a module component it is possible to specify a name alias using the <code>as</code> declaration. This allows the inclusion and the invocation of the same component multiple times using different names:</p> hello.nf<pre><code>#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\n\ninclude { SPLITLETTERS as SPLITLETTERS_one } from './modules.nf'\ninclude { SPLITLETTERS as SPLITLETTERS_two } from './modules.nf'\n\ninclude { CONVERTTOUPPER as CONVERTTOUPPER_one } from './modules.nf'\ninclude { CONVERTTOUPPER as CONVERTTOUPPER_two } from './modules.nf'\n\nworkflow {\n    letters_ch1 = SPLITLETTERS_one(greeting_ch)\n    results_ch1 = CONVERTTOUPPER_one(letters_ch1.flatten())\n    results_ch1.view { it }\n\n    letters_ch2 = SPLITLETTERS_two(greeting_ch)\n    results_ch2 = CONVERTTOUPPER_two(letters_ch2.flatten())\n    results_ch2.view { it }\n}\n</code></pre> <p>Note how the <code>SPLITLETTERS</code> and <code>CONVERTTOUPPER</code> processes are imported twice, each time with a different alias, and how these aliases are used to invoke the processes:</p> Output<pre><code>N E X T F L O W  ~  version 23.10.1\nLaunching `hello.nf` [crazy_shirley] DSL2 - revision: 99f6b6e40e\nexecutor &gt;  local (6)\n[2b/ec0395] process &gt; SPLITLETTERS_one (1)   [100%] 1 of 1 \u2714\n[d7/be3b77] process &gt; CONVERTTOUPPER_one (1) [100%] 2 of 2 \u2714\n[04/9ffc05] process &gt; SPLITLETTERS_two (1)   [100%] 1 of 1 \u2714\n[d9/91b029] process &gt; CONVERTTOUPPER_two (2) [100%] 2 of 2 \u2714\nWORLD!\nHELLO\nHELLO\nWORLD!\n</code></pre> <p>Tip</p> <p>You can store each process in separate files within separate sub-folders or combined in one big file (both are valid). You can find examples of this on public repos such as the Seqera RNA-Seq tutorial or within nf-core workflows, such as nf-core/rnaseq.</p>"},{"location":"archive/basic_training/modules/#output-definition","title":"Output definition","text":"<p>Nextflow allows the use of alternative output definitions within workflows to simplify your code.</p> <p>In the previous example (<code>hello.nf</code>), you defined the channel names to specify the input to the next process:</p> hello.nf<pre><code>workflow  {\n    greeting_ch = Channel.of(params.greeting)\n    letters_ch = SPLITLETTERS(greeting_ch)\n    results_ch = CONVERTTOUPPER(letters_ch.flatten())\n    results_ch.view { it }\n}\n</code></pre> <p>You can also explicitly define the output of one channel to another using the <code>.out</code> attribute, removing the channel definitions completely:</p> hello.nf<pre><code>workflow  {\n    greeting_ch = Channel.of(params.greeting)\n    SPLITLETTERS(greeting_ch)\n    CONVERTTOUPPER(SPLITLETTERS.out.flatten())\n    CONVERTTOUPPER.out.view()\n}\n</code></pre> <p>If a process defines two or more output channels, each channel can be accessed by indexing the <code>.out</code> attribute, e.g., <code>.out[0]</code>, <code>.out[1]</code>, etc. In the example below, the <code>[0]'th</code> output is shown:</p> hello.nf<pre><code>workflow  {\n    greeting_ch = Channel.of(params.greeting)\n    SPLITLETTERS(greeting_ch)\n    CONVERTTOUPPER(SPLITLETTERS.out.flatten())\n    CONVERTTOUPPER.out[0].view()\n}\n</code></pre> <p>Alternatively, the process <code>output</code> definition allows the use of the <code>emit</code> statement to define a named identifier that can be used to reference the channel in the external scope.</p> <p>In the example below, an <code>emit</code> statement has been added to the <code>CONVERTTOUPPER</code> process and is then used in the workflow definition:</p> modules.nf<pre><code>process SPLITLETTERS {\n    input:\n    val x\n\n    output:\n    path 'chunk_*'\n\n    script:\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout emit: upper\n\n    script:\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]'\n    \"\"\"\n}\n\nworkflow {\n    greeting_ch = Channel.of(params.greeting)\n    SPLITLETTERS(greeting_ch)\n    CONVERTTOUPPER(SPLITLETTERS.out.flatten())\n    CONVERTTOUPPER.out.upper.view { it }\n}\n</code></pre>"},{"location":"archive/basic_training/modules/#using-piped-outputs","title":"Using piped outputs","text":"<p>Another way to deal with outputs in the workflow scope is to use pipes <code>|</code>.</p> <p>Exercise</p> <p>Try changing the workflow script to the snippet below:</p> <pre><code>workflow {\n    Channel.of(params.greeting) | SPLITLETTERS | flatten | CONVERTTOUPPER | view\n}\n</code></pre> <p>Here, a pipe passes the output as a channel to the next process without the need of applying <code>.out</code> to the process name.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to import modules</li> <li>How to import multiple modules</li> <li>How to use module aliases</li> <li>How to use alternative output definitions</li> <li>How to use piped outputs</li> </ol>"},{"location":"archive/basic_training/modules/#workflow-definition","title":"Workflow definition","text":"<p>The <code>workflow</code> scope allows the definition of components that define the invocation of one or more processes or operators:</p> hello.nf<pre><code>#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\n\ninclude { SPLITLETTERS } from './modules.nf'\ninclude { CONVERTTOUPPER } from './modules.nf'\n\n\nworkflow my_workflow {\n    greeting_ch = Channel.of(params.greeting)\n    SPLITLETTERS(greeting_ch)\n    CONVERTTOUPPER(SPLITLETTERS.out.flatten())\n    CONVERTTOUPPER.out.upper.view { it }\n}\n\nworkflow {\n    my_workflow()\n}\n</code></pre> <p>For example, the snippet above defines a <code>workflow</code> named <code>my_workflow</code>, that is invoked via another <code>workflow</code> definition.</p> <p>Note</p> <p>Make sure that your <code>modules.nf</code> file is the one containing the <code>emit</code> on the <code>CONVERTTOUPPER</code> process.</p> <p>Warning</p> <p>A workflow component can access any variable or parameter defined in the outer scope. In the running example, you can also access <code>params.greeting</code> directly within the <code>workflow</code> definition.</p>"},{"location":"archive/basic_training/modules/#workflow-inputs","title":"Workflow inputs","text":"<p>A <code>workflow</code> component can declare one or more input channels using the <code>take</code> statement. For example:</p> hello.nf<pre><code>#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\n\ninclude { SPLITLETTERS } from './modules.nf'\ninclude { CONVERTTOUPPER } from './modules.nf'\n\nworkflow my_workflow {\n    take:\n    greeting\n\n    main:\n    SPLITLETTERS(greeting)\n    CONVERTTOUPPER(SPLITLETTERS.out.flatten())\n    CONVERTTOUPPER.out.upper.view { it }\n}\n</code></pre> <p>Note</p> <p>When the <code>take</code> statement is used, the <code>workflow</code> definition needs to be declared within the <code>main</code> block.</p> <p>The input for the <code>workflow</code> can then be specified as an argument:</p> hello.nf<pre><code>workflow {\n    my_workflow(Channel.of(params.greeting))\n}\n</code></pre>"},{"location":"archive/basic_training/modules/#workflow-outputs","title":"Workflow outputs","text":"<p>A <code>workflow</code> can declare one or more output channels using the <code>emit</code> statement. For example:</p> hello.nf<pre><code>#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\n\nprocess SPLITLETTERS {\n    input:\n    val x\n\n    output:\n    path 'chunk_*'\n\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout emit: upper\n\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]'\n    \"\"\"\n}\n\nworkflow my_workflow {\n    take:\n    greeting\n\n    main:\n    SPLITLETTERS(greeting)\n    CONVERTTOUPPER(SPLITLETTERS.out.flatten())\n\n    emit:\n    CONVERTTOUPPER.out.upper\n}\n\nworkflow {\n    my_workflow(Channel.of(params.greeting))\n    my_workflow.out.view()\n}\n</code></pre> <p>As a result, you can use the <code>my_workflow.out</code> notation to access the outputs of <code>my_workflow</code> in the invoking <code>workflow</code>.</p> <p>You can also declare named outputs within the <code>emit</code> block.</p> hello.nf<pre><code>#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\n\nprocess SPLITLETTERS {\n    input:\n    val x\n\n    output:\n    path 'chunk_*'\n\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout emit: upper\n\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]'\n    \"\"\"\n}\n\nworkflow my_workflow {\n    take:\n    greeting\n\n    main:\n    SPLITLETTERS(greeting)\n    CONVERTTOUPPER(SPLITLETTERS.out.flatten())\n\n    emit:\n    my_data = CONVERTTOUPPER.out.upper\n}\n\nworkflow {\n    my_workflow(Channel.of(params.greeting))\n    my_workflow.out.my_data.view()\n}\n</code></pre> <p>The result of the above snippet can then be accessed using <code>my_workflow.out.my_data</code>.</p>"},{"location":"archive/basic_training/modules/#calling-named-workflows","title":"Calling named workflows","text":"<p>Within a <code>main.nf</code> script (called <code>hello.nf</code> in our example) you can also have multiple workflows. In which case you may want to call a specific workflow when running the code. For this you could use the entrypoint call <code>-entry &lt;workflow_name&gt;</code>.</p> <p>The following snippet has two named workflows (<code>my_workflow_one</code> and <code>my_workflow_two</code>):</p> hello2.nf<pre><code>#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\n\ninclude { SPLITLETTERS as SPLITLETTERS_one } from './modules.nf'\ninclude { SPLITLETTERS as SPLITLETTERS_two } from './modules.nf'\n\ninclude { CONVERTTOUPPER as CONVERTTOUPPER_one } from './modules.nf'\ninclude { CONVERTTOUPPER as CONVERTTOUPPER_two } from './modules.nf'\n\n\nworkflow my_workflow_one {\n    letters_ch1 = SPLITLETTERS_one(params.greeting)\n    results_ch1 = CONVERTTOUPPER_one(letters_ch1.flatten())\n    results_ch1.view { it }\n}\n\nworkflow my_workflow_two {\n    letters_ch2 = SPLITLETTERS_two(params.greeting)\n    results_ch2 = CONVERTTOUPPER_two(letters_ch2.flatten())\n    results_ch2.view { it }\n}\n\nworkflow {\n    my_workflow_one(Channel.of(params.greeting))\n    my_workflow_two(Channel.of(params.greeting))\n}\n</code></pre> <p>You can choose which workflow to run by using the <code>entry</code> flag:</p> <pre><code>nextflow run hello2.nf -entry my_workflow_one\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to define workflow inputs</li> <li>How to define workflow outputs</li> <li>How to use named workflows</li> </ol>"},{"location":"archive/basic_training/modules/#dsl2-migration-notes","title":"DSL2 migration notes","text":"<p>To view a summary of the changes introduced when Nextflow migrated from DSL1 to DSL2 please refer to the DSL2 migration notes in the official Nextflow documentation.</p>"},{"location":"archive/basic_training/operators/","title":"Operators","text":"<p>Nextflow operators are methods that allow you to manipulate channels. Every operator, with the exception of <code>set</code> and <code>subscribe</code>, produces one or more new channels, allowing you to chain operators to fit your needs.</p> <p>There are seven main groups of operators are described in greater detail within the Nextflow Reference Documentation, linked below:</p> <ol> <li>Filtering operators</li> <li>Transforming operators</li> <li>Splitting operators</li> <li>Combining operators</li> <li>Forking operators</li> <li>Maths operators</li> <li>Other operators</li> </ol>"},{"location":"archive/basic_training/operators/#basic-example","title":"Basic example","text":"<p>The <code>map</code> operator applies a function of your choosing to every item emitted by a channel, and returns the items so obtained as a new channel. The function applied is called the mapping function and is expressed with a closure as shown in the example below:</p> <p>Click the  icons in the code for explanations.</p> snippet.nf<pre><code>nums = Channel.of(1, 2, 3, 4) // (1)!\nsquare = nums.map { it -&gt; it * it } // (2)!\nsquare.view() // (3)!\n</code></pre> <ol> <li>Creates a queue channel emitting four values</li> <li>Creates a new channel, transforming each number into its square</li> <li>Prints the channel content</li> </ol> <p>Operators can also be chained to implement custom behaviors, so the previous snippet can also be written as:</p> snippet.nf<pre><code>Channel\n    .of(1, 2, 3, 4)\n    .map { it -&gt; it * it }\n    .view()\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>The basic features of an operator</li> </ol>"},{"location":"archive/basic_training/operators/#commonly-used-operators","title":"Commonly used operators","text":"<p>Here you will explore some of the most commonly used operators.</p>"},{"location":"archive/basic_training/operators/#view","title":"<code>view()</code>","text":"<p>The <code>view</code> operator prints the items emitted by a channel to the console standard output, appending a new line character to each item. For example:</p> snippet.nf<pre><code>Channel\n    .of('foo', 'bar', 'baz')\n    .view()\n</code></pre> Output<pre><code>foo\nbar\nbaz\n</code></pre> <p>An optional closure parameter can be specified to customize how items are printed. For example:</p> snippet.nf<pre><code>Channel\n    .of('foo', 'bar', 'baz')\n    .view { \"- $it\" }\n</code></pre> Output<pre><code>- foo\n- bar\n- baz\n</code></pre>"},{"location":"archive/basic_training/operators/#map","title":"<code>map()</code>","text":"<p>The <code>map</code> operator applies a function of your choosing to every item emitted by a channel and returns the items obtained as a new channel. The function applied is called the mapping function and is expressed with a closure. In the example below the groovy <code>reverse</code> method has been used to reverse the order of the characters in each string emitted by the channel.</p> snippet.nf<pre><code>Channel\n    .of('hello', 'world')\n    .map { it -&gt; it.reverse() }\n    .view()\n</code></pre> <p>A <code>map</code> can associate a generic tuple to each element and can contain any data. In the example below the groovy <code>size</code> method is used to return the length of each string emitted by the channel.</p> snippet.nf<pre><code>Channel\n    .of('hello', 'world')\n    .map { word -&gt; [word, word.size()] }\n    .view()\n</code></pre> Output<pre><code>[hello, 5]\n[world, 5]\n</code></pre> <p>Exercise</p> <p>Use <code>fromPath</code> to create a channel emitting the fastq files matching the pattern <code>data/ggal/*.fq</code>, then use <code>map</code> to return a pair containing the file name and the file path. Finally, use <code>view</code> to print the resulting channel.</p> <p>Hint</p> <p>You can use the <code>name</code> method to get the file name.</p> Solution <p>Here is one possible solution:</p> snippet.nf<pre><code>Channel\n    .fromPath('data/ggal/*.fq')\n    .map { file -&gt; [file.name, file] }\n    .view()\n</code></pre> <p>Your output should look like this:</p> Output<pre><code>[gut_1.fq, /workspaces/training/nf-training/data/ggal/gut_1.fq]\n[gut_2.fq, /workspaces/training/nf-training/data/ggal/gut_2.fq]\n[liver_1.fq, /workspaces/training/nf-training/data/ggal/liver_1.fq]\n[liver_2.fq, /workspaces/training/nf-training/data/ggal/liver_2.fq]\n[lung_1.fq, /workspaces/training/nf-training/data/ggal/lung_1.fq]\n[lung_2.fq, /workspaces/training/nf-training/data/ggal/lung_2.fq]\n</code></pre>"},{"location":"archive/basic_training/operators/#mix","title":"<code>mix()</code>","text":"<p>The <code>mix</code> operator combines the items emitted by two (or more) channels.</p> snippet.nf<pre><code>my_channel_1 = Channel.of(1, 2, 3)\nmy_channel_2 = Channel.of('a', 'b')\nmy_channel_3 = Channel.of('z')\n\nmy_channel_1\n    .mix(my_channel_2, my_channel_3)\n    .view()\n</code></pre> <p>It prints a single channel containing all the items emitted by the three channels:</p> Output<pre><code>1\n2\na\n3\nb\nz\n</code></pre> <p>Warning</p> <p>The items in the resulting channel have the same order as in the respective original channels. However, there is no guarantee that the elements of the second channel are appended after the elements of the first. Indeed, in the example above, the element <code>a</code> has been printed before <code>3</code>.</p>"},{"location":"archive/basic_training/operators/#flatten","title":"<code>flatten()</code>","text":"<p>The <code>flatten</code> operator transforms a channel in such a way that every tuple is flattened so that each entry is emitted as a sole element by the resulting channel.</p> snippet.nf<pre><code>foo = [1, 2, 3]\nbar = [4, 5, 6]\n\nChannel\n    .of(foo, bar)\n    .flatten()\n    .view()\n</code></pre> Output<pre><code>1\n2\n3\n4\n5\n6\n</code></pre>"},{"location":"archive/basic_training/operators/#collect","title":"<code>collect()</code>","text":"<p>The <code>collect</code> operator collects all of the items emitted by a channel in a list and returns the object as a sole emission.</p> snippet.nf<pre><code>Channel\n    .of(1, 2, 3, 4)\n    .collect()\n    .view()\n</code></pre> Output<pre><code>[1, 2, 3, 4]\n</code></pre> <p>Info</p> <p>The result of the <code>collect</code> operator is a value channel.</p>"},{"location":"archive/basic_training/operators/#grouptuple","title":"<code>groupTuple()</code>","text":"<p>The <code>groupTuple</code> operator collects tuples (or lists) of values emitted by the source channel, grouping the elements that share the same key. Finally, it emits a new tuple object for each distinct key collected.</p> snippet.nf<pre><code>Channel\n    .of([1, 'A'], [1, 'B'], [2, 'C'], [3, 'B'], [1, 'C'], [2, 'A'], [3, 'D'])\n    .groupTuple()\n    .view()\n</code></pre> Output<pre><code>[1, [A, B, C]]\n[2, [C, A]]\n[3, [B, D]]\n</code></pre> <p>This operator is especially useful to process a group together with all the elements that share a common property or grouping key.</p> <p>Exercise</p> <p>Use <code>fromPath</code> to create a channel emitting all of the files in the folder <code>data/meta/</code>, then use a <code>map</code> to associate the <code>baseName</code> method to each file. Finally, group all files that have the same common prefix.</p> Solution snippet.nf<pre><code>Channel\n    .fromPath('data/meta/*')\n    .map { file -&gt; tuple(file.baseName, file) }\n    .groupTuple()\n    .view()\n</code></pre> Output<pre><code>[patients_1, [/workspaces/training/nf-training/data/meta/patients_1.csv]]\n[patients_2, [/workspaces/training/nf-training/data/meta/patients_2.csv]]\n[random, [/workspaces/training/nf-training/data/meta/random.txt]]\n[regions, [/workspaces/training/nf-training/data/meta/regions.json, /workspaces/training/nf-training/data/meta/regions.tsv, /workspaces/training/nf-training/data/meta/regions.yml]]\n[regions2, [/workspaces/training/nf-training/data/meta/regions2.json]]\n</code></pre>"},{"location":"archive/basic_training/operators/#join","title":"<code>join()</code>","text":"<p>The <code>join</code> operator creates a channel that joins together the items emitted by two channels with a matching key. The key is defined, by default, as the first element in each item emitted.</p> snippet.nf<pre><code>left = Channel.of(['X', 1], ['Y', 2], ['Z', 3], ['P', 7])\nright = Channel.of(['Z', 6], ['Y', 5], ['X', 4])\nleft.join(right).view()\n</code></pre> Output<pre><code>[Z, 3, 6]\n[Y, 2, 5]\n[X, 1, 4]\n</code></pre> <p>Note</p> <p>Notice P is missing in the final result.</p>"},{"location":"archive/basic_training/operators/#branch","title":"<code>branch()</code>","text":"<p>The <code>branch</code> operator allows you to forward the items emitted by a source channel to one or more output channels.</p> <p>The selection criterion is defined by specifying a closure that provides one or more boolean expressions, each of which is identified by a unique label. For the first expression that evaluates to a true value, the item is bound to a named channel as the label identifier.</p> snippet.nf<pre><code>Channel\n    .of(1, 2, 3, 40, 50)\n    .branch {\n        small: it &lt; 10\n        large: it &gt; 10\n    }\n    .set { result }\n\nresult.small.view { \"$it is small\" }\nresult.large.view { \"$it is large\" }\n</code></pre> Output<pre><code>1 is small\n40 is large\n2 is small\n3 is small\n50 is large\n</code></pre> <p>Info</p> <p>The <code>branch</code> operator returns a multi-channel object (i.e., a variable that holds more than one channel object).</p> <p>Note</p> <p>In the above example, what would happen to a value of 10? To deal with this, you can also use <code>&gt;=</code>.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to use the <code>view</code> operator to print the content of a channel</li> <li>How to use the <code>map</code> operator to transform the content of a channel</li> <li>How to use the <code>mix</code> operator to combine the content of two or more channels</li> <li>How to use the <code>flatten</code> operator to flatten the content of a channel</li> <li>How to use the <code>collect</code> operator to collect the content of a channel</li> <li>How to use the <code>groupTuple</code> operator to group the content of a channel</li> <li>How to use the <code>join</code> operator to join the content of two channels</li> <li>How to use the <code>branch</code> operator to split the content of a channel</li> </ol>"},{"location":"archive/basic_training/operators/#text-files","title":"Text files","text":""},{"location":"archive/basic_training/operators/#splittext","title":"<code>splitText()</code>","text":"<p>The <code>splitText</code> operator allows you to split multi-line strings or text file items, emitted by a source channel into chunks containing n lines, which will be emitted by the resulting channel.</p> snippet.nf<pre><code>Channel\n    .fromPath('data/meta/random.txt') // (1)!\n    .splitText() // (2)!\n    .view() // (3)!\n</code></pre> <ol> <li>Instructs Nextflow to make a channel from the path <code>data/meta/random.txt</code></li> <li>The <code>splitText</code> operator splits each item into chunks of one line by default.</li> <li>View contents of the channel.</li> </ol> Output<pre><code>Lorem Ipsum is simply dummy text of the printing and typesetting industry.\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s,\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\nIt has survived not only five centuries, but also the leap into electronic typesetting,\n...\n</code></pre> <p>You can define the number of lines in each chunk by using the parameter <code>by</code>, as shown in the following example:</p> snippet.nf<pre><code>Channel\n    .fromPath('data/meta/random.txt')\n    .splitText(by: 2)\n    .view()\n</code></pre> Output<pre><code>Lorem Ipsum is simply dummy text of the printing and typesetting industry.\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\nIt has survived not only five centuries, but also the leap into electronic typesetting,\n...\n</code></pre> <p>An optional closure can also be specified in order to transform the text chunks produced by the operator. The following example shows how to split text files into chunks of 2 lines and transform them into capital letters:</p> snippet.nf<pre><code>Channel\n    .fromPath('data/meta/random.txt')\n    .splitText(by: 2) { it.toUpperCase() }\n    .view()\n</code></pre> Output<pre><code>LOREM IPSUM IS SIMPLY DUMMY TEXT OF THE PRINTING AND TYPESETTING INDUSTRY.\nLOREM IPSUM HAS BEEN THE INDUSTRY'S STANDARD DUMMY TEXT EVER SINCE THE 1500S,\n\nWHEN AN UNKNOWN PRINTER TOOK A GALLEY OF TYPE AND SCRAMBLED IT TO MAKE A TYPE SPECIMEN BOOK.\nIT HAS SURVIVED NOT ONLY FIVE CENTURIES, BUT ALSO THE LEAP INTO ELECTRONIC TYPESETTING,\n...\n</code></pre>"},{"location":"archive/basic_training/operators/#splitcsv","title":"<code>splitCsv()</code>","text":"<p>The <code>splitCsv</code> operator allows you to parse text items emitted by a channel, that are CSV formatted.</p> <p>It then splits them into records or groups them as a list of records with a specified length.</p> <p>In the simplest case, just apply the <code>splitCsv</code> operator to a channel emitting a CSV formatted text file or text entries. For example, to view only the first and fourth columns:</p> snippet.nf<pre><code>Channel\n    .fromPath(\"data/meta/patients_1.csv\")\n    .splitCsv()\n    .view { row -&gt; \"${row[0]}, ${row[3]}\" }\n</code></pre> Output<pre><code>patient_id, num_samples\nATX-TBL-001-GB-02-117, 3\nATX-TBL-001-GB-01-110, 3\nATX-TBL-001-GB-03-101, 3\nATX-TBL-001-GB-04-201, 3\nATX-TBL-001-GB-02-120, 3\nATX-TBL-001-GB-04-102, 3\nATX-TBL-001-GB-03-104, 3\nATX-TBL-001-GB-03-103, 3\n</code></pre> <p>When the CSV begins with a header line defining the column names, you can specify the parameter <code>header: true</code> which allows you to reference each value by its column name, as shown in the following example:</p> snippet.nf<pre><code>Channel\n    .fromPath(\"data/meta/patients_1.csv\")\n    .splitCsv(header: true)\n    // row is a list object\n    .view { row -&gt; \"${row.patient_id}, ${row.num_samples}\" }\n</code></pre> <p>Alternatively, you can provide custom header names by specifying a list of strings in the header parameter as shown below:</p> snippet.nf<pre><code>Channel\n    .fromPath(\"data/meta/patients_1.csv\")\n    .splitCsv(header: ['col1', 'col2', 'col3', 'col4', 'col5'])\n    .view { row -&gt; \"${row.col1}, ${row.col4}\" }\n</code></pre> Output<pre><code>patient_id, num_samples\nATX-TBL-001-GB-02-117, 3\nATX-TBL-001-GB-01-110, 3\nATX-TBL-001-GB-03-101, 3\nATX-TBL-001-GB-04-201, 3\nATX-TBL-001-GB-02-120, 3\nATX-TBL-001-GB-04-102, 3\nATX-TBL-001-GB-03-104, 3\nATX-TBL-001-GB-03-103, 3\n</code></pre> <p>You can also process multiple CSV files at the same time:</p> snippet.nf<pre><code>Channel\n    .fromPath(\"data/meta/patients_*.csv\") // &lt;-- just use a pattern\n    .splitCsv(header: true)\n    .view { row -&gt; \"${row.patient_id}\\t${row.num_samples}\" }\n</code></pre> Output<pre><code>ATX-TBL-001-GB-02-117   3\nATX-TBL-001-GB-01-110   3\nATX-TBL-001-GB-03-101   3\nATX-TBL-001-GB-04-201   3\nATX-TBL-001-GB-02-120   3\nATX-TBL-001-GB-04-102   3\nATX-TBL-001-GB-03-104   3\nATX-TBL-001-GB-03-103   3\nATX-TBL-001-GB-01-111   2\nATX-TBL-001-GB-01-112   3\nATX-TBL-001-GB-04-202   3\nATX-TBL-001-GB-02-124   3\nATX-TBL-001-GB-02-107   3\nATX-TBL-001-GB-01-105   3\nATX-TBL-001-GB-02-108   3\nATX-TBL-001-GB-01-113   3\n</code></pre> <p>Tip</p> <p>Notice that you can change the output format simply by adding a different delimiter.</p> <p>Finally, you can also operate on CSV files outside the channel context:</p> <pre><code>def f = file('data/meta/patients_1.csv')\ndef lines = f.splitCsv()\nfor (List row : lines) {\n    log.info \"${row[0]} -- ${row[2]}\"\n}\n</code></pre> <p>Exercise</p> <p>Create a CSV file and use it as input for <code>script7.nf</code>, part of the Simple RNA-Seq workflow tutorial.</p> Solution <p>Add a CSV text file containing the following, as an example input with the name \"fastq.csv\":</p> fastq.csv<pre><code>gut,/workspaces/training/nf-training/data/ggal/gut_1.fq,/workspaces/training/nf-training/data/ggal/gut_2.fq\n</code></pre> <p>Then replace the input channel for the reads in <code>script7.nf</code>. Changing the following lines:</p> <pre><code>Channel\n    .fromFilePairs(params.reads, checkIfExists: true)\n    .set { read_pairs_ch }\n</code></pre> <p>To a splitCsv channel factory input:</p> script7.nf<pre><code>Channel\n    .fromPath(\"fastq.csv\")\n    .splitCsv()\n    .view { row -&gt; \"${row[0]}, ${row[1]}, ${row[2]}\" }\n    .set { read_pairs_ch }\n</code></pre> <p>Finally, change the cardinality of the processes that use the input data:</p> script7.nf<pre><code>process QUANTIFICATION {\n    tag \"$sample_id\"\n\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads1), path(reads2)\n\n    output:\n    path sample_id, emit: quant_ch\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U -i $salmon_index -1 ${reads1} -2 ${reads2} -o $sample_id\n    \"\"\"\n}\n</code></pre> <p>Repeat the above for the fastqc step.</p> script7.nf<pre><code>process FASTQC {\n    tag \"FASTQC on $sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads1), path(reads2)\n\n    output:\n    path \"fastqc_${sample_id}_logs\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads1} ${reads2}\n    \"\"\"\n}\n</code></pre> <p>Now the workflow should run from a CSV file.</p>"},{"location":"archive/basic_training/operators/#tab-separated-values-tsv","title":"Tab separated values (.tsv)","text":"<p>Parsing TSV files works in a similar way. Simply add the <code>sep: '\\t'</code> option in the <code>splitCsv</code> context:</p> snippet.nf<pre><code>Channel\n    .fromPath(\"data/meta/regions.tsv\", checkIfExists: true)\n    // use `sep` option to parse TAB separated files\n    .splitCsv(sep: '\\t')\n    .view()\n</code></pre> <p>Exercise</p> <p>Use the tab separation technique on the file <code>data/meta/regions.tsv</code>, but print just the first column, and remove the header.</p> Solution snippet.nf<pre><code>Channel\n    .fromPath(\"data/meta/regions.tsv\", checkIfExists: true)\n    // use `sep` option to parse TAB separated files\n    .splitCsv(sep: '\\t', header: true)\n    // row is a list object\n    .view { row -&gt; \"${row.patient_id}\" }\n</code></pre>"},{"location":"archive/basic_training/operators/#splitjson","title":"<code>splitJson()</code>","text":"<p>You can parse the JSON file format using the <code>splitJson</code> channel operator.</p> <p>The <code>splitJson</code> operator supports JSON arrays:</p> snippet.nf<pre><code>Channel\n    .of('[\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]')\n    .splitJson()\n    .view()\n</code></pre> Output<pre><code>Sunday\nMonday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\n</code></pre> <p>As well as JSON arrays in objects:</p> snippet.nf<pre><code>Channel\n    .of('{\"player\": {\"name\": \"Bob\", \"height\": 180, \"champion\": false}}')\n    .splitJson()\n    .view()\n</code></pre> Output<pre><code>[value:[name:Bob, height:180, champion:false], key:player]\n</code></pre> <p>And even a JSON array of JSON objects:</p> snippet.nf<pre><code>Channel\n    .of('[{\"name\": \"Bob\", \"height\": 180, \"champion\": false}, \\\n          {\"name\": \"Alice\", \"height\": 170, \"champion\": false}]')\n    .splitJson()\n    .view()\n</code></pre> Output<pre><code>[name:Bob, height:180, champion:false]\n[name:Alice, height:170, champion:false]\n</code></pre> <p>You can also parse JSON files directly:</p> file.json<pre><code>[\n  { \"name\": \"Bob\", \"height\": 180, \"champion\": false },\n  { \"name\": \"Alice\", \"height\": 170, \"champion\": false }\n]\n</code></pre> snippet.nf<pre><code>Channel\n    .fromPath('file.json')\n    .splitJson()\n    .view()\n</code></pre> Output<pre><code>[name:Bob, height:180, champion:false]\n[name:Alice, height:170, champion:false]\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to use the <code>splitText</code> operator to split text files of various formats</li> <li>How to use the <code>splitJson</code> operator to split JSON files of various formats</li> </ol>"},{"location":"archive/basic_training/operators/#more-resources","title":"More resources","text":"<p>Check the operators documentation on Nextflow web site.</p>"},{"location":"archive/basic_training/orientation/","title":"Orientation","text":"<p>The GitHub Codespaces environment contains some test data that will be used in this workshop.</p> <p>Note</p> <p>Follow this link if you have not yet setup your GitHub Codespaces environment.</p>"},{"location":"archive/basic_training/orientation/#getting-started","title":"Getting started","text":"<p>You will complete this module in the <code>nf-training/</code> folder.</p> <p>In this folder you will find a series of data files (<code>ggal</code>, <code>index</code>, <code>meta</code>...) and several script and configuration files.</p> <pre><code>.\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 ggal\n\u2502   \u2502   \u2514\u2500\u2500 &lt;data files&gt;\n\u2502   \u251c\u2500\u2500 index\n\u2502   \u2502   \u2514\u2500\u2500 &lt;data files&gt;\n\u2502   \u251c\u2500\u2500 meta\n\u2502   \u2502   \u2514\u2500\u2500 &lt;data files&gt;\n\u2502   \u251c\u2500\u2500 prots\n\u2502   \u2502   \u2514\u2500\u2500 &lt;data files&gt;\n\u2502   \u251c\u2500\u2500 reads\n\u2502   \u2502   \u2514\u2500\u2500 &lt;data files&gt;\n\u2502   \u2514\u2500\u2500 test\n\u2502       \u2514\u2500\u2500 &lt;data files&gt;\n\u251c\u2500\u2500 env.yml\n\u251c\u2500\u2500 hello.nf\n\u251c\u2500\u2500 hello_py.nf\n\u251c\u2500\u2500 modules.hello.nf\n\u251c\u2500\u2500 nextflow.config\n\u251c\u2500\u2500 script1.nf\n\u251c\u2500\u2500 script2.nf\n\u251c\u2500\u2500 script3.nf\n\u251c\u2500\u2500 script4.nf\n\u251c\u2500\u2500 script5.nf\n\u251c\u2500\u2500 script6.nf\n\u251c\u2500\u2500 script7.nf\n\u2514\u2500\u2500 snippet.nf\n</code></pre> <p>Each file will be used in this training module.</p>"},{"location":"archive/basic_training/orientation/#selecting-a-nextflow-version","title":"Selecting a Nextflow version","text":"<p>By default, Nextflow will pull the latest stable version into your environment.</p> <p>However, Nextflow is constantly evolving as improvements are made.</p> <p>The latest releases can be viewed on GitHub here.</p> <p>If you want to use a specific version of Nextflow, you can set the <code>NXF_VER</code> variable as shown below:</p> <pre><code>export NXF_VER=23.10.1\n</code></pre> <p>You can double-check <code>NXF_VER</code> by running:</p> <pre><code>nextflow -version\n</code></pre> <p>Exercise</p> <p>Open the GitHub Codespaces training environment and use the following command to switch to the <code>nf-customize</code> folder. View the files in this folder using the <code>tree</code> command:</p> <pre><code>cd /workspaces/training/nf-training\ntree .\n</code></pre>"},{"location":"archive/basic_training/processes/","title":"Processes","text":"<p>In Nextflow, a <code>process</code> is the basic computing primitive to execute foreign functions (i.e., custom scripts or tools).</p> <p>The <code>process</code> definition starts with the keyword <code>process</code>, followed by the process name and finally the process body delimited by curly brackets.</p> <p>A basic <code>process</code>, only using the <code>script</code> definition block, looks like the following:</p> snippet.nf<pre><code>process SAYHELLO {\n    script:\n    \"\"\"\n    echo 'Hello world!'\n    \"\"\"\n}\n</code></pre> <p>Info</p> <p>The <code>process</code> name is commonly written in upper case by convention.</p> <p>However, the process body can contain up to five definition blocks:</p> <ol> <li>Directives are initial declarations that define optional settings</li> <li>Input defines the expected input channel(s)</li> <li>Output defines the expected output channel(s)</li> <li>When is an optional clause statement to allow conditional processes</li> <li>Script is a string statement that defines the command to be executed by the process' task</li> </ol> <p>The full process syntax is defined as follows:</p> <p>Click the  icons in the code for explanations.</p> <pre><code>process &lt; name &gt; {\n    [ directives ] // (1)!\n\n    input: // (2)!\n    &lt; process inputs &gt;\n\n    output: // (3)!\n    &lt; process outputs &gt;\n\n    when: // (4)!\n    &lt; condition &gt;\n\n    [script|shell|exec]: // (5)!\n    \"\"\"\n    &lt; user script to be executed &gt;\n    \"\"\"\n}\n</code></pre> <ol> <li>Zero, one, or more process directives</li> <li>Zero, one, or more process inputs</li> <li>Zero, one, or more process outputs</li> <li>An optional boolean conditional to trigger the process execution</li> <li>The command to be executed</li> </ol>"},{"location":"archive/basic_training/processes/#script","title":"Script","text":"<p>The <code>script</code> block is a string statement that defines the command to be executed by the process.</p> <p>A process can execute only one <code>script</code> block. It must be the last statement when the process contains <code>input</code> and <code>output</code> declarations.</p> <p>The <code>script</code> block can be a single or a multi-line string. The latter simplifies the writing of non-trivial scripts composed of multiple commands spanning over multiple lines. For example:</p> snippet.nf<pre><code>process EXAMPLE {\n    script:\n    \"\"\"\n    echo 'Hello world!\\nHola mundo!\\nCiao mondo!\\nHallo Welt!' &gt; file\n    cat file | head -n 1 | head -c 5 &gt; chunk_1.txt\n    gzip -c chunk_1.txt  &gt; chunk_archive.gz\n    \"\"\"\n}\n\nworkflow {\n    EXAMPLE()\n}\n</code></pre> <p>Tip</p> <p>In the snippet below the directive <code>debug</code> is used to enable the debug mode for the process. This is useful to print the output of the process script in the console.</p> <p>By default, the <code>process</code> command is interpreted as a Bash script. However, any other scripting language can be used by simply starting the script with the corresponding Shebang declaration. For example:</p> snippet.nf<pre><code>process PYSTUFF {\n    debug true\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    x = 'Hello'\n    y = 'world!'\n    print (\"%s - %s\" % (x, y))\n    \"\"\"\n}\n\nworkflow {\n    PYSTUFF()\n}\n</code></pre> Output<pre><code>Hello-world\n</code></pre> <p>Tip</p> <p>Multiple programming languages can be used within the same workflow script. However, for large chunks of code it is better to save them into separate files and invoke them from the process script. One can store the specific scripts in the <code>./bin/</code> folder.</p>"},{"location":"archive/basic_training/processes/#script-parameters","title":"Script parameters","text":"<p>Script parameters (<code>params</code>) can be defined dynamically using variable values. For example:</p> snippet.nf<pre><code>params.data = 'World'\n\nprocess FOO {\n    debug true\n\n    script:\n    \"\"\"\n    echo Hello $params.data\n    \"\"\"\n}\n\nworkflow {\n    FOO()\n}\n</code></pre> Output<pre><code>Hello World\n</code></pre> <p>Info</p> <p>A process script can contain any string format supported by the Groovy programming language. This allows us to use string interpolation as in the script above or multiline strings. Refer to String interpolation for more information.</p> <p>Warning</p> <p>Since Nextflow uses the same Bash syntax for variable substitutions in strings, Bash environment variables need to be escaped using the <code>\\</code> character. The escaped version will be resolved later, returning the task directory (e.g. work/7f/f285b80022d9f61e82cd7f90436aa4/), while <code>$PWD</code> would show the directory where you're running Nextflow.</p> snippet.nf<pre><code>process FOO {\n    debug true\n\n    script:\n    \"\"\"\n    echo \"The current directory is \\$PWD\"\n    \"\"\"\n}\n\nworkflow {\n    FOO()\n}\n</code></pre> <p>Your expected output will look something like this:</p> Output<pre><code>The current directory is /workspaces/training/nf-training/work/7a/4b050a6cdef4b6c1333ce29f7059a0\n</code></pre> <p>It can be tricky to write a script that uses many Bash variables. One possible alternative is to use a <code>script</code> string delimited by single-quote characters (<code>'</code>).</p> snippet.nf<pre><code>process BAR {\n    debug true\n\n    script:\n    '''\n    echo \"The current directory is $PWD\"\n    '''\n}\n\nworkflow {\n    BAR()\n}\n</code></pre> <p>Your expected output will look something like this:</p> Output<pre><code>The current directory is /workspaces/training/nf-training/work/7a/4b050a6cdef4b6c1333ce29f7059a0\n</code></pre> <p>However, using the single quotes (<code>'</code>) will block the usage of Nextflow variables in the command script.</p> <p>Another alternative is to use a <code>shell</code> statement instead of <code>script</code> and use a different syntax for Nextflow variables, e.g., <code>!{..}</code>. This allows the use of both Nextflow and Bash variables in the same script.</p> snippet.nf<pre><code>params.data = 'le monde'\n\nprocess BAZ {\n    shell:\n    '''\n    X='Bonjour'\n    echo $X !{params.data}\n    '''\n}\n\nworkflow {\n    BAZ()\n}\n</code></pre>"},{"location":"archive/basic_training/processes/#conditional-script","title":"Conditional script","text":"<p>The process script can also be defined in a completely dynamic manner using an <code>if</code> statement or any other expression for evaluating a string value. For example:</p> snippet.nf<pre><code>params.compress = 'gzip'\nparams.file2compress = \"$projectDir/data/ggal/transcriptome.fa\"\n\nprocess FOO {\n    debug true\n\n    input:\n    path file\n\n    script:\n    if (params.compress == 'gzip')\n        \"\"\"\n        echo \"gzip -c $file &gt; ${file}.gz\"\n        \"\"\"\n    else if (params.compress == 'bzip2')\n        \"\"\"\n        echo \"bzip2 -c $file &gt; ${file}.bz2\"\n        \"\"\"\n    else\n        throw new IllegalArgumentException(\"Unknown compressor $params.compress\")\n}\n\nworkflow {\n    FOO(params.file2compress)\n}\n</code></pre> <p>Exercise</p> <p>Execute this script using the command line to choose <code>bzip2</code> compression.</p> Solution <p>Execute the following command:</p> <pre><code>nextflow run snippet.nf --compress bzip2\n</code></pre> <p>The output will look like this:</p> Output<pre><code>bzip2 -c transcriptome.fa &gt; transcriptome.fa.bz2\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to use the <code>script</code> declaration to define the command to be executed by the process</li> <li>How to use the <code>params</code> variable to define dynamic script parameters</li> <li>How to use the <code>shell</code> declaration to define the command to be executed by the process</li> <li>How to use the <code>if</code> statement to define a conditional script</li> </ol>"},{"location":"archive/basic_training/processes/#inputs","title":"Inputs","text":"<p>Nextflow process instances (tasks) are isolated from each other but can communicate between themselves by sending values through channels.</p> <p>Inputs implicitly determine the dependencies and the parallel execution of the process. The process execution is fired each time new data is ready to be consumed from the input channel:</p> <p>The <code>input</code> block defines the names and qualifiers of variables that refer to channel elements directed at the process. You can only define one <code>input</code> block at a time, and it must contain one or more input declarations.</p> <p>The <code>input</code> block follows the syntax shown below:</p> <pre><code>input:\n&lt;input qualifier&gt; &lt;input name&gt;\n</code></pre> <p>There are several input qualifiers that can be used to define the input declaration. The most common are outlined in detail below.</p>"},{"location":"archive/basic_training/processes/#input-values","title":"Input values","text":"<p>The <code>val</code> qualifier allows you to receive data of any type as input. It can be accessed in the process script by using the specified input name. For example:</p> snippet.nf<pre><code>num = Channel.of(1, 2, 3)\n\nprocess BASICEXAMPLE {\n    debug true\n\n    input:\n    val x\n\n    script:\n    \"\"\"\n    echo process job $x\n    \"\"\"\n}\n\nworkflow {\n    BASICEXAMPLE(num)\n}\n</code></pre> <p>In the above example the process is executed three times, each time a value is received from the channel <code>num</code> it is used by the script. Thus, it results in an output similar to the one shown below:</p> Output<pre><code>process job 1\nprocess job 2\nprocess job 3\n</code></pre> <p>Warning</p> <p>The channel guarantees that items are delivered in the same order as they have been sent - but - since the process is executed in a parallel manner, there is no guarantee that they are processed in the same order as they are received.</p>"},{"location":"archive/basic_training/processes/#input-files","title":"Input files","text":"<p>The <code>path</code> qualifier allows the handling of file values in the process execution context. This means that Nextflow will stage it in the process execution directory, and it can be accessed by the script using the name specified in the input declaration. For example:</p> snippet.nf<pre><code>reads = Channel.fromPath('data/ggal/*.fq')\n\nprocess FOO {\n    debug true\n\n    input:\n    path 'sample.fastq'\n\n    script:\n    \"\"\"\n    ls sample.fastq\n    \"\"\"\n}\n\nworkflow {\n    result = FOO(reads)\n}\n</code></pre> <p>In this case, the process is executed six times and will print the name of the file <code>sample.fastq</code> six times as this is the name of the file in the input declaration and despite the input file name being different in each execution (e.g., <code>lung_1.fq</code>).</p> Output<pre><code>sample.fastq\nsample.fastq\nsample.fastq\nsample.fastq\nsample.fastq\nsample.fastq\n</code></pre> <p>The input file name can also be defined using a variable reference as shown below:</p> snippet.nf<pre><code>reads = Channel.fromPath('data/ggal/*.fq')\n\nprocess FOO {\n    debug true\n\n    input:\n    path sample\n\n    script:\n    \"\"\"\n    ls  $sample\n    \"\"\"\n}\n\nworkflow {\n    result = FOO(reads)\n}\n</code></pre> <p>In this case, the process is executed six times and will print the name of the variable input file six times (e.g., <code>lung_1.fq</code>).</p> Output<pre><code>lung_1.fq\ngut_2.fq\nliver_2.fq\nlung_2.fq\nliver_1.fq\ngut_1.fq\n</code></pre> <p>The same syntax is also able to handle more than one input file in the same execution and only requires changing the channel composition using an operator (e.g., <code>collect</code>).</p> snippet.nf<pre><code>reads = Channel.fromPath('data/ggal/*.fq')\n\nprocess FOO {\n    debug true\n\n    input:\n    path sample\n\n    script:\n    \"\"\"\n    ls $sample\n    \"\"\"\n}\n\nworkflow {\n    FOO(reads.collect())\n}\n</code></pre> <p>Note that while the output looks the same, this process is only executed once.</p> Output<pre><code>lung_1.fq\ngut_2.fq\nliver_2.fq\nlung_2.fq\nliver_1.fq\ngut_1.fq\n</code></pre> <p>Warning</p> <p>In the past, the <code>file</code> qualifier was used for files, but the <code>path</code> qualifier should be preferred over file to handle process input files when using Nextflow 19.10.0 or later. When a process declares an input file, the corresponding channel elements must be file objects created with the file helper function from the file specific channel factories (e.g., <code>Channel.fromPath</code> or <code>Channel.fromFilePairs</code>).</p>"},{"location":"archive/basic_training/processes/#combine-input-channels","title":"Combine input channels","text":"<p>A key feature of processes is the ability to handle inputs from multiple channels. However, it\u2019s important to understand how channel contents and their semantics affect the execution of a process.</p> <p>Consider the following example:</p> snippet.nf<pre><code>ch1 = Channel.of(1, 2, 3)\nch2 = Channel.of('a', 'b', 'c')\n\nprocess FOO {\n    debug true\n\n    input:\n    val x\n    val y\n\n    script:\n    \"\"\"\n    echo $x and $y\n    \"\"\"\n}\n\nworkflow {\n    FOO(ch1, ch2)\n}\n</code></pre> <p>Both channels emit three values, therefore the process is executed three times, each time with a different pair:</p> Output<pre><code>1 and a\n3 and c\n2 and b\n</code></pre> <p>The process waits until there\u2019s a complete input configuration, i.e., it receives an input value from all the channels declared as input.</p> <p>When this condition is verified, it consumes the input values coming from the respective channels, spawns a task execution, then repeats the same logic until one or more channels have no more content.</p> <p>This means channel values are consumed serially one after another and the first empty channel causes the process execution to stop, even if there are other values in other channels.</p> <p>What happens when channels do not have the same cardinality (i.e., they emit a different number of elements)?</p> snippet.nf<pre><code>ch1 = Channel.of(1, 2, 3)\nch2 = Channel.of('a')\n\nprocess FOO {\n    debug true\n\n    input:\n    val x\n    val y\n\n    script:\n    \"\"\"\n    echo $x and $y\n    \"\"\"\n}\n\nworkflow {\n    FOO(ch1, ch2)\n}\n</code></pre> <p>In the above example, the process is only executed once because the process stops when a channel has no more data to be processed.</p> Output<pre><code>1 and a\n</code></pre> <p>However, replacing <code>ch2</code> with a <code>value</code> channel will cause the process to be executed three times, each time with the same value of <code>a</code>:</p> snippet.nf<pre><code>ch1 = Channel.of(1, 2, 3)\nch2 = Channel.value('a')\n\nprocess FOO {\n    debug true\n\n    input:\n    val x\n    val y\n\n    script:\n    \"\"\"\n    echo $x and $y\n    \"\"\"\n}\n\nworkflow {\n    FOO(ch1, ch2)\n}\n</code></pre> Script output<pre><code>1 and a\n2 and a\n3 and a\n</code></pre> <p>As <code>ch2</code> is now a value channel, it can be consumed multiple times and does not affect process termination.</p> <p>Exercise</p> <p>Write a process that is executed for each read file matching the pattern <code>data/ggal/*_1.fq</code> and use the same <code>data/ggal/transcriptome.fa</code> in each execution.</p> Solution <p>One possible solution is shown below:</p> snippet.nf<pre><code>params.reads = \"$projectDir/data/ggal/*_1.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\n\nChannel\n    .fromPath(params.reads)\n    .set { read_ch }\n\nprocess COMMAND {\n    debug true\n\n    input:\n    path reads\n    path transcriptome\n\n    script:\n    \"\"\"\n    echo $reads $transcriptome\n    \"\"\"\n}\n\nworkflow {\n    COMMAND(read_ch, params.transcriptome_file)\n}\n</code></pre> <p>You may also consider using other Channel factories or operators to create your input channels.</p>"},{"location":"archive/basic_training/processes/#input-repeaters","title":"Input repeaters","text":"<p>The <code>each</code> qualifier allows you to repeat the execution of a process for each item in a collection every time new data is received. For example:</p> snippet.nf<pre><code>sequences = Channel.fromPath(\"$projectDir/data/ggal/*_1.fq\")\nmethods = ['regular', 'espresso']\n\nprocess ALIGNSEQUENCES {\n    debug true\n\n    input:\n    path seq\n    each mode\n\n    script:\n    \"\"\"\n    echo t_coffee -in $seq -mode $mode\n    \"\"\"\n}\n\nworkflow {\n    ALIGNSEQUENCES(sequences, methods)\n}\n</code></pre> Output<pre><code>t_coffee -in gut_1.fq -mode regular\nt_coffee -in lung_1.fq -mode espresso\nt_coffee -in liver_1.fq -mode regular\nt_coffee -in gut_1.fq -mode espresso\nt_coffee -in lung_1.fq -mode regular\nt_coffee -in liver_1.fq -mode espresso\n</code></pre> <p>In the above example, every time a file of sequences is received as an input by the process, it executes three tasks, each running a different alignment method set as a <code>mode</code> variable. This is useful when you need to repeat the same task for a given set of parameters.</p> <p>Exercise</p> <p>Extend the previous example so a task is executed for an additional type of coffee.</p> Solution <p>Modify the methods list and add another coffee type:</p> snippet.nf<pre><code>sequences = Channel.fromPath(\"$projectDir/data/ggal/*_1.fq\")\nmethods = ['regular', 'espresso', 'cappuccino']\n\nprocess ALIGNSEQUENCES {\n    debug true\n\n    input:\n    path seq\n    each mode\n\n    script:\n    \"\"\"\n    echo t_coffee -in $seq -mode $mode\n    \"\"\"\n}\n\nworkflow {\n    ALIGNSEQUENCES(sequences, methods)\n}\n</code></pre> <p>Your output will look something like this:</p> Output<pre><code>t_coffee -in gut_1.fq -mode regular\nt_coffee -in lung_1.fq -mode regular\nt_coffee -in gut_1.fq -mode espresso\nt_coffee -in liver_1.fq -mode cappuccino\nt_coffee -in liver_1.fq -mode espresso\nt_coffee -in lung_1.fq -mode espresso\nt_coffee -in liver_1.fq -mode regular\nt_coffee -in gut_1.fq -mode cappuccino\nt_coffee -in lung_1.fq -mode cappuccino\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to use the <code>val</code> qualifier to define the input channel(s) of a process</li> <li>How to use the <code>path</code> qualifier to define the input file(s) of a process</li> <li>How to use the <code>each</code> qualifier to repeat the execution of a process for each item in a collection</li> </ol>"},{"location":"archive/basic_training/processes/#outputs","title":"Outputs","text":"<p>The output declaration block defines the channels used by the process to send out the results produced.</p> <p>Only one output block, that can contain one or more output declaration, can be defined. The output block follows the syntax shown below:</p> <pre><code>output:\n&lt;output qualifier&gt; &lt;output name&gt;, emit: &lt;output channel&gt;\n</code></pre>"},{"location":"archive/basic_training/processes/#output-values","title":"Output values","text":"<p>The <code>val</code> qualifier specifies a defined value in the script context. Values are frequently defined in the <code>input</code> and/or <code>output</code> declaration blocks, as shown in the following example:</p> snippet.nf<pre><code>greeting = \"Hello world!\"\n\nprocess FOO {\n    input:\n    val x\n\n    output:\n    val x\n\n    script:\n    \"\"\"\n    echo $x &gt; file\n    \"\"\"\n}\n\nworkflow {\n    FOO(Channel.of(greeting))\n        .view()\n}\n</code></pre>"},{"location":"archive/basic_training/processes/#output-files","title":"Output files","text":"<p>The <code>path</code> qualifier specifies one or more files produced by the process into the specified channel as an output.</p> snippet.nf<pre><code>process RANDOMNUM {\n    output:\n    path 'result.txt'\n\n    script:\n    \"\"\"\n    echo \\$RANDOM &gt; result.txt\n    \"\"\"\n}\n\nworkflow {\n    receiver_ch = RANDOMNUM()\n    receiver_ch.view()\n}\n</code></pre> <p>In the above example the process <code>RANDOMNUM</code> creates a file named <code>result.txt</code> containing a random number.</p> <p>Since a file parameter using the same name is declared in the output block, the file is sent over the <code>receiver_ch</code> channel when the task is complete. A downstream <code>process</code> declaring the same channel as input will be able to receive it.</p>"},{"location":"archive/basic_training/processes/#multiple-output-files","title":"Multiple output files","text":"<p>When an output file name contains a wildcard character (<code>*</code> or <code>?</code>) it is interpreted as a glob path matcher. This allows us to capture multiple files into a list object and output them as a sole emission. For example:</p> snippet.nf<pre><code>process SPLITLETTERS {\n    output:\n    path 'chunk_*'\n\n    script:\n    \"\"\"\n    printf 'Hola' | split -b 1 - chunk_\n    \"\"\"\n}\n\nworkflow {\n    letters = SPLITLETTERS()\n    letters.view()\n}\n</code></pre> <p>Prints the following:</p> Output<pre><code>[/workspaces/training/nf-training/work/ca/baf931d379aa7fa37c570617cb06d1/chunk_aa, /workspaces/training/nf-training/work/ca/baf931d379aa7fa37c570617cb06d1/chunk_ab, /workspaces/training/nf-training/work/ca/baf931d379aa7fa37c570617cb06d1/chunk_ac, /workspaces/training/nf-training/work/ca/baf931d379aa7fa37c570617cb06d1/chunk_ad]\n</code></pre> <p>Some caveats on glob pattern behavior:</p> <ul> <li>Input files are not included in the list of possible matches</li> <li>Glob pattern matches both files and directory paths</li> <li>When a two asterisks pattern <code>**</code> is used to recourse across directories, only file paths are matched i.e., directories are not included in the result list.</li> </ul> <p>Exercise</p> <p>Add the <code>flatMap</code> operator and see out the output changes. The documentation for the <code>flatMap</code> operator is available at this link.</p> Solution <p>Add the <code>flatMap</code> operator to the <code>letters</code> channel.</p> snippet.nf<pre><code>process SPLITLETTERS {\n    output:\n    path 'chunk_*'\n\n    script:\n    \"\"\"\n    printf 'Hola' | split -b 1 - chunk_\n    \"\"\"\n}\n\nworkflow {\n    letters = SPLITLETTERS()\n    letters.flatMap().view()\n}\n</code></pre> <p>Your output will look something like this:</p> Output<pre><code>/workspaces/training/nf-training/work/54/9d79f9149f15085e00dde2d8ead150/chunk_aa\n/workspaces/training/nf-training/work/54/9d79f9149f15085e00dde2d8ead150/chunk_ab\n/workspaces/training/nf-training/work/54/9d79f9149f15085e00dde2d8ead150/chunk_ac\n/workspaces/training/nf-training/work/54/9d79f9149f15085e00dde2d8ead150/chunk_ad\n</code></pre>"},{"location":"archive/basic_training/processes/#dynamic-output-file-names","title":"Dynamic output file names","text":"<p>When an output file name needs to be expressed dynamically, it is possible to define it using a dynamic string that references values defined in the input declaration block or in the script global context. For example:</p> snippet.nf<pre><code>species = ['cat', 'dog', 'sloth']\nsequences = ['AGATAG', 'ATGCTCT', 'ATCCCAA']\n\nChannel\n    .fromList(species)\n    .set { species_ch }\n\nprocess ALIGN {\n    input:\n    val x\n    val seq\n\n    output:\n    path \"${x}.aln\"\n\n    script:\n    \"\"\"\n    echo align -in $seq &gt; ${x}.aln\n    \"\"\"\n}\n\nworkflow {\n    genomes = ALIGN(species_ch, sequences)\n    genomes.view()\n}\n</code></pre> <p>In the above example, each time the process is executed an alignment file is produced whose name depends on the actual value of the <code>x</code> input.</p>"},{"location":"archive/basic_training/processes/#composite-inputs-and-outputs","title":"Composite inputs and outputs","text":"<p>So far you have seen how to declare multiple input and output channels that can handle one value at a time. However, Nextflow can also handle a tuple of values.</p> <p>The <code>input</code> and <code>output</code> declarations for tuples must be declared with a <code>tuple</code> qualifier followed by the definition of each element in the tuple.</p> snippet.nf<pre><code>reads_ch = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')\n\nprocess FOO {\n    input:\n    tuple val(sample_id), path(sample_id_paths)\n\n    output:\n    tuple val(sample_id), path('sample.bam')\n\n    script:\n    \"\"\"\n    echo your_command_here --sample $sample_id_paths &gt; sample.bam\n    \"\"\"\n}\n\nworkflow {\n    sample_ch = FOO(reads_ch)\n    sample_ch.view()\n}\n</code></pre> <p>The output will looks something like this:</p> Output<pre><code>[lung, /workspaces/training/nf-training/work/23/fe268295bab990a40b95b7091530b6/sample.bam]\n[liver, /workspaces/training/nf-training/work/32/656b96a01a460f27fa207e85995ead/sample.bam]\n[gut, /workspaces/training/nf-training/work/ae/3cfc7cf0748a598c5e2da750b6bac6/sample.bam]\n</code></pre> <p>Exercise</p> <p>Modify the script of the previous exercise so that the --sample file is named as the given <code>sample_id</code>.</p> Solution snippet.nf<pre><code>reads_ch = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')\n\nprocess FOO {\n    input:\n    tuple val(sample_id), path(sample_id_paths)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}.bam\")\n\n    script:\n    \"\"\"\n    echo your_command_here --sample $sample_id_paths &gt; ${sample_id}.bam\n    \"\"\"\n}\n\nworkflow {\n    sample_ch = FOO(reads_ch)\n    sample_ch.view()\n}\n</code></pre>"},{"location":"archive/basic_training/processes/#output-definitions","title":"Output definitions","text":"<p>Nextflow allows the use of alternative output definitions within workflows to simplify your code.</p> <p>You can also explicitly define the output of a channel using the <code>.out</code> attribute:</p> snippet.nf<pre><code>reads_ch = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')\n\nprocess FOO {\n    input:\n    tuple val(sample_id), path(sample_id_paths)\n\n    output:\n    tuple val(sample_id), path('sample.bam')\n    tuple val(sample_id), path('sample.bai')\n\n    script:\n    \"\"\"\n    echo your_command_here --sample $sample_id_paths &gt; sample.bam\n    echo your_command_here --sample $sample_id_paths &gt; sample.bai\n    \"\"\"\n}\n\nworkflow {\n    FOO(reads_ch)\n    FOO.out.view()\n}\n</code></pre> <p>This command will produce an error message, because <code>.view()</code> operates on single channels, and FOO.out contains multiple channels.</p> <p>If a process defines two or more output channels, each channel can be accessed by indexing the <code>.out</code> attribute, e.g., <code>.out[0]</code>, <code>.out[1]</code>, etc. In this example you only have the <code>[0]'th</code> output:</p> snippet.nf<pre><code>reads_ch = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')\n\nprocess FOO {\n    input:\n    tuple val(sample_id), path(sample_id_paths)\n\n    output:\n    tuple val(sample_id), path('sample.bam')\n    tuple val(sample_id), path('sample.bai')\n\n    script:\n    \"\"\"\n    echo your_command_here --sample $sample_id_paths &gt; sample.bam\n    echo your_command_here --sample $sample_id_paths &gt; sample.bai\n    \"\"\"\n}\n\nworkflow {\n    FOO(reads_ch)\n    FOO.out[0].view()\n}\n</code></pre> <p>Alternatively, the process <code>output</code> definition allows the use of the <code>emit</code> statement to define a named identifier that can be used to reference the channel in the external scope.</p> snippet.nf<pre><code>reads_ch = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')\n\nprocess FOO {\n    input:\n    tuple val(sample_id), path(sample_id_paths)\n\n    output:\n    tuple val(sample_id), path('sample.bam'), emit: bam\n    tuple val(sample_id), path('sample.bai'), emit: bai\n\n    script:\n    \"\"\"\n    echo your_command_here --sample $sample_id_paths &gt; sample.bam\n    echo your_command_here --sample $sample_id_paths &gt; sample.bai\n    \"\"\"\n}\n\nworkflow {\n    FOO(reads_ch)\n    FOO.out.bam.view()\n}\n</code></pre> <p>Exercise</p> <p>Modify the previous example so that the <code>bai</code> output channel is printed to your terminal.</p> Solution <p>Your workflow will look something like this:</p> snippet.nf<pre><code>reads_ch = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')\n\nprocess FOO {\n    input:\n    tuple val(sample_id), path(sample_id_paths)\n\n    output:\n    tuple val(sample_id), path('sample.bam'), emit: bam\n    tuple val(sample_id), path('sample.bai'), emit: bai\n\n    script:\n    \"\"\"\n    echo your_command_here --sample $sample_id_paths &gt; sample.bam\n    echo your_command_here --sample $sample_id_paths &gt; sample.bai\n    \"\"\"\n}\n\nworkflow {\n    FOO(reads_ch)\n    FOO.out.bai.view()\n}\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to use the <code>val</code> qualifier to define the output channel(s) of a process</li> <li>How to use the <code>path</code> qualifier to define the output file(s) of a process</li> <li>How to use the <code>tuple</code> qualifier to define the output channel(s) of a process</li> <li>How to manage multiple output files using glob patterns</li> <li>How to use dynamic output file names</li> <li>How to use composite inputs and outputs</li> <li>How to define outputs</li> </ol>"},{"location":"archive/basic_training/processes/#when","title":"When","text":"<p>The <code>when</code> declaration allows you to define a condition that must be verified in order to execute the process. This can be any expression that evaluates a boolean value.</p> <p>Warning</p> <p>Deprecated since version 24.10.0: Use conditional logic (e.g. <code>if</code> statement, filter operator) in the calling workflow instead.</p> <p>It is useful to enable/disable the process execution depending on the state of various inputs and parameters. For example:</p> snippet.nf<pre><code>params.dbtype = 'nr'\nparams.prot = 'data/prots/*.tfa'\nproteins = Channel.fromPath(params.prot)\n\nprocess FIND {\n    debug true\n\n    input:\n    path fasta\n    val type\n\n    when:\n    fasta.name =~ /^BB11.*/ &amp;&amp; type == 'nr'\n\n    script:\n    \"\"\"\n    echo blastp -query $fasta -db nr\n    \"\"\"\n}\n\nworkflow {\n    result = FIND(proteins, params.dbtype)\n}\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to use the <code>when</code> declaration to allow conditional processes</li> </ol>"},{"location":"archive/basic_training/processes/#directives","title":"Directives","text":"<p>Directive declarations allow the definition of optional settings that affect the execution of the current process without affecting the semantic of the task itself.</p> <p>They must be entered at the top of the process body, before any other declaration blocks (i.e., input, output, etc.).</p> <p>Directives are commonly used to define the amount of computing resources to be used or other meta directives that allow the definition of extra configuration of logging information. For example:</p> snippet.nf<pre><code>process FOO {\n    cpus 2\n    memory 1.GB\n    container 'image/name'\n\n    script:\n    \"\"\"\n    echo your_command --this --that\n    \"\"\"\n}\n</code></pre> <p>The complete list of directives is available at this link. Some of the most common are described in detail below.</p>"},{"location":"archive/basic_training/processes/#resource-allocation","title":"Resource allocation","text":"<p>Directives that allow you to define the amount of computing resources to be used by the process. These are:</p> Name Description <code>cpus</code> Allows you to define the number of (logical) CPUs required by the process\u2019 task. <code>time</code> Allows you to define how long the task is allowed to run (e.g., time 1h: 1 hour, 1s 1 second, 1m 1 minute, 1d 1 day). <code>memory</code> Allows you to define how much memory the task is allowed to use (e.g., 2 GB is 2 GB). Can also use B, KB,MB,GB and TB. <code>disk</code> Allows you to define how much local disk storage the task is allowed to use. <p>These directives can be used in combination with each other to allocate specific resources to each process. For example:</p> snippet.nf<pre><code>process FOO {\n    cpus 2\n    memory 1.GB\n    time '1h'\n    disk '10 GB'\n\n    script:\n    \"\"\"\n    echo your_command --this --that\n    \"\"\"\n}\n</code></pre>"},{"location":"archive/basic_training/processes/#publishdir-directive","title":"PublishDir directive","text":"<p>Given each task is being executed in separate temporary <code>work/</code> folder (e.g., <code>work/f1/850698\u2026</code>), you may want to save important, non-intermediary, and/or final files in a results folder.</p> <p>To store our workflow result files, you need to explicitly mark them using the directive publishDir in the process that\u2019s creating the files. For example:</p> snippet.nf<pre><code>reads_ch = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')\n\nprocess FOO {\n    publishDir \"results\", pattern: \"*.bam\"\n\n    input:\n    tuple val(sample_id), path(sample_id_paths)\n\n    output:\n    tuple val(sample_id), path(\"*.bam\")\n    tuple val(sample_id), path(\"*.bai\")\n\n    script:\n    \"\"\"\n    echo your_command_here --sample $sample_id_paths &gt; ${sample_id}.bam\n    echo your_command_here --sample $sample_id_paths &gt; ${sample_id}.bai\n    \"\"\"\n}\n\nworkflow {\n    FOO(reads_ch)\n}\n</code></pre> <p>The above example will copy all BAM files created by the <code>FOO</code> process into the directory path <code>results</code>.</p> <p>Tip</p> <p>The publish directory can be local or remote. For example, output files could be stored using an AWS S3 bucket by using the <code>s3://</code> prefix in the target path.</p> <p>You can use more than one <code>publishDir</code> to keep different outputs in separate directories. For example:</p> snippet.nf<pre><code>reads_ch = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')\n\nprocess FOO {\n    publishDir \"results/bam\", pattern: \"*.bam\"\n    publishDir \"results/bai\", pattern: \"*.bai\"\n\n    input:\n    tuple val(sample_id), path(sample_id_paths)\n\n    output:\n    tuple val(sample_id), path(\"*.bam\")\n    tuple val(sample_id), path(\"*.bai\")\n\n    script:\n    \"\"\"\n    echo your_command_here --sample $sample_id_paths &gt; ${sample_id}.bam\n    echo your_command_here --sample $sample_id_paths &gt; ${sample_id}.bai\n    \"\"\"\n}\n\nworkflow {\n    FOO(reads_ch)\n}\n</code></pre> <p>Exercise</p> <p>Edit the <code>publishDir</code> directive in the previous example to store the output files for each sample type in a different directory.</p> Solution <p>Your solution could look something like this:</p> snippet.nf<pre><code>reads_ch = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')\n\nprocess FOO {\n    publishDir \"results/$sample_id\", pattern: \"*.{bam,bai}\"\n\n    input:\n...\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to use the cpus, time, memory, and disk directives to define the amount of computing resources to be used by the process</li> <li>How to use the publishDir directive to store the output files in a results folder</li> </ol>"},{"location":"archive/basic_training/rnaseq_pipeline/","title":"Simple RNA-Seq workflow","text":"<p>To demonstrate a real-world biomedical scenario, you will implement a proof of concept RNA-Seq workflow which:</p> <ol> <li>Indexes a transcriptome file</li> <li>Performs quality controls</li> <li>Performs quantification</li> <li>Creates a MultiQC report</li> </ol> <p>This will be done using a series of seven scripts. Each script will build on the previous to create a complete workflow. You can find these in the tutorial folder (<code>script1.nf</code> - <code>script7.nf</code>). These scripts will make use of third-party tools that are known by many bioinformaticians:</p> <ol> <li>Salmon is a tool for quantifying molecules known as transcripts through a type of data called RNA-seq data.</li> <li>FastQC is a tool for quality analysis of high throughput sequence data. You can think of it as a way to assess the quality of your data.</li> <li>MultiQC searches a given directory for analysis logs and compiles a HTML report for easy viewing. It's a general use tool, perfect for summarizing the output from numerous bioinformatics tools.</li> </ol> <p>Although you may be unfamiliar with these tools, they represent real world bioinformatic tools and will be used to teach you how to use Nextflow to create a workflow.</p>"},{"location":"archive/basic_training/rnaseq_pipeline/#define-the-workflow-parameters","title":"Define the workflow parameters","text":"<p>Parameters are inputs and options that can be modified when the workflow is executed.</p> <p>The script <code>script1.nf</code> defines three workflow input parameters and uses the groovy <code>println</code> command to print one of these to the console.</p> script1.nf<pre><code>params.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\n\nprintln \"reads: $params.reads\"\n</code></pre> <p>Run it by using the following command:</p> <pre><code>nextflow run script1.nf\n</code></pre> <p>Parameters are special in Nextflow as they can be modified at the time you execute your command, for example:</p> <pre><code>nextflow run script1.nf --reads '/workspaces/training/nf-training/data/ggal/lung_{1,2}.fq'\n</code></pre> <p>Your output will look something like this:</p> Output<pre><code>N E X T F L O W  ~  version 23.10.1\nLaunching `script1.nf` [big_baekeland] DSL2 - revision: 86d466d737\nreads: /workspaces/training/nf-training/data/ggal/lung_{1,2}.fq\n</code></pre> <p>Exercise</p> <p>Add a fourth parameter named <code>outdir</code> to <code>script1.nf</code> and give it the string \"results\".</p> Solution script1.nf<pre><code>params.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\nparams.outdir = \"results\"\n</code></pre> <p>The <code>log.info</code> command can be used to print multiline information using groovy\u2019s logger functionality. Instead of writing a series of <code>println</code> commands, it can be used to include a multiline message.</p> example.nf<pre><code>log.info \"\"\"\\\n    This is\n    a multiline\n    message\n\"\"\"\n</code></pre> <p>Exercise</p> <p>Modify <code>script1.nf</code> to print all of the workflow parameters by using a single <code>log.info</code> command as a multiline string statement.</p> <p> See an example here.</p> Solution <p>Add the following to your script file:</p> script1.nf<pre><code>log.info \"\"\"\\\n    R N A S E Q - N F   P I P E L I N E\n    ===================================\n    transcriptome: ${params.transcriptome_file}\n    reads        : ${params.reads}\n    outdir       : ${params.outdir}\n    \"\"\"\n    .stripIndent(true)\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to define parameters in your workflow script</li> <li>How to pass parameters by using the command line</li> <li>How to use <code>log.info</code> to print information and save it in the log execution file</li> </ol>"},{"location":"archive/basic_training/rnaseq_pipeline/#create-a-transcriptome-index-file","title":"Create a transcriptome index file","text":"<p>Nextflow allows the execution of any command or script by using a <code>process</code> definition.</p> <p>A <code>process</code> is defined by providing three main declarations:</p> <ul> <li><code>input</code></li> <li><code>output</code></li> <li><code>script</code></li> </ul> <p>To add a transcriptome <code>INDEX</code> processing step to your pipeline, you will need to add the following code block to your <code>script1.nf</code>. Alternatively, this code block has already been added to <code>script2.nf</code>.</p> script2.nf<pre><code>/*\n * define the `INDEX` process that creates a binary index\n * given the transcriptome file\n */\nprocess INDEX {\n    input:\n    path transcriptome\n\n    output:\n    path 'salmon_index'\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_index\n    \"\"\"\n}\n</code></pre> <p>Additionally, you will need to add a workflow scope containing an input channel definition and the index process:</p> script2.nf<pre><code>workflow {\n    index_ch = INDEX(params.transcriptome_file)\n}\n</code></pre> <p>Here, the <code>params.transcriptome_file</code> parameter is used as the input for the <code>INDEX</code> process. The <code>INDEX</code> process (using the <code>salmon</code> tool) creates <code>salmon_index</code>, an indexed transcriptome that is passed as an output to the <code>index_ch</code> channel.</p> <p>Info</p> <p>The <code>input</code> declaration defines a <code>transcriptome</code> path variable which is used in the <code>script</code> as a reference (using the dollar symbol) in the Salmon execution command.</p> <p>Warning</p> <p>Resource requirements such as CPUs and memory limits can change with different workflow executions and platforms. Nextflow can use <code>$task.cpus</code> as a variable for the number of CPUs. See process directives documentation for more details.</p> <p>Exercise</p> <p>Use the <code>nextflow run</code> command to execute <code>script2.nf</code>:</p> <pre><code>nextflow run script2.nf\n</code></pre> <p>This execution will fail because <code>salmon</code> is not installed in your environment. Fortunately, a docker container image with the salmon software is available and has already been defined in your <code>nextflow.config</code> file.</p> <p>Nextflow has support for managing the execution of processes in Docker containers. This is useful when you need to execute a process that requires a specific software version or a specific operating system.</p> <p>Exercise</p> <p>Add the command line option <code>-with-docker</code> to launch <code>script2.nf</code> with the docker container:</p> <pre><code>nextflow run script2.nf -with-docker\n</code></pre> <p>This time the execution will work because it uses the Docker container <code>nextflow/rnaseq-nf</code> that is defined in the <code>nextflow.config</code> file in your current directory. If you are running this script locally, you will need to download Docker to your machine, log in, activate Docker, and allow the script to download the container containing the run scripts.</p> <p>You can learn more about Docker here.</p> <p>To avoid being required to add <code>-with-docker</code> to your execution command every time you execute the script, you can enable docker in your <code>nextflow.config</code> file.</p> <p>Exercise</p> <p>Enable docker by adding <code>docker.enabled = true</code> to your <code>nextflow.config</code> file.</p> <p>Viewing a channel with the <code>view</code> operator is a useful way to see what is in a channel and is useful for testing and debugging:</p> <p>Exercise</p> <p>Print the output of the <code>index_ch</code> channel by using the view operator.</p> Solution <p>Add the following to the end of your workflow block in your script file</p> script2.nf<pre><code>workflow {\n    index_ch = INDEX(params.transcriptome_file)\n    index_ch.view()\n}\n</code></pre> <p>Directives are used to specify the execution requirements of a process. For example, the <code>cpus</code> directive specifies the number of CPUs required to execute the process. Directives can be added under the <code>process</code> declaration.</p> script2.nf<pre><code>process PROCESS_NAME {\n    cpus 2\n    ...\n}\n</code></pre> <p>Exercise</p> <p>Add the <code>cpus</code> directive to the <code>INDEX</code> process to modify the number of CPUs allocated for its execution.</p> Solution <p>Add <code>cpus 2</code> to the top of the index process:</p> script2.nf<pre><code>process INDEX {\n    cpus 2\n\n    input:\n    ...\n</code></pre> <p>You can check the directive has been applied by viewing the script executed in the work directory. Look for the hexadecimal (e.g. <code>work/7f/f285b80022d9f61e82cd7f90436aa4/</code>), then <code>cat</code> the <code>.command.sh</code> file.</p> <pre><code>cat work/7f/f285b80022d9f61e82cd7f90436aa4/.command.sh\n</code></pre> <p>Nextflow will organize the process work directory into a series of folders. The hexadecimal folder name is the process identifier. You can view the structure of these files using the <code>tree</code> command.</p> <p>For example, executing <code>tree work</code> should look something like this:</p> Output<pre><code>work\n\u251c\u2500\u2500 17\n\u2502   \u2514\u2500\u2500 263d3517b457de4525513ae5e34ea8\n\u2502       \u251c\u2500\u2500 index\n\u2502       \u2502   \u251c\u2500\u2500 complete_ref_lens.bin\n\u2502       \u2502   \u251c\u2500\u2500 ctable.bin\n\u2502       \u2502   \u251c\u2500\u2500 ctg_offsets.bin\n\u2502       \u2502   \u251c\u2500\u2500 duplicate_clusters.tsv\n\u2502       \u2502   \u251c\u2500\u2500 eqtable.bin\n\u2502       \u2502   \u251c\u2500\u2500 info.json\n\u2502       \u2502   \u251c\u2500\u2500 mphf.bin\n\u2502       \u2502   \u251c\u2500\u2500 pos.bin\n\u2502       \u2502   \u251c\u2500\u2500 pre_indexing.log\n\u2502       \u2502   \u251c\u2500\u2500 rank.bin\n\u2502       \u2502   \u251c\u2500\u2500 refAccumLengths.bin\n\u2502       \u2502   \u251c\u2500\u2500 ref_indexing.log\n\u2502       \u2502   \u251c\u2500\u2500 reflengths.bin\n\u2502       \u2502   \u251c\u2500\u2500 refseq.bin\n\u2502       \u2502   \u251c\u2500\u2500 seq.bin\n\u2502       \u2502   \u2514\u2500\u2500 versionInfo.json\n\u2502       \u2514\u2500\u2500 transcriptome.fa -&gt; /workspaces/training/data/ggal/transcriptome.fa\n\u251c\u2500\u2500 7f\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to define a process executing a custom command</li> <li>How process inputs are declared</li> <li>How process outputs are declared</li> <li>How to view a channel</li> <li>How to add a directive to a process</li> </ol>"},{"location":"archive/basic_training/rnaseq_pipeline/#collect-read-files-by-pairs","title":"Collect read files by pairs","text":"<p>There are numerous channel factories that can be used to create channels. In this step, you will use the fromFilePairs channel factory to create a channel of read pairs.</p> <p>The <code>fromFilePairs</code> channel factory takes a glob pattern as input and returns a channel of tuples. Each tuple contains two items: the first is the read pair prefix and the second is a list of paths to the read files.</p> <p>By adding the <code>view</code> operator to the <code>read_pairs_ch</code> channel, you can see the contents of the channel.</p> <p>Exercise</p> <p>Add the <code>read_pairs_ch.view()</code> command to the end of your workflow block in your script file.</p> Solution <p>Add the following to the end of your workflow block in your script file</p> script3.nf<pre><code>read_pairs_ch.view()\n</code></pre> <p>It will print something similar to this:</p> Output<pre><code>[gut, [/.../data/ggal/gut_1.fq, /.../data/ggal/gut_2.fq]]\n</code></pre> <p>The above exercise shows how the <code>read_pairs_ch</code> channel emits tuples composed of two items, where the first is the read pair prefix and the second is a list representing the actual files.</p> <p>Glob patterns can also be used to create channels of files. For example, the following command creates a channel of all the files in the <code>data/ggal</code> directory:</p> <pre><code>nextflow run script3.nf --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>Warning</p> <p>File paths that include one or more wildcards ie. <code>*</code>, <code>?</code>, etc., MUST be wrapped in single-quoted characters to avoid Bash expanding the glob.</p> <p>The <code>set</code> operator can also be used to define a new channel variable in place of an <code>=</code> assignment.</p> <p>Exercise</p> <p>Use the set operator in place of <code>=</code> assignment to define the <code>read_pairs_ch</code> channel.</p> Solution script3.nf<pre><code>Channel\n    .fromFilePairs(params.reads)\n    .set { read_pairs_ch }\n</code></pre> <p>Channel factories also have options that can be used to modify their behaviour. For example, the <code>checkIfExists</code> option can be used to check if the specified path contains file pairs. If the path does not contain file pairs, an error is thrown. A full list of options can be found in the channel factory documentation.</p> <p>Exercise</p> <p>Use the <code>checkIfExists</code> option for the fromFilePairs channel factory to check if the specified path contains file pairs.</p> Solution script3.nf<pre><code>Channel\n    .fromFilePairs(params.reads, checkIfExists: true)\n    .set { read_pairs_ch }\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to use <code>fromFilePairs</code> to handle read pair files</li> <li>How to use the <code>set</code> operator to define a new channel variable</li> <li>How to use the <code>checkIfExists</code> option to check for the existence of input files</li> </ol> <p>Info</p> <p>The declaration of a channel can be before the workflow scope or within it. As long as it is upstream of the process that requires the specific channel.</p>"},{"location":"archive/basic_training/rnaseq_pipeline/#perform-expression-quantification","title":"Perform expression quantification","text":"<p><code>script4.nf</code> adds a gene expression <code>QUANTIFICATION</code> process and a call to it within the workflow scope. Quantification requires the index transcriptome and RNA-Seq read pair fastq files.</p> <p>In the workflow scope, note how the <code>index_ch</code> channel is assigned as output in the <code>INDEX</code> process.</p> <p>Next, note that the first input channel for the <code>QUANTIFICATION</code> process is the previously declared <code>index_ch</code>, which contains the <code>path</code> to the <code>salmon_index</code>.</p> <p>Also, note that the second input channel for the <code>QUANTIFICATION</code> process, is the <code>read_pair_ch</code> you just created. This being a <code>tuple</code> composed of two items (a value: <code>sample_id</code> and a list of paths to the fastq reads: <code>reads</code>) in order to match the structure of the items emitted by the <code>fromFilePairs</code> channel factory.</p> <p>Execute it by using the following command:</p> <pre><code>nextflow run script4.nf -resume\n</code></pre> <p>You will see the execution of the <code>QUANTIFICATION</code> process.</p> <p>When using the <code>-resume</code> option, any step that has already been processed is skipped.</p> <p>Try to execute the same script again with more read files, as shown below:</p> <pre><code>nextflow run script4.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>You will notice that the <code>QUANTIFICATION</code> process is executed multiple times.</p> <p>Nextflow parallelizes the execution of your workflow simply by providing multiple sets of input data to your script.</p> <p>Tip</p> <p>It may be useful to apply optional settings to a specific process using directives by specifying them in the process body.</p> <p>Exercise</p> <p>Add a tag directive to the <code>QUANTIFICATION</code> process to provide a more readable execution log.</p> Solution <p>Add the following before the input declaration:</p> script4.nf<pre><code>process QUANTIFICATION {\n    tag \"Salmon on $sample_id\"\n\n    input:\n    ...\n</code></pre> <p>Exercise</p> <p>Add a publishDir directive to the <code>QUANTIFICATION</code> process to store the process results in a directory of your choice.</p> Solution <p>Add the following before the <code>input</code> declaration in the <code>QUANTIFICATION</code> process:</p> script4.nf<pre><code>process QUANTIFICATION {\n    tag \"Salmon on $sample_id\"\n    publishDir params.outdir, mode: 'copy'\n\n    input:\n    ...\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to connect two processes together by using the channel declarations</li> <li>How to <code>resume</code> the script execution and skip cached steps</li> <li>How to use the <code>tag</code> directive to provide a more readable execution output</li> <li>How to use the <code>publishDir</code> directive to store a process results in a path of your choice</li> </ol>"},{"location":"archive/basic_training/rnaseq_pipeline/#quality-control","title":"Quality control","text":"<p>Next, you will implement a <code>FASTQC</code> quality control step for your input reads (using the label <code>fastqc</code>). The inputs are the same as the read pairs used in the <code>QUANTIFICATION</code> step.</p> <p>You can run it by using the following command:</p> <pre><code>nextflow run script5.nf -resume\n</code></pre> <p>Nextflow DSL2 knows to split the <code>reads_pair_ch</code> into two identical channels as they are required twice as an input for both of the <code>FASTQC</code> and the <code>QUANTIFICATION</code> process.</p>"},{"location":"archive/basic_training/rnaseq_pipeline/#multiqc-report","title":"MultiQC report","text":"<p>This step collects the outputs from the <code>QUANTIFICATION</code> and <code>FASTQC</code> processes to create a final report using the MultiQC tool.</p> <p>You can execute <code>script6.nf</code> with the following command:</p> <pre><code>nextflow run script6.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>It creates the final report in the <code>results</code> folder in the current <code>work</code> directory.</p> <p>In this script, note the use of the mix and collect operators chained together to gather the outputs of the <code>QUANTIFICATION</code> and <code>FASTQC</code> processes as a single input. Operators can be used in combinations to combine, split, and transform channels.</p> script6.nf<pre><code>MULTIQC(quant_ch.mix(fastqc_ch).collect())\n</code></pre> <p>You will only want one task of MultiQC to be executed to produce one report. Therefore, you can use the <code>mix</code> channel operator to combine the <code>quant_ch</code> and the <code>fastqc_ch</code> channels, followed by the <code>collect</code> operator, to return the complete channel contents as a single element.</p> <p>Exercise</p> <p>Remove the <code>collect</code> operators from the <code>MULTIQC</code> process and run the script again. See what happens.</p> Solution <p>Modify the workflow block to look like this:</p> script6.nf<pre><code>workflow {\n    Channel\n        .fromFilePairs(params.reads, checkIfExists: true)\n        .set { read_pairs_ch }\n\n    index_ch = INDEX(params.transcriptome_file)\n    quant_ch = QUANTIFICATION(index_ch, read_pairs_ch)\n    fastqc_ch = FASTQC(read_pairs_ch)\n    MULTIQC(quant_ch.mix(fastqc_ch))\n}\n</code></pre> <p>Note how the <code>MULTIQC</code> process is executed 6 times.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to collect many outputs to a single input with the <code>collect</code> operator</li> <li>How to <code>mix</code> two channels into a single channel</li> <li>How to chain two or more operators together</li> </ol>"},{"location":"archive/basic_training/rnaseq_pipeline/#handle-completion-event","title":"Handle completion event","text":"<p>This step shows how to execute an action when the workflow completes the execution.</p> <p>Note that Nextflow processes define the execution of asynchronous tasks ,i.e., they are not executed one after another as if they were written in the workflow script in a common imperative programming language.</p> <p>The script uses the <code>workflow.onComplete</code> event handler to print a confirmation message when the script completes.</p> <p>Try to run it by using the following command:</p> <pre><code>nextflow run script7.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to add completion events to your workflow</li> </ol>"},{"location":"archive/basic_training/rnaseq_pipeline/#email-notifications","title":"Email notifications","text":"<p>Send a notification email when the workflow execution completes using the <code>-N &lt;email address&gt;</code> command-line option.</p> <p>Note: this requires the configuration of a SMTP server in the nextflow config file. Below is an example <code>nextflow.config</code> file showing the settings you would have to configure:</p> nextflow.config<pre><code>mail {\n    from = 'info@nextflow.io'\n    smtp.host = 'email-smtp.eu-west-1.amazonaws.com'\n    smtp.port = 587\n    smtp.user = \"xxxxx\"\n    smtp.password = \"yyyyy\"\n    smtp.auth = true\n    smtp.starttls.enable = true\n    smtp.starttls.required = true\n}\n</code></pre> <p>See mail documentation for details.</p>"},{"location":"archive/basic_training/rnaseq_pipeline/#custom-scripts","title":"Custom scripts","text":"<p>Real-world workflows use a lot of custom user scripts (BASH, R, Python, etc.). Nextflow allows you to consistently use and manage these scripts. Simply put them in a directory named <code>bin</code> in the workflow project root. They will be automatically added to the workflow execution <code>PATH</code>.</p> <p>For example, the <code>FASTQC</code> process in <code>script7.nf</code> could be replaced by creating an executable script named <code>fastqc.sh</code> in the <code>bin</code> directory as shown below:</p> <p>Create a new file named <code>fastqc.sh</code> with the following content:</p> fastqc.sh<pre><code>#!/bin/bash\nset -e\nset -u\n\nsample_id=${1}\nreads=${2}\n\nmkdir fastqc_${sample_id}_logs\nfastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n</code></pre> <p>Give it execute permission and move it into the <code>bin</code> directory:</p> <pre><code>chmod +x fastqc.sh\nmkdir -p bin\nmv fastqc.sh bin\n</code></pre> <p>Open the <code>script7.nf</code> file and replace the <code>FASTQC</code> process script block with the following code:</p> script7.nf<pre><code>script:\n\"\"\"\nfastqc.sh \"$sample_id\" \"$reads\"\n\"\"\"\n</code></pre> <p>Exercise</p> <p>Use the example above to replace the <code>FASTQC</code> process script block in <code>script7.nf</code> with an executable <code>fastqc.sh</code> script.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to write or use existing custom scripts in your Nextflow workflow</li> <li>How to avoid the use of absolute paths by having your scripts in the <code>bin/</code> folder</li> </ol>"},{"location":"archive/basic_training/rnaseq_pipeline/#metrics-and-reports","title":"Metrics and reports","text":"<p>Nextflow can produce multiple reports and charts providing several runtime metrics and execution information. These can be enabled by using the following command line options:</p> <p>The <code>-with-report</code> option enables the creation of the workflow execution report.</p> <p>The <code>-with-trace</code> option enables the creation of a tab separated value (TSV) file containing runtime information for each executed task.</p> <p>The <code>-with-timeline</code> option enables the creation of the workflow timeline report showing how processes were executed over time. This may be useful to identify the most time consuming tasks and bottlenecks.</p> <p>Finally, the <code>-with-dag</code> option enables the rendering of the workflow execution direct acyclic graph representation. The dag needs to be given a name (<code>-with-dag dag.png</code>). Note: This feature requires the installation of Graphviz on your computer. See here for further details. You can also output HTML DAGs, and the <code>-preview</code> command my allow you to have a look at an approximate DAG without having to run the pipeline.</p> <p>Exercise</p> <p>Execute <code>script7.nf</code> and and generate a report (<code>-with-report</code>), trace (<code>-with-trace</code>), timeline (<code>-with-timeline</code>), and dag ('-with-dag dag.png').</p> Solution <pre><code>nextflow run script7.nf -with-docker -with-report -with-trace -with-timeline -with-dag dag.png\n</code></pre> <p>You can view the DAG by using the following command:</p> <pre><code>open dag.png\n</code></pre> <p>You can view the HTML files by right-clicking on the file name in the left side-bar and choosing the Show Preview menu item.</p> <p>Warning</p> <p>Run time metrics may be incomplete for runs with short running tasks, as in the case of this tutorial.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to generate reports and charts for your executions</li> </ol>"},{"location":"archive/basic_training/rnaseq_pipeline/#run-a-project-from-github","title":"Run a project from GitHub","text":"<p>Nextflow allows the execution of a workflow project directly from a GitHub repository (or similar services, e.g., BitBucket and GitLab).</p> <p>This simplifies the sharing and deployment of complex projects and tracking changes in a consistent manner.</p> <p>The following GitHub repository hosts a version of the workflow introduced in this tutorial: https://github.com/nextflow-io/rnaseq-nf</p> <p>You can run it by specifying the project name and launching each task of the execution as a Docker container run command:</p> <pre><code>nextflow run nextflow-io/rnaseq-nf -with-docker\n</code></pre> <p>It automatically downloads the container and stores it in the <code>$HOME/.nextflow</code> folder.</p> <p>The nextflow <code>info</code> command can be used to show the project information:</p> <pre><code>nextflow info nextflow-io/rnaseq-nf\n</code></pre> <p>Nextflow allows the execution of a specific revision of your project by using the <code>-r</code> command line option. For example:</p> <pre><code>nextflow run nextflow-io/rnaseq-nf -r v2.1 -with-docker\n</code></pre> <p>Revisions are defined by using Git tags or branches defined in the project repository.</p> <p>Tags enable precise control of the changes in your project files and dependencies over time.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to execute a project directly from GitHub</li> <li>How to specify a specific revision of a project</li> </ol>"},{"location":"archive/basic_training/seqera_platform/","title":"Get started with Seqera Platform","text":"<p>Seqera Platform, previously known as Nextflow Tower, is the centralized command post for data management and workflows. It brings monitoring, logging and observability to distributed workflows and simplifies the deployment of workflows on any cloud, cluster or laptop. In Seqera Platform terminology, a workflow is what we've been working on so far, and pipelines are pre-configured workflows that can be used by all users in a workspace. It is composed of a workflow repository, launch parameters, and a compute environment. We'll stick to these definitions in this section.</p> <p>Seqera core features include:</p> <ul> <li>The launching of pre-configured pipelines with ease.</li> <li>Programmatic integration to meet the needs of an organization.</li> <li>Publishing pipelines to shared workspaces.</li> <li>Management of the infrastructure required to run data analysis at scale.</li> </ul> <p>Tip</p> <p>Sign up to try Seqera for free or request a demo for deployments in your own on-premise or cloud environment.</p> <p>You can use Seqera Platform via either the CLI, through the online GUI or through the API.</p>"},{"location":"archive/basic_training/seqera_platform/#cli","title":"CLI","text":"<p>You will need to set up your environment to use Seqera Platform. This is a one-time setup.</p> <p>Create an account and login into Seqera Platform.</p> <p>1. Create a new token</p> <p>You can access your tokens from the Settings drop-down menu:</p> <p></p> <p>2. Name your token</p> <p></p> <p>3. Save your token safely</p> <p>Copy and keep your new token in a safe place.</p> <p></p> <p>4. Export your token</p> <p>Once your token has been created, open a terminal and type:</p> <pre><code>export TOWER_ACCESS_TOKEN=eyxxxxxxxxxxxxxxxQ1ZTE=\n</code></pre> <p>Where <code>eyxxxxxxxxxxxxxxxQ1ZTE=</code> is the token you have just created.</p> <p>Note</p> <p>Check your <code>nextflow -version</code>. Bearer tokens require Nextflow version 20.10.0 or later and can be set with the second command shown above. You can change the version if necessary.</p> <p>To submit a pipeline to a Workspace using the Nextflow command-line tool, add the workspace ID to your environment. For example:</p> <pre><code>export TOWER_WORKSPACE_ID=000000000000000\n</code></pre> <p>The workspace ID can be found on the organization\u2019s Workspaces overview page.</p> <p>5. Run Nextflow with Seqera Platform</p> <p>Run your Nextflow workflows as usual with the addition of the <code>-with-tower</code> command:</p> <pre><code>nextflow run hello.nf -with-tower\n</code></pre> <p>You will see and be able to monitor your Nextflow jobs in Seqera Platform.</p> <p>To configure and execute Nextflow jobs in Cloud environments, visit the Compute environments section.</p> <p>Exercise</p> <p>Run the RNA-Seq <code>script7.nf</code> using the <code>-with-tower</code> flag, after correctly completing the token settings outlined above.</p> Tip <p>Go to https://cloud.seqera.io/, login, then click the run tab, and select the run that you just submitted. If you can\u2019t find it, double check your token was entered correctly.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to create and add your token and workspace.</li> <li>How to launch a pipeline with Seqera Platform.</li> </ol>"},{"location":"archive/basic_training/seqera_platform/#online-gui","title":"Online GUI","text":"<p>To run using the GUI, there are three main steps:</p> <ol> <li>Create an account and login into Seqera Platform, available free of charge, at cloud.seqera.io.</li> <li>Create and configure a new compute environment.</li> <li>Start launching pipelines.</li> </ol>"},{"location":"archive/basic_training/seqera_platform/#configuring-your-compute-environment","title":"Configuring your compute environment","text":"<p>Seqera Platform uses the concept of Compute Environments to define the execution platform where a workflow will run.</p> <p>It supports the launching of workflows into a growing number of cloud and on-premise infrastructures.</p> <p></p> <p>Each compute environment must be pre-configured to enable Seqera Platform to submit tasks. You can read more on how to set up each environment using the links below.</p> <p>The following guides describe how to configure each of these compute environments.</p> <ul> <li>AWS Batch</li> <li>Azure Batch</li> <li>Google Batch</li> <li>Google Life Sciences</li> <li>HPC (LSF, Slurm, Grid Engine, Altair PBS Pro)</li> <li>Amazon Kubernetes (EKS)</li> <li>Google Kubernetes (GKE)</li> <li>Hosted Kubernetes</li> </ul>"},{"location":"archive/basic_training/seqera_platform/#selecting-a-default-compute-environment","title":"Selecting a default compute environment","text":"<p>If you have more than one Compute Environment, you can select which one will be used by default when launching a pipeline.</p> <ol> <li>Navigate to your compute environments.</li> <li>Choose your default environment by selecting the Make primary button.</li> </ol> <p>Congratulations!</p> <p>You are now ready to launch workflows with your primary compute environment.</p>"},{"location":"archive/basic_training/seqera_platform/#launchpad","title":"Launchpad","text":"<p>Launchpad makes it easy for any workspace user to launch a pre-configured pipeline.</p> <p></p> <p>A pipeline is a repository containing a Nextflow workflow, a compute environment and workflow parameters.</p>"},{"location":"archive/basic_training/seqera_platform/#pipeline-parameters-form","title":"Pipeline Parameters Form","text":"<p>Launchpad automatically detects the presence of a <code>nextflow_schema.json</code> in the root of the repository and dynamically creates a form where users can easily update the parameters.</p> <p>Info</p> <p>The parameter forms view will appear if the pipeline has a Nextflow schema file for the parameters. Please refer to the Nextflow Schema guide to learn more about the schema file use-cases and how to create them.</p> <p>This makes it trivial for users without any expertise in Nextflow to enter their workflow parameters and launch.</p> <p></p>"},{"location":"archive/basic_training/seqera_platform/#adding-a-new-pipeline","title":"Adding a new pipeline","text":"<p>Adding a pipeline to the pre-saved workspace launchpad is detailed in full on the Seqera webpage docs.</p> <p>In brief, these are the steps you need to follow to set up a pipeline.</p> <ol> <li>Select the Launchpad button in the navigation bar. This will open the Launch Form.</li> <li>Select a compute environment.</li> <li>Enter the repository of the workflow you want to launch. e.g. https://github.com/nf-core/rnaseq.git</li> <li>Select a workflow Revision number. The Git default branch (main/master) or <code>manifest.defaultBranch</code> in the Nextflow configuration will be used by default.</li> <li>Set the Work directory location of the Nextflow work directory. The location associated with the compute environment will be selected by default.</li> <li>Enter the name(s) of each of the Nextflow Config profiles followed by the <code>Enter</code> key. See the Nextflow Config profiles documentation for more details.</li> <li>Enter any workflow parameters in YAML or JSON format. YAML example:</li> </ol> <pre><code>reads: \"s3://nf-bucket/exome-data/ERR013140_{1,2}.fastq.bz2\"\npaired_end: true\n</code></pre> <ol> <li>Select Launch to begin the pipeline execution.</li> </ol> <p>Info</p> <p>Nextflow workflows are simply Git repositories and can be changed to any public or private Git-hosting platform. See Git Integration in the Seqera Platform docs and Pipeline Sharing in the Nextflow docs for more details.</p> <p>Note</p> <p>The credentials associated with the compute environment must be able to access the work directory.</p> <p>Info</p> <p>In the configuration, the full path to a bucket must be specified with single quotes around strings and no quotes around booleans or numbers.</p> <p>Tip</p> <p>To create your own customized Nextflow Schema for your workflow, see the examples from the <code>nf-core</code> workflows that have adopted this approach. For example, eager and rnaseq.</p> <p>For advanced settings options check out this page.</p> <p>There is also community support available if you get into trouble, join the Nextflow Slack by following this link.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to create an account and login into Seqera Platform</li> <li>How to configure your compute environment.</li> <li>How to add, customize, and launch a pipeline with Seqera Platform.</li> </ol>"},{"location":"archive/basic_training/seqera_platform/#api","title":"API","text":"<p>To learn more about using the Seqera Platform API, visit the API section in the documentation.</p>"},{"location":"archive/basic_training/seqera_platform/#workspaces-and-organizations","title":"Workspaces and Organizations","text":"<p>Seqera Platform simplifies the development and execution of pipeline by providing a centralized interface for users and organizations.</p> <p>Each user has a unique workspace where they can interact and manage all resources such as workflows, compute environments and credentials. Details of this can be found here.</p> <p>Organisations can have multiple workspaces with customized access for specific organisation members and collaborators.</p>"},{"location":"archive/basic_training/seqera_platform/#organization-resources","title":"Organization resources","text":"<p>You can create your own organization and participant workspace by following the docs at Seqera.</p> <p>Seqera Platform allows the creation of multiple organizations, each of which can contain multiple workspaces with shared users and resources. This allows any organization to customize and organize the usage of resources while maintaining an access control layer for users associated with a workspace.</p>"},{"location":"archive/basic_training/seqera_platform/#organization-users","title":"Organization users","text":"<p>Any user can be added or removed from a particular organization or a workspace and can be allocated a specific access role within that workspace.</p> <p>The Teams feature provides a way for organizations to group various users and participants together into teams. For example, <code>workflow-developers</code> or <code>analysts</code>, and apply access control to all the users within this team collectively.</p> <p>For further information, please refer to the User Management section.</p>"},{"location":"archive/basic_training/seqera_platform/#setting-up-a-new-organization","title":"Setting up a new organization","text":"<p>Organizations are the top-level structure and contain Workspaces, Members, Teams and Collaborators.</p> <p>To create a new Organization:</p> <ol> <li>Click on the dropdown next to your name and select New organization to open the creation dialog.</li> <li> <p>On the dialog, fill in the fields as per your organization. The Name and Full name fields are compulsory.</p> <p>Note</p> <p>A valid name for the organization must follow a specific pattern. Please refer to the UI for further instructions.</p> </li> <li> <p>The rest of the fields such as Description, Location, Website URL and Logo Url are optional.</p> </li> <li> <p>Once the details are filled in, you can access the newly created organization using the organization\u2019s page, which lists all of your organizations.</p> <p>Note</p> <p>It is possible to change the values of the optional fields either using the Edit option on the organization\u2019s page or by using the Settings tab within the organization page, provided that you are the Owner of the organization.</p> <p>Tip</p> <p>A list of all the included Members, Teams and Collaborators can be found on the organization page.</p> </li> </ol> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to create a new organization</li> <li>How to access the newly created organization</li> <li>How to change organization settings</li> </ol>"},{"location":"envsetup/","title":"Environment Setup","text":"<p>The training courses offered on the Nextflow community training portal are optimized for use within our GitHub Codespaces environment.</p> <p>GitHub Codespaces offers a virtual machine with everything already set up for you, accessible from your web browser or built into your code editor (e.g., VSCode).</p> <p>If you already have a GitHub Codespaces account, click on the button below, otherwise continue in this module to set up your account for the first time.</p> <p>Let's get started!</p> <p></p> <p>For more detailed instructions for GitHub Codespaces, see the GitHub Codespaces env-setup docs. If you cannot use GitHub Codespaces and wish to use a local development environment, see the documentation for local installation. If you prefer using VS Code with a pre-configured environment, see the VS Code Devcontainers setup guide.</p> <p>Deprecation of Gitpod</p> <p>Nextflow Training used to use Gitpod until February 2025. However, the makers of Gitpod have decided to retire the free functionality in favor of their new Gitpod Flex system. For that reason, we have switched to using GitHub Codespaces, which also offer a one-click developer environment with no prior setup.</p> <p>Depending on when you signed up to Gitpod and when exactly they retire the service, you may still be able to launch the training in their old cloud IDE, though we cannot guarantee reliable access going forward: Open in Gitpod.</p>"},{"location":"envsetup/01_setup/","title":"GitHub Codespaces","text":"<p>GitHub Codespaces is a cloud development environment for teams to develop software efficiently and securely. We use it as a training environment because it allows us to provide learners with a consistent and thoroughly tested environment.</p>"},{"location":"envsetup/01_setup/#creating-a-github-account","title":"Creating a GitHub account","text":"<p>You can create a free GitHub account from the GitHub home page.</p>"},{"location":"envsetup/01_setup/#running-github-codespaces","title":"Running GitHub Codespaces","text":"<p>Once you are logged in to GitHub, open this link in your browser to open the training environment: https://codespaces.new/nextflow-io/training?quickstart=1&amp;ref=master</p> <p>Alternatively, you can click on the button shown below from the many pages in the training portal where it is displayed.</p> <p></p> <p>You should be presented with a page where you can create a new GitHub Codespace:</p> <p></p> <p>You can click \"Change options\" to configure the machine used. Using a machine with more cores allows you to take greater advantage of Nextflow's ability to parallelize workflow execution.</p> <p>For our Hello Nextflow, Nextflow For Science, and nf-core training courses, we recommend using a 4-core machine.</p> <p>The free GitHub plan includes 120 core-hours of Codespaces compute per month, which amounts to 30 hours of a 4-core machine. (See below for more information about quotas.)</p> <p>Warning</p> <p>Opening a new GitHub Codespaces environment for the first time can take several minutes. Just enough time to brew a cup of tea and check your emails, or go over the intro materials if you're in a group training.</p>"},{"location":"envsetup/01_setup/#explore-your-github-codespaces-ide","title":"Explore your GitHub Codespaces IDE","text":"<p>After GitHub Codespaces has loaded, you should see something similar to the following (which may open in light mode depending on your account preferences):</p> <p></p> <p>This is the interface of the VSCode IDE, a popular code development application that we recommend using for Nextflow development.</p> <ul> <li>The sidebar allows you to customize your GitHub Codespaces environment and perform basic tasks (copy, paste, open files, search, git, etc.). You can click the explorer button to see which files are in this repository.</li> <li>The terminal allows you to run all the programs in the repository. For example, both <code>nextflow</code> and <code>docker</code> are installed and can be executed.</li> <li>The file explorer allows you to view and edit files. Clicking on a file in the explorer will open it within the main window.</li> <li>The main editor showing you a preview of the <code>README.md</code> file. When you open code or data files, they will open there.</li> </ul>"},{"location":"envsetup/01_setup/#reopening-a-github-codespaces-session","title":"Reopening a GitHub Codespaces session","text":"<p>Once you have created an environment, you can easily resume or restart it and continue from where you left off. Your environment will time out after 30 minutes of inactivity and will save your changes for up to 2 weeks.</p> <p>You can reopen an environment from https://github.com/codespaces/. Previous environments will be listed. Click a session to resume it.</p> <p></p> <p>If you have saved the URL for your previous GitHub Codespaces environment, you can simply open it in your browser. Alternatively, click the same button that you used to create it in the first place:</p> <p></p> <p>You should see the previous session, the default option is to resume it:</p> <p></p>"},{"location":"envsetup/01_setup/#saving-files-from-github-codespaces-to-your-local-machine","title":"Saving files from GitHub Codespaces to your local machine","text":"<p>To save any file from the explorer panel, right-click the file and select <code>Download</code>.</p>"},{"location":"envsetup/01_setup/#github-codespaces-quotas","title":"GitHub Codespaces quotas","text":"<p>GitHub Codespaces gives you up to 15 GB-month storage per month, and 120 core-hours per month. This is equivalent to around 60 hours of the default environment runtime using the standard workspace (up to 2 cores, 8 GB RAM, and 32 GB storage).</p> <p>GitHub Codespaces environments are configurable. You can create them with more resources, but this will consume your free usage faster and you will have fewer hours of access to this space. Optionally, you can purchase access to more resources.</p> <p>More information can be found in the GitHub docs: About billing for GitHub Codespaces</p>"},{"location":"envsetup/02_local/","title":"Local installation","text":"<p>If you cannot use GitHub Codespaces session for any reason, you have the option of installing everything locally instead.</p> <p>Some requirements may be different depending on your local machine.</p>"},{"location":"envsetup/02_local/#requirements","title":"Requirements","text":"<p>Nextflow can be used on any POSIX-compatible system (Linux, macOS, Windows Subsystem for Linux, etc.).</p> <p>Requirements</p> <ul> <li>Bash</li> <li>Java 11 (or later, up to 21)</li> <li>Git</li> <li>Docker</li> </ul> <p>Optional requirements</p> <ul> <li>Singularity 2.5.x (or later)</li> <li>Conda 4.5 (or later)</li> <li>Graphviz</li> <li>AWS CLI</li> <li>A configured AWS Batch computing environment</li> </ul>"},{"location":"envsetup/02_local/#download-nextflow","title":"Download Nextflow","text":"<p>Execute this command in your terminal:</p> <pre><code>wget -qO- https://get.nextflow.io | bash\n</code></pre> <p>Alternatively, you could use the <code>curl</code> command:</p> <pre><code>curl -s https://get.nextflow.io | bash\n</code></pre> <p>Next, ensure that the downloaded binary is executable:</p> <pre><code>chmod +x nextflow\n</code></pre> <p>Finally, ensure the <code>nextflow</code> executable is in your <code>$PATH</code>. The executable could be in <code>/usr/local/bin</code>, <code>/bin/</code>, etc.</p>"},{"location":"envsetup/02_local/#docker","title":"Docker","text":"<p>Ensure you have Docker Desktop running on your machine. You can download Docker here.</p>"},{"location":"envsetup/02_local/#training-material","title":"Training material","text":"<p>You can view the training material here.</p> <p>To download the material, execute this command:</p> <pre><code>git clone https://github.com/nextflow-io/training.git\n</code></pre> <p>Then <code>cd</code> into the relevant directory. By default, that is <code>hello-nextflow</code>.</p>"},{"location":"envsetup/02_local/#checking-your-installation","title":"Checking your installation","text":"<p>Check that you have correctly installed <code>nextflow</code> by running the following command:</p> <pre><code>nextflow info\n</code></pre> <p>This should print the current version, system, and runtime.</p> <p>Exercise</p> <p>To test that the environment is working correctly, execute the following command:</p> <pre><code>nextflow info\n</code></pre> <p>This should come up with the Nextflow version and runtime information (actual versions may differ):</p> <pre><code>Version: 23.10.1 build 5891\nCreated: 12-01-2024 22:01 UTC\nSystem: Linux 6.1.75-060175-generic\nRuntime: Groovy 3.0.19 on OpenJDK 64-Bit Server VM 11.0.1-internal+0-adhoc..src\nEncoding: UTF-8 (UTF-8)\n</code></pre>"},{"location":"envsetup/03_devcontainer/","title":"Local installation using VSCode Devcontainers extension","text":"<p>If you have a local Docker installation or are happy to install one, the easiest way to work locally with these materials is to use Visual Studio Code's devcontainer feature. This approach provides all the necessary tools and dependencies without requiring manual installation.</p>"},{"location":"envsetup/03_devcontainer/#requirements","title":"Requirements","text":"<p>To use the devcontainer setup, you'll need:</p> <ul> <li>Visual Studio Code</li> <li>A local Docker installation, for example:</li> <li>Docker Desktop (for Windows/macOS)</li> <li>Docker Engine (for Linux)</li> <li>Colima (alternative for macOS)</li> <li>Docker Buildx (included in Docker Desktop, but may need separate installation with other Docker setups)</li> <li>Dev Containers extension for VS Code</li> </ul> <p>Your Docker installation must be running before you attempt to open the devcontainer.</p> <p>To verify that Docker buildx is available, run:</p> <pre><code>docker buildx version\n</code></pre> <p>If this command fails, you'll need to install the buildx extension before proceeding.</p>"},{"location":"envsetup/03_devcontainer/#setup-instructions","title":"Setup Instructions","text":"<p>Follow these steps to set up your local environment using VS Code devcontainers:</p>"},{"location":"envsetup/03_devcontainer/#install-the-dev-containers-extension-in-vs-code","title":"Install the \"Dev Containers\" extension in VS Code","text":"<ul> <li>Open VS Code</li> <li>Go to Extensions (Ctrl+Shift+X or Cmd+Shift+X on macOS)</li> <li>Search for \"Dev Containers\"</li> <li>Click \"Install\"</li> </ul>"},{"location":"envsetup/03_devcontainer/#clone-the-repository","title":"Clone the repository:","text":"<pre><code>git clone https://github.com/nextflow-io/training.git\ncd training\n</code></pre>"},{"location":"envsetup/03_devcontainer/#open-the-repository-in-vs-code","title":"Open the repository in VS Code:","text":"<ul> <li>Launch VS Code</li> <li>Select File -&gt; Open Folder from the menu</li> <li>Navigate to and select the training repository folder you just cloned</li> <li>Click Open</li> </ul>"},{"location":"envsetup/03_devcontainer/#reopen-in-container","title":"Reopen in Container","text":"<p>If prompted by VS Code to \"Reopen in Container\", click on it. Alternatively:</p> <ul> <li>Press F1 (or Ctrl+Shift+P / Cmd+Shift+P on macOS)</li> <li>Type \"Dev Containers: Reopen in Container\"</li> <li>Important: When prompted to select a configuration, choose the local-dev devcontainer configuration</li> </ul> <p></p> <p></p> <p>Wait for the container to build. This may take a few minutes the first time as it downloads and sets up all the necessary components.</p> <p>Once the container is built and running, you'll have a fully configured environment with all the necessary tools installed, including:</p> <ul> <li>Java</li> <li>Nextflow</li> <li>Docker</li> <li>Git</li> <li>And all other dependencies required for the training</li> </ul> <p></p>"},{"location":"envsetup/03_devcontainer/#benefits-of-using-devcontainers","title":"Benefits of Using Devcontainers","text":"<p>Using the devcontainer approach offers several advantages:</p> <ul> <li>Consistency: Ensures a consistent development environment across different machines</li> <li>Simplicity: All dependencies are pre-installed and configured</li> <li>Isolation: The development environment is isolated from your local system</li> <li>Reproducibility: Everyone using the devcontainer gets the same setup</li> <li>No manual installation: No need to manually install Java, Nextflow, and other tools</li> </ul>"},{"location":"envsetup/03_devcontainer/#checking-your-environment","title":"Checking Your Environment","text":"<p>Once your devcontainer is running, you can verify that everything is set up correctly by running:</p> <pre><code>nextflow info\n</code></pre> <p>This should display the Nextflow version and runtime information, confirming that your environment is properly configured.</p>"},{"location":"envsetup/03_devcontainer/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with the devcontainer setup:</p> <ol> <li>Ensure your Docker installation (Docker Desktop, Colima, Docker Engine, etc.) is running before opening the devcontainer</li> <li>Check that you've selected the local-dev configuration when prompted</li> <li>Verify that Docker buildx is installed and working by running <code>docker buildx version</code></li> <li>If the container fails to build, try rebuilding it by running the \"Dev Containers: Rebuild Container\" command</li> <li>For persistent issues, refer to the VS Code Dev Containers troubleshooting guide</li> </ol>"},{"location":"hello_nextflow/","title":"Hello Nextflow","text":"<p>Hello! You are now on the path to writing reproducible and scalable scientific workflows using Nextflow.</p> <p>The rise of big data has made it increasingly necessary to be able to analyze and perform experiments on large datasets in a portable and reproducible manner. Parallelization and distributed computing are the best ways to tackle this challenge, but the tools commonly available to computational scientists often lack good support for these techniques, or they provide a model that fits poorly with the needs of computational scientists. Nextflow was particularly created to address these challenges.</p> <p>During this training, you will be introduced to Nextflow in a series of complementary hands-on workshops.</p> <p>Let's get started! Click on the \"Open in GitHub Codespaces\" button below to launch the training environment (preferably in a separate tab), then read on while it loads.</p> <p></p>   Follow the videos   <p>The Hello Nextflow training has a video for each chapter, embedded at the top of each page.</p> <p>You can also find the whole playlist on the Nextflow YouTube channel.</p>"},{"location":"hello_nextflow/#learning-objectives","title":"Learning objectives","text":"<p>In this workshop, you will learn foundational concepts for building pipelines.</p> <p>By the end of this workshop you will be able to:</p> <ul> <li>Describe and utilize core Nextflow components sufficient to build a simple multi-step workflow</li> <li>Describe next-step concepts such as operators and channel factories</li> <li>Launch a Nextflow workflow locally</li> <li>Find and interpret outputs (results) and log files generated by Nextflow</li> <li>Troubleshoot basic issues</li> </ul>"},{"location":"hello_nextflow/#audience-prerequisites","title":"Audience &amp; prerequisites","text":"<p>This is a workshop for those who are completely new to Nextflow. Some basic familiarity with the command line, and common file formats is assumed.</p> <p>Prerequisites</p> <ul> <li>A GitHub account OR a local installation as described here.</li> <li>Experience with command line and basic scripting</li> </ul>"},{"location":"hello_nextflow/00_orientation/","title":"Orientation","text":"<p> See the whole playlist on the Nextflow YouTube channel.</p> <p> The video transcript is available here.</p> <p>Tip</p> <p>The YouTube videos have some super powers!</p> <ul> <li> High quality (manually curated) captions / subtitles. Switch them on with the  icon</li> <li> Video chapters in the timeline that correspond to page headings.</li> </ul>"},{"location":"hello_nextflow/00_orientation/#github-codespaces","title":"GitHub Codespaces","text":"<p>The GitHub Codespaces environment contains all the software, code and data necessary to work through this training course, so you don't need to install anything yourself. However, you do need a (free) GitHub account to log in, and you should take a few minutes to familiarize yourself with the interface.</p> <p>If you have not yet done so, please go through the Environment Setup mini-course before going any further.</p>"},{"location":"hello_nextflow/00_orientation/#working-directory","title":"Working directory","text":"<p>Throughout this training course, we'll be working in the <code>hello-nextflow/</code> directory.</p> <p>Change directory now by running this command in the terminal:</p> <pre><code>cd hello-nextflow/\n</code></pre> <p>Tip</p> <p>If for whatever reason you move out of this directory, you can always use the full path to return to it, assuming you're running this within the Github Codespaces training environment:</p> <pre><code>cd /workspaces/training/hello-nextflow\n</code></pre> <p>Now let's have a look at the contents of this directory.</p>"},{"location":"hello_nextflow/00_orientation/#materials-provided","title":"Materials provided","text":"<p>You can explore the contents of this directory by using the file explorer on the left-hand side of the training workspace. Alternatively, you can use the <code>tree</code> command.</p> <p>Throughout the course, we use the output of <code>tree</code> to represent directory structure and contents in a readable form, sometimes with minor modifications for clarity.</p> <p>Here we generate a table of contents to the second level down:</p> <pre><code>tree . -L 2\n</code></pre> <p>If you run this inside <code>hello-nextflow</code>, you should see the following output:</p> Directory contents<pre><code>.\n\u251c\u2500\u2500 greetings.csv\n\u251c\u2500\u2500 hello-channels.nf\n\u251c\u2500\u2500 hello-config.nf\n\u251c\u2500\u2500 hello-containers.nf\n\u251c\u2500\u2500 hello-modules.nf\n\u251c\u2500\u2500 hello-workflow.nf\n\u251c\u2500\u2500 hello-world.nf\n\u251c\u2500\u2500 nextflow.config\n\u251c\u2500\u2500 solutions\n\u2502   \u251c\u2500\u2500 1-hello-world\n\u2502   \u251c\u2500\u2500 2-hello-channels\n\u2502   \u251c\u2500\u2500 3-hello-workflow\n\u2502   \u251c\u2500\u2500 4-hello-modules\n\u2502   \u251c\u2500\u2500 5-hello-containers\n\u2502   \u2514\u2500\u2500 6-hello-config\n\u2514\u2500\u2500 test-params.json\n\n7 directories, 9 files\n</code></pre> <p>Here's a summary of what you should know to get started:</p> <ul> <li> <p>The <code>.nf</code> files are workflow scripts that are named based on what part of the course they're used in.</p> </li> <li> <p>The file <code>nextflow.config</code> is a configuration file that sets minimal environment properties.   You can ignore it for now.</p> </li> <li> <p>The file <code>greetings.csv</code> contains input data we'll use in most of the course. It is described in Part 1, when we introduce it for the first time.</p> </li> <li> <p>The file <code>test-params.json</code> is a file we'll use in Part 6. You can ignore it for now.</p> </li> <li> <p>The <code>solutions</code> directory contains the completed workflow scripts that result from each step of the course.   They are intended to be used as a reference to check your work and troubleshoot any issues.   The name and number in the filename correspond to the step of the relevant part of the course.   For example, the file <code>hello-world-4.nf</code> is the expected result of completing steps 1 through 4 of Part 1: Hello World.</p> </li> </ul> <p>Now, to begin the course, click on the arrow in the bottom right corner of this page.</p>"},{"location":"hello_nextflow/01_hello_world/","title":"Part 1: Hello World","text":"<p> See the whole playlist on the Nextflow YouTube channel.</p> <p> The video transcript is available here.</p> <p>In this first part of the Hello Nextflow training course, we ease into the topic with a very basic domain-agnostic Hello World example, which we'll progressively build up to demonstrate the usage of foundational Nextflow logic and components.</p> <p>Note</p> <p>A \"Hello World!\" is a minimalist example that is meant to demonstrate the basic syntax and structure of a programming language or software framework. The example typically consists of printing the phrase \"Hello, World!\" to the output device, such as the console or terminal, or writing it to a file.</p>"},{"location":"hello_nextflow/01_hello_world/#0-warmup-run-hello-world-directly","title":"0. Warmup: Run Hello World directly","text":"<p>Let's demonstrate this with a simple command that we run directly in the terminal, to show what it does before we wrap it in Nextflow.</p> <p>Tip</p> <p>Remember that you should now be inside the <code>hello-nextflow/</code> directory as described in the Orientation.</p>"},{"location":"hello_nextflow/01_hello_world/#01-make-the-terminal-say-hello","title":"0.1. Make the terminal say hello","text":"<pre><code>echo 'Hello World!'\n</code></pre> <p>This outputs the text 'Hello World' to the terminal.</p> Output<pre><code>Hello World!\n</code></pre>"},{"location":"hello_nextflow/01_hello_world/#02-now-make-it-write-the-text-output-to-a-file","title":"0.2. Now make it write the text output to a file","text":"<pre><code>echo 'Hello World!' &gt; output.txt\n</code></pre> <p>This does not output anything to the terminal.</p> Output<pre><code>\n</code></pre>"},{"location":"hello_nextflow/01_hello_world/#03-show-the-file-contents","title":"0.3. Show the file contents","text":"<pre><code>cat output.txt\n</code></pre> <p>The text 'Hello World' is now in the output file we specified.</p> output.txt<pre><code>Hello World!\n</code></pre> <p>Tip</p> <p>In the training environment, you can also find the output file in the file explorer, and view its contents by clicking on it. Alternatively, you can use the <code>code</code> command to open the file for viewing.</p> <pre><code>code output.txt\n</code></pre>"},{"location":"hello_nextflow/01_hello_world/#takeaway","title":"Takeaway","text":"<p>You now know how to run a simple command in the terminal that outputs some text, and optionally, how to make it write the output to a file.</p>"},{"location":"hello_nextflow/01_hello_world/#whats-next","title":"What's next?","text":"<p>Find out what that would look like written as a Nextflow workflow.</p>"},{"location":"hello_nextflow/01_hello_world/#1-examine-the-hello-world-workflow-starter-script","title":"1. Examine the Hello World workflow starter script","text":"<p>As mentioned in the orientation, we provide you with a fully functional if minimalist workflow script named <code>hello-world.nf</code> that does the same thing as before (write out 'Hello World!') but with Nextflow.</p> <p>To get you started, we'll first open up the workflow script so you can get a sense of how it's structured.</p>"},{"location":"hello_nextflow/01_hello_world/#11-examine-the-overall-code-structure","title":"1.1. Examine the overall code structure","text":"<p>Let's open the <code>hello-world.nf</code> script in the editor pane.</p> <p>Note</p> <p>The file is in the <code>hello-nextflow</code> directory, which should be your current working directory. You can either click on the file in the file explorer, or type <code>ls</code> in the terminal and Cmd+Click (MacOS) or Ctrl+Click (PC) on the file to open it.</p> hello-world.nf<pre><code>#!/usr/bin/env nextflow\n\n/*\n * Use echo to print 'Hello World!' to a file\n */\nprocess sayHello {\n\n    output:\n        path 'output.txt'\n\n    script:\n    \"\"\"\n    echo 'Hello World!' &gt; output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // emit a greeting\n    sayHello()\n}\n</code></pre> <p>As you can see, a Nextflow script involves two main types of core components: one or more processes, and the workflow itself. Each process describes what operation(s) the corresponding step in the pipeline should accomplish, while the workflow describes the dataflow logic that connects the various steps.</p> <p>Let's take a closer look at the process block first, then we'll look at the workflow block.</p>"},{"location":"hello_nextflow/01_hello_world/#12-the-process-definition","title":"1.2. The <code>process</code> definition","text":"<p>The first block of code describes a process. The process definition starts with the keyword <code>process</code>, followed by the process name and finally the process body delimited by curly braces. The process body must contain a script block which specifies the command to run, which can be anything you would be able to run in a command line terminal.</p> <p>Here we have a process called <code>sayHello</code> that writes its output to a file named <code>output.txt</code>.</p> hello-world.nf<pre><code>/*\n * Use echo to print 'Hello World!' to a file\n */\nprocess sayHello {\n\n    output:\n        path 'output.txt'\n\n    script:\n    \"\"\"\n    echo 'Hello World!' &gt; output.txt\n    \"\"\"\n}\n</code></pre> <p>This is a very minimal process definition that just contains an <code>output</code> definition and the <code>script</code> to execute.</p> <p>The <code>output</code> definition includes the <code>path</code> qualifier, which tells Nextflow this should be handled as a path (includes both directory paths and files). Another common qualifier is <code>val</code>.</p> <p>Note</p> <p>The output definition does not determine what output will be created. It simply declares what is the expected output, so that Nextflow can look for it once execution is complete. This is necessary for verifying that the command was executed successfully and for passing the output to downstream processes if needed. Output produced that doesn't match what is declared in the output block will not be passed to downstream processes.</p> <p>Warning</p> <p>This example is brittle because we hardcoded the output filename in two separate places (the script and the output blocks). If we change one but not the other, the script will break. Later, you'll learn how to use variables to avoid this problem.</p> <p>In a real-world pipeline, a process usually contains additional blocks such as directives and inputs, which we'll introduce in a little bit.</p>"},{"location":"hello_nextflow/01_hello_world/#13-the-workflow-definition","title":"1.3. The <code>workflow</code> definition","text":"<p>The second block of code describes the workflow itself. The workflow definition starts with the keyword <code>workflow</code>, followed by an optional name, then the workflow body delimited by curly braces.</p> <p>Here we have a workflow that consists of one call to the <code>sayHello</code> process.</p> hello-world.nf<pre><code>workflow {\n\n    // emit a greeting\n    sayHello()\n}\n</code></pre> <p>This is a very minimal workflow definition. In a real-world pipeline, the workflow typically contains multiple calls to processes connected by channels, and the processes expect one or more variable input(s).</p> <p>You'll learn how to add variable inputs later in this training module; and you'll learn how to add more processes and connect them by channels in Part 3 of this course.</p>"},{"location":"hello_nextflow/01_hello_world/#takeaway_1","title":"Takeaway","text":"<p>You now know how a simple Nextflow workflow is structured.</p>"},{"location":"hello_nextflow/01_hello_world/#whats-next_1","title":"What's next?","text":"<p>Learn to launch the workflow, monitor execution and find your outputs.</p>"},{"location":"hello_nextflow/01_hello_world/#2-run-the-workflow","title":"2. Run the workflow","text":"<p>Looking at code is not nearly as fun as running it, so let's try this out in practice.</p>"},{"location":"hello_nextflow/01_hello_world/#21-launch-the-workflow-and-monitor-execution","title":"2.1. Launch the workflow and monitor execution","text":"<p>In the terminal, run the following command:</p> <pre><code>nextflow run hello-world.nf\n</code></pre> <p>You console output should look something like this:</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-world.nf` [goofy_torvalds] DSL2 - revision: c33d41f479\n\nexecutor &gt;  local (1)\n[a3/7be2fa] sayHello | 1 of 1 \u2714\n</code></pre> <p>Congratulations, you just ran your first Nextflow workflow!</p> <p>The most important output here is the last line (line 6):</p> Output<pre><code>[a3/7be2fa] sayHello | 1 of 1 \u2714\n</code></pre> <p>This tells us that the <code>sayHello</code> process was successfully executed once (<code>1 of 1 \u2714</code>).</p> <p>Importantly, this line also tells you where to find the output of the <code>sayHello</code> process call. Let's look at that now.</p>"},{"location":"hello_nextflow/01_hello_world/#22-find-the-output-and-logs-in-the-work-directory","title":"2.2. Find the output and logs in the <code>work</code> directory","text":"<p>When you run Nextflow for the first time in a given directory, it creates a directory called <code>work</code> where it will write all files (and any symlinks) generated in the course of execution.</p> <p>Within the <code>work</code> directory, Nextflow organizes outputs and logs per process call. For each process call, Nextflow creates a nested subdirectory, named with a hash in order to make it unique, where it will stage all necessary inputs (using symlinks by default), write helper files, and write out logs and any outputs of the process.</p> <p>The path to that subdirectory is shown in truncated form in square brackets in the console output. Looking at what we got for the run shown above, the console log line for the sayHello process starts with <code>[a3/7be2fa]</code>. That corresponds to the following directory path: <code>work/a3/7be2fa7be2fad5e71e5f49998f795677fd68</code></p> <p>Let's take a look at what's in there.</p> <p>Tip</p> <p>If you browse the contents of the task subdirectory in the VSCode file explorer, you'll see all the files right away. However, the log files are set to be invisible in the terminal, so if you want to use <code>ls</code> or <code>tree</code> to view them, you'll need to set the relevant option for displaying invisible files.</p> <pre><code>tree -a work\n</code></pre> <p>You should see something like this, though the exact subdirectory names will be different on your system:</p> Directory contents<pre><code>work\n\u2514\u2500\u2500 a3\n    \u2514\u2500\u2500 7be2fad5e71e5f49998f795677fd68\n        \u251c\u2500\u2500 .command.begin\n        \u251c\u2500\u2500 .command.err\n        \u251c\u2500\u2500 .command.log\n        \u251c\u2500\u2500 .command.out\n        \u251c\u2500\u2500 .command.run\n        \u251c\u2500\u2500 .command.sh\n        \u251c\u2500\u2500 .exitcode\n        \u2514\u2500\u2500 output.txt\n</code></pre> <p>These are the helper and log files:</p> <ul> <li><code>.command.begin</code>: Metadata related to the beginning of the execution of the process call</li> <li><code>.command.err</code>: Error messages (<code>stderr</code>) emitted by the process call</li> <li><code>.command.log</code>: Complete log output emitted by the process call</li> <li><code>.command.out</code>: Regular output (<code>stdout</code>) by the process call</li> <li><code>.command.run</code>: Full script run by Nextflow to execute the process call</li> <li><code>.command.sh</code>: The command that was actually run by the process call</li> <li><code>.exitcode</code>: The exit code resulting from the command</li> </ul> <p>The <code>.command.sh</code> file is especially useful because it tells you what command Nextflow actually executed. In this case it's very straightforward, but later in the course you'll see commands that involve some interpolation of variables. When you're dealing with that, you need to be able to check exactly what was run, especially when troubleshooting an issue.</p> <p>The actual output of the <code>sayHello</code> process is <code>output.txt</code>. Open it and you will find the <code>Hello World!</code> greeting, which was the expected result of our minimalist workflow.</p> output.txt<pre><code>Hello World!\n</code></pre>"},{"location":"hello_nextflow/01_hello_world/#takeaway_2","title":"Takeaway","text":"<p>You know how to decipher a simple Nextflow script, run it and find the output and relevant log files in the work directory.</p>"},{"location":"hello_nextflow/01_hello_world/#whats-next_2","title":"What's next?","text":"<p>Learn how to manage your workflow executions conveniently.</p>"},{"location":"hello_nextflow/01_hello_world/#3-manage-workflow-executions","title":"3. Manage workflow executions","text":"<p>Knowing how to launch workflows and retrieve outputs is great, but you'll quickly find there are a few other aspects of workflow management that will make your life easier, especially if you're developing your own workflows.</p> <p>Here we show you how to use the <code>publishDir</code> directive to store in an output folder all the main results from your pipeline run, the <code>resume</code> feature for when you need to re-launch the same workflow, and how to delete older work directories with <code>nextflow clean</code>.</p>"},{"location":"hello_nextflow/01_hello_world/#31-publish-outputs","title":"3.1. Publish outputs","text":"<p>As you have just learned, the output produced by our pipeline is buried in a working directory several layers deep. This is done on purpose; Nextflow is in control of this directory and we are not supposed to interact with it.</p> <p>However, that makes it inconvenient to retrieve outputs that we care about.</p> <p>Fortunately, Nextflow provides a way to manage this more conveniently, called the <code>publishDir</code> directive, which acts at the process level. This directive tells Nextflow to publish the output(s) of the process to a designated output directory. By default, the outputs are published as symbolic links from the <code>work</code> directory. It allows us to retrieve the desired output file without having to dig down into the work directory.</p>"},{"location":"hello_nextflow/01_hello_world/#311-add-a-publishdir-directive-to-the-sayhello-process","title":"3.1.1. Add a <code>publishDir</code> directive to the <code>sayHello</code> process","text":"<p>In the workflow script file <code>hello-world.nf</code>, make the following code modification:</p> AfterBefore hello-world.nf<pre><code>process sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    output:\n        path 'output.txt'\n</code></pre> hello-world.nf<pre><code>process sayHello {\n\n    output:\n        path 'output.txt'\n</code></pre>"},{"location":"hello_nextflow/01_hello_world/#312-run-the-workflow-again","title":"3.1.2. Run the workflow again","text":"<p>Now run the modified workflow script:</p> <pre><code>nextflow run hello-world.nf\n</code></pre> <p>The log output should look very familiar:</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-world.nf` [jovial_mayer] DSL2 - revision: 35bd3425e5\n\nexecutor &gt;  local (1)\n[62/49a1f8] sayHello | 1 of 1 \u2714\n</code></pre> <p>This time, Nextflow has created a new directory called <code>results/</code>. Our <code>output.txt</code> file is in this directory. If you check the contents it should match the output in the work subdirectory. This is how we publish results files outside of the working directories conveniently.</p> <p>When you're dealing with very large files that you don't need to retain for long, you may prefer to set the <code>publishDir</code> directive to make a symbolic link to the file instead of copying it. However, if you delete the work directory as part of a cleanup operation, you will lose access to the file, so always make sure you have actual copies of everything you care about before deleting anything.</p> <p>Note</p> <p>A newer syntax option documented here has been proposed to make it possible to declare and publish workflow-level outputs. This will eventually make using <code>publishDir</code> at the process level redundant for completed pipelines. However, we expect that <code>publishDir</code> will still remain very useful during pipeline development.</p>"},{"location":"hello_nextflow/01_hello_world/#32-re-launch-a-workflow-with-resume","title":"3.2. Re-launch a workflow with <code>-resume</code>","text":"<p>Sometimes, you're going to want to re-run a pipeline that you've already launched previously without redoing any steps that already completed successfully.</p> <p>Nextflow has an option called <code>-resume</code> that allows you to do this. Specifically, in this mode, any processes that have already been run with the exact same code, settings and inputs will be skipped. This means Nextflow will only run processes that you've added or modified since the last run, or to which you're providing new settings or inputs.</p> <p>There are two key advantages to doing this:</p> <ul> <li>If you're in the middle of developing your pipeline, you can iterate more rapidly since you only have to run the process(es) you're actively working on in order to test your changes.</li> <li>If you're running a pipeline in production and something goes wrong, in many cases you can fix the issue and relaunch the pipeline, and it will resume running from the point of failure, which can save you a lot of time and compute.</li> </ul> <p>To use it, simply add <code>-resume</code> to your command and run it:</p> <pre><code>nextflow run hello-world.nf -resume\n</code></pre> <p>The console output should look similar.</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-world.nf` [golden_cantor] DSL2 - revision: 35bd3425e5\n\n[62/49a1f8] sayHello | 1 of 1, cached: 1 \u2714\n</code></pre> <p>Look for the <code>cached:</code> bit that has been added in the process status line (line 5), which means that Nextflow has recognized that it has already done this work and simply re-used the result from the previous successful run.</p> <p>You can also see that the work subdirectory hash is the same as in the previous run. Nextflow is literally pointing you to the previous execution and saying \"I already did that over there.\"</p> <p>Note</p> <p>When your re-run a pipeline with <code>resume</code>, Nextflow does not overwrite any files written to a <code>publishDir</code> directory by any process call that was previously run successfully.</p>"},{"location":"hello_nextflow/01_hello_world/#33-delete-older-work-directories","title":"3.3. Delete older work directories","text":"<p>During the development process, you'll typically run your draft pipelines a large number of times, which can lead to an accumulation of very many files across many subdirectories. Since the subdirectories are named randomly, it is difficult to tell from their names what are older vs. more recent runs.</p> <p>Nextflow includes a convenient <code>clean</code> subcommand that can automatically delete the work subdirectories for past runs that you no longer care about, with several options to control what will be deleted.</p> <p>Here we show you an example that deletes all subdirectories from runs before a given run, specified using its run name. The run name is the machine-generated two-part string shown in square brackets in the <code>Launching (...)</code> console output line.</p> <p>First we use the dry run flag <code>-n</code> to check what will be deleted given the command:</p> <pre><code>nextflow clean -before golden_cantor -n\n</code></pre> <p>The output should look like this:</p> Output<pre><code>Would remove /workspaces/training/hello-nextflow/work/a3/7be2fad5e71e5f49998f795677fd68\n</code></pre> <p>If you don't see any lines output, you either did not provide a valid run name or there are no past runs to delete.</p> <p>If the output looks as expected and you want to proceed with the deletion, re-run the command with the <code>-f</code> flag instead of <code>-n</code>:</p> <pre><code>nextflow clean -before golden_cantor -f\n</code></pre> <p>You should now see the following:</p> Output<pre><code>Removed /workspaces/training/hello-nextflow/work/a3/7be2fad5e71e5f49998f795677fd68\n</code></pre> <p>Warning</p> <p>Deleting work subdirectories from past runs removes them from Nextflow's cache and deletes any outputs that were stored in those directories. That means it breaks Nextflow's ability to resume execution without re-running the corresponding processes.</p> <p>You are responsible for saving any outputs that you care about or plan to rely on! If you're using the <code>publishDir</code> directive for that purpose, make sure to use the <code>copy</code> mode, not the <code>symlink</code> mode.</p>"},{"location":"hello_nextflow/01_hello_world/#takeaway_3","title":"Takeaway","text":"<p>You know how to publish outputs to a specific directory, relaunch a pipeline without repeating steps that were already run in an identical way, and use the <code>nextflow clean</code> command to clean up old work directories.</p>"},{"location":"hello_nextflow/01_hello_world/#whats-next_3","title":"What's next?","text":"<p>Learn to provide a variable input via a command-line parameter and utilize default values effectively.</p>"},{"location":"hello_nextflow/01_hello_world/#4-use-a-variable-input-passed-on-the-command-line","title":"4. Use a variable input passed on the command line","text":"<p>In its current state, our workflow uses a greeting hardcoded into the process command. We want to add some flexibility by using an input variable, so that we can more easily change the greeting at runtime.</p>"},{"location":"hello_nextflow/01_hello_world/#41-modify-the-workflow-to-take-and-use-a-variable-input","title":"4.1. Modify the workflow to take and use a variable input","text":"<p>This requires us to make three changes to our script:</p> <ol> <li>Tell the process to expect a variable input by adding an <code>input:</code> block</li> <li>Edit the process to use the input</li> <li>Set up a command-line parameter and provide its value as an input to the process call</li> </ol> <p>Let's make these changes one at a time.</p>"},{"location":"hello_nextflow/01_hello_world/#411-add-an-input-block-to-the-process-definition","title":"4.1.1. Add an input block to the process definition","text":"<p>First we need to adapt the process definition to accept an input called <code>greeting</code>.</p> <p>In the process block, make the following code change:</p> AfterBefore hello-world.nf<pre><code>process sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        val greeting\n\n    output:\n        path 'output.txt'\n</code></pre> hello-world.nf<pre><code>process sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    output:\n        path 'output.txt'\n</code></pre> <p>The <code>greeting</code> variable is prefixed by <code>val</code> to tell Nextflow it's a value (not a path).</p>"},{"location":"hello_nextflow/01_hello_world/#412-edit-the-process-command-to-use-the-input-variable","title":"4.1.2. Edit the process command to use the input variable","text":"<p>Now we swap the original hardcoded value for the value of the input variable we expect to receive.</p> <p>In the process block, make the following code change:</p> AfterBefore hello-channels.nf<pre><code>script:\n\"\"\"\necho '$greeting' &gt; output.txt\n\"\"\"\n</code></pre> hello-channels.nf<pre><code>script:\n\"\"\"\necho 'Hello World!' &gt; output.txt\n\"\"\"\n</code></pre> <p>Make sure to prepend the <code>$</code> symbol to tell Nextflow this is a variable name that needs to be replaced with the actual value (=interpolated).</p>"},{"location":"hello_nextflow/01_hello_world/#413-set-up-a-cli-parameter-and-provide-it-as-input-to-the-process-call","title":"4.1.3. Set up a CLI parameter and provide it as input to the process call","text":"<p>Now we need to actually set up a way to provide an input value to the <code>sayHello()</code> process call.</p> <p>We could simply hardcode it directly by writing <code>sayHello('Hello World!')</code>. However, when we're doing real work with our workflow, we're often going to want to be able to control its inputs from the command line.</p> <p>Good news: Nextflow has a built-in workflow parameter system called <code>params</code>, which makes it easy to declare and use CLI parameters. The general syntax is to declare <code>params.&lt;parameter_name&gt;</code> to tell Nextflow to expect a <code>--&lt;parameter_name&gt;</code> parameter on the command line.</p> <p>Here, we want to create a parameter called <code>--greeting</code>, so we need to declare <code>params.greeting</code> somewhere in the workflow. In principle we can write it anywhere; but since we're going to want to give it to the <code>sayHello()</code> process call, we can plug it in there directly by writing <code>sayHello(params.greeting)</code>.</p> <p>Note</p> <p>The parameter name (at the workflow level) does not have to match the input variable name (at the process level). We're just using the same word because that's what makes sense and keeps the code readable.</p> <p>In the workflow block, make the following code change:</p> AfterBefore hello-world.nf<pre><code>// emit a greeting\nsayHello(params.greeting)\n</code></pre> hello-world.nf<pre><code>// emit a greeting\nsayHello()\n</code></pre> <p>This tells Nextflow to run the <code>sayHello</code> process on the value provided through the <code>--greeting</code> parameter.</p>"},{"location":"hello_nextflow/01_hello_world/#414-run-the-workflow-command-again","title":"4.1.4. Run the workflow command again","text":"<p>Let's run it!</p> <pre><code>nextflow run hello-world.nf --greeting 'Bonjour le monde!'\n</code></pre> <p>If you made all three edits correctly, you should get another successful execution:</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-world.nf` [elated_lavoisier] DSL2 - revision: 7c031b42ea\n\nexecutor &gt;  local (1)\n[4b/654319] sayHello | 1 of 1 \u2714\n</code></pre> <p>Be sure to open up the output file to check that you now have the new version of the greeting.</p> results/output.txt<pre><code>Bonjour le monde!\n</code></pre> <p>Voil\u00e0!</p> <p>Tip</p> <p>You can readily distinguish Nextflow-level parameters from pipeline-level parameters.</p> <ul> <li>Parameters that apply to a pipeline always take a double hyphen (<code>--</code>).</li> <li>Parameters that modify a Nextflow setting, e.g. the <code>-resume</code> feature we used earlier, take a single hyphen (<code>-</code>).</li> </ul>"},{"location":"hello_nextflow/01_hello_world/#42-use-default-values-for-command-line-parameters","title":"4.2. Use default values for command line parameters","text":"<p>In many cases, it makes sense to supply a default value for a given parameter so that you don't have to specify it for every run.</p>"},{"location":"hello_nextflow/01_hello_world/#421-set-a-default-value-for-the-cli-parameter","title":"4.2.1. Set a default value for the CLI parameter","text":"<p>Let's give the <code>greeting</code> parameter with a default value by declaring it before the workflow definition.</p> hello-world.nf<pre><code>/*\n * Pipeline parameters\n */\nparams.greeting = 'Hol\u00e0 mundo!'\n</code></pre> <p>Tip</p> <p>You can put the parameter declaration inside the workflow block if you prefer. Whatever you choose, try to group similar things in the same place so you don't end up with declarations all over the place.</p>"},{"location":"hello_nextflow/01_hello_world/#422-run-the-workflow-again-without-specifying-the-parameter","title":"4.2.2. Run the workflow again without specifying the parameter","text":"<p>Now that you have a default value set, you can run the workflow again without having to specify a value in the command line.</p> <pre><code>nextflow run hello-world.nf\n</code></pre> <p>The console output should look the same.</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-world.nf` [determined_edison] DSL2 - revision: 3539118582\n\nexecutor &gt;  local (1)\n[72/394147] sayHello | 1 of 1 \u2714\n</code></pre> <p>Check the output in the results directory:</p> results/output.txt<pre><code>Hol\u00e0 mundo!\n</code></pre> <p>Nextflow used the default value of the greeting parameter to create the output.</p>"},{"location":"hello_nextflow/01_hello_world/#423-run-the-workflow-again-with-the-parameter-to-override-the-default-value","title":"4.2.3. Run the workflow again with the parameter to override the default value","text":"<p>If you provide the parameter on the command line, the CLI value will override the default value.</p> <p>Try it out:</p> <pre><code>nextflow run hello-world.nf --greeting 'Konnichiwa!'\n</code></pre> <p>The console output should look the same.</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-world.nf` [elegant_faraday] DSL2 - revision: 3539118582\n\nexecutor &gt;  local (1)\n[6f/a12a91] sayHello | 1 of 1 \u2714\n</code></pre> <p>Now you will have the corresponding new output in your results directory.</p> results/output.txt<pre><code>Konnichiwa!\n</code></pre> <p>Note</p> <p>In Nextflow, there are multiple places where you can specify values for parameters. If the same parameter is set to different values in multiple places, Nexflow will determine what value to use based on the order of precedence that is described here.</p>"},{"location":"hello_nextflow/01_hello_world/#takeaway_4","title":"Takeaway","text":"<p>You know how to use a simple variable input provided at runtime via a command-line parameter, as well as set up, use and override default values.</p> <p>More generally, you know how to interpret a simple Nextflow workflow, manage its execution, and retrieve outputs.</p>"},{"location":"hello_nextflow/01_hello_world/#whats-next_4","title":"What's next?","text":"<p>Take a little break, you've earned it! When you're ready, move on to Part 2 to learn how to use channels to feed inputs into your workflow, which will allow you to take advantage of Nextflow's built-in dataflow parallelism and other powerful features.</p>"},{"location":"hello_nextflow/02_hello_channels/","title":"Part 2: Hello Channels","text":"<p> See the whole playlist on the Nextflow YouTube channel.</p> <p> The video transcript is available here.</p> <p>In Part 1 of this course (Hello World), we showed you how to provide a variable input to a process by providing the input in the process call directly: <code>sayHello(params.greeting)</code>. That was a deliberately simplified approach. In practice, that approach has major limitations; namely that it only works for very simple cases where we only want to run the process once, on a single value. In most realistic workflow use cases, we want to process multiple values (experimental data for multiple samples, for example), so we need a more sophisticated way to handle inputs.</p> <p>That is what Nextflow channels are for. Channels are queues designed to handle inputs efficiently and shuttle them from one step to another in multi-step workflows, while providing built-in parallelism and many additional benefits.</p> <p>In this part of the course, you will learn how to use a channel to handle multiple inputs from a variety of different sources. You will also learn to use operators to transform channel contents as needed.</p> <p>For training on using channels to connect steps in a multi-step workflow, see Part 3 of this course.</p>"},{"location":"hello_nextflow/02_hello_channels/#0-warmup-run-hello-channelsnf","title":"0. Warmup: Run <code>hello-channels.nf</code>","text":"<p>We're going to use the workflow script <code>hello-channels.nf</code> as a starting point. It is equivalent to the script produced by working through Part 1 of this training course.</p> <p>Just to make sure everything is working, run the script once before making any changes:</p> <pre><code>nextflow run hello-channels.nf --greeting 'Hello Channels!'\n</code></pre> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-channels.nf` [insane_lichterman] DSL2 - revision: c33d41f479\n\nexecutor &gt;  local (1)\n[86/9efa08] sayHello | 1 of 1 \u2714\n</code></pre> <p>As previously, you will find the output file named <code>output.txt</code> in the <code>results</code> directory (specified by the <code>publishDir</code> directive).</p> results/output.txt<pre><code>Hello Channels!\n</code></pre> <p>If that worked for you, you're ready to learn about channels.</p>"},{"location":"hello_nextflow/02_hello_channels/#1-provide-variable-inputs-via-a-channel-explicitly","title":"1. Provide variable inputs via a channel explicitly","text":"<p>We are going to create a channel to pass the variable input to the <code>sayHello()</code> process instead of relying on the implicit handling, which has certain limitations.</p>"},{"location":"hello_nextflow/02_hello_channels/#11-create-an-input-channel","title":"1.1. Create an input channel","text":"<p>There are a variety of channel factories that we can use to set up a channel. To keep things simple for now, we are going to use the most basic channel factory, called <code>Channel.of</code>, which will create a channel containing a single value. Functionally this will be similar to how we had it set up before, but instead of having Nextflow create a channel implicitly, we are doing this explicitly now.</p> <p>This is the line of code we're going to use:</p> Syntax<pre><code>greeting_ch = Channel.of('Hello Channels!')\n</code></pre> <p>This creates a channel called <code>greeting_ch</code> using the <code>Channel.of()</code> channel factory, which sets up a simple queue channel, and loads the string <code>'Hello Channels!'</code> to use as the greeting value.</p> <p>Note</p> <p>We are temporarily switching back to hardcoded strings instead of using a CLI parameter for the sake of readability. We'll go back to using CLI parameters once we've covered what's happening at the level of the channel.</p> <p>In the workflow block, add the channel factory code:</p> AfterBefore hello-channels.nf<pre><code>workflow {\n\n    // create a channel for inputs\n    greeting_ch = Channel.of('Hello Channels!')\n\n    // emit a greeting\n    sayHello(params.greeting)\n}\n</code></pre> hello-channels.nf<pre><code>workflow {\n\n    // emit a greeting\n    sayHello(params.greeting)\n}\n</code></pre> <p>This is not yet functional since we haven't yet switched the input to the process call.</p>"},{"location":"hello_nextflow/02_hello_channels/#12-add-the-channel-as-input-to-the-process-call","title":"1.2. Add the channel as input to the process call","text":"<p>Now we need to actually plug our newly created channel into the <code>sayHello()</code> process call, replacing the CLI parameter which we were providing directly before.</p> <p>In the workflow block, make the following code change:</p> AfterBefore hello-channels.nf<pre><code>workflow {\n\n    // create a channel for inputs\n    greeting_ch = Channel.of('Hello Channels!')\n\n    // emit a greeting\n    sayHello(greeting_ch)\n}\n</code></pre> hello-channels.nf<pre><code>workflow {\n\n    // create a channel for inputs\n    greeting_ch = Channel.of('Hello Channels!')\n\n    // emit a greeting\n    sayHello(params.greeting)\n}\n</code></pre> <p>This tells Nextflow to run the <code>sayHello</code> process on the contents of the <code>greeting_ch</code> channel.</p> <p>Now our workflow is properly functional; it is the explicit equivalent of writing <code>sayHello('Hello Channels!')</code>.</p>"},{"location":"hello_nextflow/02_hello_channels/#13-run-the-workflow-command-again","title":"1.3. Run the workflow command again","text":"<p>Let's run it!</p> <pre><code>nextflow run hello-channels.nf\n</code></pre> <p>If you made both edits correctly, you should get another successful execution:</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-channels.nf` [nice_heisenberg] DSL2 - revision: 41b4aeb7e9\n\nexecutor &gt;  local (1)\n[3b/f2b109] sayHello (1) | 1 of 1 \u2714\n</code></pre> <p>You can check the results directory to satisfy yourself that the outcome is still the same as previously.</p> results/output.txt<pre><code>Hello Channels!\n</code></pre> <p>So far we're just progressively tweaking the code to increase the flexibility of our workflow while achieving the same end result.</p> <p>Note</p> <p>This may seem like we're writing more code for no tangible benefit, but the value will become clear as soon as we start handling more inputs.</p>"},{"location":"hello_nextflow/02_hello_channels/#takeaway","title":"Takeaway","text":"<p>You know how to use a basic channel factory to provide an input to a process.</p>"},{"location":"hello_nextflow/02_hello_channels/#whats-next","title":"What's next?","text":"<p>Learn how to use channels to make the workflow iterate over multiple input values.</p>"},{"location":"hello_nextflow/02_hello_channels/#2-modify-the-workflow-to-run-on-multiple-input-values","title":"2. Modify the workflow to run on multiple input values","text":"<p>Workflows typically run on batches of inputs that are meant to be processed in bulk, so we want to upgrade the workflow to accept multiple input values.</p>"},{"location":"hello_nextflow/02_hello_channels/#21-load-multiple-greetings-into-the-input-channel","title":"2.1. Load multiple greetings into the input channel","text":"<p>Conveniently, the <code>Channel.of()</code> channel factory we've been using is quite happy to accept more than one value, so we don't need to modify that at all. We just have to load more values into the channel.</p>"},{"location":"hello_nextflow/02_hello_channels/#211-add-more-greetings","title":"2.1.1. Add more greetings","text":"<p>Before the workflow block, make the following code change:</p> AfterBefore hello-channels.nf<pre><code>// create a channel for inputs\ngreeting_ch = Channel.of('Hello','Bonjour','Hol\u00e0')\n</code></pre> hello-channels.nf<pre><code>// create a channel for inputs\ngreeting_ch = Channel.of('Hello Channels')\n</code></pre> <p>The documentation tells us this should work. Can it really be so simple?</p>"},{"location":"hello_nextflow/02_hello_channels/#212-run-the-command-and-look-at-the-log-output","title":"2.1.2. Run the command and look at the log output","text":"<p>Let's try it.</p> <pre><code>nextflow run hello-channels.nf\n</code></pre> <p>It certainly seems to run just fine:</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-channels.nf` [suspicious_lamport] DSL2 - revision: 778deadaea\n\nexecutor &gt;  local (3)\n[cd/77a81f] sayHello (3) | 3 of 3 \u2714\n</code></pre> <p>However... This seems to indicate that '3 of 3' calls were made for the process, which is encouraging, but this only shows us a single run of the process, with one subdirectory path (<code>cd/77a81f</code>). What's going on?</p> <p>By default, the ANSI logging system writes the logging from multiple calls to the same process on the same line. Fortunately, we can disable that behavior to see the full list of process calls.</p>"},{"location":"hello_nextflow/02_hello_channels/#213-run-the-command-again-with-the-ansi-log-false-option","title":"2.1.3. Run the command again with the <code>-ansi-log false</code> option","text":"<p>To expand the logging to display one line per process call, add <code>-ansi-log false</code> to the command.</p> <pre><code>nextflow run hello-channels.nf -ansi-log false\n</code></pre> <p>This time we see all three process runs and their associated work subdirectories listed in the output:</p> Output<pre><code>N E X T F L O W  ~  version 25.04.3\nLaunching `hello-channels.nf` [pensive_poitras] DSL2 - revision: 778deadaea\n[76/f61695] Submitted process &gt; sayHello (1)\n[6e/d12e35] Submitted process &gt; sayHello (3)\n[c1/097679] Submitted process &gt; sayHello (2)\n</code></pre> <p>That's much better; at least for a simple workflow. For a complex workflow, or a large number of inputs, having the full list output to the terminal might get a bit overwhelming, so you might not choose to use <code>-ansi-log false</code> in those cases.</p> <p>Note</p> <p>The way the status is reported is a bit different between the two logging modes. In the condensed mode, Nextflow reports whether calls were completed successfully or not. In this expanded mode, it only reports that they were submitted.</p> <p>That being said, we have another problem. If you look in the <code>results</code> directory, there is only one file: <code>output.txt</code>!</p> Directory contents<pre><code>results\n\u2514\u2500\u2500 output.txt\n</code></pre> <p>What's up with that? Shouldn't we be expecting a separate file per input greeting, so three files in all? Did all three greetings go into a single file?</p> <p>You can check the contents of <code>output.txt</code>; you will find only one of the three, containing one of the three greetings we provided.</p> output.txt<pre><code>Bonjour\n</code></pre> <p>You may recall that we hardcoded the output file name for the <code>sayHello</code> process, so all three calls produced a file called <code>output.txt</code>. You can check the work subdirectories for each of the three processes; each of them contains a file called <code>output.txt</code> as expected.</p> <p>As long as the output files stay there, isolated from the other processes, that is okay. But when the <code>publishDir</code> directive copies each of them to the same <code>results</code> directory, whichever got copied there first gets overwritten by the next one, and so on.</p>"},{"location":"hello_nextflow/02_hello_channels/#22-ensure-the-output-file-names-will-be-unique","title":"2.2. Ensure the output file names will be unique","text":"<p>We can continue publishing all the outputs to the same results directory, but we need to ensure they will have unique names. Specifically, we need to modify the first process to generate a file name dynamically so that the final file names will be unique.</p> <p>So how do we make the file names unique? A common way to do that is to use some unique piece of metadata from the inputs (received from the input channel) as part of the output file name. Here, for convenience, we'll just use the greeting itself since it's just a short string, and prepend it to the base output filename.</p>"},{"location":"hello_nextflow/02_hello_channels/#221-construct-a-dynamic-output-file-name","title":"2.2.1. Construct a dynamic output file name","text":"<p>In the process block, make the following code changes:</p> AfterBefore hello-channels.nf<pre><code>process sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        val greeting\n\n    output:\n        path \"${greeting}-output.txt\"\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; '$greeting-output.txt'\n    \"\"\"\n}\n</code></pre> hello-channels.nf<pre><code>process sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        val greeting\n\n    output:\n        path 'output.txt'\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; output.txt\n    \"\"\"\n}\n</code></pre> <p>Make sure to replace <code>output.txt</code> in both the output definition and in the <code>script:</code> command block.</p> <p>Tip</p> <p>In the output definition, you MUST use double quotes around the output filename expression (NOT single quotes), otherwise it will fail.</p> <p>This should produce a unique output file name every time the process is called, so that it can be distinguished from the outputs from other iterations of the same process in the output directory.</p>"},{"location":"hello_nextflow/02_hello_channels/#222-run-the-workflow","title":"2.2.2. Run the workflow","text":"<p>Let's run it:</p> <pre><code>nextflow run hello-channels.nf\n</code></pre> <p>Reverting back to the summary view, the output looks like this again:</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-channels.nf` [astonishing_bell] DSL2 - revision: f57ff44a69\n\nexecutor &gt;  local (3)\n[2d/90a2e2] sayHello (1) | 3 of 3 \u2714\n</code></pre> <p>Importantly, now we have three new files in addition to the one we already had in the <code>results</code> directory:</p> Directory contents<pre><code>results\n\u251c\u2500\u2500 Bonjour-output.txt\n\u251c\u2500\u2500 Hello-output.txt\n\u251c\u2500\u2500 Hol\u00e0-output.txt\n\u2514\u2500\u2500 output.txt\n</code></pre> <p>They each have the expected contents:</p> Bonjour-output.txt<pre><code>Bonjour\n</code></pre> Hello-output.txt<pre><code>Hello\n</code></pre> Hol\u00e0-output.txt<pre><code>Hol\u00e0\n</code></pre> <p>Success! Now we can add as many greetings as we like without worrying about output files being overwritten.</p> <p>Note</p> <p>In practice, naming files based on the input data itself is almost always impractical. The better way to generate dynamic filenames is to pass metadata to a process along with the input files. The metadata is typically provided via a 'sample sheet' or equivalents. You'll learn how to do that later in your Nextflow training.</p>"},{"location":"hello_nextflow/02_hello_channels/#takeaway_1","title":"Takeaway","text":"<p>You know how to feed multiple input elements through a channel.</p>"},{"location":"hello_nextflow/02_hello_channels/#whats-next_1","title":"What's next?","text":"<p>Learn to use an operator to transform the contents of a channel.</p>"},{"location":"hello_nextflow/02_hello_channels/#3-use-an-operator-to-transform-the-contents-of-a-channel","title":"3. Use an operator to transform the contents of a channel","text":"<p>In Nextflow, operators allow us to transform the contents of a channel.</p> <p>We just showed you how to handle multiple input elements that were hardcoded directly in the channel factory. What if we wanted to provide those multiple inputs in a different form?</p> <p>For example, imagine we set up an input variable containing an array of elements like this:</p> <p><code>greetings_array = ['Hello','Bonjour','Hol\u00e0']</code></p> <p>Can we load that into our output channel and expect it to work? Let's find out.</p>"},{"location":"hello_nextflow/02_hello_channels/#31-provide-an-array-of-values-as-input-to-the-channel","title":"3.1. Provide an array of values as input to the channel","text":"<p>Common sense suggests we should be able to simply pass in an array of values instead of a single value. Right?</p>"},{"location":"hello_nextflow/02_hello_channels/#311-set-up-the-input-variable","title":"3.1.1. Set up the input variable","text":"<p>Let's take the <code>greetings_array</code> variable we just imagined and make it a reality by adding it to the workflow block:</p> AfterBefore hello-channels.nf<pre><code>workflow {\n\n    // declare an array of input greetings\n    greetings_array = ['Hello','Bonjour','Hol\u00e0']\n\n    // create a channel for inputs\n    greeting_ch = Channel.of('Hello','Bonjour','Hol\u00e0')\n</code></pre> hello-channels.nf<pre><code>workflow {\n\n    // create a channel for inputs\n    greeting_ch = Channel.of('Hello','Bonjour','Hol\u00e0')\n</code></pre>"},{"location":"hello_nextflow/02_hello_channels/#312-set-array-of-greetings-as-the-input-to-the-channel-factory","title":"3.1.2. Set array of greetings as the input to the channel factory","text":"<p>We're going to replace the values <code>'Hello','Bonjour','Hol\u00e0'</code> currently hardcoded in the channel factory with the <code>greetings_array</code> we just created.</p> <p>In the workflow block, make the following change:</p> AfterBefore hello-channels.nf<pre><code>    // create a channel for inputs\n    greeting_ch = Channel.of(greetings_array)\n</code></pre> hello-channels.nf<pre><code>    // create a channel for inputs\n    greeting_ch = Channel.of('Hello','Bonjour','Hol\u00e0')\n</code></pre>"},{"location":"hello_nextflow/02_hello_channels/#313-run-the-workflow","title":"3.1.3. Run the workflow","text":"<p>Let's try running this:</p> <pre><code>nextflow run hello-channels.nf\n</code></pre> <p>Oh no! Nextflow throws an error that starts like this:</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-channels.nf` [friendly_koch] DSL2 - revision: 97256837a7\n\nexecutor &gt;  local (1)\n[22/57e015] sayHello (1) | 0 of 1\nERROR ~ Error executing process &gt; 'sayHello (1)'\n\nCaused by:\n  Missing output file(s) `[Hello, Bonjour, Hol\u00e0]-output.txt` expected by process `sayHello (1)`\n</code></pre> <p>It looks like Nextflow tried to run a single process call, using <code>[Hello, Bonjour, Hol\u00e0]</code> as a string value, instead of using the three strings in the array as separate values.</p> <p>How do we get Nextflow to unpack the array and load the individual strings into the channel?</p>"},{"location":"hello_nextflow/02_hello_channels/#32-use-an-operator-to-transform-channel-contents","title":"3.2. Use an operator to transform channel contents","text":"<p>This is where operators come in.</p> <p>If you skim through the list of operators in the Nextflow documentation, you'll find <code>flatten()</code>, which does exactly what we need: unpack the contents of an array and emits them as individual items.</p> <p>Note</p> <p>It is technically possible to achieve the same results by using a different channel factory, <code>Channel.fromList</code>, which includes an implicit mapping step in its operation. Here we chose not to use that in order to demonstrate the use of an operator on a fairly simple use case.</p>"},{"location":"hello_nextflow/02_hello_channels/#321-add-the-flatten-operator","title":"3.2.1. Add the <code>flatten()</code> operator","text":"<p>To apply the <code>flatten()</code> operator to our input channel, we append it to the channel factory declaration.</p> <p>In the workflow block, make the following code change:</p> AfterBefore hello-channels.nf<pre><code>    // create a channel for inputs\n    greeting_ch = Channel.of(greetings_array)\n                         .flatten()\n</code></pre> hello-channels.nf<pre><code>    // create a channel for inputs\n    greeting_ch = Channel.of(greetings_array)\n</code></pre> <p>Here we added the operator on the next line for readability, but you can add operators on the same line as the channel factory if you prefer, like this: <code>greeting_ch = Channel.of(greetings_array).flatten()</code></p>"},{"location":"hello_nextflow/02_hello_channels/#322-add-view-to-inspect-channel-contents","title":"3.2.2. Add <code>view()</code> to inspect channel contents","text":"<p>We could run this right away to test if it works, but while we're at it, we're also going to add a couple of <code>view()</code> operators, which allow us to inspect the contents of a channel. You can think of <code>view()</code> as a debugging tool, like a <code>print()</code> statement in Python, or its equivalent in other languages.</p> <p>In the workflow block, make the following code change:</p> AfterBefore hello-channels.nf<pre><code>    // create a channel for inputs\n    greeting_ch = Channel.of(greetings_array)\n                         .view { greeting -&gt; \"Before flatten: $greeting\" }\n                         .flatten()\n                         .view { greeting -&gt; \"After flatten: $greeting\" }\n</code></pre> hello-channels.nf<pre><code>    // create a channel for inputs\n    greeting_ch = Channel.of(greetings_array)\n                         .flatten()\n</code></pre> <p>We are using an operator closure here - the curly brackets. This code executes for each item in the channel. We define a temporary variable for the inner value, here called <code>greeting</code> (it could be anything). This variable is only used within the scope of that closure.</p> <p>In this example, <code>$greeting</code> represents each individual item loaded in a channel.</p> <p>Note on <code>$it</code></p> <p>In some pipelines you may see a special variable called <code>$it</code> used inside operator closures. This is an implicit variable that allows a short-hand access to the inner variable, without needing to define it with a <code>-&gt;</code>.</p> <p>We prefer to be explicit to aid code clarity, as such the <code>$it</code> syntax is discouraged and will slowly be phased out of the Nextflow language.</p>"},{"location":"hello_nextflow/02_hello_channels/#323-run-the-workflow","title":"3.2.3. Run the workflow","text":"<p>Finally, you can try running the workflow again!</p> <pre><code>nextflow run hello-channels.nf\n</code></pre> <p>This time it works AND gives us the additional insight into what the contents of the channel look like before and after we run the <code>flatten()</code> operator:</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-channels.nf` [tiny_elion] DSL2 - revision: 1d834f23d2\n\nexecutor &gt;  local (3)\n[8e/bb08f3] sayHello (2) | 3 of 3 \u2714\nBefore flatten: [Hello, Bonjour, Hol\u00e0]\nAfter flatten: Hello\nAfter flatten: Bonjour\nAfter flatten: Hol\u00e0\n</code></pre> <p>You see that we get a single <code>Before flatten:</code> statement because at that point the channel contains one item, the original array. Then we get three separate <code>After flatten:</code> statements, one for each greeting, which are now individual items in the channel.</p> <p>Importantly, this means each item can now be processed separately by the workflow.</p> <p>Tip</p> <p>You should delete or comment out the <code>view()</code> statements before moving on.</p> hello-channels.nf<pre><code>// create a channel for inputs\ngreeting_ch = Channel.of(greetings_array)\n                     .flatten()\n</code></pre> <p>We left them in the <code>hello-channels-3.nf</code> solution file for reference purposes.</p>"},{"location":"hello_nextflow/02_hello_channels/#takeaway_2","title":"Takeaway","text":"<p>You know how to use an operator like <code>flatten()</code> to transform the contents of a channel, and how to use the <code>view()</code> operator to inspect channel contents before and after applying an operator.</p>"},{"location":"hello_nextflow/02_hello_channels/#whats-next_2","title":"What's next?","text":"<p>Learn how to make the workflow take a file as its source of input values.</p>"},{"location":"hello_nextflow/02_hello_channels/#4-use-an-operator-to-parse-input-values-from-a-csv-file","title":"4. Use an operator to parse input values from a CSV file","text":"<p>It's often the case that, when we want to run on multiple inputs, the input values are contained in a file. As an example, we prepared a CSV file called <code>greetings.csv</code> containing several greetings, one on each line (like a column of data).</p> greetings.csv<pre><code>Hello\nBonjour\nHol\u00e0\n</code></pre> <p>So now we need to modify our workflow to read in the values from a file like that.</p>"},{"location":"hello_nextflow/02_hello_channels/#41-modify-the-script-to-expect-a-csv-file-as-the-source-of-greetings","title":"4.1. Modify the script to expect a CSV file as the source of greetings","text":"<p>To get started, we're going to need to make two key changes to the script:</p> <ul> <li>Switch the input parameter to point to the CSV file</li> <li>Switch to a channel factory designed to handle a file</li> </ul>"},{"location":"hello_nextflow/02_hello_channels/#411-switch-the-input-parameter-to-point-to-the-csv-file","title":"4.1.1. Switch the input parameter to point to the CSV file","text":"<p>Remember the <code>params.greeting</code> parameter we set up in Part 1? We're going to update it to point to the CSV file containing our greetings.</p> <p>Before the workflow block, make the following code change:</p> AfterBefore hello-channels.nf<pre><code>/*\n * Pipeline parameters\n */\nparams.greeting = 'greetings.csv'\n</code></pre> hello-channels.nf<pre><code>/*\n * Pipeline parameters\n */\nparams.greeting = ['Hello','Bonjour','Hol\u00e0']\n</code></pre>"},{"location":"hello_nextflow/02_hello_channels/#412-switch-to-a-channel-factory-designed-to-handle-a-file","title":"4.1.2. Switch to a channel factory designed to handle a file","text":"<p>Since we now want to use a file instead of simple strings as the input, we can't use the <code>Channel.of()</code> channel factory from before. We need to switch to using a new channel factory, <code>Channel.fromPath()</code>, which has some built-in functionality for handling file paths.</p> <p>In the workflow block, make the following code change:</p> AfterBefore hello-channels.nf<pre><code>    // create a channel for inputs from a CSV file\n    greeting_ch = Channel.fromPath(params.greeting)\n</code></pre> hello-channels.nf<pre><code>    // create a channel for inputs\n    greeting_ch = Channel.of(greetings_array)\n                         .flatten()\n</code></pre>"},{"location":"hello_nextflow/02_hello_channels/#413-run-the-workflow","title":"4.1.3. Run the workflow","text":"<p>Let's try running the workflow with the new channel factory and the input file.</p> <pre><code>nextflow run hello-channels.nf\n</code></pre> <p>Oh no, it doesn't work. Here's the start of the console output and error message:</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-channels.nf` [adoring_bhabha] DSL2 - revision: 8ce25edc39\n\n[-        ] sayHello | 0 of 1\nERROR ~ Error executing process &gt; 'sayHello (1)'\n\nCaused by:\n  File `/workspaces/training/hello-nextflow/data/greetings.csv-output.txt` is outside the scope of the process work directory: /workspaces/training/hello-nextflow/work/e3/c459b3c8f4029094cc778c89a4393d\n\n\nCommand executed:\n\n  echo '/workspaces/training/hello-nextflow/data/greetings.csv' &gt; '/workspaces/training/hello-nextflow/data/greetings.\n</code></pre> <p>The <code>Command executed:</code> bit (lines 13-15) is especially helpful here.</p> <p>This may look a little bit familiar. It looks like Nextflow tried to run a single process call using the file path itself as a string value. So it has resolved the file path correctly, but it didn't actually parse its contents, which is what we wanted.</p> <p>How do we get Nextflow to open the file and load its contents into the channel?</p> <p>Sounds like we need another operator!</p>"},{"location":"hello_nextflow/02_hello_channels/#42-use-the-splitcsv-operator-to-parse-the-file","title":"4.2. Use the <code>splitCsv()</code> operator to parse the file","text":"<p>Looking through the list of operators again, we find <code>splitCsv()</code>, which is designed to parse and split CSV-formatted text.</p>"},{"location":"hello_nextflow/02_hello_channels/#421-apply-splitcsv-to-the-channel","title":"4.2.1. Apply <code>splitCsv()</code> to the channel","text":"<p>To apply the operator, we append it to the channel factory line like previously.</p> <p>In the workflow block, make the following code change:</p> AfterBefore hello-channels.nf<pre><code>// create a channel for inputs from a CSV file\ngreeting_ch = Channel.fromPath(params.greeting)\n                     .view { csv -&gt; \"Before splitCsv: $csv\" }\n                     .splitCsv()\n                     .view { csv -&gt; \"After splitCsv: $csv\" }\n</code></pre> hello-channels.nf<pre><code>// create a channel for inputs from a CSV file\ngreeting_ch = Channel.fromPath(params.greeting)\n</code></pre> <p>As you can see, we also include before/after view statements while we're at it.</p>"},{"location":"hello_nextflow/02_hello_channels/#422-run-the-workflow-again","title":"4.2.2. Run the workflow again","text":"<p>Let's try running the workflow with the added CSV-parsing logic.</p> <pre><code>nextflow run hello-channels.nf\n</code></pre> <p>Interestingly, this fails too, but with a different error. The console output and error starts like this:</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-channels.nf` [stoic_ride] DSL2 - revision: a0e5de507e\n\nexecutor &gt;  local (3)\n[42/8fea64] sayHello (1) | 0 of 3\nBefore splitCsv: /workspaces/training/hello-nextflow/greetings.csv\nAfter splitCsv: [Hello]\nAfter splitCsv: [Bonjour]\nAfter splitCsv: [Hol\u00e0]\nERROR ~ Error executing process &gt; 'sayHello (2)'\n\nCaused by:\n  Missing output file(s) `[Bonjour]-output.txt` expected by process `sayHello (2)`\n\n\nCommand executed:\n\n  echo '[Bonjour]' &gt; '[Bonjour]-output.txt'\n</code></pre> <p>This time Nextflow has parsed the contents of the file (yay!) but it's added brackets around the greetings.</p> <p>Long story short, <code>splitCsv()</code> reads each line into an array, and each comma-separated value in the line becomes an element in the array. So here it gives us three arrays containing one element each.</p> <p>Note</p> <p>Even if this behavior feels inconvenient right now, it's going to be extremely useful later when we deal with input files with multiple columns of data.</p> <p>We could solve this by using <code>flatten()</code>, which you already know. However, there's another operator called <code>map()</code> that's more appropriate to use here and is really useful to know; it pops up a lot in Nextflow pipelines.</p>"},{"location":"hello_nextflow/02_hello_channels/#43-use-the-map-operator-to-extract-the-greetings","title":"4.3. Use the <code>map()</code> operator to extract the greetings","text":"<p>The <code>map()</code> operator is a very handy little tool that allows us to do all kinds of mappings to the contents of a channel.</p> <p>In this case, we're going to use it to extract that one element that we want from each line of our file. This is what the syntax looks like:</p> Syntax<pre><code>.map { item -&gt; item[0] }\n</code></pre> <p>This means 'for each element in the channel, take the first of any items it contains'.</p> <p>So let's apply that to our CSV parsing.</p>"},{"location":"hello_nextflow/02_hello_channels/#431-apply-map-to-the-channel","title":"4.3.1. Apply <code>map()</code> to the channel","text":"<p>In the workflow block, make the following code change:</p> AfterBefore hello-channels.nf<pre><code>// create a channel for inputs from a CSV file\ngreeting_ch = Channel.fromPath(params.greeting)\n                     .view { csv -&gt; \"Before splitCsv: $csv\" }\n                     .splitCsv()\n                     .view { csv -&gt; \"After splitCsv: $csv\" }\n                     .map { item -&gt; item[0] }\n                     .view { csv -&gt; \"After map: $csv\" }\n</code></pre> hello-channels.nf<pre><code>// create a channel for inputs from a CSV file\ngreeting_ch = Channel.fromPath(params.greeting)\n                     .view { csv -&gt; \"Before splitCsv: $csv\" }\n                     .splitCsv()\n                     .view { csv -&gt; \"After splitCsv: $csv\" }\n</code></pre> <p>Once again we include another <code>view()</code> call to confirm that the operator does what we expect.</p>"},{"location":"hello_nextflow/02_hello_channels/#432-run-the-workflow-one-more-time","title":"4.3.2. Run the workflow one more time","text":"<p>Let's run it one more time:</p> <pre><code>nextflow run hello-channels.nf\n</code></pre> <p>This time it should run without error.</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-channels.nf` [tiny_heisenberg] DSL2 - revision: 845b471427\n\nexecutor &gt;  local (3)\n[1a/1d19ab] sayHello (2) | 3 of 3 \u2714\nBefore splitCsv: /workspaces/training/hello-nextflow/greetings.csv\nAfter splitCsv: [Hello]\nAfter splitCsv: [Bonjour]\nAfter splitCsv: [Hol\u00e0]\nAfter map: Hello\nAfter map: Bonjour\nAfter map: Hol\u00e0\n</code></pre> <p>Looking at the output of the <code>view()</code> statements, we see the following:</p> <ul> <li>A single <code>Before splitCsv:</code> statement: at that point the channel contains one item, the original file path.</li> <li>Three separate <code>After splitCsv:</code> statements: one for each greeting, but each is contained within an array that corresponds to that line in the file.</li> <li>Three separate <code>After map:</code> statements: one for each greeting, which are now individual elements in the channel.</li> </ul> <p>You can also look at the output files to verify that each greeting was correctly extracted and processed through the workflow.</p> <p>We've achieved the same result as previously, but now we have a lot more flexibility to add more elements to the channel of greetings we want to process by modifying an input file, without modifying any code.</p> <p>Note</p> <p>Here we had all greetings on one line in the CSV file. You can try adding more columns to the CSV file and see what happens; for example, try the following:</p> greetings.csv<pre><code>Hello,English\nBonjour,French\nHol\u00e0,Spanish\n</code></pre> <p>You can also try replacing <code>.map { item -&gt; item[0] }</code> with <code>.flatten()</code> and see what happens depending on how many lines and columns you have in the input file.</p> <p>You'll learn learn more advanced approaches for handling complex inputs in a later training.</p>"},{"location":"hello_nextflow/02_hello_channels/#takeaway_3","title":"Takeaway","text":"<p>You know how to use the operators <code>splitCsv()</code> and <code>map()</code> to read in a file of input values and handle them appropriately.</p> <p>More generally, you have a basic understanding of how Nextflow uses channels to manage inputs to processes and operators to transform their contents.</p>"},{"location":"hello_nextflow/02_hello_channels/#whats-next_3","title":"What's next?","text":"<p>Take a big break, you worked hard in this one! When you're ready, move on to Part 3 to learn how to add more steps and connect them together into a proper workflow.</p>"},{"location":"hello_nextflow/03_hello_workflow/","title":"Part 3: Hello Workflow","text":"<p> See the whole playlist on the Nextflow YouTube channel.</p> <p> The video transcript is available here.</p> <p>Most real-world workflows involve more than one step. In this training module, you'll learn how to connect processes together in a multi-step workflow.</p> <p>This will teach you the Nextflow way of achieving the following:</p> <ol> <li>Making data flow from one process to the next</li> <li>Collecting outputs from multiple process calls into a single process call</li> <li>Passing more than one input to a process</li> <li>Handling multiple outputs coming out of a process</li> </ol> <p>To demonstrate, we will continue building on the domain-agnostic Hello World example from Parts 1 and 2. This time, we're going to make the following changes to our workflow to better reflect how people build actual workflows:</p> <ol> <li>Add a second step that converts the greeting to uppercase.</li> <li>Add a third step that collects all the transformed greetings and writes them into a single file.</li> <li>Add a parameter to name the final output file and pass that as a secondary input to the collection step.</li> <li>Make the collection step also output a simple statistic about what was processed.</li> </ol>"},{"location":"hello_nextflow/03_hello_workflow/#0-warmup-run-hello-workflownf","title":"0. Warmup: Run <code>hello-workflow.nf</code>","text":"<p>We're going to use the workflow script <code>hello-workflow.nf</code> as a starting point. It is equivalent to the script produced by working through Part 2 of this training course.</p> <p>Just to make sure everything is working, run the script once before making any changes:</p> <pre><code>nextflow run hello-workflow.nf\n</code></pre> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-workflow.nf` [stupefied_sammet] DSL2 - revision: b9e466930b\n\nexecutor &gt;  local (3)\n[2a/324ce6] sayHello (3) | 3 of 3 \u2714\n</code></pre> <p>As previously, you will find the output files in the <code>results</code> directory (specified by the <code>publishDir</code> directive).</p> Directory contents<pre><code>results\n\u251c\u2500\u2500 Bonjour-output.txt\n\u251c\u2500\u2500 Hello-output.txt\n\u2514\u2500\u2500 Hol\u00e0-output.txt\n</code></pre> <p>Note</p> <p>There may also be a file named <code>output.txt</code> left over if you worked through Part 2 in the same environment.</p> <p>If that worked for you, you're ready to learn how to assemble a multi-step workflow.</p>"},{"location":"hello_nextflow/03_hello_workflow/#1-add-a-second-step-to-the-workflow","title":"1. Add a second step to the workflow","text":"<p>We're going to add a step to convert the greeting to uppercase. To that end, we need to do three things:</p> <ul> <li>Define the command we're going to use to do the uppercase conversion.</li> <li>Write a new process that wraps the uppercasing command.</li> <li>Call the new process in the workflow block and set it up to take the output of the <code>sayHello()</code> process as input.</li> </ul>"},{"location":"hello_nextflow/03_hello_workflow/#11-define-the-uppercasing-command-and-test-it-in-the-terminal","title":"1.1. Define the uppercasing command and test it in the terminal","text":"<p>To do the conversion of the greetings to uppercase, we're going to use a classic UNIX tool called <code>tr</code> for 'text replacement', with the following syntax:</p> Syntax<pre><code>tr '[a-z]' '[A-Z]'\n</code></pre> <p>This is a very naive text replacement one-liner that does not account for accented letters, so for example 'Hol\u00e0' will become 'HOL\u00e0', but it will do a good enough job for demonstrating the Nextflow concepts and that's what matters.</p> <p>To test it out, we can run the <code>echo 'Hello World'</code> command and pipe its output to the <code>tr</code> command:</p> <pre><code>echo 'Hello World' | tr '[a-z]' '[A-Z]' &gt; UPPER-output.txt\n</code></pre> <p>The output is a text file called <code>UPPER-output.txt</code> that contains the uppercase version of the <code>Hello World</code> string:</p> UPPER-output.txt<pre><code>HELLO WORLD\n</code></pre> <p>That's basically what we're going to try to do with our workflow.</p>"},{"location":"hello_nextflow/03_hello_workflow/#12-write-the-uppercasing-step-as-a-nextflow-process","title":"1.2. Write the uppercasing step as a Nextflow process","text":"<p>We can model our new process on the first one, since we want to use all the same components.</p> <p>Add the following process definition to the workflow script:</p> hello-workflow.nf<pre><code>/*\n * Use a text replacement tool to convert the greeting to uppercase\n */\nprocess convertToUpper {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        path input_file\n\n    output:\n        path \"UPPER-${input_file}\"\n\n    script:\n    \"\"\"\n    cat '$input_file' | tr '[a-z]' '[A-Z]' &gt; 'UPPER-${input_file}'\n    \"\"\"\n}\n</code></pre> <p>Here, we compose the second output filename based on the input filename, similarly to what we did originally for the output of the first process.</p> <p>Note</p> <p>Nextflow will determine the order of operations based on the chaining of inputs and outputs, so the order of the process definitions in the workflow script does not matter. However, we do recommend you be kind to your collaborators and to your future self, and try to write them in a logical order for the sake of readability.</p>"},{"location":"hello_nextflow/03_hello_workflow/#13-add-a-call-to-the-new-process-in-the-workflow-block","title":"1.3. Add a call to the new process in the workflow block","text":"<p>Now we need to tell Nextflow to actually call the process that we just defined.</p> <p>In the workflow block, make the following code change:</p> AfterBefore hello-workflow.nf<pre><code>    // emit a greeting\n    sayHello(greeting_ch)\n\n    // convert the greeting to uppercase\n    convertToUpper()\n}\n</code></pre> hello-workflow.nf<pre><code>    // emit a greeting\n    sayHello(greeting_ch)\n}\n</code></pre> <p>This is not yet functional because we have not specified what should be input to the <code>convertToUpper()</code> process.</p>"},{"location":"hello_nextflow/03_hello_workflow/#14-pass-the-output-of-the-first-process-to-the-second-process","title":"1.4. Pass the output of the first process to the second process","text":"<p>Now we need to make the output of the <code>sayHello()</code> process flow into the <code>convertToUpper()</code> process.</p> <p>Conveniently, Nextflow automatically packages the output of a process into a channel called <code>&lt;process&gt;.out</code>. So the output of the <code>sayHello</code> process is a channel called <code>sayHello.out</code>, which we can plug straight into the call to <code>convertToUpper()</code>.</p> <p>In the workflow block, make the following code change:</p> AfterBefore hello-workflow.nf<pre><code>    // convert the greeting to uppercase\n    convertToUpper(sayHello.out)\n}\n</code></pre> hello-workflow.nf<pre><code>    // convert the greeting to uppercase\n    convertToUpper()\n}\n</code></pre> <p>For a simple case like this (one output to one input), that's all we need to do to connect two processes!</p>"},{"location":"hello_nextflow/03_hello_workflow/#15-run-the-workflow-again-with-resume","title":"1.5. Run the workflow again with <code>-resume</code>","text":"<p>Let's run this using the <code>-resume</code> flag, since we've already run the first step of the workflow successfully.</p> <pre><code>nextflow run hello-workflow.nf -resume\n</code></pre> <p>You should see the following output:</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-workflow.nf` [disturbed_darwin] DSL2 - revision: 4e252c048f\n\nexecutor &gt;  local (3)\n[79/33b2f0] sayHello (2)       | 3 of 3, cached: 3 \u2714\n[b3/d52708] convertToUpper (3) | 3 of 3 \u2714\n</code></pre> <p>There is now an extra line in the console output (line 7), which corresponds to the new process we just added.</p> <p>Let's have a look inside the work directory of one of the calls to the second process.</p> Directory contents<pre><code>work/b3/d52708edba8b864024589285cb3445/\n\u251c\u2500\u2500 Bonjour-output.txt -&gt; /workspaces/training/hello-nextflow/work/79/33b2f0af8438486258d200045bd9e8/Bonjour-output.txt\n\u2514\u2500\u2500 UPPER-Bonjour-output.txt\n</code></pre> <p>We find two output files: the output of the first process AND the output of the second.</p> <p>The output of the first process is in there because Nextflow staged it there in order to have everything needed for execution within the same subdirectory. However, it is actually a symbolic link pointing to the the original file in the subdirectory of the first process call. By default, when running on a single machine as we're doing here, Nextflow uses symbolic links rather than copies to stage input and intermediate files.</p> <p>You'll also find the final outputs in the <code>results</code> directory since we used the <code>publishDir</code> directive in the second process too.</p> Directory contents<pre><code>results\n\u251c\u2500\u2500 Bonjour-output.txt\n\u251c\u2500\u2500 Hello-output.txt\n\u251c\u2500\u2500 Hol\u00e0-output.txt\n\u251c\u2500\u2500 UPPER-Bonjour-output.txt\n\u251c\u2500\u2500 UPPER-Hello-output.txt\n\u2514\u2500\u2500 UPPER-Hol\u00e0-output.txt\n</code></pre> <p>Think about how all we did was connect the output of <code>sayHello</code> to the input of <code>convertToUpper</code> and the two processes could be run in series. Nextflow did the hard work of handling individual input and output files and passing them between the two commands for us.</p> <p>This is one of the reasons Nextflow channels are so powerful: they take care of the busywork involved in connecting workflow steps together.</p>"},{"location":"hello_nextflow/03_hello_workflow/#takeaway","title":"Takeaway","text":"<p>You know how to add a second step that takes the output of the first step as input.</p>"},{"location":"hello_nextflow/03_hello_workflow/#whats-next","title":"What's next?","text":"<p>Learn how to collect outputs from batched process calls and feed them into a single process.</p>"},{"location":"hello_nextflow/03_hello_workflow/#2-add-a-third-step-to-collect-all-the-greetings","title":"2. Add a third step to collect all the greetings","text":"<p>When we use a process to apply a transformation to each of the elements in a channel, like we're doing here to the multiple greetings, we sometimes want to collect elements from the output channel of that process, and feed them into another process that performs some kind of analysis or summation.</p> <p>In the next step we're simply going to write all the elements of a channel to a single file, using the UNIX <code>cat</code> command.</p>"},{"location":"hello_nextflow/03_hello_workflow/#21-define-the-collection-command-and-test-it-in-the-terminal","title":"2.1. Define the collection command and test it in the terminal","text":"<p>The collection step we want to add to our workflow will use the <code>cat</code> command to concatenate multiple uppercased greetings into a single file.</p> <p>Let's run the command by itself in the terminal to verify that it works as expected, just like we've done previously.</p> <p>Run the following in your terminal:</p> <pre><code>echo 'Hello' | tr '[a-z]' '[A-Z]' &gt; UPPER-Hello-output.txt\necho 'Bonjour' | tr '[a-z]' '[A-Z]' &gt; UPPER-Bonjour-output.txt\necho 'Hol\u00e0' | tr '[a-z]' '[A-Z]' &gt; UPPER-Hol\u00e0-output.txt\ncat UPPER-Hello-output.txt UPPER-Bonjour-output.txt UPPER-Hol\u00e0-output.txt &gt; COLLECTED-output.txt\n</code></pre> <p>The output is a text file called <code>COLLECTED-output.txt</code> that contains the uppercase versions of the original greetings.</p> COLLECTED-output.txt<pre><code>HELLO\nBONJOUR\nHOL\u00e0\n</code></pre> <p>That is the result we want to achieve with our workflow.</p>"},{"location":"hello_nextflow/03_hello_workflow/#22-create-a-new-process-to-do-the-collection-step","title":"2.2. Create a new process to do the collection step","text":"<p>Let's create a new process and call it <code>collectGreetings()</code>. We can start writing it based on the previous one.</p>"},{"location":"hello_nextflow/03_hello_workflow/#221-write-the-obvious-parts-of-the-process","title":"2.2.1. Write the 'obvious' parts of the process","text":"<p>Add the following process definition to the workflow script:</p> hello-workflow.nf<pre><code>/*\n * Collect uppercase greetings into a single output file\n */\nprocess collectGreetings {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        ???\n\n    output:\n        path \"COLLECTED-output.txt\"\n\n    script:\n    \"\"\"\n    ??? &gt; 'COLLECTED-output.txt'\n    \"\"\"\n}\n</code></pre> <p>This is what we can write with confidence based on what you've learned so far. But this is not functional! It leaves out the input definition(s) and the first half of the script command because we need to figure out how to write that.</p>"},{"location":"hello_nextflow/03_hello_workflow/#222-define-inputs-to-collectgreetings","title":"2.2.2. Define inputs to <code>collectGreetings()</code>","text":"<p>We need to collect the greetings from all the calls to the <code>convertToUpper()</code> process. What do we know we can get from the previous step in the workflow?</p> <p>The channel output by <code>convertToUpper()</code> will contain the paths to the individual files containing the uppercased greetings. That amounts to one input slot; let's call it <code>input_files</code> for simplicity.</p> <p>In the process block, make the following code change:</p> AfterBefore hello-workflow.nf<pre><code>        input:\n            path input_files\n</code></pre> hello-workflow.nf<pre><code>        input:\n            ???\n</code></pre> <p>Notice we use the <code>path</code> prefix even though we expect this to contain multiple files. Nextflow doesn't mind, so it doesn't matter.</p>"},{"location":"hello_nextflow/03_hello_workflow/#223-compose-the-concatenation-command","title":"2.2.3. Compose the concatenation command","text":"<p>This is where things could get a little tricky, because we need to be able to handle an arbitrary number of input files. Specifically, we can't write the command up front, so we need to tell Nextflow how to compose it at runtime based on what inputs flow into the process.</p> <p>In other words, if we have an input channel containing the element <code>[file1.txt, file2.txt, file3.txt]</code>, we need Nextflow to turn that into <code>cat file1.txt file2.txt file3.txt</code>.</p> <p>Fortunately, Nextflow is quite happy to do that for us if we simply write <code>cat ${input_files}</code> in the script command.</p> <p>In the process block, make the following code change:</p> AfterBefore hello-workflow.nf<pre><code>    script:\n    \"\"\"\n    cat ${input_files} &gt; 'COLLECTED-output.txt'\n    \"\"\"\n</code></pre> hello-workflow.nf<pre><code>    script:\n    \"\"\"\n    ??? &gt; 'COLLECTED-output.txt'\n    \"\"\"\n</code></pre> <p>In theory this should handle any arbitrary number of input files.</p> <p>Tip</p> <p>Some command-line tools require providing an argument (like <code>-input</code>) for each input file. In that case, we would have to do a little bit of extra work to compose the command. You can see an example of this in the Nextflow for Genomics training course.</p>"},{"location":"hello_nextflow/03_hello_workflow/#23-add-the-collection-step-to-the-workflow","title":"2.3. Add the collection step to the workflow","text":"<p>Now we should just need to call the collection process on the output of the uppercasing step.</p>"},{"location":"hello_nextflow/03_hello_workflow/#231-connect-the-process-calls","title":"2.3.1. Connect the process calls","text":"<p>In the workflow block, make the following code change:</p> AfterBefore hello-workflow.nf<pre><code>    // convert the greeting to uppercase\n    convertToUpper(sayHello.out)\n\n    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out)\n}\n</code></pre> hello-workflow.nf<pre><code>    // convert the greeting to uppercase\n    convertToUpper(sayHello.out)\n}\n</code></pre> <p>This connects the output of <code>convertToUpper()</code> to the input of <code>collectGreetings()</code>.</p>"},{"location":"hello_nextflow/03_hello_workflow/#232-run-the-workflow-with-resume","title":"2.3.2. Run the workflow with <code>-resume</code>","text":"<p>Let's try it.</p> <pre><code>nextflow run hello-workflow.nf -resume\n</code></pre> <p>It runs successfully, including the third step:</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-workflow.nf` [mad_gilbert] DSL2 - revision: 6acfd5e28d\n\nexecutor &gt;  local (3)\n[79/33b2f0] sayHello (2)         | 3 of 3, cached: 3 \u2714\n[99/79394f] convertToUpper (3)   | 3 of 3, cached: 3 \u2714\n[47/50fe4a] collectGreetings (1) | 3 of 3 \u2714\n</code></pre> <p>However, look at the number of calls for <code>collectGreetings()</code> on line 8. We were only expecting one, but there are three.</p> <p>And have a look at the contents of the final output file too:</p> results/COLLECTED-output.txt<pre><code>Hol\u00e0\n</code></pre> <p>Oh no. The collection step was run individually on each greeting, which is NOT what we wanted.</p> <p>We need to do something to tell Nextflow explicitly that we want that third step to run on all the elements in the channel output by <code>convertToUpper()</code>.</p>"},{"location":"hello_nextflow/03_hello_workflow/#24-use-an-operator-to-collect-the-greetings-into-a-single-input","title":"2.4. Use an operator to collect the greetings into a single input","text":"<p>Yes, once again the answer to our problem is an operator.</p> <p>Specifically, we are going to use the aptly-named <code>collect()</code> operator.</p>"},{"location":"hello_nextflow/03_hello_workflow/#241-add-the-collect-operator","title":"2.4.1. Add the <code>collect()</code> operator","text":"<p>This time it's going to look a bit different because we're not adding the operator in the context of a channel factory, but to an output channel.</p> <p>We take the <code>convertToUpper.out</code> and append the <code>collect()</code> operator, which gives us <code>convertToUpper.out.collect()</code>. We can plug that directly into the <code>collectGreetings()</code> process call.</p> <p>In the workflow block, make the following code change:</p> AfterBefore hello-workflow.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect())\n}\n</code></pre> hello-workflow.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out)\n}\n</code></pre>"},{"location":"hello_nextflow/03_hello_workflow/#242-add-some-view-statements","title":"2.4.2. Add some <code>view()</code> statements","text":"<p>Let's also include a couple of <code>view()</code> statements to visualize the before and after states of the channel contents.</p> AfterBefore hello-workflow.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect())\n\n    // optional view statements\n    convertToUpper.out.view { greeting -&gt; \"Before collect: $greeting\" }\n    convertToUpper.out.collect().view { greeting -&gt; \"After collect: $greeting\" }\n}\n</code></pre> hello-workflow.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect())\n}\n</code></pre> <p>The <code>view()</code> statements can go anywhere you want; we put them after the call for readability.</p>"},{"location":"hello_nextflow/03_hello_workflow/#243-run-the-workflow-again-with-resume","title":"2.4.3. Run the workflow again with <code>-resume</code>","text":"<p>Let's try it:</p> <pre><code>nextflow run hello-workflow.nf -resume\n</code></pre> <p>It runs successfully, although the log output may look a little messier than this (we cleaned it up for readability).</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-workflow.nf` [soggy_franklin] DSL2 - revision: bc8e1b2726\n\n[d6/cdf466] sayHello (1)       | 3 of 3, cached: 3 \u2714\n[99/79394f] convertToUpper (2) | 3 of 3, cached: 3 \u2714\n[1e/83586c] collectGreetings   | 1 of 1 \u2714\nBefore collect: /workspaces/training/hello-nextflow/work/b3/d52708edba8b864024589285cb3445/UPPER-Bonjour-output.txt\nBefore collect: /workspaces/training/hello-nextflow/work/99/79394f549e3040dfc2440f69ede1fc/UPPER-Hello-output.txt\nBefore collect: /workspaces/training/hello-nextflow/work/aa/56bfe7cf00239dc5badc1d04b60ac4/UPPER-Hol\u00e0-output.txt\nAfter collect: [/workspaces/training/hello-nextflow/work/b3/d52708edba8b864024589285cb3445/UPPER-Bonjour-output.txt, /workspaces/training/hello-nextflow/work/99/79394f549e3040dfc2440f69ede1fc/UPPER-Hello-output.txt, /workspaces/training/hello-nextflow/work/aa/56bfe7cf00239dc5badc1d04b60ac4/UPPER-Hol\u00e0-output.txt]\n</code></pre> <p>This time the third step was only called once!</p> <p>Looking at the output of the <code>view()</code> statements, we see the following:</p> <ul> <li>Three <code>Before collect:</code> statements, one for each greeting: at that point the file paths are individual items in the channel.</li> <li>A single <code>After collect:</code> statement: the three file paths are now packaged into a single element.</li> </ul> <p>Have a look at the contents of the final output file too:</p> results/COLLECTED-output.txt<pre><code>BONJOUR\nHELLO\nHOL\u00e0\n</code></pre> <p>This time we have all three greetings in the final output file. Success! Remove the optional view calls to make the next outputs less verbose.</p> <p>Note</p> <p>If you run this several times without <code>-resume</code>, you will see that the order of the greetings changes from one run to the next. This shows you that the order in which elements flow through process calls is not guaranteed to be consistent.</p>"},{"location":"hello_nextflow/03_hello_workflow/#takeaway_1","title":"Takeaway","text":"<p>You know how to collect outputs from a batch of process calls and feed them into a joint analysis or summation step.</p>"},{"location":"hello_nextflow/03_hello_workflow/#whats-next_1","title":"What's next?","text":"<p>Learn how to pass more than one input to a process.</p>"},{"location":"hello_nextflow/03_hello_workflow/#3-pass-more-than-one-input-to-a-process-in-order-to-name-the-final-output-file-uniquely","title":"3. Pass more than one input to a process in order to name the final output file uniquely","text":"<p>We want to be able to name the final output file something specific in order to process subsequent batches of greetings without overwriting the final results.</p> <p>To that end, we're going to make the following refinements to the workflow:</p> <ul> <li>Modify the collector process to accept a user-defined name for the output file</li> <li>Add a command-line parameter to the workflow and pass it to the collector process</li> </ul>"},{"location":"hello_nextflow/03_hello_workflow/#31-modify-the-collector-process-to-accept-a-user-defined-name-for-the-output-file","title":"3.1. Modify the collector process to accept a user-defined name for the output file","text":"<p>We're going to need to declare the additional input and integrate it into the output file name.</p>"},{"location":"hello_nextflow/03_hello_workflow/#311-declare-the-additional-input-in-the-process-definition","title":"3.1.1. Declare the additional input in the process definition","text":"<p>Good news: we can declare as many input variables as we want. Let's call this one <code>batch_name</code>.</p> <p>In the process block, make the following code change:</p> AfterBefore hello-workflow.nf<pre><code>    input:\n        path input_files\n        val batch_name\n</code></pre> hello-workflow.nf<pre><code>    input:\n        path input_files\n</code></pre> <p>You can set up your processes to expect as many inputs as you want. Later on, you will learn how to manage required vs. optional inputs.</p>"},{"location":"hello_nextflow/03_hello_workflow/#312-use-the-batch_name-variable-in-the-output-file-name","title":"3.1.2. Use the <code>batch_name</code> variable in the output file name","text":"<p>In the process block, make the following code change:</p> AfterBefore hello-workflow.nf<pre><code>    output:\n        path \"COLLECTED-${batch_name}-output.txt\"\n\n    script:\n    \"\"\"\n    cat ${input_files} &gt; 'COLLECTED-${batch_name}-output.txt'\n    \"\"\"\n</code></pre> hello-workflow.nf<pre><code>    output:\n        path \"COLLECTED-output.txt\"\n\n    script:\n    \"\"\"\n    cat ${input_files} &gt; 'COLLECTED-output.txt'\n    \"\"\"\n</code></pre> <p>This sets up the process to use the <code>batch_name</code> value to generate a specific filename for the final output of the workflow.</p>"},{"location":"hello_nextflow/03_hello_workflow/#32-add-a-batch-command-line-parameter","title":"3.2. Add a <code>batch</code> command-line parameter","text":"<p>Now we need a way to supply the value for <code>batch_name</code> and feed it to the process call.</p>"},{"location":"hello_nextflow/03_hello_workflow/#321-use-params-to-set-up-the-parameter","title":"3.2.1. Use <code>params</code> to set up the parameter","text":"<p>You already know how to use the <code>params</code> system to declare CLI parameters. Let's use that to declare a <code>batch</code> parameter (with a default value because we are lazy).</p> <p>In the pipeline parameters section, make the following code changes:</p> AfterBefore hello-workflow.nf<pre><code>/*\n * Pipeline parameters\n */\nparams.greeting = 'greetings.csv'\nparams.batch = 'test-batch'\n</code></pre> hello-workflow.nf<pre><code>/*\n * Pipeline parameters\n */\nparams.greeting = 'greetings.csv'\n</code></pre> <p>Remember you can override that default value by specifying a value with <code>--batch</code> on the command line.</p>"},{"location":"hello_nextflow/03_hello_workflow/#322-pass-the-batch-parameter-to-the-process","title":"3.2.2. Pass the <code>batch</code> parameter to the process","text":"<p>To provide the value of the parameter to the process, we need to add it in the process call.</p> <p>In the workflow block, make the following code change:</p> AfterBefore hello-workflow.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect(), params.batch)\n</code></pre> hello-workflow.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect())\n</code></pre> <p>Warning</p> <p>You MUST provide the inputs to a process in the EXACT SAME ORDER as they are listed in the input definition block of the process.</p>"},{"location":"hello_nextflow/03_hello_workflow/#33-run-the-workflow","title":"3.3. Run the workflow","text":"<p>Let's try running this with a batch name on the command line.</p> <pre><code>nextflow run hello-workflow.nf -resume --batch trio\n</code></pre> <p>It runs successfully:</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-workflow.nf` [confident_rutherford] DSL2 - revision: bc58af409c\n\nexecutor &gt;  local (1)\n[79/33b2f0] sayHello (2)       | 3 of 3, cached: 3 \u2714\n[99/79394f] convertToUpper (2) | 3 of 3, cached: 3 \u2714\n[b5/f19efe] collectGreetings   | 1 of 1 \u2714\n</code></pre> <p>And produces the desired output:</p> bash<pre><code>cat results/COLLECTED-trio-output.txt\n</code></pre> Output<pre><code>HELLO\nBONJOUR\nHOL\u00e0\n</code></pre> <p>Now, subsequent runs on other batches of inputs won't clobber previous results (as long as we specify the parameter appropriately).</p>"},{"location":"hello_nextflow/03_hello_workflow/#takeaway_2","title":"Takeaway","text":"<p>You know how to pass more than one input to a process.</p>"},{"location":"hello_nextflow/03_hello_workflow/#whats-next_2","title":"What's next?","text":"<p>Learn how to emit multiple outputs and handle them conveniently.</p>"},{"location":"hello_nextflow/03_hello_workflow/#4-add-an-output-to-the-collector-step","title":"4. Add an output to the collector step","text":"<p>When a process produces only one output, it's easy to access it (in the workflow block) using the <code>&lt;process&gt;.out</code> syntax. When there are two or more outputs, the default way to select a specific output is to use the corresponding (zero-based) index; for example, you would use <code>&lt;process&gt;.out[0]</code> to get the first output. This is not terribly convenient; it's too easy to grab the wrong index.</p> <p>Let's have a look at how we can select and use a specific output of a process when there are more than one.</p> <p>For demonstration purposes, let's say we want to count and report the number of greetings that are being collected for a given batch of inputs.</p> <p>To that end, we're going to make the following refinements to the workflow:</p> <ul> <li>Modify the process to count and output the number of greetings</li> <li>Once the process has run, select the count and report it using <code>view</code> (in the workflow block)</li> </ul>"},{"location":"hello_nextflow/03_hello_workflow/#41-modify-the-process-to-count-and-output-the-number-of-greetings","title":"4.1. Modify the process to count and output the number of greetings","text":"<p>This will require two key changes to the process definition: we need a way to count the greetings, then we need to add that count to the <code>output</code> block of the process.</p>"},{"location":"hello_nextflow/03_hello_workflow/#411-count-the-number-of-greetings-collected","title":"4.1.1. Count the number of greetings collected","text":"<p>Conveniently, Nextflow lets us add arbitrary code in the <code>script:</code> block of the process definition, which comes in really handy for doing things like this.</p> <p>That means we can use the built-in <code>size()</code> function to get the number of files in the <code>input_files</code> array.</p> <p>In the <code>collectGreetings</code> process block, make the following code change:</p> AfterBefore hello-workflow.nf<pre><code>    script:\n        count_greetings = input_files.size()\n    \"\"\"\n    cat ${input_files} &gt; 'COLLECTED-${batch_name}-output.txt'\n    \"\"\"\n</code></pre> hello-workflow.nf<pre><code>    script:\n    \"\"\"\n    cat ${input_files} &gt; 'COLLECTED-${batch_name}-output.txt'\n    \"\"\"\n</code></pre> <p>The <code>count_greetings</code> variable will be computed at runtime.</p>"},{"location":"hello_nextflow/03_hello_workflow/#412-emit-the-count-as-a-named-output","title":"4.1.2. Emit the count as a named output","text":"<p>In principle all we need to do is to add the <code>count_greetings</code> variable to the <code>output:</code> block.</p> <p>However, while we're at it, we're also going to add some <code>emit:</code> tags to our output declarations. These will enable us to select the outputs by name instead of having to use positional indices.</p> <p>In the process block, make the following code change:</p> AfterBefore hello-workflow.nf<pre><code>    output:\n        path \"COLLECTED-${batch_name}-output.txt\" , emit: outfile\n        val count_greetings , emit: count\n</code></pre> hello-workflow.nf<pre><code>    output:\n        path \"COLLECTED-${batch_name}-output.txt\"\n</code></pre> <p>The <code>emit:</code> tags are optional, and we could have added a tag to only one of the outputs. But as the saying goes, why not both?</p>"},{"location":"hello_nextflow/03_hello_workflow/#42-report-the-output-at-the-end-of-the-workflow","title":"4.2. Report the output at the end of the workflow","text":"<p>Now that we have two outputs coming out of the <code>collectGreetings</code> process, the <code>collectGreetings.out</code> output contains two channels:</p> <ul> <li><code>collectGreetings.out.outfile</code> contains the final output file</li> <li><code>collectGreetings.out.count</code> contains the count of greetings</li> </ul> <p>We could send either or both of these to another process for further work. However, in the interest of wrapping this up, we're just going to use <code>view()</code> to demonstrate that we can access and report the count of greetings.</p> <p>In the workflow block, make the following code change:</p> AfterBefore hello-workflow.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect(), params.batch)\n\n    // emit a message about the size of the batch\n    collectGreetings.out.count.view { num_greetings -&gt; \"There were $num_greetings greetings in this batch\" }\n</code></pre> hello-workflow.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect(), params.batch)\n</code></pre> <p>Note</p> <p>There are a few other ways we could achieve a similar result, including some more elegant ones like the <code>count()</code> operator, but this allows us to show how to handle multiple outputs, which is what we care about.</p>"},{"location":"hello_nextflow/03_hello_workflow/#43-run-the-workflow","title":"4.3. Run the workflow","text":"<p>Let's try running this with the current batch of greetings.</p> <pre><code>nextflow run hello-workflow.nf -resume --batch trio\n</code></pre> <p>This runs successfully:</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-workflow.nf` [evil_sinoussi] DSL2 - revision: eeca64cdb1\n\n[d6/cdf466] sayHello (1)       | 3 of 3, cached: 3 \u2714\n[99/79394f] convertToUpper (2) | 3 of 3, cached: 3 \u2714\n[9e/1dfda7] collectGreetings   | 1 of 1, cached: 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>The last line (line 8) shows that we correctly retrieved the count of greetings processed. Feel free to add more greetings to the CSV and see what happens.</p>"},{"location":"hello_nextflow/03_hello_workflow/#takeaway_3","title":"Takeaway","text":"<p>You know how to make a process emit a named output and how to access it from the workflow block.</p> <p>More generally, you understand the key principles involved in connecting processes together in common ways.</p>"},{"location":"hello_nextflow/03_hello_workflow/#whats-next_3","title":"What's next?","text":"<p>Take an extra long break, you've earned it. When you're ready, move on to Part 4 to learn how to modularize your code for better maintainability and code efficiency.</p>"},{"location":"hello_nextflow/04_hello_modules/","title":"Part 4: Hello Modules","text":"<p> See the whole playlist on the Nextflow YouTube channel.</p> <p> The video transcript is available here.</p> <p>This section covers how to organize your workflow code to make development and maintenance of your pipeline more efficient and sustainable. Specifically, we are going to demonstrate how to use modules.</p> <p>In Nextflow, a module is a single process definition that is encapsulated by itself in a standalone code file. To use a module in a workflow, you just add a single-line import statement to your workflow code file; then you can integrate the process into the workflow the same way you normally would.</p> <p>When we started developing our workflow, we put everything in one single code file.</p> <p>Putting processes into individual modules makes it possible to reuse process definitions in multiple workflows without producing multiple copies of the code. This makes the code more shareable, flexible and maintainable.</p> <p>Note</p> <p>It is also possible to encapsulate a section of a workflow as a 'subworkflow' that can be imported into a larger pipeline, but that is outside the scope of this course.</p>"},{"location":"hello_nextflow/04_hello_modules/#0-warmup-run-hello-modulesnf","title":"0. Warmup: Run <code>hello-modules.nf</code>","text":"<p>We're going to use the workflow script <code>hello-modules.nf</code> as a starting point. It is equivalent to the script produced by working through Part 3 of this training course.</p> <p>Just to make sure everything is working, run the script once before making any changes:</p> <pre><code>nextflow run hello-modules.nf\n</code></pre> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-modules.nf` [festering_nobel] DSL2 - revision: eeca64cdb1\n\nexecutor &gt;  local (7)\n[25/648bdd] sayHello (2)       | 3 of 3 \u2714\n[60/bc6831] convertToUpper (1) | 3 of 3 \u2714\n[1a/bc5901] collectGreetings   | 1 of 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>As previously, you will find the output files in the <code>results</code> directory (specified by the <code>publishDir</code> directive).</p> Directory contents<pre><code>results\n\u251c\u2500\u2500 Bonjour-output.txt\n\u251c\u2500\u2500 COLLECTED-output.txt\n\u251c\u2500\u2500 COLLECTED-test-batch-output.txt\n\u251c\u2500\u2500 COLLECTED-trio-output.txt\n\u251c\u2500\u2500 Hello-output.txt\n\u251c\u2500\u2500 Hol\u00e0-output.txt\n\u251c\u2500\u2500 UPPER-Bonjour-output.txt\n\u251c\u2500\u2500 UPPER-Hello-output.txt\n\u2514\u2500\u2500 UPPER-Hol\u00e0-output.txt\n</code></pre> <p>Note</p> <p>There may also be a file named <code>output.txt</code> left over if you worked through Part 2 in the same environment.</p> <p>If that worked for you, you're ready to learn how to modularize your workflow code.</p>"},{"location":"hello_nextflow/04_hello_modules/#1-create-a-directory-to-store-modules","title":"1. Create a directory to store modules","text":"<p>It is best practice to store your modules in a specific directory. You can call that directory anything you want, but the convention is to call it <code>modules/</code>.</p> <pre><code>mkdir modules\n</code></pre> <p>Note</p> <p>Here we are showing how to use local modules, meaning modules stored locally in the same repository as the rest of the workflow code, in contrast to remote modules, which are stored in other (remote) repositories. For more information about remote modules, see the documentation.</p>"},{"location":"hello_nextflow/04_hello_modules/#2-create-a-module-for-sayhello","title":"2. Create a module for <code>sayHello()</code>","text":"<p>In its simplest form, turning an existing process into a module is little more than a copy-paste operation. We're going to create a file stub for the module, copy the relevant code over then delete it from the main workflow file.</p> <p>Then all we'll need to do is add an import statement so that Nextflow will know to pull in the relevant code at runtime.</p>"},{"location":"hello_nextflow/04_hello_modules/#21-create-a-file-stub-for-the-new-module","title":"2.1. Create a file stub for the new module","text":"<p>Let's create an empty file for the module called <code>sayHello.nf</code>.</p> <pre><code>touch modules/sayHello.nf\n</code></pre> <p>This gives us a place to put the process code.</p>"},{"location":"hello_nextflow/04_hello_modules/#22-move-the-sayhello-process-code-to-the-module-file","title":"2.2. Move the <code>sayHello</code> process code to the module file","text":"<p>Copy the whole process definition over from the workflow file to the module file, making sure to copy over the <code>#!/usr/bin/env nextflow</code> shebang too.</p> modules/sayHello.nf<pre><code>#!/usr/bin/env nextflow\n\n/*\n * Use echo to print 'Hello World!' to a file\n */\nprocess sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        val greeting\n\n    output:\n        path \"${greeting}-output.txt\"\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; '$greeting-output.txt'\n    \"\"\"\n}\n</code></pre> <p>Once that is done, delete the process definition from the workflow file, but make sure to leave the shebang in place.</p>"},{"location":"hello_nextflow/04_hello_modules/#23-add-an-import-declaration-before-the-workflow-block","title":"2.3. Add an import declaration before the workflow block","text":"<p>The syntax for importing a local module is fairly straightforward:</p> Syntax: Import declaration<pre><code>include { &lt;MODULE_NAME&gt; } from '&lt;path_to_module&gt;'\n</code></pre> <p>Let's insert that above the workflow block and fill it out appropriately.</p> AfterBefore hello-modules.nf<pre><code>// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\n\nworkflow {\n</code></pre> hello-modules.nf<pre><code>workflow {\n</code></pre>"},{"location":"hello_nextflow/04_hello_modules/#24-run-the-workflow-to-verify-that-it-does-the-same-thing-as-before","title":"2.4. Run the workflow to verify that it does the same thing as before","text":"<p>We're running the workflow with essentially the same code and inputs as before, so let's run with the <code>-resume</code> flag and see what happens.</p> <pre><code>nextflow run hello-modules.nf -resume\n</code></pre> <p>This runs quickly very quickly because everything is cached.</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-modules.nf` [romantic_poisson] DSL2 - revision: 96edfa9ad3\n\n[f6/cc0107] sayHello (1)       | 3 of 3, cached: 3 \u2714\n[3c/4058ba] convertToUpper (2) | 3 of 3, cached: 3 \u2714\n[1a/bc5901] collectGreetings   | 1 of 1, cached: 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>Nextflow recognized that it's still all the same work to be done, even if the code is split up into multiple files.</p>"},{"location":"hello_nextflow/04_hello_modules/#takeaway","title":"Takeaway","text":"<p>You know how to extract a process into a local module and you know doing this doesn't break the resumability of the workflow.</p>"},{"location":"hello_nextflow/04_hello_modules/#whats-next","title":"What's next?","text":"<p>Practice making more modules. Once you've done one, you can do a million modules... But let's just do two more for now.</p>"},{"location":"hello_nextflow/04_hello_modules/#3-modularize-the-converttoupper-process","title":"3. Modularize the <code>convertToUpper()</code> process","text":""},{"location":"hello_nextflow/04_hello_modules/#31-create-a-file-stub-for-the-new-module","title":"3.1. Create a file stub for the new module","text":"<p>Create an empty file for the module called <code>convertToUpper.nf</code>.</p> <pre><code>touch modules/convertToUpper.nf\n</code></pre>"},{"location":"hello_nextflow/04_hello_modules/#32-move-the-converttoupper-process-code-to-the-module-file","title":"3.2. Move the <code>convertToUpper</code> process code to the module file","text":"<p>Copy the whole process definition over from the workflow file to the module file, making sure to copy over the <code>#!/usr/bin/env nextflow</code> shebang too.</p> modules/convertToUpper.nf<pre><code>#!/usr/bin/env nextflow\n\n/*\n * Use a text replacement tool to convert the greeting to uppercase\n */\nprocess convertToUpper {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        path input_file\n\n    output:\n        path \"UPPER-${input_file}\"\n\n    script:\n    \"\"\"\n    cat '$input_file' | tr '[a-z]' '[A-Z]' &gt; 'UPPER-${input_file}'\n    \"\"\"\n}\n</code></pre> <p>Once that is done, delete the process definition from the workflow file, but make sure to leave the shebang in place.</p>"},{"location":"hello_nextflow/04_hello_modules/#33-add-an-import-declaration-before-the-workflow-block","title":"3.3. Add an import declaration before the workflow block","text":"<p>Insert the import declaration above the workflow block and fill it out appropriately.</p> AfterBefore hello-modules.nf<pre><code>// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\ninclude { convertToUpper } from './modules/convertToUpper.nf'\n\nworkflow {\n</code></pre> hello-modules.nf<pre><code>// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\n\nworkflow {\n</code></pre>"},{"location":"hello_nextflow/04_hello_modules/#34-run-the-workflow-to-verify-that-it-does-the-same-thing-as-before","title":"3.4. Run the workflow to verify that it does the same thing as before","text":"<p>Run this with the <code>-resume</code> flag.</p> <pre><code>nextflow run hello-modules.nf -resume\n</code></pre> <p>This should still produce the same output as previously.</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-modules.nf` [nauseous_heisenberg] DSL2 - revision: a04a9f2da0\n\n[c9/763d42] sayHello (3)       | 3 of 3, cached: 3 \u2714\n[60/bc6831] convertToUpper (3) | 3 of 3, cached: 3 \u2714\n[1a/bc5901] collectGreetings   | 1 of 1, cached: 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>Two done, one more to go!</p>"},{"location":"hello_nextflow/04_hello_modules/#4-modularize-the-collectgreetings-process","title":"4. Modularize the <code>collectGreetings()</code> process","text":""},{"location":"hello_nextflow/04_hello_modules/#41-create-a-file-stub-for-the-new-module","title":"4.1. Create a file stub for the new module","text":"<p>Create an empty file for the module called <code>collectGreetings.nf</code>.</p> <pre><code>touch modules/collectGreetings.nf\n</code></pre>"},{"location":"hello_nextflow/04_hello_modules/#42-move-the-collectgreetings-process-code-to-the-module-file","title":"4.2. Move the <code>collectGreetings</code> process code to the module file","text":"<p>Copy the whole process definition over from the workflow file to the module file, making sure to copy over the <code>#!/usr/bin/env nextflow</code> shebang too.</p> modules/collectGreetings.nf<pre><code>#!/usr/bin/env nextflow\n\n/*\n * Collect uppercase greetings into a single output file\n */\nprocess collectGreetings {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        path input_files\n        val batch_name\n\n    output:\n        path \"COLLECTED-${batch_name}-output.txt\" , emit: outfile\n        val count_greetings , emit: count\n\n    script:\n        count_greetings = input_files.size()\n    \"\"\"\n    cat ${input_files} &gt; 'COLLECTED-${batch_name}-output.txt'\n    \"\"\"\n}\n</code></pre> <p>Once that is done, delete the process definition from the workflow file, but make sure to leave the shebang in place.</p>"},{"location":"hello_nextflow/04_hello_modules/#43-add-an-import-declaration-before-the-workflow-block","title":"4.3. Add an import declaration before the workflow block","text":"<p>Insert the import declaration above the workflow block and fill it out appropriately.</p> AfterBefore hello-modules.nf<pre><code>// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\ninclude { convertToUpper } from './modules/convertToUpper.nf'\ninclude { collectGreetings } from './modules/collectGreetings.nf'\n\nworkflow {\n</code></pre> hello-modules.nf<pre><code>// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\ninclude { convertToUpper } from './modules/convertToUpper.nf'\n\nworkflow {\n</code></pre>"},{"location":"hello_nextflow/04_hello_modules/#44-run-the-workflow-to-verify-that-it-does-the-same-thing-as-before","title":"4.4. Run the workflow to verify that it does the same thing as before","text":"<p>Run this with the <code>-resume</code> flag.</p> <pre><code>nextflow run hello-modules.nf -resume\n</code></pre> <p>This should still produce the same output as previously.</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-modules.nf` [friendly_coulomb] DSL2 - revision: 7aa2b9bc0f\n\n[f6/cc0107] sayHello (1)       | 3 of 3, cached: 3 \u2714\n[3c/4058ba] convertToUpper (2) | 3 of 3, cached: 3 \u2714\n[1a/bc5901] collectGreetings   | 1 of 1, cached: 1 \u2714\nThere were 3 greetings in this batch\n</code></pre>"},{"location":"hello_nextflow/04_hello_modules/#takeaway_1","title":"Takeaway","text":"<p>You know how to modularize multiple processes in a workflow.</p> <p>Congratulations, you've done all this work and absolutely nothing has changed to how the pipeline works!</p> <p>Jokes aside, now your code is more modular, and if you decide to write another pipeline that calls on one of those processes, you just need to type one short import statement to use the relevant module. This is better than just copy-pasting the code, because if later you decide to improve the module, all your pipelines will inherit the improvements.</p>"},{"location":"hello_nextflow/04_hello_modules/#whats-next_1","title":"What's next?","text":"<p>Take a short break if you feel like it. When you're ready, move on to Part 5 to learn how to use containers to manage software dependencies more conveniently and reproducibly.</p>"},{"location":"hello_nextflow/05_hello_containers/","title":"Part 5: Hello Containers","text":"<p> See the whole playlist on the Nextflow YouTube channel.</p> <p> The video transcript is available here.</p> <p>In Parts 1-4 of this training course, you learned how to use the basic building blocks of Nextflow to assemble a simple workflow capable of processing some text, parallelizing execution if there were multiple inputs, and collecting the results for further processing.</p> <p>However, you were limited to basic UNIX tools available in your environment. Real-world tasks often require various tools and packages not included by default. Typically, you'd need to install these tools, manage their dependencies, and resolve any conflicts.</p> <p>That is all very tedious and annoying, so we're going to show you how to use containers to solve this problem much more conveniently.</p> <p>A container is a lightweight, standalone, executable unit of software created from a container image that includes everything needed to run an application including code, system libraries and settings.</p> <p>Note</p> <p>We'll be teaching this using the technology Docker, but Nextflow supports several other container technologies as well.</p>"},{"location":"hello_nextflow/05_hello_containers/#0-warmup-run-hello-containersnf","title":"0. Warmup: Run <code>hello-containers.nf</code>","text":"<p>We're going to use the workflow script <code>hello-containers.nf</code> as a starting point for the second section. It is equivalent to the script produced by working through Part 4 of this training course.</p> <p>Just to make sure everything is working, run the script once before making any changes:</p> <pre><code>nextflow run hello-containers.nf\n</code></pre> <p>This should produce the following output:</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-containers.nf` [tender_becquerel] DSL2 - revision: f7cat8e223\n\nexecutor &gt;  local (7)\n[bd/4bb541] sayHello (1)         [100%] 3 of 3 \u2714\n[85/b627e8] convertToUpper (3)   [100%] 3 of 3 \u2714\n[7d/f7961c] collectGreetings     [100%] 1 of 1 \u2714\n</code></pre> <p>As previously, you will find the output files in the <code>results</code> directory (specified by the <code>publishDir</code> directive).</p> Directory contents<pre><code>results\n\u251c\u2500\u2500 Bonjour-output.txt\n\u251c\u2500\u2500 COLLECTED-output.txt\n\u251c\u2500\u2500 COLLECTED-test-batch-output.txt\n\u251c\u2500\u2500 COLLECTED-trio-output.txt\n\u251c\u2500\u2500 Hello-output.txt\n\u251c\u2500\u2500 Hol\u00e0-output.txt\n\u251c\u2500\u2500 UPPER-Bonjour-output.txt\n\u251c\u2500\u2500 UPPER-Hello-output.txt\n\u2514\u2500\u2500 UPPER-Hol\u00e0-output.txt\n</code></pre> <p>Note</p> <p>There may also be a file named <code>output.txt</code> left over if you worked through Part 2 in the same environment.</p> <p>If that worked for you, you're ready to learn how to use containers.</p>"},{"location":"hello_nextflow/05_hello_containers/#1-use-a-container-manually","title":"1. Use a container 'manually'","text":"<p>What we want to do is add a step to our workflow that will use a container for execution.</p> <p>However, we are first going to go over some basic concepts and operations to solidify your understanding of what containers are before we start using them in Nextflow.</p>"},{"location":"hello_nextflow/05_hello_containers/#11-pull-the-container-image","title":"1.1. Pull the container image","text":"<p>To use a container, you usually download or \"pull\" a container image from a container registry, and then run the container image to create a container instance.</p> <p>The general syntax is as follows:</p> Syntax<pre><code>docker pull '&lt;container&gt;'\n</code></pre> <p>The <code>docker pull</code> part is the instruction to the container system to pull a container image from a repository.</p> <p>The <code>'&lt;container&gt;'</code> part is the URI address of the container image.</p> <p>As an example, let's pull a container image that contains cowpy, a python implementation of a tool called <code>cowsay</code> that generates ASCII art to display arbitrary text inputs in a fun way.</p> <p>There are various repositories where you can find published containers. We used the Seqera Containers service to generate this Docker container image from the <code>cowpy</code> Conda package: <code>'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'</code>.</p> <p>Run the complete pull command:</p> <pre><code>docker pull 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n</code></pre> <p>This gives you the following console output as the system downloads the image:</p> Output<pre><code>Unable to find image 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273' locally\n131d6a1b707a8e65: Pulling from library/cowpy\ndafa2b0c44d2: Pull complete\ndec6b097362e: Pull complete\nf88da01cff0b: Pull complete\n4f4fb700ef54: Pull complete\n92dc97a3ef36: Pull complete\n403f74b0f85e: Pull complete\n10b8c00c10a5: Pull complete\n17dc7ea432cc: Pull complete\nbb36d6c3110d: Pull complete\n0ea1a16bbe82: Pull complete\n030a47592a0a: Pull complete\n622dd7f15040: Pull complete\n895fb5d0f4df: Pull complete\nDigest: sha256:fa50498b32534d83e0a89bb21fec0c47cc03933ac95c6b6587df82aaa9d68db3\nStatus: Downloaded newer image for community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273\ncommunity.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273\n</code></pre> <p>Once the download is complete, you have a local copy of the container image.</p>"},{"location":"hello_nextflow/05_hello_containers/#12-use-the-container-to-run-cowpy-as-a-one-off-command","title":"1.2. Use the container to run <code>cowpy</code> as a one-off command","text":"<p>One very common way that people use containers is to run them directly, i.e. non-interactively. This is great for running one-off commands.</p> <p>The general syntax is as follows:</p> Syntax<pre><code>docker run --rm '&lt;container&gt;' [tool command]\n</code></pre> <p>The <code>docker run --rm '&lt;container&gt;'</code> part is the instruction to the container system to spin up a container instance from a container image and execute a command in it. The <code>--rm</code> flag tells the system to shut down the container instance after the command has completed.</p> <p>The <code>[tool command]</code> syntax depends on the tool you are using and how the container is set up. Let's just start with <code>cowpy</code>.</p> <p>Fully assembled, the container execution command looks like this:</p> <pre><code>docker run --rm 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273' cowpy\n</code></pre> <p>Run it to produce the following output:</p> Output<pre><code> ______________________________________________________\n&lt; Cowacter, eyes:default, tongue:False, thoughts:False &gt;\n ------------------------------------------------------\n     \\   ^__^\n      \\  (oo)\\_______\n         (__)\\       )\\/\\\n           ||----w |\n           ||     ||\n</code></pre> <p>The system spun up the container, ran the <code>cowpy</code> command with its parameters, sent the output to the console and finally, shut down the container instance.</p>"},{"location":"hello_nextflow/05_hello_containers/#13-use-the-container-to-run-cowpy-interactively","title":"1.3. Use the container to run <code>cowpy</code> interactively","text":"<p>You can also run a container interactively, which gives you a shell prompt inside the container and allows you to play with the command.</p>"},{"location":"hello_nextflow/05_hello_containers/#131-spin-up-the-container","title":"1.3.1. Spin up the container","text":"<p>To run interactively, we just add <code>-it</code> to the <code>docker run</code> command. Optionally, we can specify the shell we want to use inside the container by appending e.g. <code>/bin/bash</code> to the command.</p> <pre><code>docker run --rm -it 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273' /bin/bash\n</code></pre> <p>Notice that your prompt changes to something like <code>(base) root@b645838b3314:/tmp#</code>, which indicates that you are now inside the container.</p> <p>You can verify this by running <code>ls</code> to list directory contents:</p> <pre><code>ls /\n</code></pre> Output<pre><code>bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\n</code></pre> <p>You can see that the filesystem inside the container is different from the filesystem on your host system.</p> <p>Note</p> <p>When you run a container, it is isolated from the host system by default. This means that the container can't access any files on the host system unless you explicitly allow it to do so.</p> <p>You will learn how to do that in a minute.</p>"},{"location":"hello_nextflow/05_hello_containers/#132-run-the-desired-tool-commands","title":"1.3.2. Run the desired tool command(s)","text":"<p>Now that you are inside the container, you can run the <code>cowpy</code> command directly and give it some parameters. For example, the tool documentation says we can change the character ('cowacter') with <code>-c</code>.</p> <pre><code>cowpy \"Hello Containers\" -c tux\n</code></pre> <p>Now the output shows the Linux penguin, Tux, instead of the default cow, because we specified the <code>-c tux</code> parameter.</p> Output<pre><code> __________________\n&lt; Hello Containers &gt;\n ------------------\n   \\\n    \\\n        .--.\n       |o_o |\n       |:_/ |\n      //   \\ \\\n     (|     | )\n    /'\\_   _/`\\\n    \\___)=(___/\n</code></pre> <p>Because you're inside the container, you can run the cowpy command as many times as you like, varying the input parameters, without having to bother with Docker commands.</p> <p>Tip</p> <p>Use the '-c' flag to pick a different character, including: <code>beavis</code>, <code>cheese</code>, <code>daemon</code>, <code>dragonandcow</code>, <code>ghostbusters</code>, <code>kitty</code>, <code>moose</code>, <code>milk</code>, <code>stegosaurus</code>, <code>turkey</code>, <code>turtle</code>, <code>tux</code></p> <p>This is neat. What would be even neater is if we could feed our <code>greetings.csv</code> as input into this. But since we don't have access to the filesystem, we can't.</p> <p>Let's fix that.</p>"},{"location":"hello_nextflow/05_hello_containers/#133-exit-the-container","title":"1.3.3. Exit the container","text":"<p>To exit the container, you can type <code>exit</code> at the prompt or use the Ctrl+D keyboard shortcut.</p> <pre><code>exit\n</code></pre> <p>Your prompt should now be back to what it was before you started the container.</p>"},{"location":"hello_nextflow/05_hello_containers/#134-mount-data-into-the-container","title":"1.3.4. Mount data into the container","text":"<p>When you run a container, it is isolated from the host system by default. This means that the container can't access any files on the host system unless you explicitly allow it to do so.</p> <p>One way to do this is to mount a volume from the host system into the container using the following syntax:</p> Syntax<pre><code>-v &lt;outside_path&gt;:&lt;inside_path&gt;\n</code></pre> <p>In our case <code>&lt;outside_path&gt;</code> will be the current working directory, so we can just use a dot (<code>.</code>), and <code>&lt;inside_path&gt;</code> is just a name we make up; let's call it <code>/data</code>.</p> <p>To mount a volume, we replace the paths and add the volume mounting argument to the docker run command as follows:</p> <pre><code>docker run --rm -it -v .:/data 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273' /bin/bash\n</code></pre> <p>This mounts the current working directory as a volume that will be accessible under <code>/data</code> inside the container.</p> <p>You can check that it works by listing the contents of <code>/data</code>:</p> <pre><code>ls /data\n</code></pre> <p>Depending on what part of this training you've done before, the output below my look slightly different.</p> Output<pre><code>greetings.csv      hello-config.nf      hello-modules.nf   hello-world.nf  nextflow.config  solutions         work\nhello-channels.nf  hello-containers.nf  hello-workflow.nf  modules         results          test-params.json\n</code></pre> <p>You can now see the contents of the <code>data</code> directory from inside the container, including the <code>greetings.csv</code> file.</p> <p>This effectively established a tunnel through the container wall that you can use to access that part of your filesystem.</p>"},{"location":"hello_nextflow/05_hello_containers/#135-use-the-mounted-data","title":"1.3.5. Use the mounted data","text":"<p>Now that we have mounted the <code>data</code> directory into the container, we can use the <code>cowpy</code> command to display the contents of the <code>greetings.csv</code> file.</p> <p>To do this, we'll use <code>cat /data/greetings.csv |</code> to pipe the contents of the CSV file into the <code>cowpy</code> command.</p> <pre><code>cat /data/greetings.csv | cowpy -c turkey\n</code></pre> <p>This produces the desired ASCII art of a turkey rattling off our example greetings:</p> Output<pre><code> _________\n/ Hello   \\\n| Bonjour |\n\\ Hol\u00e0    /\n ---------\n  \\                                  ,+*^^*+___+++_\n   \\                           ,*^^^^              )\n    \\                       _+*                     ^**+_\n     \\                    +^       _ _++*+_+++_,         )\n              _+^^*+_    (     ,+*^ ^          \\+_        )\n             {       )  (    ,(    ,_+--+--,      ^)      ^\\\n            { (\\@)    } f   ,(  ,+-^ __*_*_  ^^\\_   ^\\       )\n           {:;-/    (_+*-+^^^^^+*+*&lt;_ _++_)_    )    )      /\n          ( /  (    (        ,___    ^*+_+* )   &lt;    &lt;      \\\n           U _/     )    *--&lt;  ) ^\\-----++__)   )    )       )\n            (      )  _(^)^^))  )  )\\^^^^^))^*+/    /       /\n          (      /  (_))_^)) )  )  ))^^^^^))^^^)__/     +^^\n         (     ,/    (^))^))  )  ) ))^^^^^^^))^^)       _)\n          *+__+*       (_))^)  ) ) ))^^^^^^))^^^^^)____*^\n          \\             \\_)^)_)) ))^^^^^^^^^^))^^^^)\n           (_             ^\\__^^^^^^^^^^^^))^^^^^^^)\n             ^\\___            ^\\__^^^^^^))^^^^^^^^)\\\\\n                  ^^^^^\\uuu/^^\\uuu/^^^^\\^\\^\\^\\^\\^\\^\\^\\\n                     ___) &gt;____) &gt;___   ^\\_\\_\\_\\_\\_\\_\\)\n                    ^^^//\\\\_^^//\\\\_^       ^(\\_\\_\\_\\)\n                      ^^^ ^^ ^^^ ^\n</code></pre> <p>Feel free to play around with this command. When you're done, exit the container as previously:</p> <pre><code>exit\n</code></pre> <p>You will find yourself back in your normal shell.</p>"},{"location":"hello_nextflow/05_hello_containers/#takeaway","title":"Takeaway","text":"<p>You know how to pull a container and run it either as a one-off or interactively. You also know how to make your data accessible from within your container, which lets you try any tool you're interested in on real data without having to install any software on your system.</p>"},{"location":"hello_nextflow/05_hello_containers/#whats-next","title":"What's next?","text":"<p>Learn how to use containers for the execution of Nextflow processes.</p>"},{"location":"hello_nextflow/05_hello_containers/#2-use-containers-in-nextflow","title":"2. Use containers in Nextflow","text":"<p>Nextflow has built-in support for running processes inside containers to let you run tools you don't have installed in your compute environment. This means that you can use any container image you like to run your processes, and Nextflow will take care of pulling the image, mounting the data, and running the process inside it.</p> <p>To demonstrate this, we are going to add a <code>cowpy</code> step to the pipeline we've been developing, after the <code>collectGreetings</code> step.</p>"},{"location":"hello_nextflow/05_hello_containers/#21-write-a-cowpy-module","title":"2.1. Write a <code>cowpy</code> module","text":""},{"location":"hello_nextflow/05_hello_containers/#211-create-a-file-stub-for-the-new-module","title":"2.1.1. Create a file stub for the new module","text":"<p>Create an empty file for the module called <code>cowpy.nf</code>.</p> <pre><code>touch modules/cowpy.nf\n</code></pre> <p>This gives us a place to put the process code.</p>"},{"location":"hello_nextflow/05_hello_containers/#212-copy-the-cowpy-process-code-in-the-module-file","title":"2.1.2. Copy the <code>cowpy</code> process code in the module file","text":"<p>We can model our <code>cowpy</code> process on the other processes we've written previously.</p> modules/cowpy.nf<pre><code>#!/usr/bin/env nextflow\n\n// Generate ASCII art with cowpy\nprocess cowpy {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        path input_file\n        val character\n\n    output:\n        path \"cowpy-${input_file}\"\n\n    script:\n    \"\"\"\n    cat $input_file | cowpy -c \"$character\" &gt; cowpy-${input_file}\n    \"\"\"\n\n}\n</code></pre> <p>The output will be a new text file containing the ASCII art generated by the <code>cowpy</code> tool.</p>"},{"location":"hello_nextflow/05_hello_containers/#22-add-cowpy-to-the-workflow","title":"2.2. Add cowpy to the workflow","text":"<p>Now we need to import the module and call the process.</p>"},{"location":"hello_nextflow/05_hello_containers/#221-import-the-cowpy-process-into-hello-containersnf","title":"2.2.1. Import the <code>cowpy</code> process into <code>hello-containers.nf</code>","text":"<p>Insert the import declaration above the workflow block and fill it out appropriately.</p> AfterBefore hello-containers.nf<pre><code>// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\ninclude { convertToUpper } from './modules/convertToUpper.nf'\ninclude { collectGreetings } from './modules/collectGreetings.nf'\ninclude { cowpy } from './modules/cowpy.nf'\n\nworkflow {\n</code></pre> hello-containers.nf<pre><code>// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\ninclude { convertToUpper } from './modules/convertToUpper.nf'\ninclude { collectGreetings } from './modules/collectGreetings.nf'\n\nworkflow {\n</code></pre>"},{"location":"hello_nextflow/05_hello_containers/#222-add-a-call-to-the-cowpy-process-in-the-workflow","title":"2.2.2. Add a call to the <code>cowpy</code> process in the workflow","text":"<p>Let's connect the <code>cowpy()</code> process to the output of the <code>collectGreetings()</code> process, which as you may recall produces two outputs:</p> <ul> <li><code>collectGreetings.out.outfile</code> contains the output file</li> <li><code>collectGreetings.out.count</code> contains the count of greetings per batch</li> </ul> <p>In the workflow block, make the following code change:</p> AfterBefore hello-containers.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect(), params.batch)\n\n    // emit a message about the size of the batch\n    collectGreetings.out.count.view{ num_greetings -&gt; \"There were $num_greetings greetings in this batch\" }\n\n    // generate ASCII art of the greetings with cowpy\n    cowpy(collectGreetings.out.outfile, params.character)\n</code></pre> hello-containers.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect(), params.batch)\n\n    // emit a message about the size of the batch\n    collectGreetings.out.count.view{ num_greetings -&gt; \"There were $num_greetings greetings in this batch\" }\n</code></pre> <p>Notice that we include a new CLI parameter, <code>params.character</code>, in order to specify which character we want to have say the greetings.</p>"},{"location":"hello_nextflow/05_hello_containers/#223-set-a-default-value-for-paramscharacter","title":"2.2.3. Set a default value for <code>params.character</code>","text":"<p>We like to be lazy and skip typing parameters in our command lines.</p> AfterBefore hello-containers.nf<pre><code>/*\n * Pipeline parameters\n */\nparams.greeting = 'greetings.csv'\nparams.batch = 'test-batch'\nparams.character = 'turkey'\n</code></pre> hello-containers.nf<pre><code>/*\n * Pipeline parameters\n */\nparams.greeting = 'greetings.csv'\nparams.batch = 'test-batch'\n</code></pre> <p>That should be all we need to make this work.</p>"},{"location":"hello_nextflow/05_hello_containers/#224-run-the-workflow-to-verify-that-it-works","title":"2.2.4. Run the workflow to verify that it works","text":"<p>Run this with the <code>-resume</code> flag.</p> <pre><code>nextflow run hello-containers.nf -resume\n</code></pre> <p>Oh no, there's an error!</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-containers.nf` [special_lovelace] DSL2 - revision: 028a841db1\n\nexecutor &gt;  local (1)\n[f6/cc0107] sayHello (1)       | 3 of 3, cached: 3 \u2714\n[2c/67a06b] convertToUpper (3) | 3 of 3, cached: 3 \u2714\n[1a/bc5901] collectGreetings   | 1 of 1, cached: 1 \u2714\n[b2/488871] cowpy             | 0 of 1\nThere were 3 greetings in this batch\nERROR ~ Error executing process &gt; 'cowpy'\n\nCaused by:\n  Process `cowpy` terminated with an error exit status (127)\n\nCommand executed:\n\n  cat COLLECTED-test-batch-output.txt | cowpy -c \"turkey\" &gt; cowpy-COLLECTED-test-batch-output.txt\n\nCommand exit status:\n  127\n\nCommand output:\n  (empty)\n\nCommand error:\n  .command.sh: line 2: cowpy: command not found\n\n(trimmed output)\n</code></pre> <p>This error code, <code>error exit status (127)</code> means the executable we asked for was not found.</p> <p>Of course, since we're calling the <code>cowpy</code> tool but we haven't actually specified a container yet.</p>"},{"location":"hello_nextflow/05_hello_containers/#23-use-a-container-to-run-it","title":"2.3. Use a container to run it","text":"<p>We need to specify a container and tell Nextflow to use it for the <code>cowpy()</code> process.</p>"},{"location":"hello_nextflow/05_hello_containers/#231-specify-a-container-for-the-cowpy-process-to-use","title":"2.3.1. Specify a container for the <code>cowpy</code> process to use","text":"<p>Edit the <code>cowpy.nf</code> module to add the <code>container</code> directive to the process definition as follows:</p> AfterBefore modules/cowpy.nf<pre><code>process cowpy {\n\n    publishDir 'containers/results', mode: 'copy'\n    container 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n</code></pre> modules/cowpy.nf<pre><code>process cowpy {\n\n    publishDir 'containers/results', mode: 'copy'\n</code></pre> <p>This tells Nextflow that if the use of Docker is enabled, it should use the container image specified here to execute the process.</p>"},{"location":"hello_nextflow/05_hello_containers/#232-enable-use-of-docker-via-the-nextflowconfig-file","title":"2.3.2. Enable use of Docker via the <code>nextflow.config</code> file","text":"<p>Here we are going to slightly anticipate the topic of the next and last part of this course (Part 6), which covers configuration.</p> <p>One of the main ways Nextflow offers for configuring workflow execution is to use a <code>nextflow.config</code> file. When such a file is present in the current directory, Nextflow will automatically load it in and apply any configuration it contains.</p> <p>We provided a <code>nextflow.config</code> file with a single line of code that disables Docker: <code>docker.enabled = false</code>.</p> <p>Now, let's switch that to <code>true</code> to enable Docker:</p> AfterBefore nextflow.config<pre><code>docker.enabled = true\n</code></pre> nextflow.config<pre><code>docker.enabled = false\n</code></pre> <p>Note</p> <p>It is possible to enable Docker execution from the command-line, on a per-run basis, using the <code>-with-docker &lt;container&gt;</code> parameter. However, that only allows us to specify one container for the entire workflow, whereas the approach we just showed you allows us to specify a different container per process. This is better for modularity, code maintenance and reproducibility.</p>"},{"location":"hello_nextflow/05_hello_containers/#233-run-the-workflow-with-docker-enabled","title":"2.3.3. Run the workflow with Docker enabled","text":"<p>Run the workflow with the <code>-resume</code> flag:</p> <pre><code>nextflow run hello-containers.nf -resume\n</code></pre> <p>This time it does indeed work.</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-containers.nf` [elegant_brattain] DSL2 - revision: 028a841db1\n\nexecutor &gt;  local (1)\n[95/fa0bac] sayHello (3)       | 3 of 3, cached: 3 \u2714\n[92/32533f] convertToUpper (3) | 3 of 3, cached: 3 \u2714\n[aa/e697a2] collectGreetings   | 1 of 1, cached: 1 \u2714\n[7f/caf718] cowpy              | 1 of 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>You can find the cowpy'ed output in the <code>results</code> directory.</p> results/cowpy-COLLECTED-test-batch-output.txt<pre><code> _________\n/ HOL\u00e0    \\\n| HELLO   |\n\\ BONJOUR /\n ---------\n  \\                                  ,+*^^*+___+++_\n   \\                           ,*^^^^              )\n    \\                       _+*                     ^**+_\n     \\                    +^       _ _++*+_+++_,         )\n              _+^^*+_    (     ,+*^ ^          \\+_        )\n             {       )  (    ,(    ,_+--+--,      ^)      ^\\\n            { (\\@)    } f   ,(  ,+-^ __*_*_  ^^\\_   ^\\       )\n           {:;-/    (_+*-+^^^^^+*+*&lt;_ _++_)_    )    )      /\n          ( /  (    (        ,___    ^*+_+* )   &lt;    &lt;      \\\n           U _/     )    *--&lt;  ) ^\\-----++__)   )    )       )\n            (      )  _(^)^^))  )  )\\^^^^^))^*+/    /       /\n          (      /  (_))_^)) )  )  ))^^^^^))^^^)__/     +^^\n         (     ,/    (^))^))  )  ) ))^^^^^^^))^^)       _)\n          *+__+*       (_))^)  ) ) ))^^^^^^))^^^^^)____*^\n          \\             \\_)^)_)) ))^^^^^^^^^^))^^^^)\n           (_             ^\\__^^^^^^^^^^^^))^^^^^^^)\n             ^\\___            ^\\__^^^^^^))^^^^^^^^)\\\\\n                  ^^^^^\\uuu/^^\\uuu/^^^^\\^\\^\\^\\^\\^\\^\\^\\\n                     ___) &gt;____) &gt;___   ^\\_\\_\\_\\_\\_\\_\\)\n                    ^^^//\\\\_^^//\\\\_^       ^(\\_\\_\\_\\)\n                      ^^^ ^^ ^^^ ^\n</code></pre> <p>You see that the character is saying all the greetings, just as it did when we ran the <code>cowpy</code> command on the <code>greetings.csv</code> file from inside the container.</p>"},{"location":"hello_nextflow/05_hello_containers/#234-inspect-how-nextflow-launched-the-containerized-task","title":"2.3.4. Inspect how Nextflow launched the containerized task","text":"<p>Let's take a look at the work subdirectory for one of the <code>cowpy</code> process calls to get a bit more insight on how Nextflow works with containers under the hood.</p> <p>Check the output from your <code>nextflow run</code> command to find the path to the work subdirectory for the <code>cowpy</code> process. Looking at what we got for the run shown above, the console log line for the <code>cowpy</code> process starts with <code>[7f/caf718]</code>. That corresponds to the following truncated directory path: <code>work/7f/caf718</code>. In it, you will find the <code>.command.run</code> file that contains all the commands Nextflow ran on your behalf in the course of executing the pipeline.</p> <p>Open the <code>.command.run</code> file and search for <code>nxf_launch</code>; you should see something like this:</p> <pre><code>nxf_launch() {\n    docker run -i --cpu-shares 1024 -e \"NXF_TASK_WORKDIR\" -v /workspaces/training/hello-nextflow/work:/workspaces/training/hello-nextflow/work -w \"$NXF_TASK_WORKDIR\" --name $NXF_BOXID community.wave.seqera.io/library/pip_cowpy:131d6a1b707a8e65 /bin/bash -ue /workspaces/training/hello-nextflow/work/7f/caf7189fca6c56ba627b75749edcb3/.command.sh\n}\n</code></pre> <p>As you can see, Nextflow is using the <code>docker run</code> command to launch the process call. It also mounts the corresponding work subdirectory into the container, sets the working directory inside the container accordingly, and runs our templated bash script in the <code>.command.sh</code> file.</p> <p>All the hard work we had to do manually in the previous section is done for us by Nextflow!</p>"},{"location":"hello_nextflow/05_hello_containers/#takeaway_1","title":"Takeaway","text":"<p>You know how to use containers in Nextflow to run processes.</p>"},{"location":"hello_nextflow/05_hello_containers/#whats-next_1","title":"What's next?","text":"<p>Take a break! When you're ready, move on to Part 6 to learn how to configure the execution of your pipeline to fit your infrastructure as well as manage configuration of inputs and parameters. It's the very last part and then you're done!</p>"},{"location":"hello_nextflow/06_hello_config/","title":"Part 6: Hello Config","text":"<p> See the whole playlist on the Nextflow YouTube channel.</p> <p> The video transcript is available here.</p> <p>This section will explore how to set up and manage the configuration of your Nextflow pipeline so that you'll be able to customize its behavior, adapt it to different environments, and optimize resource usage without altering a single line of the workflow code itself.</p> <p>There are multiple ways to do this; here we are going to use the simplest and most common configuration file mechanism, the <code>nextflow.config</code> file. Whenever there is a file named <code>nextflow.config</code> in the current directory, Nextflow will automatically load configuration from it.</p> <p>Note</p> <p>Anything you put into the <code>nextflow.config</code> can be overridden at runtime by providing the relevant process directives or parameters and values on the command line, or by importing another configuration file, according to the order of precedence described here.</p> <p>In this part of the training, we're going to use the <code>nextflow.config</code> file to demonstrate essential components of Nextflow configuration such as process directives, executors, profiles, and parameter files.</p> <p>By learning to utilize these configuration options effectively, you can enhance the flexibility, scalability, and performance of your pipelines.</p>"},{"location":"hello_nextflow/06_hello_config/#0-warmup-check-that-docker-is-enabled-and-run-the-hello-config-workflow","title":"0. Warmup: Check that Docker is enabled and run the Hello Config workflow","text":"<p>First, a quick check. There is a <code>nextflow.config</code> file in the current directory that contains the line <code>docker.enabled = &lt;setting&gt;</code>, where <code>&lt;setting&gt;</code> is either <code>true</code> or <code>false</code> depending on whether or not you've worked through Part 5 of this course in the same environment.</p> <p>If it is set to <code>true</code>, you don't need to do anything.</p> <p>If it is set to <code>false</code>, switch it to <code>true</code> now.</p> nextflow.config<pre><code>docker.enabled = true\n</code></pre> <p>Once you've done that, verify that the initial workflow runs properly:</p> <pre><code>nextflow run hello-config.nf\n</code></pre> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-config.nf` [reverent_heisenberg] DSL2 - revision: 028a841db1\n\nexecutor &gt;  local (8)\n[7f/0da515] sayHello (1)       | 3 of 3 \u2714\n[f3/42f5a5] convertToUpper (3) | 3 of 3 \u2714\n[04/fe90e4] collectGreetings   | 1 of 1 \u2714\n[81/4f5fa9] cowpy              | 1 of 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>If everything works, you're ready to learn how to modify basic configuration properties to adapt to your compute environment's requirements.</p>"},{"location":"hello_nextflow/06_hello_config/#1-determine-what-software-packaging-technology-to-use","title":"1. Determine what software packaging technology to use","text":"<p>The first step toward adapting your workflow configuration to your compute environment is specifying where the software packages that will get run in each step are going to be coming from. Are they already installed in the local compute environment? Do we need to retrieve images and run them via a container system? Or do we need to retrieve Conda packages and build a local Conda environment?</p> <p>In the very first part of this training course (Parts 1-4) we just used locally installed software in our workflow. Then in Part 5, we introduced Docker containers and the <code>nextflow.config</code> file, which we used to enable the use of Docker containers.</p> <p>In the warmup to this section, you checked that Docker was enabled in <code>nextflow.config</code> file and ran the workflow, which used a Docker container to execute the <code>cowpy()</code> process.</p> <p>Note</p> <p>If that doesn't sound familiar, you should probably go back and work through Part 5 before continuing.</p> <p>Now let's see how we can configure an alternative software packaging option via the <code>nextflow.config</code> file.</p>"},{"location":"hello_nextflow/06_hello_config/#11-disable-docker-and-enable-conda-in-the-config-file","title":"1.1. Disable Docker and enable Conda in the config file","text":"<p>Let's pretend we're working on an HPC cluster and the admin doesn't allow the use of Docker for security reasons.</p> <p>Fortunately for us, Nextflow supports multiple other container technologies such as including Singularity (which is more widely used on HPC), and software package managers such as Conda.</p> <p>We can change our configuration file to use Conda instead of Docker. To do so, we switch the value of <code>docker.enabled</code> to <code>false</code>, and add a directive enabling the use of Conda:</p> AfterBefore nextflow.config<pre><code>docker.enabled = false\nconda.enabled = true\n</code></pre> nextflow.config<pre><code>docker.enabled = true\n</code></pre> <p>This will allow Nextflow to create and utilize Conda environments for processes that have Conda packages specified. Which means we now need to add one of those to our <code>cowpy</code> process!</p>"},{"location":"hello_nextflow/06_hello_config/#12-specify-a-conda-package-in-the-process-definition","title":"1.2. Specify a Conda package in the process definition","text":"<p>We've already retrieved the URI for a Conda package containing the <code>cowpy</code> tool: <code>conda-forge::cowpy==1.1.5</code></p> <p>Note</p> <p>There are a few different ways to get the URI for a given conda package. We recommend using the Seqera Containers search query, which will give you a URI that you can copy and paste, even if you're not planning to create a container from it.</p> <p>Now we add the URI to the <code>cowpy</code> process definition using the <code>conda</code> directive:</p> AfterBefore modules/cowpy.nf<pre><code>process cowpy {\n\n    container 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n    conda 'conda-forge::cowpy==1.1.5'\n\n    publishDir 'results', mode: 'copy'\n</code></pre> modules/cowpy.nf<pre><code>process cowpy {\n\n    container 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n\n    publishDir 'results', mode: 'copy'\n</code></pre> <p>To be clear, we're not replacing the <code>docker</code> directive, we're adding an alternative option.</p>"},{"location":"hello_nextflow/06_hello_config/#13-run-the-workflow-to-verify-that-it-can-use-conda","title":"1.3. Run the workflow to verify that it can use Conda","text":"<p>Let's try it out.</p> <pre><code>nextflow run hello-config.nf\n</code></pre> <p>This should work without issue.</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-config.nf` [trusting_lovelace] DSL2 - revision: 028a841db1\n\nexecutor &gt;  local (8)\n[ee/4ca1f2] sayHello (3)       | 3 of 3 \u2714\n[20/2596a7] convertToUpper (1) | 3 of 3 \u2714\n[b3/e15de5] collectGreetings   | 1 of 1 \u2714\n[c5/af5f88] cowpy              | 1 of 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>Behind the scenes, Nextflow has retrieved the Conda packages and created the environment, which normally takes a bit of work; so it's nice that we don't have to do any of that ourselves!</p> <p>Note</p> <p>This runs quickly because the <code>cowpy</code> package is quite small, but if you're working with large packages, it may take a bit longer than usual the first time, and you might see the console output stay 'stuck' for a minute or so before completing. This is normal and is due to the extra work Nextflow does the first time you use a new package.</p> <p>From our standpoint, it looks like it works exactly the same as running with Docker, even though on the backend the mechanics are a bit different.</p> <p>This means we're all set to run with Conda environments if needed.</p> <p>Note</p> <p>Since these directives are assigned per process, it is possible 'mix and match', i.e. configure some of the processes in your workflow to run with Docker and others with Conda, for example, if the compute infrastructure you are using supports both. In that case, you would enable both Docker and Conda in your configuration file. If both are available for a given process, Nextflow will prioritize containers.</p> <p>And as noted earlier, Nextflow supports multiple other software packaging and container technologies, so you are not limited to just those two.</p>"},{"location":"hello_nextflow/06_hello_config/#takeaway","title":"Takeaway","text":"<p>You know how to configure which software package each process should use, and how to switch between technologies.</p>"},{"location":"hello_nextflow/06_hello_config/#whats-next","title":"What's next?","text":"<p>Learn how to change the executor used by Nextflow to actually do the work.</p>"},{"location":"hello_nextflow/06_hello_config/#2-allocate-compute-resources-with-process-directives","title":"2. Allocate compute resources with process directives","text":"<p>Most high-performance computing platforms allow (and sometimes require) that you specify certain resource allocation parameters such as number of CPUs and memory.</p> <p>By default, Nextflow will use a single CPU and 2GB of memory for each process. The corresponding process directives are called <code>cpus</code> and <code>memory</code>, so the following configuration is implied:</p> Built-in configuration<pre><code>process {\n    cpus = 1\n    memory = 2.GB\n}\n</code></pre> <p>You can modify these values, either for all processes or for specific named processes, using additional process directives in your configuration file. Nextflow will translate them into the appropriate instructions for the chosen executor.</p> <p>But how do you know what values to use?</p>"},{"location":"hello_nextflow/06_hello_config/#21-run-the-workflow-to-generate-a-resource-utilization-report","title":"2.1. Run the workflow to generate a resource utilization report","text":"<p>If you don't know up front how much CPU and memory your processes are likely to need, you can do some resource profiling, meaning you run the workflow with some default allocations, record how much each process used, and from there, estimate how to adjust the base allocations.</p> <p>Conveniently, Nextflow includes built-in tools for doing this, and will happily generate a report for you on request.</p> <p>To do so, add <code>-with-report &lt;filename&gt;.html</code> to your command line.</p> <pre><code>nextflow run hello-config.nf -with-report report-config-1.html\n</code></pre> <p>The report is an html file, which you can download and open in your browser. You can also right click it in the file explorer on the left and click on <code>Show preview</code> in order to view it in the training environment.</p> <p>Take a few minutes to look through the report and see if you can identify some opportunities for adjusting resources. Make sure to click on the tabs that show the utilization results as a percentage of what was allocated. There is some documentation describing all the available features.</p>"},{"location":"hello_nextflow/06_hello_config/#22-set-resource-allocations-for-all-processes","title":"2.2. Set resource allocations for all processes","text":"<p>The profiling shows that the processes in our training workflow are very lightweight, so let's reduce the default memory allocation to 1GB per process.</p> <p>Add the following to your <code>nextflow.config</code> file:</p> nextflow.config<pre><code>process {\n    memory = 1.GB\n}\n</code></pre>"},{"location":"hello_nextflow/06_hello_config/#23-set-resource-allocations-for-an-individual-process","title":"2.3. Set resource allocations for an individual process","text":"<p>At the same time, we're going to pretend that the <code>cowpy</code> process requires more resources than the others, just so we can demonstrate how to adjust allocations for an individual process.</p> AfterBefore nextflow.config<pre><code>process {\n    memory = 1.GB\n    withName: 'cowpy' {\n        memory = 2.GB\n        cpus = 2\n    }\n}\n</code></pre> nextflow.config<pre><code>process {\n    memory = 1.GB\n}\n</code></pre> <p>With this configuration, all processes will request 1GB of memory and a single CPU (the implied default), except the <code>cowpy</code> process, which will request 2GB and 2 CPUs.</p> <p>Note</p> <p>If you have a machine with few CPUs and you allocate a high number per process, you might see process calls getting queued behind each other. This is because Nextflow ensures we don't request more CPUs than are available.</p>"},{"location":"hello_nextflow/06_hello_config/#24-run-the-workflow-with-the-modified-configuration","title":"2.4. Run the workflow with the modified configuration","text":"<p>Let's try that out, supplying a different filename for the profiling report so we can compare performance before and after the configuration changes.</p> <pre><code>nextflow run hello-config.nf -with-report report-config-2.html\n</code></pre> <p>You will probably not notice any real difference since this is such a small workload, but this is the approach you would use to analyze the performance and resource requirements of a real-world workflow.</p> <p>It is very useful when your processes have different resource requirements. It empowers you to right-size the resource allocations you set up for each process based on actual data, not guesswork.</p> <p>Note</p> <p>This is just a tiny taster of what you can do to optimize your use of resources. Nextflow itself has some really neat dynamic retry logic built in to retry jobs that fail due to resource limitations. Additionally, the Seqera Platform offers AI-driven tooling for optimizing your resource allocations automatically as well.</p> <p>We'll cover both of those approaches in an upcoming part of this training course.</p>"},{"location":"hello_nextflow/06_hello_config/#25-add-resource-limits","title":"2.5. Add resource limits","text":"<p>Depending on what computing executor and compute infrastructure you're using, there may be some constraints on what you can (or must) allocate. For example, your cluster may require you to stay within certain limits.</p> <p>You can use the <code>resourceLimits</code> directive to set the relevant limitations. The syntax looks like this when it's by itself in a process block:</p> Syntax example<pre><code>process {\n    resourceLimits = [\n        memory: 750.GB,\n        cpus: 200,\n        time: 30.d\n    ]\n}\n</code></pre> <p>Nextflow will translate these values into the appropriate instructions depending on the executor that you specified.</p> <p>We're not going to run this, since we don't have access to relevant infrastructure in the training environment. However, if you were to try running the workflow with resource allocations that exceed these limits, then look up the <code>sbatch</code> command in the <code>.command.run</code> script file, you would see that the requests that actually get sent to the executor are capped at the values specified by <code>resourceLimits</code>.</p> <p>Note</p> <p>The nf-core project has compiled a collection of configuration files shared by various institutions around the world, covering a wide range of HPC and cloud executors.</p> <p>Those shared configs are valuable both for people who work there and can therefore just utilize their institution's configuration out of the box, and as a model for people who are looking to develop a configuration for their own infrastructure.</p>"},{"location":"hello_nextflow/06_hello_config/#takeaway_1","title":"Takeaway","text":"<p>You know how to generate a profiling report to assess resource utilization and how to modify resource allocations for all processes and/or for individual processes, as well as set resource limitations for running on HPC.</p>"},{"location":"hello_nextflow/06_hello_config/#whats-next_1","title":"What's next?","text":"<p>Learn to use a parameter file to store workflow parameters.</p>"},{"location":"hello_nextflow/06_hello_config/#3-use-a-parameter-file-to-store-workflow-parameters","title":"3. Use a parameter file to store workflow parameters","text":"<p>So far we've been looking at configuration from the technical point of view of the compute infrastructure. Now let's consider another aspect of workflow configuration that is very important for reproducibility: the configuration of the workflow parameters.</p> <p>Currently, our workflow is set up to accept several parameter values via the command-line, with default values set in the workflow script itself. This is fine for a simple workflow with very few parameters that need to be set for a given run. However, many real-world workflows will have many more parameters that may be run-specific, and putting all of them in the command line would be tedious and error-prone.</p> <p>Nextflow allows us to specify parameters via a parameter file in JSON format, which makes it very convenient to manage and distribute alternative sets of default values, for example, as well as run-specific parameter values.</p> <p>We provide an example parameter file in the current directory, called <code>test-params.json</code>:</p> test-params.json<pre><code>{\n  \"greeting\": \"greetings.csv\",\n  \"batch\": \"Trio\",\n  \"character\": \"turkey\"\n}\n</code></pre> <p>This parameter file contains a key-value pair for each of the inputs our workflow expects.</p>"},{"location":"hello_nextflow/06_hello_config/#31-run-the-workflow-using-a-parameter-file","title":"3.1. Run the workflow using a parameter file","text":"<p>To run the workflow with this parameter file, simply add <code>-params-file &lt;filename&gt;</code> to the base command.</p> <pre><code>nextflow run hello-config.nf -params-file test-params.json\n</code></pre> <p>It works! And as expected, this produces the same outputs as previously.</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-config.nf` [disturbed_sammet] DSL2 - revision: ede9037d02\n\nexecutor &gt;  local (8)\n[f0/35723c] sayHello (2)       | 3 of 3 \u2714\n[40/3efd1a] convertToUpper (3) | 3 of 3 \u2714\n[17/e97d32] collectGreetings   | 1 of 1 \u2714\n[98/c6b57b] cowpy              | 1 of 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>This may seem like overkill when you only have a few parameters to specify, but some pipelines expect dozens of parameters. In those cases, using a parameter file will allow us to provide parameter values at runtime without having to type massive command lines and without modifying the workflow script.</p>"},{"location":"hello_nextflow/06_hello_config/#takeaway_2","title":"Takeaway","text":"<p>You know how to manage parameter defaults and override them at runtime using a parameter file.</p>"},{"location":"hello_nextflow/06_hello_config/#whats-next_2","title":"What's next?","text":"<p>Learn how to use profiles to conveniently switch between alternative configurations.</p>"},{"location":"hello_nextflow/06_hello_config/#4-determine-what-executors-should-be-used-to-do-the-work","title":"4. Determine what executor(s) should be used to do the work","text":"<p>Until now, we have been running our pipeline with the local executor. This executes each task on the machine that Nextflow is running on. When Nextflow begins, it looks at the available CPUs and memory. If the resources of the tasks ready to run exceed the available resources, Nextflow will hold the last tasks back from execution until one or more of the earlier tasks have finished, freeing up the necessary resources.</p> <p>For very large workloads, you may discover that your local machine is a bottleneck, either because you have a single task that requires more resources than you have available, or because you have so many tasks that waiting for a single machine to run them would take too long. The local executor is convenient and efficient, but is limited to that single machine. Nextflow supports many different execution backends, including HPC schedulers (Slurm, LSF, SGE, PBS, Moab, OAR, Bridge, HTCondor and others) as well as cloud execution backends such (AWS Batch, Google Cloud Batch, Azure Batch, Kubernetes and more).</p> <p>Each of these systems uses different technologies, syntaxes and configurations for defining how a job should be defined. For example, /if we didn't have Nextflow/, a job requiring 8 CPUs and 4GB of RAM to be executed on the queue \"my-science-work\" would need to include the following configuration on SLURM and submit the job using <code>sbatch</code>:</p> <pre><code>#SBATCH -o /path/to/my/task/directory/my-task-1.log\n#SBATCH --no-requeue\n#SBATCH -c 8\n#SBATCH --mem 4096M\n#SBATCH -p my-science-work\n</code></pre> <p>If I wanted to make the workflow available to a colleague running on PBS, I'd need to remember to use a different submission program <code>qsub</code> and I'd need to change my scripts to use a new syntax for resources:</p> <pre><code>#PBS -o /path/to/my/task/directory/my-task-1.log\n#PBS -j oe\n#PBS -q my-science-work\n#PBS -l nodes=1:ppn=5\n#PBS -l mem=4gb\n</code></pre> <p>If I wanted to use SGE, the configuration would be slightly different again:</p> <pre><code>#$ -o /path/to/my/task/directory/my-task-1.log\n#$ -j y\n#$ -terse\n#$ -notify\n#$ -q my-science-work\n#$ -l slots=5\n#$ -l h_rss=4096M,mem_free=4096M\n</code></pre> <p>Running on a single cloud execution engine would require a new approach again, likely using an SDK that uses the cloud platform's APIs.</p> <p>Nextflow makes it easy to write a single workflow that can be run on each of these different infrastructures and systems, without having to modify the workflow. The executor is subject to a process directive called <code>executor</code>. By default it is set to <code>local</code>, so the following configuration is implied:</p> Built-in configuration<pre><code>process {\n    executor = 'local'\n}\n</code></pre>"},{"location":"hello_nextflow/06_hello_config/#41-targeting-a-different-backend","title":"4.1. Targeting a different backend","text":"<p>By default, this training environment does not include a running HPC schedulder, but if you were running on a system with SLURM installed, for example, you can have Nextflow convert the <code>cpus</code>, <code>memory</code>, <code>queue</code> and other process directives into the correct syntax at runtime by adding following lines to the <code>nextflow.config</code> file:</p> nextflow.config<pre><code>process {\n    executor = 'slurm'\n}\n</code></pre> <p>And... that's it! As noted before, this does assume that Slurm itself is already set up for you, but this is really all Nextflow itself needs to know.</p> <p>Basically we are telling Nextflow to generate a Slurm submission script and submit it using an <code>sbatch</code> command.</p>"},{"location":"hello_nextflow/06_hello_config/#takeaway_3","title":"Takeaway","text":"<p>You now know how to change the executor to use different kinds of computing infrastructure.</p>"},{"location":"hello_nextflow/06_hello_config/#whats-next_3","title":"What's next?","text":"<p>Learn how to control the resources allocated for executing processes.</p>"},{"location":"hello_nextflow/06_hello_config/#5-use-profiles-to-select-preset-configurations","title":"5. Use profiles to select preset configurations","text":"<p>You may want to switch between alternative settings depending on what computing infrastructure you're using. For example, you might want to develop and run small-scale tests locally on your laptop, then run full-scale workloads on HPC or cloud.</p> <p>Nextflow lets you set up profiles that describe different configurations, which you can then select at runtime using a command-line argument, rather than having to modify the configuration file itself.</p>"},{"location":"hello_nextflow/06_hello_config/#51-create-profiles-for-switching-between-local-development-and-execution-on-hpc","title":"5.1. Create profiles for switching between local development and execution on HPC","text":"<p>Let's set up two alternative profiles; one for running small scale loads on a regular computer, where we'll use Docker containers, and one for running on a university HPC with a Slurm scheduler, where we'll use Conda packages.</p> <p>Add the following to your <code>nextflow.config</code> file:</p> nextflow.config<pre><code>profiles {\n    my_laptop {\n        process.executor = 'local'\n        docker.enabled = true\n    }\n    univ_hpc {\n        process.executor = 'slurm'\n        conda.enabled = true\n        process.resourceLimits = [\n            memory: 750.GB,\n            cpus: 200,\n            time: 30.d\n        ]\n    }\n}\n</code></pre> <p>You see that for the university HPC, we're also specifying resource limitations.</p>"},{"location":"hello_nextflow/06_hello_config/#52-run-the-workflow-with-a-profile","title":"5.2. Run the workflow with a profile","text":"<p>To specify a profile in our Nextflow command line, we use the <code>-profile</code> argument.</p> <p>Let's try running the workflow with the <code>my_laptop</code> configuration.</p> <pre><code>nextflow run hello-config.nf -profile my_laptop\n</code></pre> <p>This still produces the following output:</p> <pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-config.nf` [gigantic_brazil] DSL2 - revision: ede9037d02\n\nexecutor &gt;  local (8)\n[58/da9437] sayHello (3)       | 3 of 3 \u2714\n[35/9cbe77] convertToUpper (2) | 3 of 3 \u2714\n[67/857d05] collectGreetings   | 1 of 1 \u2714\n[37/7b51b5] cowpy              | 1 of 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>As you can see, this allows us to toggle between configurations very conveniently at runtime.</p> <p>Warning</p> <p>The <code>univ_hpc</code> profile will not run properly in the training environment since we do not have access to a Slurm scheduler.</p> <p>If in the future we find other elements of configuration that are always co-occurring with these, we can simply add them to the corresponding profile(s). We can also create additional profiles if there are other elements of configuration that we want to group together.</p>"},{"location":"hello_nextflow/06_hello_config/#53-create-a-test-profile","title":"5.3. Create a test profile","text":"<p>Profiles are not only for infrastructure configuration. We can also use them to set default values for workflow parameters, to make it easier for others to try out the workflow without having to gather appropriate input values themselves. This is intended as an alternative to using a parameter file.</p> <p>The syntax for expressing default values is the same as when writing them into the workflow file itself, except we wrap them in a block named <code>test</code>:</p> Syntax example<pre><code>    test {\n        params.&lt;parameter1&gt;\n        params.&lt;parameter2&gt;\n        ...\n    }\n</code></pre> <p>If we add a test profile for our workflow, the <code>profiles</code> block becomes:</p> nextflow.config<pre><code>profiles {\n    my_laptop {\n        process.executor = 'local'\n        docker.enabled = true\n    }\n    univ_hpc {\n        process.executor = 'slurm'\n        conda.enabled = true\n        process.resourceLimits = [\n            memory: 750.GB,\n            cpus: 200,\n            time: 30.d\n        ]\n    }\n    test {\n        params.greeting = 'greetings.csv'\n        params.batch = 'test-batch'\n        params.character = 'turkey'\n    }\n}\n</code></pre> <p>Just like for technical configuration profiles, you can set up multiple different profiles specifying parameters under any arbitrary name you like.</p>"},{"location":"hello_nextflow/06_hello_config/#54-run-the-workflow-locally-with-the-test-profile","title":"5.4. Run the workflow locally with the test profile","text":"<p>Conveniently, profiles are not mutually exclusive, so we can specify multiple profiles in our command line using the following syntax <code>-profile &lt;profile1&gt;,&lt;profile2&gt;</code> (for any number of profiles).</p> <p>Note</p> <p>If you combine profiles that set values for the same elements of configuration and are described in the same configuration file, Nextflow will resolve the conflict by using whichever value it read in last (i.e. whatever comes later in the file). If the conflicting settings are set in different configuration sources, the default order of precedence applies.</p> <p>Let's try adding the test profile to our previous command:</p> <pre><code>nextflow run hello-config.nf -profile my_laptop,test\n</code></pre> <p>This should produce the following:</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `hello-config.nf` [gigantic_brazil] DSL2 - revision: ede9037d02\n\nexecutor &gt;  local (8)\n[58/da9437] sayHello (3)       | 3 of 3 \u2714\n[35/9cbe77] convertToUpper (2) | 3 of 3 \u2714\n[67/857d05] collectGreetings   | 1 of 1 \u2714\n[37/7b51b5] cowpy              | 1 of 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>This means that as long as we distribute any test data files with the workflow code, anyone can quickly try out the workflow without having to supply their own inputs via the command line or a parameter file.</p> <p>Note</p> <p>We can even point to URLs for larger files that are stored externally. Nextflow will download them automatically as long as there is an open connection.</p>"},{"location":"hello_nextflow/06_hello_config/#takeaway_4","title":"Takeaway","text":"<p>You know how to use profiles to select a preset configuration at runtime with minimal hassle. More generally, you know how to configure your workflow executions to suit different compute platforms and enhance the reproducibility of your analyses.</p>"},{"location":"hello_nextflow/06_hello_config/#whats-next_4","title":"What's next?","text":"<p>Celebrate and give yourself a big pat on the back! You have completed your very first Nextflow developer course.</p> <p>Next, we ask you to complete a very short survey about your experience with this training course, then we'll take you to a page with links to further training resources and helpful links.</p>"},{"location":"hello_nextflow/next_steps/","title":"Next Steps","text":"<p> See the whole playlist on the Nextflow YouTube channel.</p> <p> You can read the video transcript alongside the video.</p> <p>Congrats again on completing the Hello Nextflow training course and thank you for completing our survey!</p>"},{"location":"hello_nextflow/next_steps/#1-top-3-ways-to-level-up-your-nextflow-skills","title":"1. Top 3 ways to level up your Nextflow skills","text":"<p>Here are our top three recommendations for what to do next based on the course you just completed.</p>"},{"location":"hello_nextflow/next_steps/#11-apply-nextflow-to-a-scientific-analysis-use-case","title":"1.1. Apply Nextflow to a scientific analysis use case","text":"<p>Check out the Nextflow for Science page for a list of short standalone courses that demonstrate how to apply the basic concepts and mechanisms presented in Hello Nextflow to common scientific analysis use cases.</p> <p>If you don't see your domain represented by a relatable use case, let us know in the Community forum so we can add it to our development list.</p>"},{"location":"hello_nextflow/next_steps/#12-get-started-with-nf-core","title":"1.2. Get started with nf-core","text":"<p>nf-core is a worldwide collaborative effort to develop standardized open-source pipelines for a wide range of scientific research applications.** The project includes over 100 pipelines that are available for use out of the box and well over 1400 process modules that can be integrated into your own projects, as well as a rich set of developer tools.</p> <p>The Hello nf-core training course will introduce you to the nf-core community-curated pipelines and development framework, designed to help you write reproducible, scalable, and standardized workflows. You\u2019ll learn how to use existing nf-core pipelines, contribute to their development, and even start building your own, supported by best practices and a vibrant community. If you\u2019re ready to apply your Nextflow skills in real-world projects, this is the perfect next step.</p>"},{"location":"hello_nextflow/next_steps/#13-master-more-advanced-nextflow-features","title":"1.3. Master more advanced Nextflow features","text":"<p>In the Hello Nextflow course, we keep the level of technical complexity low on purpose to avoid overloading you with information you don't need in order to get started with Nextflow. As you move forward with your work, you're going to want to learn how to use the full feature set and power of Nextflow.</p> <p>To that end, we are currently working on a collection of Side Quests, which are meant to be short standalone courses that go deep into specific topics like testing, metadata handling, using conditional statements and the differences between working on HPC vs. cloud.</p> <p>For any topics that's not covered there yet, browse the Fundamentals Training and Advanced Training to find training materials about the topics that interest you.</p>"},{"location":"hello_nextflow/next_steps/#2-check-out-seqera-platform","title":"2. Check out Seqera Platform","text":"<p>Seqera Platform is the best way to run Nextflow in practice.</p> <p>It is a cloud-based platform developed by the creators of Nextflow that you can connect to your own compute infrastructure (whether local, HPC or cloud) to make it much easier to launch and manage your workflows, as well as manage your data and run analyses interactively in a cloud environment.</p> <p>The Free Tier is available for free use by everyone (with usage quotas). Qualifying academics can get free Pro-level access (no usage limitations) through the Academic Program.</p> <p>Have a look at the Seqera Platform tutorials to see if this might be useful to you.</p>"},{"location":"hello_nextflow/next_steps/#thats-it-for-now","title":"That's it for now!","text":"<p>Good luck in your Nextflow journey and don't hesitate to let us know in the Community forum what else we could do to help.</p>"},{"location":"hello_nextflow/survey/","title":"Feedback survey","text":"<p>Before you move on, please complete this short 4-question survey to rate the training, share any feedback you may have about your experience, and let us know what else we could do to help you in your Nextflow journey.</p> <p>This should take you less than a minute to complete. Thank you for helping us improve our training materials for everyone!</p>"},{"location":"hello_nextflow/transcripts/00_orientation/","title":"Orientation - Video Transcript","text":"<p>Important note</p> <p>This page shows the transcript only. For full step-by-step instructions, return to the course material.</p>"},{"location":"hello_nextflow/transcripts/00_orientation/#welcome","title":"Welcome","text":"<p>Hi, welcome to Hello Nextflow. My name is Phil Ewels. I'm Product Manager for Open Source at Seqera, and I'm delighted to be here today to take you through this first Nextflow training course.</p> <p>We're going to go through the basics of Nextflow, explaining how to write and run pipelines and configure them.</p> <p>And you're going to build your own simple multi-step pipeline. We'll cover terminology like operators and channel factories, and by the end of the course, you'll be ready to get started building your own bioformatics pipelines.</p> <p>If you have any questions, please reach out on community.seqera.io. We have a really active Nextflow community there's a section dedicated to training, so just let us know where you're stuck and someone will be able to help out.</p> <p>Right. Let's get started.</p>"},{"location":"hello_nextflow/transcripts/00_orientation/#training-website","title":"Training Website","text":"<p>All of the training material for the Nextflow courses lives at training.nextflow.io. You can go to it in your web browser. So boot that up now and we can have a look through.</p> <p>I'll be running this with version 2.1.1. We push small updates and fixes here and there, so don't worry if it's a little bit off, but if a material's drifted too far, you can always use this version picker at the top to pick the exact version of the materials that I'm going to be talking through.</p> <p>If you're more of a light mode person, you can change the theme for the website here.</p> <p>See translations here, though at the time of recording, it's really only English, which covers this new material.</p> <p>And also see all the source code for the training website and everything we'll be working with on GitHub.</p> <p>The homepage here lists all of the different training material courses that we have. So I scroll down, we'll see Nextflow for newcomers with the Hello Nextflow course we'll be doing here. You can see all the other courses we also have, which work in a similar way.</p>"},{"location":"hello_nextflow/transcripts/00_orientation/#environment-setup","title":"Environment Setup","text":"<p>I am actually going to start off by using this first one at the top, which is common for all of the training courses, and it's specifically about setting up our environment.</p> <p>I click through, it takes me to this section, and we can see instructions for developing locally. If you want to use your own laptop with your own copy of VS Code and your own software installations, or what we expect most people to do, which is to use something called GitHub Codespaces.</p> <p>Codespaces is a service provided by GitHub where they run a web server in the cloud, which you can connect to. That server has VS code installed, where you can run that in your web browser, or if you prefer, connect that to your local installation of VS code. All of the computation, all of the files, all of the editing happens remotely, which means that all the software you need comes pre-installed and is the same for everybody.</p>"},{"location":"hello_nextflow/transcripts/00_orientation/#creating-a-github-codespace","title":"Creating a GitHub Codespace","text":"<p>In order to create the codespace with everything we need, look for the buttons in the docs material, which say \"Open in GitHub Codespaces\". I'm going to click that now, open it in a new tab. And I'm presented it with this webpage. Now you can see this is pre-configured to set with an nextflow-io training.</p> <p>I can just click create new codespace. But actually we recommend that we use a slightly bigger machine for the Nextflow training with four CPUs instead of two. You can change which version of a material it uses. So this is defaulting to 2.1.1 because that's the version of the docs I followed the link from. But I could also set it to a specific branch of the repository if I want to.</p> <p>Now I'm going to click create codespace. And it's going to start setting up the environment for me.</p>"},{"location":"hello_nextflow/transcripts/00_orientation/#codespace-creation","title":"Codespace creation","text":"<p>Now, the first time you do this, it's going to take quite a long time, so now is a good time to go and have a cup of tea. Get yourself comfortable, chat to the person you're sat next to.</p> <p>If you're interested, you can click building codespace down here to see the logs of the setup. And you can see here it's pulling a Docker image with everything I need and configuring of the environment.</p> <p>Now, you only have to wait like this the first time you create a codespace. If you go to github.com/codespaces here, you'll see all the different Codespaces you have open. Here's the one I've just created. Next time you do this, you can go here and you can select the previous codespace and just jump straight back into it. And it's a much, much faster process to warm up that existing environment. That will also keep all of the changes that you've made to VS Code and to the files , so you won't lose your progress if you leave and come back.</p> <p>You can click the three dots here to do other actions. For example, if you configured it with two CPUs and now you want four, you can change the machine type. Or if you wanna start from scratch and fresh, you can delete the codespace.</p>"},{"location":"hello_nextflow/transcripts/00_orientation/#intro-to-vs-code","title":"Intro to VS Code","text":"<p>Okay, Codespaces is finished setting up my environment and now presented with VS Code in the web browser.</p> <p>If you're used to VS code. This will feel very familiar if you've not used it before, it's pretty simple. There's a few different parts to the page you need to be aware of.</p> <p>Over on the left here, we've got the sidebar. You can see the Explorer set up with all the different files in the GitHub repository from the training repo.</p> <p>On these buttons down the left, can be different tooling. In the sidebar. I can search all the files in all the project. I can work with Git, can work with GitHub, all different things like that.</p> <p>At the top here is the main menu. The file explorer is the one we'll have up most here, and you can right click any of these files and do the normal things you'd expect. You might need to click through some warnings like this where it like cut copy and you can download to your local machine as well.</p> <p>When the codespace loads, it gives us a preview of the markdown file in this main area here. This is just the same as the one that renders on github.com. I can close that and if I double click that read Readme file, you'll see it opens it as code in the code editor and just as with any other file, we can edit this code directly.</p> <p>Finally at the bottom here, we've got the terminal window. I was looking at the logs as it built, so that's what the current thing it's showing. I can also press this plus button to start a new terminal session. This isn't running on my machine. Remember, this is running in the cloud, and if I do tree three to depth of two, you'll see all the same files here, which were over on left.</p>"},{"location":"hello_nextflow/transcripts/00_orientation/#showing-just-hello-nextflow-files","title":"Showing just \"hello-nextflow\" files","text":"<p>This GitHub repository contains all the different training sets, not just the one we're doing. So if you like, you can focus on just the Hello Nextflow folder. One way to clean this up a little bit is to go to the menu file and then add folder to workspace.</p> <p>We click that go to training. Hello nextflow, and click add. It will refresh your screen. And then over in the Explorer, we now have two different workspaces, the one we had before for training and one with just Hello Nextflow .</p> <p>If you like, you can right click on training and click remove folder from workspace to get rid of it from the sidebar completely.</p> <p>Now we've got just files for this particular training course in the side. I can hide that warning and now I can do the same thing in the terminal here and do CD for change directory. Hello, Nextflow. And again, we have the same files here, which are on the sidebar.</p>"},{"location":"hello_nextflow/transcripts/00_orientation/#hello-nextflow-files","title":"Hello Nextflow: files","text":"<p>Looking at these files for the Hello Nextflow course.</p> <p>We have a bunch of .nf files, which are for Nextflow, and there's one of these files for each of the chapters of the training course. We'll work through these files and modify them in the exercises.</p> <p>We also have a nextflow.config file, which just has basic config settings for running Nextflow in this environment, which you don't really need to worry about at this point. A greetings.csv file, which we'll use for processing data, which will be introduced in the next part of this course, and a test-params.json file, which will be used in part six and you can ignore for now.</p> <p>These Nextflow files are just the start of each exercise. If you want to see how they should look when they're finished, you can go into a solutions directory and there are the answers for each part of the training course, so you can see a working version of what you're aiming towards.</p>"},{"location":"hello_nextflow/transcripts/00_orientation/#opening-a-terminal","title":"Opening a terminal","text":"<p>If at any point you close the terminal and can't remember how to get back, don't worry about it. These buttons at the top, right open and close different panels in the workspace. So click this one for bottom panel and it will reappear. And just make sure you've got terminal selected here. You can also click this button here, the arrow on the right hand side of a terminal to make it full screen.</p> <p>You'll see me doing that quite a lot because I have VS Code zoomed in so that you can read the text. Depending on your screen size, you may or may not need to do this. The same goes for minimizing the side panel.</p> <p>Right. That's enough for environment. I think we're ready to get started. Join me back in the next video for chapter one.</p> <p>Next video transcript </p>"},{"location":"hello_nextflow/transcripts/01_hello_world/","title":"Part 1: Hello World - Transcript","text":"<p>Important notes</p> <p>This page shows the transcript only. For full step-by-step instructions, return to the course material.</p> <p>The section numbers shown in the transcript are provided for indicative purposes only and may not include all section numbers in the materials.</p>"},{"location":"hello_nextflow/transcripts/01_hello_world/#welcome","title":"Welcome","text":"<p>Hi, welcome to Chapter One of Hello Nextflow.</p> <p>In this first part of a six part course, we're going to go into the very basics of Nextflow. We're going to start off by running some commands in a terminal, and then we'll take those Bash commands and see how to build them into a Nextflow script.</p> <p>We'll try running that first Nextflow pipeline, see what Nextflow does, where it runs, what files it creates, and what the purpose of where those files is.</p> <p>All right, let's get started.</p>"},{"location":"hello_nextflow/transcripts/01_hello_world/#trainingnextflowio","title":"training.nextflow.io","text":"<p>First things first, go to training.nextflow.io. Just as before, all of the material is written here, and I'll be working through it step by step. I'll be showing my screen as I do the steps of a training, but. Everything I'm saying is in the training material so you can follow it at your own speed, and you can find it all written there.</p> <p>This video also has video subtitles enabled, so feel free to put those up and track exactly what I'm saying as I say it.</p> <p>Okay, let's go to Hello Nextflow. That's the course we're going to be doing today, and we've done the orientation already in the first video, so we're going to go straight into part one. Hello World.</p> <p>Okay, I'm going to leave this training material now and hop into my Code Spaces environment. This is what we set up in the first video. Hopefully you have something that looks very similar to this in your own system. I'm using VS Code and I'm looking at the training material and I've changed directories into the hello Nextflow directory.</p>"},{"location":"hello_nextflow/transcripts/01_hello_world/#0-warmup-run-hello-world-directly","title":"0. Warmup: Run Hello World directly","text":"<p>Okay. Let's start off with a couple of basics, which hopefully will feel familiar to everybody. I'm going to start off just by writing very basic command in the terminal. Down here going to say 'echo Hello World!\"' press enter and, no surprises, the terminal does what I ask it and returns that string. Hello world.</p> <p>Okay, then I'm going to press up to get that command and edit it a bit more. Let's this time redirect that output to a file. I'm going to write it instead to output.txt and press enter nothing on the terminal this time because the output didn't come to the terminal. It went into that file.</p> <p>I can then read that file by doing 'cat output.txt' hit tab there to auto expand the file name and there you go. The file's there.</p> <p>I can also see that file in the sidebar over in the file explorer in VS code. I can double click it and open it here. If you want to open it in VS Code without clicking anything, you can also do \"code\" and then \"output.txt\" and it does the same thing.</p> <p>Great. That's the first step. Very simple.</p>"},{"location":"hello_nextflow/transcripts/01_hello_world/#1-examine-the-hello-world-workflow-starter-script","title":"1. Examine the Hello World workflow starter script","text":"<p>Okay. We're now going to do exactly the same thing, but in Nextflow, instead of directly in the terminal.</p> <p>We're going to use the first example script to start with, this file is called Hello World. I can do \"ls\" to view it in a terminal, and I'm on Mac, so I can do command click to open that file, or I could have just double clicked in the sidebar over here.</p> <p>There are a few things we can see in this file. Right at the top, there's a hash statement saying that this is a Nextflow file and that's how it could be executed. There are some comments here, just regular code comments in light gray, which don't affect the execution, and just help us read the script.</p> <p>And then there are two main structures. There's a process here and a workflow.</p> <p>Processes in Nextflow are the steps of the pipeline. They're the parts which actually do the logic and do the processing.</p> <p>The workflow then at the bottom stitches these processes together and governs the logic of the workflow, how everything connects to one another.</p> <p>We're going to start off looking at a process. We'll come back to the workflow in a moment.</p>"},{"location":"hello_nextflow/transcripts/01_hello_world/#12-the-process-definition","title":"1.2 The process definition","text":"<p>So every process starts with a key word process. Has a name and then has some curly brackets and everything within those curly brackets is that single process.</p> <p>A process must have a script section, and contained here is a bash snippet in a multi-line string, which is the part of the code which is actually executed in the compute environment.</p> <p>We also have an output statement here, which tells Nextflow, which files are expected to be created by the script. Note that the output here has a keyword path, which tells Nextflow that this is a file, not a value, or a string.</p> <p>Within the script block, this is just a regular bash statement, and it's exactly the same as what we wrote in the terminal. We're echoing hello world to a file called output.txt. This output.txt is then picked up by the output definition. The output definition isn't actually doing anything. It's just telling Nextflow what to expect, and if this file wasn't created, Nextflow would throw an error.</p> <p>Note that this example is not a great one because we've hardcoded the file name here, output.txt and output.txt. If either of these were changed, that would cause an error in our workflow.</p> <p>There is better way to do this with variables, which we'll cover in a minute.</p>"},{"location":"hello_nextflow/transcripts/01_hello_world/#13-the-workflow-definition","title":"1.3 The workflow definition","text":"<p>Okay. Moving down to the workflow, we can see that we have a comment and then we run the process called sayHello. This is the same keyword that's up here. This is about as simple as a workflow can get. We're just calling a single process with no variable input, so we're not connecting it to anything else. In the later part of this course, we'll talk about how to make this more powerful by using variable inputs and connecting things with channels.</p>"},{"location":"hello_nextflow/transcripts/01_hello_world/#2-run-the-workflow","title":"2. Run the workflow","text":"<p>Okay, this is all we need. Let's see if we can run it and see what happens. I am going to just clear the terminal and then I'm going to do \"nextflow run\", and I'm going to call the file name, which is hello-world.nf. That's all we need to run a Nextflow pipeline. This pipeline doesn't take any input, so we don't need any other arguments.</p> <p>Let's hit enter and see what happens.</p> <p>Okay. Hopefully you should have some output, which looks like this. We have a few bits of information telling us that Nextflow ran and what version it was using. Tells us which script was launched and it gives us a randomly generated name for this particular workflow execution. In this case, mine was called \"gloomy_crick\".</p> <p>The most important part of this though, is it tells us which steps ran in the pipeline. You can see that our process called sayHello ran, and it ran once and it was a hundred percent complete.</p> <p>This part here is the hash for that particular workflow task. Each process runs one or more times, and each one of those executions is called a task.</p>"},{"location":"hello_nextflow/transcripts/01_hello_world/#22-find-the-output-and-logs-in-the-work-directory","title":"2.2. Find the output and logs in the work directory","text":"<p>Every task gets its own isolated directory where it runs, so it's separate from the rest of the execution of the workflow. This hash corresponds to the file structure Within the work directory. If I do \"tree work\", we can see a0, and then a longer version of a short hash, and then our output.txt file. You can also see it in a sidebar.</p> <p>You can see in the sidebar there are some additional files here. The reason that these didn't show up in a terminal is because they are hidden files, they start with a dot. And indeed, if I do \"tree -a\" for all, and \"work\", we can see them here.</p> <p>These dot files are present in every single work directory. That Nextflow creates, and each one has a slightly different task. Firstly .command.begin just includes some instructions for Nextflow that sets up the task before it runs. .command.run are the actual instructions executed by Nextflow itself. Then .command.sh is probably the most interesting one. This is the script that was resolved from our process block script.</p> <p>If I open it, you can see we've got our \"echo Hello World\" to output.txt file. This is exactly the same as our process in this case, but if we have any variables within our Nextflow code, every task will have a different .command.sh, and you can see how those variables were resolved.</p> <p>The other files are to do with how the task executed. So .command.err, .log and .out are the standard error, standard output and the two combined. And .exitcode tells Nextflow how this task executed with what exit code, whether it's successful or not.</p> <p>Finally, we have our output.txt file and sure enough, \"Hello World\" this is what we're expecting and this is what was created.</p> <p>Okay, great. That was your first ever Nextflow run. Congratulations. It really is that simple.</p> <p>Next, we're going to go onto how to do this a little bit more conveniently so that we don't have to edit the code every time we want to make a change to how the pipeline runs.</p>"},{"location":"hello_nextflow/transcripts/01_hello_world/#3-manage-workflow-executions","title":"3. Manage workflow executions","text":"<p>This directory structure is great for keeping all the tasks separated and everything organized, but of course, it's not very convenient to find your output files. You don't want to be digging through loads of nested directories trying to find the results of your pipeline.</p>"},{"location":"hello_nextflow/transcripts/01_hello_world/#31-publish-outputs","title":"3.1. Publish outputs","text":"<p>The good news is you're not meant to. The work directories are really just for Nextflow to use itself. So what we're going to do is we're going to use a function for Nextflow called \"publishDir\".</p> <p>We go back to our workflow, go to the process. We can add a new statement here called a directive. This is what Nextflow calls these things at the top of processes which augment how the functionality works, and the one we're going to use is called publishDir.</p> <p>You can see I've started typing here and the Nextflow extension for VS Code has suggested to directive for me, so I can just hit enter.</p> <p>Okay, I am going to follow this with a directory called \"results\" and we're going to tell it to copy the output files there . So I'm going to say mode copy. Great. going to hit save and let's run the workflow again.</p> <p>nextflow run hello-world.nf</p> <p>It runs exactly the same. Though note we have a slightly different hash this time. Nextflow, will use a different hash every time you run the workflow. And we have a different set of work directories as a result. Areas, ones called EB instead, but you can see all the files are the same . However, what's new this time is that we also have a directory called \"results\".</p> <p>Within \"results\" here we have our output file. That's what we told Nextflow to do. We said, save the results files in a directory called \"results\" and copy them there. And so this is now much easier to find. It's just there alongside where we launched a workflow and all the different files can be organized there however, we wish, irrespective of where or how Nextflow ran the actual execution.</p> <p>Note that publishDir can handle symlinks, which is good if you're working on a shared file system and you want to save on space. And also you don't have to define all the files which are created by a process as an output.</p> <p>Nextflow will only copy the things which are defined in this output block. So if you have intermediate files created by the step, which are not needed downstream of this process, you just don't define them in output and they won't turn up in the publishDir. So this is a way of keeping your output files from a pipeline clean and easily deleting intermediate files once the workplace finished.</p> <p>A quick note here. There's some new Nextflow syntax coming called workflow output definitions, which will eventually replace publishDir. This gives us a way to define all the outputs from a workflow at pipeline level down in the workflow block. This is described in the Nextflow docs if you wanna give it a try. But for now, publishDir will be around for a while, so still have that in a training for 2025.</p>"},{"location":"hello_nextflow/transcripts/01_hello_world/#32-re-launch-a-workflow-with-resume","title":"3.2. Re-launch a workflow with -resume","text":"<p>Okay. I mentioned that the work directory here now has two sets of results with a different hash from each time we run the workflow. That's good. However, sometimes we don't want to recompute steps every time if we don't need to.</p> <p>Maybe you are iteratively building your workflow and you're adding steps in and you want the first steps just to reuse the cached versions. Or maybe something went wrong on your compute system halfway through your workflow and you want it to carry on from where it left off, but skip the steps it had already completed.</p> <p>Nextflow has built-in functionality for this called resume. Let's try it out. So first off, I'm going to just have a look at the work directory so we can remember what was there.</p> <p>And then I'm going to do \"nextflow run hello-world.nf\" and I'm going to add a single command here, \"-resume\".</p> <p>Note, single dash, that's really important. I'm going to run it and the output's going to look basically exactly the same, with a couple of small differences.</p> <p>Note here it says \"cached\" in gray. That means that Nextflow didn't run the task. This time it found something that matched what were requirements and it reused those outputs directly rather than rerunning the step.</p> <p>And sure enough, if you look at the hash here, you can see this corresponds to the existing hash that we had from a previous run.</p>"},{"location":"hello_nextflow/transcripts/01_hello_world/#33-delete-older-work-directories","title":"3.3. Delete older work directories","text":"<p>Okay. But if you are developing iteratively, you're going to build up a lot of these workflow files. That can be a problem if you might be short on space.</p> <p>Nextflow can help us clean up these work directories with a couple of helper commands. If I do \"nextflow log\". That will give me a list of all the different workflow runs that I've done in this directory, and they have the run names here. You can see the gloomy quick one, which was the first one we ran, and then these two new ones.</p> <p>We can now take that name and use those with the \"nextflow clean\" command. I can specify a single run name. Or even better, I can tell Nextflow to delete everything from before a single workflow name with \"-before\", and I'm going to put in \"stupefied_shaw\". That was my most recent run, \"-n\".</p> <p>The \"-n\" command told Nextflow to do it as a dry run without actually deleting anything for real, and it tells us which of the hash directories it would've been removed. Sure enough, it's just that one from the first execution. Both of the second executions use the same hash directory.</p> <p>I am going to run it again, but now instead of \"-n\" for dry run, I'm going to do \"-f\" for force and it has removed that hash directory. Now if I do \"tree work\", we can see, we just have this output file left.</p> <p>Great. So we've managed to clean up a whole bunch of disc space there.</p> <p>A couple of things to note when deleting work directories, if you symlink stuff to your results directory, those symlink sources will now be deleted and your results will be gone forever. So that's why using copy mode is a safer thing to do, and generally what we recommend.</p> <p>Secondly, Nextflow's resume functionality relies on these work directories. So if you delete them and you run Nextflow again, the resume functionality will no longer work. So it's up to you to keep track of which things you may need or may not need, and only delete things when you're sure that it's safe to do so.</p> <p>The other thing we can do is we can just delete the entire work directory if we've finished our workflow run and we're sure we don't need it anymore.</p> <p>So I can do \"rm -r work\". I know there was nothing important in there. I've got my results that I care about in the results directory where we copied them. And so it was safe to delete the work directory. It's up to you which of these approaches you use.</p>"},{"location":"hello_nextflow/transcripts/01_hello_world/#4-use-a-variable-input-passed-on-the-command-line","title":"4. Use a variable input passed on the command line","text":"<p>Okay, what's next? I mentioned that we had hardcoded some of the values in our workflow script here, the output.txt file, and that there might be a better way to do that.</p> <p>Let's make a start on this. What we're going to do is three things. We're going to add a new input to the process. We're going to tell the process script how to use that input, and then we're going to wire it up in the workflow so that we can use it dynamically with a command line flag when running Nextflow.</p> <p>So first things first. Let's add an input block here. Just the same as output. This is a new section for the process, and I'm going to say, \"val greeting\".</p> <p>Note here, I'm saying \"val\", which says that this is a variable, not a path.</p> <p>I can then go down into the script and then I can take out this hardcoded text here and do $greeting. This works just like any other programming language. We're defining a variable here and we're referencing it within this script block. When Nextflow runs this process, the variable will be interpolated. And when we go and look at that .command.sh file, we'll see the actual hard coded string here instead.</p>"},{"location":"hello_nextflow/transcripts/01_hello_world/#413-set-up-a-cli-parameter-and-provide-it-as-input-to-the-process-call","title":"4.1.3. Set up a CLI parameter and provide it as input to the process call","text":"<p>Okay, but where do we provide the variable? Next we go down to the workflow section, and you can see that the extension here is saying, we now expect an input, and it's given me a warning.</p> <p>Now, the simplest thing we could do is just hard code it . I could write \"Hello World\" and provide that string input to the process. But again, that wouldn't really solve any problems. We'd still have to go back and edit the pipeline code every time we wanted to change something, which is no good.</p> <p>The good news is that Nextflow has a built in system to handle command line arguments called parameters. So instead, I can use one of these special variables called paras and I can call it whatever I want, but I'm going to say greeting so that it matches the workflow logic.</p> <p>Hit save and let's see what we can do with this.</p> <p>So if I go back to the terminal. So we do \"nextflow run hello-world.nf\". Just as before, but the key difference is we do --greeting</p> <p>Note, there are two dashes here because this is a parameter. When we resumed the workflow before, that was a single dash . That's because resume is a core Nextflow option, and this is a parameter which is specific to our pipeline.</p> <p>Don't mix the two up. It's easy to do that. If you did --resume instead of just one dash, then that would be \"params.resume\", which wouldn't do anything. Likewise, if you did a single dash here, Nextflow wouldn't recognize it as a key argument.</p> <p>So it's --greeting, which corresponds to parameters greeting.</p> <p>I can now follow that with whatever text I want. So I'm in Sweden at the moment, so I'm going to say, \"Hej v\u00e4rlden\".</p> <p>So let's run it, see what happens, moment of truth.</p> <p>Okay, so you can see that the process ran again, just as before, sayHello with a single execution.</p> <p>This will have overwritten the file that was in the publishDir \"results\" directory. And so be careful when you're rerunning the files because things in the published air will be overwritten.</p> <p>I can now do \"code results/output.txt\", and sure enough, our output's been updated and now says \"Hej v\u00e4rlden\".</p>"},{"location":"hello_nextflow/transcripts/01_hello_world/#42-use-default-values-for-command-line-parameters","title":"4.2. Use default values for command line parameters","text":"<p>Okay, that's great. But the problem now is our workflow relies on us always defining this parameter, and it's nice to have sensible defaults so that things will run in a sensible way for your workflow unless you override the defaults.</p> <p>So the way we do that is by setting a default value for the parameter in our workflow script.</p> <p>So if I go back to my hello-world.nf file, I can go into the script just above workflow, type \"prams.greeting\" and define it like any other variable. So let's put a string here and let's say \"Hol\u00e0 mundo!\"</p> <p>Now this parameter has got a default defined, which will be used here , or we can still override it on the command line with --greeting, just as we did before.</p> <p>So let's check it works. \"nextflow run hello-world.nf\"</p> <p>No command-line arguments this time, and check whether it did the right thing.</p> <p>\"code results/output.txt\". And there it is. We got our default.</p> <p>Okay, let's try again, just check I'm not telling you any lies. Let's run it again, but do --greeting, and use the example from a training material, let's say \"Konnichiwa!\"</p> <p>Reruns, the workflow, and sure enough, our output file up at the top is just updated with the new value which we provided on the command line.</p> <p>Great. This is a real central aspect to writing any Nextflow workflow. Defining sensible defaults in your pipeline code, but making it very easy to configure for the end user by having command line arguments on the terminal.</p> <p>Note that the end user can overwrite the config in multiple different places. You can have a config file in your home directory, which is applied to every single Nextflow run that you do. You can have a config file in a launch directory. You can have a config file in a pipeline directory. All of these different config locations are loaded in a specific order, which is described in the Nextflow docs.</p> <p>Okay, that's the end of section one. We've had our first ever workflow script in Nextflow with a process and a workflow. We've looked at inputs, outputs, scripts, and publishing, and how to wire up parameters and an input channel into our process.</p> <p>Congratulations, your first step towards writing Next low code is complete.</p> <p>Have a little break and I'll see you back in a few minutes for chapter two.</p> <p>Next video transcript </p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/","title":"Part 2: Hello Channels - Transcript","text":"<p>Important notes</p> <p>This page shows the transcript only. For full step-by-step instructions, return to the course material.</p> <p>The section numbers shown in the transcript are provided for indicative purposes only and may not include all section numbers in the materials.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#welcome","title":"Welcome","text":"<p>Hi, welcome to part two of Hello Nextflow.</p> <p>This chapter is called Hello Channels. We're gonna be talking all about this fundamental part of Nextflow.</p> <p>Channels are the things which connect with different steps in your pipeline, the way that your data and logic flows through your workflow.</p> <p>Okay, let's dig in.</p> <p>Let's start off by going to training.nextflow.io</p> <p>Hello Nextflow in the sidebar and clicking part two. Hello Channels.</p> <p>All of the material is written down here so you can follow at your own pace and catch anything that you might have missed.</p> <p>Once you've got the website open, you can load up Codespaces and we'll continue from where we were at the end of the last chapter.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#0-warmup-run-hello-channelsnf","title":"0. Warmup: Run hello-channels.nf","text":"<p>For this chapter, we're going to be editing a different file. This one's called Hello Channels, so you can find that in a side bar, double click it to open.</p> <p>Now if you've just come from chapter one, this file will look very familiar. The starting point here is basically where we finish chapter one, with our process called sayHello, our input, output, our publishDir and our params.greeting, and our simple workflow.</p> <p>We're starting with a new file, so it's a level playing ground for everybody, but you can continue with your previous file if you prefer.</p> <p>Note, I've also deleted all of the .nextflow* files and the work directories here, just so it's a clean starting point. It doesn't matter if you do that or not, it's up to you.</p> <p>Okay. Let's start off by checking that this pipeline still works as we expect. I'm going to bring up the terminal here.</p> <p>Do \"nextflow run hello-channels.nf\" and hit enter.</p> <p>It's going to run that little workflow, runs our sayHello step, generates a work directory with that hash, and here's our results folder and there's our output file, just as we expected from our default params.greeting.</p> <p>So that's great. Exactly the same as chapter one, working as we expect.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#1-provide-variable-inputs-via-a-channel-explicitly","title":"1. Provide variable inputs via a channel explicitly","text":"<p>In chapter one, you were actually already using channels, you just didn't realize it. When we specified a string here, Nextflow automatically created a channel around that string for us, just because it knew that we were calling a process, so we needed an input channel.</p> <p>The first thing we're going to do is make it explicit by actually typing out the channel itself.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#11-create-an-input-channel","title":"1.1. Create an input channel","text":"<p>So I'm going to go to the workflow here at the bottom of the script, and I'm going to say greeting_ch. This is a convention we often use in Nextflow code to have a underscore ch at the end of a variable name when it's a channel, just so it's easy to identify that it is a channel, but you don't have to do that. Equals channel of Hello Channels.</p> <p>What we've just used is something called a \"Channel Factory\" in Nextflow language. This is this thing here, we're setting this variable to a new channel, and this channel factory here is creating a channel for us in a particular way.</p> <p>There are a handful of different channel factories that Nextflow has, to create channels from different types of inputs. Dot of is the simplest one, and just takes any strings that we give it.</p> <p>Notice that when I hover over these words in VS Code, the Nextflow extension is giving me a popup explaining what this syntax does, and there's also a read more text at the bottom of that popup window.</p> <p>If I click that, it'll open the Nextflow docs. In a new tab and take me straight to the documentation for this specific thing. In this case for Channel.of.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#12-add-the-channel-as-input-to-the-process-call","title":"1.2. Add the channel as input to the process call","text":"<p>Note that the extension is also giving us a warning, saying that we've created a new channel here, but it's not being used by anything.</p> <p>So, let's fix that. I'm going to take the new channel name and I'm going to replace this params.greeting with our new channel.</p> <p>Note that we're no longer using the command line flag --greeting now, params.greeting isn't being used, we're going back to hard coding this string. That's okay. I'm just trying to keep things simple. We'll come back later and use the params again.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#13-run-the-workflow-command-again","title":"1.3. Run the workflow command again","text":"<p>Okay, let's just double check this works. Bring up the terminal and note again. Nextflow run hello channels. Check output.txt, and there it is.</p> <p>Great bit of a boring example, doing exactly the same thing as we did before, but now at least the logic is a bit clearer. We're being explicit about writing a new channel.</p> <p>We've effectively just written more code to do the same thing. But this will start to make more sense as we become a bit more complicated with how we create our channels.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#2-modify-the-workflow-to-run-on-multiple-input-values","title":"2. Modify the workflow to run on multiple input values","text":"<p>Okay, let's make this a bit more interesting. It's very rare that you want to run a Nextflow pipeline on a single input, so let's give it several inputs.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#21-load-multiple-greetings-into-the-input-channel","title":"2.1. Load multiple greetings into the input channel","text":"<p>From the docs here. I'm going to copy in these different strings, three of them. Hello, Bonjour, Ol\u00e0. Oh, get Hope. Copilot is suggesting a couple of others. So let's tab enter those.</p> <p>The Nextflow docs here tells us that we can give multiple values to this operator, so it should work, but let's try it out and see what happens.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#212-run-the-command-and-look-at-the-log-output","title":"2.1.2. Run the command and look at the log output","text":"<p>Well. Yes and no. Let's see. It says that five of five tasks have run here, but it only shows us one hash, which is a bit odd. That's okay. Everything is it's expected here. By default. Nextflow uses a special type of output to a terminal called ANSI control codes, which means it overwrites the certain lines to give a nice compressed view of all the different processes which are being run.</p> <p>This makes much more sense when you have larger workflows and are running hundreds or thousands of different samples. You can just generate so much output on the terminal, it's impossible to look at, whereas this updating view gives you a real time progress for you.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#213-run-the-command-again-with-the-ansi-log-false-option","title":"2.1.3. Run the command again with the -ansi-log false option","text":"<p>If you want, you can run it again, and this time I'm going to use an additional Nextflow core argument with a single hyphen saying, \"-ansi-log false\". This uses the previous version of the Nextflow log output. And here you can see all the individual processes which have been launched.</p> <p>It's up to you whether you do this or not. The output from Nextflow is exactly the same in both cases.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#22-ensure-the-output-file-names-will-be-unique","title":"2.2. Ensure the output file names will be unique","text":"<p>Okay, let's have a look at the output files, then we'll go to results. But there's only a single output file. What's happened? We saw that the process had run lots of times. We can go into the work directory and see all of the different hashes, all the tasks were executed properly. But if you remember in our process here, we're saving everything to an output.txt file and then publishing that to this directory.</p> <p>So the same file was created five times, and then it was overwritten five times. And we just have whichever task happen to execute last.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#221-construct-a-dynamic-output-file-name","title":"2.2.1. Construct a dynamic output file name","text":"<p>The way we fix this is by using a dynamic output file name. Here we already have a variable called greeting within the process, so we can use that in the output file name. I copy that and I do $greeting-output.txt.</p> <p>I am going to surround this in quotes, just so that bash doesn't get confused by any spaces which might creep in here. And then I'm going to take the same file name and update the output here.</p> <p>It's really important that the output matches this, because otherwise, this file won't be found and Nextflow will crash.</p> <p>I'm going to make one more really important edit, which is I'm going to change these single quotes for double quotes. Note that the color of the code changed when I did that. This variable is only expanded if we use double quotes. If I use single quotes here, it's used as a literal value, and I'd get a single file called $greeting-output, which is not what I want.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#222-run-the-workflow","title":"2.2.2. Run the workflow","text":"<p>So let's put the double quotes back and give it a try.</p> <p>I am just going to clear up my directory before I start, so it's easy to see the new files. I'm going to delete anything called .nextflow, work, and results.</p> <p>And I'm going to run that Nextflow command again and let's see what files are created. So it runs the five processes there. If you were watching very closely, you might have seen that line update as it was running.</p> <p>And now we can go into the results directory, and sure enough, we have five different outputs, and they're all prefixed with the different greeting.</p> <p>If I open each of these, we'll see that they each contain the corresponding greeting. Fantastic. That's what we want.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#3-use-an-operator-to-transform-the-contents-of-a-channel","title":"3. Use an operator to transform the contents of a channel","text":"<p>Okay, so now we know what channels are and we know what channel factories are. What about operators? This is another term for part of the Nextflow language, which is a series of functions which allow us to operate on channels to do certain things to them. Nextflow, comes with a suite of operators, which allow us to manipulate channels in a variety of different ways.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#31-provide-an-array-of-values-as-input-to-the-channel","title":"3.1. Provide an array of values as input to the channel","text":"<p>Let's work through this with an example. Let's say that we want to take these input strings, but instead of just putting them directly into a channel factory, we want to define them as an array.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#311-set-up-the-input-variable","title":"3.1.1. Set up the input variable","text":"<p>So I'm going to take these and do that as a new line above and say, greetings, array.</p> <p>There we go. I'm going to take that array variable and put it into the Channel.of, and hit save.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#313-run-the-workflow","title":"3.1.3. Run the workflow","text":"<p>Now, let's see what happens. Go back to my terminal. I'm just going to clear up all those temporary files again. And let's run the workflow.</p> <p>Not good. Okay. It broke. That's okay. I expected it to break this time. Debugging what goes wrong when an Nextflow workflow fails is a key part of being an Nextflow developer. This will happen a lot and it's important to understand what the error message says and how to deal with it.</p> <p>The Nextflow, error messages are actually quite structured. It tells us which process went wrong. It gives us an error message for a reason. It says what the command was that it tried to run within that particular task, what the exit status was, what the output was on where that task work directory was.</p> <p>Note that I can option, click this in VS Code and it opens it in a sidebar so I can go straight there and view all of these hidden files, which we talked about in the previous chapter, including the .command.sh file. This you can see is the same as the commands which was executed here.</p> <p>By looking at this file, we can get a feel for what might have gone wrong here instead of running a single task for each element in the array as it did last time, it just provided the entire array in one go as a string. So we need to unpack that array into individual values before we pass it into the channel. Let's go back and see if we can do that using an operator.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#32-use-an-operator-to-transform-channel-contents","title":"3.2. Use an operator to transform channel contents","text":"<p>In this case, we're not going to change the array before we pass it into the channel. We're going to adjust the channel so that it behaves in the way we expect. We're going to do that by using the flatten operator can do dot start typing and we can see that VS Code extension starts suggesting all the different operators we have available.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#321-add-the-flatten-operator","title":"3.2.1. Add the flatten() operator","text":"<p>And I'm going to select flatten. Note that white space doesn't matter in this context for Nextflow. So you can put these operators on a new line if you want to. So I can drop this down here and indent it so it sits underneath \".of\" and you'll see that people often chain lots of operators like this onto a channel and indent it in this way so that it is easier to read.</p> <p>You can also see, like before I can hover over this and read what the flatten operator's doing, and also follow a link to the documentation if I want to.</p> <p>So this operator is taking this channel, which has a single array within it, and separating out the array values.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#322-add-view-to-inspect-channel-contents","title":"3.2.2. Add view() to inspect channel contents","text":"<p>We can peek into the channels by using the special view operator, and I'm going to add a couple of them here. This is a bit like using print statements in other languages. So I'm going to do dot view and then I'm going to use these squiggly brackets.</p> <p>This is called a closure. This basically gives additional code to the view operator, which it will execute on each item within the channel. In this case, I'm going to say greeting before flatten. Greeting.</p> <p>I'm defining a variable here, which is just within the scope of this closure. So this variable is only used here and I could call it whatever I wanted. It doesn't really matter. I'm just using greeting to make it easy to read.</p> <p>In some Nextflow pipelines, you might see people use a special implicit variable called \"$it\". Like that. This is a special variable within Nextflow code, which is a shorthand so that you don't have to do the little definition of a variable. However, over time we're thinking, this is not very clear to people who are new to Nextflow, and we discourage usage of \"$it\" now.</p> <p>So I'm going to stick with the previous behavior of greeting and using it like this because that's more explicit and it's clearer on what's going on.</p> <p>I'm going to then copy this line and do exactly the same thing again after the flatten arguments. The view operator's a bit special because it does something on the elements, but it also just continues passing them onto the next operator so we can chain it in the middle of a chain of operations like this, and it will print the status there and keep going. So hopefully this will show us what the channel looks like before and after the flatten operator.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#323-run-the-workflow","title":"3.2.3. Run the workflow","text":"<p>Let's try it out. Clean. Clean everything up in the workspace. Run the pipeline again.</p> <p>Okay, so we can see it ran our five processes. Again, it didn't crash with an error, so that's definitely good. And now we have the before flatten and it sure enough we have our array and we have after flatten, printed five times once for each elements of the array. That's exactly what we were hoping for. So that's really good news. And that fits exactly with what we'd expect from the code.</p> <p>We don't need these debug statements anymore, so I can either comment them out or delete them. I'm going to delete them just to keep my code nice and clean. Okay, great. This example is now working nicely and we can start to see how channels can do a bit more complicated logic.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#4-use-an-operator-to-parse-input-values-from-a-csv-file","title":"4. Use an operator to parse input values from a CSV file","text":"<p>Now we're going to try and do this using a file with a series of inputs instead. This is a very common way to write Nextflow pipelines using a sample sheet or a CSV of metadata.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#41-modify-the-script-to-expect-a-csv-file-as-the-source-of-greetings","title":"4.1. Modify the script to expect a CSV file as the source of greetings","text":"<p>If I go over to the sidebar, you can see greetings.csv in the example repository, and this is a very, very simple CSV file that just contains three lines with three different greetings. Let's see if we can use this CSV file within our workflow.</p> <p>I'm now going to go back to using params like we did in chapter one, so that we can have a command line input.</p> <p>I'm going to delete this greetings array.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#411-switch-the-input-parameter-to-point-to-the-csv-file","title":"4.1.1. Switch the input parameter to point to the CSV file","text":"<p>I'm going to set params greeting to the file name, which is greetings.csv, and I'm going to use this special variable to generate the channel. I'm going to put that in there, and the errors go away. Remember that this is setting this variable by default now. So if I run the pipeline without any arguments, it'll use greetings.csv, but I could do --greeting to overwrite this variable if I wanted to.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#412-switch-to-a-channel-factory-designed-to-handle-a-file","title":"4.1.2. Switch to a channel factory designed to handle a file","text":"<p>Okay, we're passing a file now rather than a string or an array of strings, so we probably need a different channel factory.</p> <p>We're going to get rid of \"of\" which we've been using so far, and instead use .fromPath. This does exactly what it sounds like. It creates a channel with paths instead of values, using a string file name or glob. I am also going to remove the flatten operator as we no longer need this, now that we're passing a file.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#413-run-the-workflow","title":"4.1.3. Run the workflow","text":"<p>I'm going to hit save, open up the terminal, run the workflow, and then see what happens.</p> <p>Okay. It crashed again. Don't worry. I was expecting this one as well. Let's have a look at the error message and see if we can figure out what's going wrong. Here we can see the command executed, and a bit like before where we had the whole array printed. Now we have the file path being echoed into the command, rather than going through the contents of the file.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#42-use-the-splitcsv-operator-to-parse-the-file","title":"4.2. Use the splitCsv() operator to parse the file","text":"<p>So to use the contents of the file instead, we need another operator. The operator we're going to use for this one is called splitCsv. Makes sense, because it's a CSV file we're loading.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#421-apply-splitcsv-to-the-channel","title":"4.2.1. Apply splitCsv() to the channel","text":"<p>Ok, so splitCsv. Close bracket. We don't need any arguments here. And again, I'm going to use some view operators to give some insight as to what's going on here.</p> <p>.view csv after splitCsv. Before split Cv.s</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#422-run-the-workflow-again","title":"4.2.2. Run the workflow again","text":"<p>Okay, let's try running this and seeing what happens.</p> <p>Okay, we've got a bit more output this time, but it still failed. We can look at the view statements, and here you can see before split CSV, and we have a file path as we saw in the previous error message. After split CSV, we now have three values corresponding to the three lines in the CSV file.</p> <p>However, you can see that each of these values is surrounded by square brackets. So each one of those was an array in its own right, and that's given us the same area that we had before where it's trying to echo an array rather than just a single string.</p> <p>If we think about a CSV file, this kind of makes sense. Typically, a CSV file will have rows and columns, so split CSV does two dimensional array. The first dimension of the array is each row, and then there's a second dimension, which is each column for each row.</p> <p>So here we only have a single value on each line, so we have a single column, so we have a one element array for each line of the file.</p> <p>That's fine. We just need another operator to collapse that array for each line of the parsed CSV file. Let's clean this up. Get rid of a terminal and see what we can do.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#43-use-the-map-operator-to-extract-the-greetings","title":"4.3. Use the map() operator to extract the greetings","text":"<p>Now we could use the flatten operator again, which we used before. We've seen how that can collapse an array into a series of values, which would work very well here. But I'm going to use the opportunity to demonstrate another operator, which is very common within workflows called the map operator.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#431-apply-map-to-the-channel","title":"4.3.1. Apply map() to the channel","text":"<p>I'm going to do dot map and I'm going to do item item[0].</p> <p>If you write a lot of other code languages, you might be familiar with the map operator already. It takes an iterable, such as an array or a channel, and it does some operation on each value of that.</p> <p>Here we're saying that we should define a variable called item within the scope of this closure, and then we want to return, just the first value in that array. So item index zero.</p> <p>This is effectively flattening the array. You can see how we could extend this to be more complex, though: if our CSV file had six columns, but we're only interested in the fourth column, we could access a specific index here. Or do any other kind of operation on the value before we pass it onto downstream processing.</p> <p>So the map operator is extremely flexible and very powerful for modifying channels in flight. Let's put in another view statement just so we can see what it's doing in our execution. Can adjudicate that line and move it down. And after map.</p>"},{"location":"hello_nextflow/transcripts/02_hello_channels/#432-run-the-workflow-one-more-time","title":"4.3.2. Run the workflow one more time","text":"<p>Let's bring up the terminal and try running the workflow.</p> <p>Okay, no errors this time. That's a good sign. We can now go through all these different outputs from the view statements. Before split CSV, we had a single path. After split CSV, we had the single value arrays, and then after map, we have just the values without any array syntax. Let's go up to the results directory, and here are our output files behaving exactly as we wanted them to.</p> <p>There's a little bonus here. You can actually see that the view operators are slightly mixed up in the order that they've done the output. This is because Nextflow is doing parallelization of these different tasks. So after it split the CSV, there are three elements in this channel, and it's handling the processing of those three elements in parallel automatically. That means that the order of the outputs is stochastic and can vary. In this case, it just happened that some of the view operators returned after the subsequent step had been completed, and so it came in this order.</p> <p>If I run the same workflow again. Then sure enough, it's come in a different order and this time we've got the split CSVs and the maps in the order we'd expect.</p> <p>So just bear in mind, you cannot rely on the order of outputs from a process task because Nextflow's handling this parallelization for you automatically. Nextflow does that for you with its data flow logic, and that's the real power of Nextflow.</p> <p>Okay, this is probably one of the most important chapters of the whole training. Once you understand channels, channel factories and operators, you start to key into the strength of Nextflow and what makes it unique as a programming language. This functionality allows Nextflow to parallelize all of your workflows for you and generate extremely complex workflow logic with a very clean syntax and a push data flow model. It can be a bit of a strange concept at first, but once you get used to writing code like this, it will quickly feel natural and before you know it, you'll be writing fantastic workflows.</p> <p>Have a break, cup of tea, walk around and let's move on to chapter three, where we start to extend these concepts into more complex workflows. See you in the next video.</p> <p>Next video transcript </p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/","title":"Part 3: Hello Workflow - Transcript","text":"<p>Important notes</p> <p>This page shows the transcript only. For full step-by-step instructions, return to the course material.</p> <p>The section numbers shown in the transcript are provided for indicative purposes only and may not include all section numbers in the materials.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#welcome","title":"Welcome","text":"<p>Hi, welcome to part three of the \"Hello Nextflow\" training course.</p> <p>This chapter is called \"Hello Workflow\".</p> <p>In chapter two, we built a simple workflow of one process, but in reality, pipelines are useful because they can chain multiple steps of analysis together.</p> <p>In this chapter, we're going to take that initial example and extend it to be a little bit more realistic.</p> <p>We're going to add some additional steps and we're going to look at how we use channels to connect those steps.</p> <p>We are going to look at multiple tasks, which can collapse into a single process and we're going to look at processes which can have multiple inputs and multiple outputs.</p> <p>Okay, let's get started.</p> <p>So let's start off. Same as before. Let's go to training.nextflow.io. Hello Nextflow, chapter three. Hello Workflow. And let's open up our workspace. I've cleaned up all my work files from my previous chapters and I'm going to open Hello Workflow.</p> <p>Now this is the same file we've been working on until now so this should look familiar. We've got our say hello process. We've got our params.greeting with its greetings CSV file, and we've got our workflow at the bottom, which loads that CSV file, creates the channel and passes it to our process.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#0-warmup-run-hello-workflownf","title":"0. Warmup: Run hello-workflow.nf","text":"<p>If you like, we can try this out and double check it is working as we expect. Load up a terminal to nextflow run hello workflow nf and click enter.</p> <p>Okay, great. Our three processes run. We have our results directory with our three outputs. Bonjour. Hello. Hol\u00e0. So let's close those files, close the terminal, go back to the script.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#1-add-a-second-step-to-the-workflow","title":"1. Add a second step to the workflow","text":"<p>Okay. For our example, we're staying basic and we're trying to stay domain agnostic. So our second process is just going to manipulate these strings, these words, in a simple way. We're going to use the translate Unix command to take these files and make them all uppercase. We do that with the \"tr\" command.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#11-define-the-uppercasing-command-and-test-it-in-the-terminal","title":"1.1. Define the uppercasing command and test it in the terminal","text":"<p>We can try this just in the bash terminal, and see if it works. So you do echo, Hello World, and then pass that on with the pipe character to tr, and we give it a recognition pattern, a to z and what it should translate to. A to Z in uppercase.</p> <p>This is very simple because it's literally doing the A to Z characters. So it won't work on anything that's accented or anything like that. But for the purposes of the example, you should get the picture.</p> <p>Going to hit enter and it prints to a terminal, HELLO WORLD in capitals. And just as before, we could redirect this to a file if we wanted to. Outfile.</p> <p>Okay. Let's clean this up.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#11-write-the-uppercasing-step-as-a-nextflow-process","title":"1.1. Write the uppercasing step as a Nextflow process","text":"<p>Let's go back to our script and write a new process to handle this bash command. I'm going to copy the previous process, paste it underneath, and call it convert to upper. For uppercase. I am going to use the same publishDir results, but I'm going to make a few changes here. Instead of taking a val, I'm going to take a path input file, and I'm going to have a prefix here upper, so that our output files don't clobber the output. And I'm going to use the variable name from the input.And then I'm going to change a script down here, and instead I'm going to use cat on the input file and just like we did in Bash TR, a-z, upper input file .txt. Okay, let's click save.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#12-add-a-call-to-the-new-process-in-the-workflow-block","title":"1.2. Add a call to the new process in the workflow block","text":"<p>Now if I scroll down, we need to actually call this process. Just adding the process into a script isn't enough. We have to tell Nextflow that we need to run this process and where to do that.</p> <p>So I'm going to right here, convert to upper and</p> <p>okay, we're getting an error here saying it expects an argument. Sure enough, we need to pass something to this process so that it actually has something to do.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#13-pass-the-output-of-the-first-process-to-the-second-process","title":"1.3. Pass the output of the first process to the second process","text":"<p>What we're going to do is we're going to take the output from this process. So I take the name, say hello, and when I do dot out.</p> <p>For a simple example like this, where we have a process that has just one output and we're passing that to a new process, so it has one input that should be all we need. So I'm going to click save, bring up the terminal, and let's try and run this again.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#14-run-the-workflow-again","title":"1.4. Run the workflow again","text":"<p>Now, I haven't cleared up my work directory from the last time I run this workflow. I'm going to run it again and I'm going to use this as an opportunity to show how partial caching works. So if I do single dash resume. Hopefully it should reuse the outputs from that first process, which were exactly the same as last time I ran. But now we have a new process here which hasn't run before, that runs from scratch. And sure enough, you can see the first process used, the cache outputs, and the second output ran three of three. You can also see that we have both of our processes here now, our first process, say hello, run three times, and our second process convert to upper run three times.</p> <p>If I run this again, as a reminder, with -ansi-log false, we should see that six different process tasks run three for each of them. So this is doing exactly what we hoped it would. The first process is running three times, passing those outputs onto a second process, which is then running three times.</p> <p>So let's have a look inside the work directory and see how Nextflow is handling these file inputs. If I take this hash directory here from the second process we can use a tree command again with -a just to look at these files. You can see in here that we have our input file, which is the Bonjour-output.txt file, and that's actually a symlink. That's what this arrow is showing us, and it's pointing to the file in the previous work directory.</p> <p>This makes sense. Nextflow handles the execution of each task in its own encapsulated directory, so it's completely self enclosed. However, it needs to provide the files from a previous steps as an input. Rather than reaching outside of the work directory to get those files, Nextflow stages them into the work directory.</p> <p>If we have a shared file system like here, it does that using a symlink so that it doesn't use any additional file space. If we use cloud storage with buckets in different locations, it would fetch those files and actually copy them into the work directory.</p> <p>Let's have a look at the command sh file. If I do code work, command sh, you can see, sure enough, it's accessing that file from the local directory. So everything is very self-contained and clean.</p> <p>We can also check the results directory and make sure that these files were outputted properly. And sure enough, in results, we can see all the output files from the first process and all the output files from the second. And they're all in uppercase as we hoped.</p> <p>This is where the power of Nextflow starts to shine. With some very minimal code and Nextflow handled execution in parallel of these tasks with clean encapsulation within separate work directories and staging input and output files and file publishing all automatically for us just out of the box. So you can see how, as we scale this complexity of our analysis workflows, this functionality is really, really valuable.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#2-add-a-third-step-to-collect-all-the-greetings","title":"2. Add a third step to collect all the greetings","text":"<p>Okay. These steps were one-to-one. We had one output from the first process going to one input for the second process. Next, we're going to talk about how to collect these different outputs into a single process task, which is again, a very common thing to do. So let's quickly bring up the terminal and do a dry run of this.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#21-define-the-collection-command-and-test-it-in-the-terminal","title":"2.1. Define the collection command and test it in the terminal","text":"<p>I'm going to cheat and copy the example bash code from a training material and just hit enter.</p> <p>What we can see here is we ran this echo command three times to three different output files, which I can see here. And then used the cat command to print the output of each of these three different files, and redirect that to a single collected file.</p> <p>And if I do \"cat COLLECTED-output\", you can see it's got the contents of those three different files, now in a single file.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#22-create-a-new-process-to-do-the-collection-step","title":"2.2. Create a new process to do the collection step","text":"<p>So let's see if we can replicate the same thing within our Nextflow pipeline.</p> <p>Let's scroll up and create a third process. I'm going to copy this previous one, and this time I'm going to call it Collect Greetings.</p> <p>In the bash terminal, we called it collected output txt. So I'm going to say the same path output here. And I'm going to do the redirection here, so it's saved in the same way.</p> <p>Okay. We need to change what happens at the start of that command, and we need to think about what the input file is here. In fact, this process is going to take multiple input files. I'm going to keep path and I'm going to change this to a new variable called input files, plural.</p> <p>I'm then going to again, cat them like we did in our bash script. And I'm going to use the variable here.</p> <p>Now, you might think this wouldn't work. We've seen previously failures where an array of strings or an array of paths has been passed to a process and that caused an error. But in fact, here Nextflow is going to handle this automatically for us in the right way. It's going to take several different input files, and it's just going to print the different file paths here.</p> <p>Of course it helps that the cat command can take a series of file names like this. If I was using a different command that required an argument before each file path or something, we'd have to have a bit more code here and logic to be able to handle the iteration of these file paths. But in this case, it should just work.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#23-add-the-collection-step-to-the-workflow","title":"2.3. Add the collection step to the workflow","text":"<p>Okay, let's go down to the workflow and add in our new process. Collect greetings. And again, let's take the output from convert to upper out. Let's save this.</p> <p>Give it a try. nextflow run hello workflow.</p> <p>Okay, the workflow ran, but something's a bit strange here. We've got three executions of the first step, which we expect. Three tasks for the second, but we also have three tasks at the end when we expected to only have a single task here merging all the outputs.</p> <p>If we go into our results directory. We also see the collected output only has a single value rather than all three. This is because that output file was overwritten three times with three different values.</p> <p>This makes sense because we passed one output to one input here in the same way as we did in the previous step.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#24-use-an-operator-to-collect-the-greetings-into-a-single-input","title":"2.4. Use an operator to collect the greetings into a single input","text":"<p>So we need an operator here to take this channel with three elements and collapse them to a single element, so that that final process only runs once.</p> <p>To do that, we're going to use the collect operator. I can do this directly within the workflow. I can do .out and chain onto an operator here at the end .collect.</p> <p>Hit save. And then for the purposes of this training, I'm also going to do some view operators like we did before, so we can take a look at this channel before and after we use the collect operator, so we can understand what's happening.</p> <p>I am going to take this channel, get rid of the collect and dot view greetings, and then I'm going to duplicate this line, add in the collect operator. And change that to after.</p> <p>This is separate to where we're calling this, but that's fine because we're using the same operator calls on the same output channel.</p> <p>Okay, let's hit save and let's try it out in the terminal. Going to lgoing nextflow run. Hello, workflow. Rerun our script.</p> <p>Okay. This is looking better. As before we can see the first two processes run three times and now our final process only ran once.</p> <p>If we look at what was printed by the view operator, down here, we said before collect, which is this output here, and that's printed three times. And you can see there's a single path for each one of those. And then after collect, you can see that we have this array of three paths. So that's as we expect.</p> <p>Okay, let's check the results file and see if it's what we expect this time. Sure enough, there are now three lines in the file - that's successfully concatenated these three outputs into a single output file. Fantastic.</p> <p>Okay, I am going to clean up and let's go on to the next step. And I'm going to delete these view statements just to keep things clean.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#3-pass-more-than-one-input-to-a-process-in-order-to-name-the-final-output-file-uniquely","title":"3. Pass more than one input to a process in order to name the final output file uniquely","text":"<p>Okay. So far, all of our processes have only taken a single input. We're now going to do an exercise where we add more than one input to a process to see how this works. To do this, we're going to use this collect greetings example.</p> <p>Each time I ran the workflow, it overwrote that file in the results directory, which may not be what we want.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#31-modify-the-collector-process-to-accept-a-user-defined-name-for-the-output-file","title":"3.1. Modify the collector process to accept a user-defined name for the output file","text":"<p>So for this example, we're going to pass an additional parameter so that we can customize the output file name.</p> <p>Adding a second input to a process is very simple. I just add a second line in the input block. This time it's going to be a value, rather than a path, because we want to pass a string and I'm going to call it batch underscore name.</p> <p>I can now use this variable in the script block, and I'm going to say collected dash dollar batch name.</p> <p>I'm using squiggly brackets here around the variable name. That's just to keep it separate from the rest of a string, and it probably isn't needed in this case, but I think it makes it easier to read.</p> <p>Okay. Finally, remember to update the output path because now the file name has changed, so I'm going to do the same thing and put the batch name into the output of path as expected.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#32-add-a-batch-command-line-parameter","title":"3.2. Add a batch command-line parameter","text":"<p>We now need to pass in a batch name from somewhere, and I'm going to create a second parameter to do this so that we can do it on the command line when we run the workflow.</p> <p>So I'm going to do params batch name, and by default, let's call this test batch. Now I can use this special parameters variable down, where we call the process.</p> <p>And sure enough VS Code is telling us that there's not enough arguments to this process now, and that it expects a second input.</p> <p>Simply do comma and pass our new variable and the error goes away.</p> <p>Note that the order of inputs here is really important. The first process input was the path, and the second input is the name. If I change the order here, I must also change the order when I call the process. Otherwise. Next, we will pass the wrong channel to the wrong input.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#33-run-the-workflow","title":"3.3. Run the workflow","text":"<p>Okay, let's try it and see if it works. Let's do \"nextflow run hello- workflow. Okay, it ran as before. Let's have a look in the results directory.</p> <p>Sure enough, our file name here is now called \" collected test batch output txt\". Fantastic.</p> <p>And now let's see if we can overwrite that by running again. This time I'm going to do --batch_name to match that special parameter variable name here. And I'm going to call it demo output.</p> <p>Run the workflow again and we'll see if something happens.</p> <p>Okay, we now we have a collected demo output .txt. And because this file name is different to that one, it hasn't overwritten it. Both are now present in the results directory.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#4-add-an-output-to-the-collector-step","title":"4. Add an output to the collector step","text":"<p>Okay, so there we showed giving multiple inputs to a process, but how about multiple outputs? For this example, we're going to calculate the number of greetings which are processed and output that as a secondary output for this collect greeting step.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#41-modify-the-process-to-count-and-output-the-number-of-greetings","title":"4.1. Modify the process to count and output the number of greetings","text":"<p>We're going to do a bit of a trick here. Nextflow processes have this script block with a multi-line string, and that is passed as bash output to the dot command dot sh. But we can actually write any custom code above that, and that will be executed as part of a task but not included within the bash script.</p> <p>One of the built-in functions in the Nextflow syntax is called size. So I'm going to take the path input, and I'm going to say count underscore greetings, just to define a variable name. I'm going to take the input files and I'm going to call \"size\" on it.</p> <p>This function will count the size of this input channel and assign it to a variable.</p> <p>We can now return that variable as part of the output block. So we say, val, because it is value, not a file. And count greetings.</p> <p>Now this is enough by itself, and we could now access these different outputs from this process. However, we'd have to access them in a positional manner. So using an index key such as zero and one.</p> <p>To make it a little bit easier to get at the outputs, we can name them and we do that by using an emit statement.</p> <p>So we do comma emit out file or whatever I want to call this. And I do here emit count. This is basically just a decorator, which just helps us write slightly cleaner code so that we can easily reference the specific outputs later in the workflow block.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#42-report-the-output-at-the-end-of-the-workflow","title":"4.2. Report the output at the end of the workflow","text":"<p>Okay. If I scroll down to the workflow block, I can now take the outputs of collect greetings, do collect greetings, dot out, and we can see our two named outputs are suggested here by the VS Code extension. Very handy.</p> <p>So I'm going to do dot count to get the count value that we've just created, and I'm going to do view, so that it prints it in the command line. So we can see it when we run the workflow.</p> <p>Let's write something in the closure here just to make it a bit nicer. num greetings, there were greetings greetings.</p> <p>And we don't actually care about the other output because we're not using that as an input for any other processes. But you can see how we could easily pass this as an input to another process if we wanted to, downstream.</p>"},{"location":"hello_nextflow/transcripts/03_hello_workflow/#43-run-the-workflow","title":"4.3. Run the workflow","text":"<p>We're going to click save. Let's have a look at the terminal and try it out.</p> <p>Okay, fantastic. Here we go. There are three greetings. That's exactly right.</p> <p>Okay, great stuff. That's the end of this chapter. We're all done for making it this far. You're now starting to build up quite a realistic workflow, where we're able to handle inputs and outputs and logic within our workflow.</p> <p>As these workflow files get longer, they start to become a little bit unwieldy. So in the next chapter, we'll look into how we can modularize Nextflow code into separate files so that it's easier to find and maintain the code within the workflow.</p> <p>Join us in the next video for chapter four. Hello Modules.</p> <p>Next video transcript </p>"},{"location":"hello_nextflow/transcripts/04_hello_modules/","title":"Part 4: Hello Modules - Transcript","text":"<p>Important notes</p> <p>This page shows the transcript only. For full step-by-step instructions, return to the course material.</p> <p>The section numbers shown in the transcript are provided for indicative purposes only and may not include all section numbers in the materials.</p>"},{"location":"hello_nextflow/transcripts/04_hello_modules/#welcome","title":"Welcome","text":"<p>Hi, welcome to Part Four of the Hello Nextflow training course.</p> <p>This chapter is called Hello Modules, and we'll be talking about how to modularize Nextflow code. What we're going to do is take our one workflow script and break it up into separate files.</p> <p>This makes the code easier to navigate and maintain as your workflow gets bigger, and also makes it possible to share modules between pipelines so that if you have multiple pipelines using the same tool, you only need to write that process once.</p> <p>A classic example of this is the nf-core modules repository, which has thousands of different tools in ready to go modules, which you can install and use in your workflow.</p> <p>Nextflow can also work with sub workflows, which are like modules, but they have multiple processes. That's outside the scope of this training, but it works in basically the same way.</p> <p>Okay. Let's take a look.</p> <p>As usual, start by going to training.nextflow.io.</p> <p>Go to \"Hello Nextflow\" in the sidebar, and we're doing part four: \"Hello Modules\".</p> <p>I'm now going to jump into my GitHub Code Spaces environment and take a look at the file \"hello- modules\".</p> <p>Just as before, we're starting off at the end point of the previous chapter, so this script should look familiar. We've got our three processes, say hello, convert to upper and collect greetings, and in a simple workflow, which runs these three commands and emits a message at the end. We have two parameters called greeting and the batch, which specifies the name, which is used for the collected output file at the end.</p>"},{"location":"hello_nextflow/transcripts/04_hello_modules/#0-warmup-run-hello-modulesnf","title":"0. Warmup: Run hello-modules.nf","text":"<p>We can verify that this workflow still works as we expect by doing nextflow run hello, modules.</p> <p>Great. It ran three tasks with each of this process, one collected task, and it told us that there are three greetings in this batch. If we go into results, we've got our different output files here, including the collected test, batch output.</p>"},{"location":"hello_nextflow/transcripts/04_hello_modules/#1-create-a-directory-to-store-modules","title":"1. Create a directory to store modules","text":"<p>Right. Let's do some modularization.</p> <p>It is generally a good idea to put modules into a subfolder in your pipeline repository, just to keep things tidy. You can call this whatever you want, but by convention we usually call it modules.</p> <p>So let's go ahead, go into a terminal and do make the modules. You can see it pop up in the sidebar and VS Code here.</p>"},{"location":"hello_nextflow/transcripts/04_hello_modules/#2-create-a-module-for-sayhello","title":"2. Create a module for sayHello()","text":"<p>I'm then going to create a new file for my first module. You can do \"touch\" or \"code\" or you can do this in the side bar, it really doesn't matter. So I'm going to do code modules and I'm going to name it after the process. So sayHello.nf . NF is a traditional file extension for Nextflow files.</p> <p>Going to hit save here and we can see our new module file turns up.</p>"},{"location":"hello_nextflow/transcripts/04_hello_modules/#22-move-the-sayhello-process-code-to-the-module-file","title":"2.2. Move the sayHello process code to the module file","text":"<p>Right, next I'm going to take the module code from the workflow. I'm also going to take the hash bang here and copy that in first so that it's clearly a Nextflow file. And then I'm going to take this process and I'm going to cut. So I'm going to remove it from my main workflow script and I'm going to paste it into this new module.</p> <p>That's all the content that this module file is going to contain. Just a single process, no workflow, no logic, just a process alone.</p> <p>I can now close this file.</p>"},{"location":"hello_nextflow/transcripts/04_hello_modules/#23-add-an-import-declaration-before-the-workflow-block","title":"2.3. Add an import declaration before the workflow block","text":"<p>Now my workflow is missing that first process, so we need to bring it back by importing it. The syntax for this is very similar to other programming languages, so it may feel familiar. We do include squiggly brackets, the name of the process, say hello, and then from the file path modules, say hello, nf. Fantastic.</p> <p>A couple of tricks here. The VS Code extension is clever about this. It recognizes this file path and you can hover over it and do follow link . Or I'm on Mac, I can do option click and it opens this file. So we can quickly jump to it.</p> <p>This process name is now being used by the workflow down here, and we can do the same thing here. It shows us a little bit of information about that process, and again, I can hold option, click on it, and it will open it in the editor.</p> <p>So it's a really fast way when you have lots of files for your different processes to quickly navigate around your code base in VS Code.</p> <p>Okay. That's basically it for this chapter. Now we just do the same thing again for the other processes.</p>"},{"location":"hello_nextflow/transcripts/04_hello_modules/#3-modularize-the-converttoupper-process","title":"3. Modularize the convertToUpper() process","text":"<p>So let's create a new file here. Call it Convert to upper nf. Again, copy the hash bang. And then cut the process out.</p> <p>Copy the process name there, include a new include statement with the new process name.</p>"},{"location":"hello_nextflow/transcripts/04_hello_modules/#4-modularize-the-collectgreetings-process","title":"4. Modularize the collectGreetings() process","text":"<p>And then do the same for the third process. New file, connect. Greetings,</p> <p>do the hash bang. Cut the process, paste the process, and do a new include statement.</p> <p>Now you can see here I've got an error underline here saying invalid include source. And this is actually a genuine error that I made because I was moving a bit too quickly. If you look closely, you can see I missed the T and convert to upper</p> <p>So VS Code very usefully has told me that I've made a mistake there. If I fix that file name, the error goes away. It's a good example of why the error checking within VS Code is so useful for writing Nextflow code. Otherwise I wouldn't have spotted that and I would've only found out much later when I tried to run the workflow.</p> <p>Our main pipeline script is now looking much simpler. It doesn't have any processes in, we just have three include statements and our workflow. We haven't changed any of the logic of the workflow. We haven't changed any of the process code, so hopefully it should work in exactly the same way.</p>"},{"location":"hello_nextflow/transcripts/04_hello_modules/#44-run-the-workflow-to-verify-that-it-does-the-same-thing-as-before","title":"4.4. Run the workflow to verify that it does the same thing as before","text":"<p>Let's check. I'm going to open a terminal and I'm going to run exactly the same command as before.</p> <p>Sure enough, it's run our processes, say hello, convert to upper collect greetings, and given us three greetings again.</p> <p>So we've moved our code around, but we haven't changed anything about how the workflow executes and it's completely unchanged. The only difference is we now have cleaner code, easier to maintain, and easier to share with others.</p> <p>And that's it. It was a short chapter. It's a simple concept, but it's very powerful and key to how we write more complex Nextflow workflows. So it's important that you understand it and get into the habit of using it.</p> <p>In the next chapter, we're going to have a bit of a change of pace and stop thinking so much about the syntax of writing Nextflow code, and think a little bit about how we use software in the processes themselves. Join us in part five for Hello Containers.</p> <p>Next video transcript </p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/","title":"Part 5: Hello Containers - Transcript","text":"<p>Important notes</p> <p>This page shows the transcript only. For full step-by-step instructions, return to the course material.</p> <p>The section numbers shown in the transcript are provided for indicative purposes only and may not include all section numbers in the materials.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#welcome","title":"Welcome","text":"<p>Hi, welcome to Part Five of the Hello Nextflow training course.</p> <p>This chapter's called Hello Containers. We're going to talk about how Nextflow integrates with tools such as Docker and Singularity to use software containers to provision software to the users of your pipeline.</p> <p>This means that when people run your pipeline, they don't have to go and install all the different tools themselves. Nextflow will do it for them.</p> <p>Containers are an extremely powerful technology and crucial in reproducibility and ease of use. We're going to start off by doing a brief introduction to containers themselves, running some docker commands manually, and then we'll take those same containers and put them into our Nextflow pipeline.</p> <p>Okay. Let's get started.</p> <p>So just as before, let's start off by loading up the training material. Go to training.nextflow.io. Hello Nextflow, Chapter Five, Hello Containers.</p> <p>I'm going to jump into my Codespaces environment and on the left here we see hello containers dot nf.</p> <p>Just as before, this is the same script that we finished previous chapter four on, so it should look familiar.</p> <p>We've got our command line parameters to specify the input file and the batch name. We are including our three modules, and we have our workflow where we run the three processes.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#0-warmup-run-hello-containersnf","title":"0. Warmup: Run hello-containers.nf","text":"<p>Feel free to run this workflow again and double check that it is producing the outputs that you expect. For now, I'm actually going to close it and dive into the terminal.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#1-use-a-container-manually","title":"1. Use a container 'manually'","text":"<p>To start off this chapter, we're going to do a bit of a recap over container technology. If you're very used to docker or singularity or other container techs, then treat this as a refresher , or feel free to skip over it completely.</p> <p>Nextflow supports many different types of container technologies. That includes Docker, Singularity, Podman, Shifter, Charliecloud, and more.</p> <p>In this training, we're going to be focusing on Docker. That comes pre-installed in the code spaces and is one of the most popular container technologies, especially if you're developing on your own computer or your own laptop.</p> <p>If you're working in an academic environment on a shared HPC, you might find that Singularity is available and not Docker. That's fine. All of the concepts are exactly the same. A few of the manual commands are different, but if you understand Docker, you'll also understand singularity.</p> <p>In fact, Singularity is also installed in the Code Spaces environment. So if you like, you can try and do the same tasks using Singularity instead of Docker.</p> <p>Okay, so what is container technology? The idea behind Docker is that it can fetch an image from a remote source. Pull it to your local machine and then create a container based on that image.</p> <p>This running container is a little bit like a virtual machine running on your computer. It's isolated from your environment, and it comes prepackaged with an operating system and a set of available software.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#11-pull-the-container-image","title":"1.1. Pull the container image","text":"<p>The syntax that we need to fetch a preexisting image is \"docker pull\". So I'm going to type that into my terminal, but now we need an image to play with.</p> <p>You can build images yourself. You can find them on public registries like Docker Hub or quay.io. But a really good way to get images quickly is using Seqera Containers.</p> <p>This is a free to use community service that we've built in 2024, which you can use without login or anything.</p> <p>If you go to seqera.io/containers or click containers at the top here, you're presented with a search interface and you can type in the name of any tool available in Conda or on the Python package Index.</p> <p>By default, it searches the Bioconda and Conda Forge channels, but you can prefix any Conda channel. I'm here if you want to.</p> <p>For a bit of fun, let's use cowpy. I'm going to type in cowpy. It gives me results from Python Package Index and Conda Forge. I'm going to click that to add it to my container. I could add multiple packages here if I wanted to. Select Docker, select linux/amd64, and click Get Container.</p> <p>This builds the image for me on demand if it hasn't already been created, and gives me a URL that I can copy.</p> <p>If you're interested, you can click view Build Details, and that takes you to a page, which shows the conda environment file that was used and the complete build log for the build, along with the security scan results.</p> <p>If I go back to my code spaces, I can now paste this container name and hit enter.</p> <p>Docker now downloads all the different layers within this container image, and now tells us that this image is available to use.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#pulling-a-singularity-image","title":"Pulling a Singularity image","text":"<p>If you are using singularity, the process is basically the same. We select our image packages, select cowpy. Now we choose Singularity instead of Docker and click Get Container. That gives us an image URL using oras://. Or if you prefer, you can use https:// by checking that box. Copy that URL. Now go to Code Spaces. We actually have Apptainer installed in this space, which is the same as Singularity, but they're aliased to one another. So I'm going to do apptainer pull and then I'm going to call it cowpy sif, but you can call it whatever you want. Paste URL. And that's going to download that image for me.</p> <p>I could do ls -lh and see cowpy.sif</p> <p>Singularity is different to Docker, that singularity stores all images in flat files, whereas Docker has a registry where it keeps all the layers separately on your host machine, and it has a running demon to keep track of all of that.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#12-use-the-container-to-run-cowpy-as-a-one-off-command","title":"1.2. Use the container to run cowpy as a one-off command","text":"<p>Okay, let's go back to Docker. We can now try running this image that we created by doing docker run.</p> <p>I'm going to do dash dash rm, which just does a one-off execution of the image. And I'm going to paste the image URL. And then finally, you finish this with a command that you want to run.</p> <p>The image we generated had cowpy installed, so let's try cowpy.</p> <p>There you go. It ran our command. I don't have cowpy installed locally. You can see if I try and run it, it doesn't exist. However, in this command, I ran it using Docker and it correctly generated this output.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#13-use-the-container-to-run-cowpy-interactively","title":"1.3. Use the container to run cowpy interactively","text":"<p>We can go further than this if we like and spin up a container interactively and look around inside. Again, I do \"docker run dash dash rm\". Now I'm going to do dash it, which tells Docker that we want an interactive terminal. I do the image URL again, and this time, instead of doing cowpy, I'm going to do bin bash because the command that we want to run is bash.</p> <p>This takes us into this running container and you can see that the prompt has changed now.</p> <p>If I do LS slash you can see the directories here are different.</p> <p>If I open a second terminal here on the right hand side, which is just running in GitHub Code Spaces and do LS slash, you see we have directories like workspaces and temp, whereas over here in Docker it's different.</p> <p>So this environment is completely separate within Docker and isolated from my host environment. That's a good thing, because that isolates the execution of this command into the Docker image and keeps it reproducible between different people on different host systems.</p> <p>If you want to use data from your host system within the Docker image, you have to explicitly mount that into the container.</p> <p>We're going to do that in a second.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#132-run-the-desired-tool-commands","title":"1.3.2. Run the desired tool command(s)","text":"<p>First though, let's see if we can run cowpy. There again, the command is available now directly on the command line, and we can start to do more complex things and pass arguments. Hello containers and instead of the cow, let's do the tux penguin. Let's see what else we have.</p> <p>Let's do cheese. Wonderful. How about Dragon and Cow? Pretty good.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#133-exit-the-container","title":"1.3.3. Exit the container","text":"<p>Okay. I can't do much more because I don't have any data in this container. So let's exit out this running image and see if we can mount some data into the container. I can do that by doing control D or typing exit. Okay, I am now back in my regular GitHub code space.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#134-mount-data-into-the-container","title":"1.3.4. Mount data into the container","text":"<p>In order to mount some data into the Docker container, I need to use dash V. So I'm going to take my previous docker command, go back to the start do dash v. I'm going to do \".\" for the current local working directory, and then a colon to say where that should be mounted in the host directory and do slash data. So that's mounting this particular directory into the container at slash data.</p> <p>Now if I do LS slash we can see we have a new directory called data, and if I do LS data, you can see all of the files that we have in the sidebar here. Fantastic.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#135-use-the-mounted-data","title":"1.3.5. Use the mounted data","text":"<p>Now we can start to use some of the files which are on the host system within the Docker image. So I can say cat data greetings csv. If you remember, this is our CSV file with our different greetings from before, and I can pipe that to cowpy. Fantastic. Now we're getting somewhere.</p> <p>Okay. That's enough for running Docker interactively. Hopefully you now have a feel for roughly what Docker is and how to use it both to run a command in a one-off way, and also to use an image interactively. If you are using singularity. The commands are all very similar except you do things like apptainer exec or apptainer run, or singularity exec or singularity run.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#2-use-containers-in-nextflow","title":"2. Use containers in Nextflow","text":"<p>Next we're going to go back to our Nextflow workflow and see how to use this technology within the Nextflow pipeline.</p> <p>Let's close the terminal and open up Hello Containers again.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#21-write-a-cowpy-module","title":"2.1. Write a cowpy module","text":"<p>To stick with our cowpy example, let's create a new process in our workflow, which uses cowpy. Let's go up to modules, create a new file and call it cowpy nf. I'm now going to cheat a little bit and copy in the code from the training material and hit save. And let's have a look.</p> <p>So this is a simple process. Hopefully now you understand what the building blocks of a process look like. We've got our publishDir again, going to results. We have two inputs, an input file and a string called character. We have an output cowpy input file, and we have a script which looks exactly the same as what we ran manually inside our docker image a second ago: cat to print a file, piping it onto cowpy, saying which type of cowpy character we want to use, and outputting that to the output file, which we pass as the output here.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#22-add-cowpy-to-the-workflow","title":"2.2. Add cowpy to the workflow","text":"<p>Okay, let's go back to our workflow, import this new process. So cowpy from modules cowpy nf. Let's create a new parameter so that we can specify which character we wanted. Let's say Turkey by default. And then let's call this new process at the end of the workflow,</p> <p>cowpy. And let's use the output here from Collect Greetings. So collect greetings out, out file here. And then we need a second argument, which is the new params that we've just made. params dot character.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#224-run-the-workflow-to-verify-that-it-works","title":"2.2.4. Run the workflow to verify that it works","text":"<p>Okay, let's see if our new process works. Nextflow run hello containers. This should run those first three processes and then try and run cowpy at the end.</p> <p>We've got an error. What it's saying here, cowpy had an error and it had an exit status 127 and sure enough, command sh cowpy command not found.</p> <p>We didn't tell Nextflow that we have a Docker image available for cowpy, so it tried to run it on our host system and we don't have cowpy installed on our host system, so it triggered an error.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#23-use-a-container-to-run-it","title":"2.3. Use a container to run it","text":"<p>So what we need to do is we need to tell Nextflow that we have a container available. Let's go to our cowpy process and we're going to add a new directive at the top of the process called container.</p> <p>We then find our image, copy the URL, and put that in a string.</p> <p>This isn't enough by itself because an X Flow pipeline can have several ways to specify software. I could also do conda conda-forge cowpy, for example. And Nextflow needs to know which of these technologies you want to use.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#232-enable-use-of-docker-via-the-nextflowconfig-file","title":"2.3.2. Enable use of Docker via the nextflow.config file","text":"<p>So in order to run with Docker enabled, we're going to get ahead of ourselves slightly and use the Nextflow config file, which is something we're going to cover in more detail in the next chapter. You can see in this directory that we have a file called Nextflow Config, and in here you already have docker.enabled False.</p> <p>We're going to change that to True to enable Docker, and then we can try and run the workflow again.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#233-run-the-workflow-with-docker-enabled","title":"2.3.3. Run the workflow with Docker enabled","text":"<p>Nextflow run hello containers nf and this time cowpy ran successfully. Let's look in Results. cowpy collected test and there's our Turkey. Wonderful.</p> <p>So in the background there, Nextflow knew that it had a container available for that process.</p> <p>It fetched the image and it ran the commands for us.</p>"},{"location":"hello_nextflow/transcripts/05_hello_containers/#234-inspect-how-nextflow-launched-the-containerized-task","title":"2.3.4. Inspect how Nextflow launched the containerized task","text":"<p>If you're curious, we can actually see exactly what it did by looking in the work directory. If I do code work, and then the hash and then command run, which if you remember is the actual file that's executed for that task, we can go in and we can look for a function called NXF launch. And here you can see the exact docker command that Nextflow used, which looks a lot like what we were doing manually in the terminal earlier. Docker run. Binding this host directory into the container, and then specifying the container URL.</p> <p>So there's no magic here. It's just that Nextflow is automatically doing the heavy lifting for you in a way that means you can easily specify containers in your pipeline, which are then readily available for anyone else to use who runs your workflow. And those people no longer have to think about managing software to run your analysis pipeline.</p> <p>Very, very simple, very convenient, and also really reproducible. Good all around.</p> <p>Okay, great work. That's the end of Chapter Five. Join us in the next video for part six, which is the final part of this Hello Nextflow training, where we'll talk about Nextflow configuration in more detail.</p> <p>See you in the next video.</p> <p>Next video transcript </p>"},{"location":"hello_nextflow/transcripts/06_hello_config/","title":"Part 6: Hello Config - Transcript","text":"<p>Important notes</p> <p>This page shows the transcript only. For full step-by-step instructions, return to the course material.</p> <p>The section numbers shown in the transcript are provided for indicative purposes only and may not include all section numbers in the materials.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#welcome","title":"Welcome","text":"<p>Hi, welcome to part six of the Hello Nextflow training course.</p> <p>This chapter is called Hello Config, and it's the final part of our training course.</p> <p>In this chapter, we're going to be talking about Nextflow configuration. Nextflow configuration is really powerful. It allows us to run the same pipeline on multiple different compute infrastructures with different software provisioning. and different options in the pipeline itself.</p> <p>This means that you can take Nextflow pipelines built by other people and run them on your system, even though they may have been built for an entirely different infrastructure. This ability to configure Nextflow makes workflows truly portable and shareable.</p> <p>In this chapter, we'll be using the workflow we've built in previous parts, but we're not going to edit the workflow code at all. We're just going to look at our Nextflow config file and see how changing the config alters the way that Nextflow runs.</p> <p>Okay, let's get started.</p> <p>Just as before, let's start off by going to training.nextflow.io. Go over to the left on Hello Nextflow and chapter six. Hello config. I'm now going to go into my GitHub code Spaces environment and double check the script that we'll be using.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#0-warmup-check-that-docker-is-enabled-and-run-the-hello-config-workflow","title":"0. Warmup: Check that Docker is enabled and run the Hello Config workflow","text":"<p>This one's called Hello Config, and it's starting off from where we were before. So looking exactly the same with our three parameters. Greetings for the CSV file, batch for the output batch name and the character for the cowpy name. We have our four imports of the different processes, and then we have a workflow where we chain them together.</p> <p>I am actually going to close this file now because we're not going to touch the Nextflow file at all in this chapter. We're going to work purely within configuration file. If I look into the Nextflow dot config file that we briefly looked at in the previous chapter five, we can see that we have a single statement here: Docker enabled equals true, which which is telling Nextflow to use Docker when it executes this workflow.</p> <p>I am using Nextflow dot config in the pipeline root here, which is loaded automatically when I run Nextflow. But remember, Nextflow can load config files from multiple places.</p> <p>If I check with Nextflow docs go to Configuration, you can see a list of these places and a priority which they load in.</p> <p>Okay. Let's check that our workflow is executing as we expect it to. Bring up a terminal. Do Nextflow. Run. Hello, config. And hit enter. We should have those four processes running, ending up with a cowpy command. Sure enough, this worked properly. I had Docker enabled it, pulled Docker and ran cowpy for me, just as it did at the end of chapter five.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#1-determine-what-software-packaging-technology-to-use","title":"1. Determine what software packaging technology to use","text":"<p>Okay. Let's say I'm running on an HPC and I don't have Docker installed. The best thing to do in this scenario would be to use Singularity or Apptainer. If I was going to do that, I'd go into the module cowpy and change this container to use the singularity image like I showed in the previous chapter, with an oras://, which you can also get from Seqera Containers.</p> <p>I'd then go to Nextflow dot config set Docker enabled to false and do singularity enabled equals true. Or, if using Apptainer, apptainer enabled equals true and that would work.</p> <p>Nextflow, it does support other technologies as well apart from containers though, something you might be familiar with is conda. Here we can do conda enabled equals true and set Docker to false. conda doesn't use the same container directive. Instead, we can add a new one here called conda. We then specify the conda package we want to use. It's good practice to be as specific as possible to try and make the pipeline as reproducible as possible. So I'm going to specify the conda channel, conda- forge, and then cowpy, and the exact version, which was 1.1.5.</p> <p>I could also just write cowpy if I wanted, but that might resolve to a different version of cowpy on different executions of the pipeline.</p> <p>The nice thing about this is that I haven't touched the docker directive at all. This Docker image is still there. I'm just providing two alternatives now, and these can be switched on or off by using a config file alone.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#13-run-the-workflow-to-verify-that-it-can-use-conda","title":"1.3. Run the workflow to verify that it can use Conda","text":"<p>Conda's now enabled, so let's try it out.</p> <p>Great. It's running and you can see there's a message from Nextflow here saying that Nextflow is creating a conda environment for me, and it's using this cache location.</p> <p>In the background, Nextflow is running \"conda create\" commands for me to create a new isolated conda environment with just the packages I want, and then installing and fetching those conda packages so that it can run the process.</p> <p>You can see it took a little bit of time there because it was creating the environment and installing the software for the first time. However, it's cached this environment, so if I run the same Nextflow command again, it should be a lot quicker because it will reuse the same conda environment.</p> <p>One of the cool things about this is that these directives can be specified at process level, not just the entire workflow. So if you want to, you can mix and match what technology is used for different processes.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#2-allocate-compute-resources-with-process-directives","title":"2. Allocate compute resources with process directives","text":"<p>The Nextflow configuration file can do a lot more than just software packaging. We can also tell Nextflow how to actually run the steps in the pipeline. One example is telling a host system what resources should be made available to each executing task.</p> <p>By default, Nextflow doesn't give very much. It gives a single CPU and only two gigabytes of memory to each process.</p> <p>This is probably something we'd want to change, so that processes which take a long time to run can have more resources and run more quickly, but it can be difficult to know what to allocate to a process. Nextflow has some nice tricks up its sleeve to help you with this.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#21-run-the-workflow-to-generate-a-resource-utilization-report","title":"2.1. Run the workflow to generate a resource utilization report","text":"<p>Let's run the workflow again. This time, I'm going to add an additional argument, which is dash with reports. It's a core Nextflow option, so it's a single hyphen. And then whatever file name I like. In this case, I'm going to call it report config one html.</p> <p>I'm going to run the workflow again. It's going to run exactly as before, but it's going to give me an additional helper report, which you can see has now popped up here in the sidebar.</p> <p>I am going to right click on this file, click download, which downloads it from GitHub code Spaces to my local system, so that I can easily then view it in the web browser up here.</p> <p>This report can be generated for any Nextflow run, and it has a lot of information in. It starts off at the top with some metadata about what command was used, when the workflow ran, how long it took, but as you scroll down, we get more detailed information about the resources, which were used by every step in the pipeline.</p> <p>Because each process runs multiple times for different tasks. We have a box plot showing the variation of the resources that we used for each process.</p> <p>If I scroll down a bit further, I see similar information about memory used and job duration. Also disk read write.</p> <p>You can imagine for a large pipeline with long running tasks, this can be very informative about how to fine tune the configuration of the resources which you're requesting so that you don't over request, but also so that you can provide enough that it runs quickly.</p> <p>If I keep scrolling down the report, we also see a task table, which shows us detailed information about every single task that was run in the workflow. This includes information such as the resolved script, which was run.</p> <p>Okay, let's go back to our config file. We saw that we really didn't need much for our workflow, so let's tell Nextflow that we only need one gigabyte of memory for every process in the workflow.</p> <p>Now when we define it like this at process level, this is applied to every single process in the pipeline.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#23-set-resource-allocations-for-an-individual-process","title":"2.3. Set resource allocations for an individual process","text":"<p>For the sake of argument, let's pretend that cowpy is really doing a lot of heavy lifting and it needs more resources than the other tasks. We can define an extra block of config here, which applies to just that process by using, with name cowpy.</p> <p>This is called a config selector, and we can define different patterns here to match different processes. For example, I could do cow star. I then follow that with some curly brackets and let's give it two gigabytes of memory instead of one and let's say two CPUs.</p> <p>Now Nextflow will be giving every process in the workflow one gigabyte apart from this request, which is more specific. So it overrides it. And just for any processes which are called cowpy, will get two gigs of memory and two CPUs.</p> <p>Note that Nextflow is clever about resource utilization. So if you start putting these numbers to higher values, you'll see that Nextflow starts to queue job submissions one after another, rather than running all of them in parallel, so that it doesn't over request the resources which are available.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#24-run-the-workflow-with-the-modified-configuration","title":"2.4. Run the workflow with the modified configuration","text":"<p>Let's try running a workflow again and let's save a new report this time.</p> <p>Okay, we can download this file and take a look.</p> <p>Yeah, unsurprisingly, it looks basically exactly the same because this is a dummy workflow, which is not doing anything real. But you can imagine how this iterative approach of defining limits and doing real life workflows with this kind of reporting allows you to do an evidence-based approach to setting appropriate configuration and really making the most of the computational resources that you have available to you.</p> <p>You can start to be really clever about this. Nextflow has a built-in ability to retry failures, and you can take advantage in your config file by using a closure like this and dynamically setting the resources which are made available. So here I've told Nextflow to multiply that two gigabyte by the retry attempt. So the second retry will get four gigs, the third retry will get six gigs and so on. This is a bit beyond the scope of this training course, but if you're interested, check out the Nextflow docs, which has a nice section about dynamic retry logic.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#25-add-resource-limits","title":"2.5. Add resource limits","text":"<p>Now, one thing you might notice about this is this kind of thing may make it quite easy to accidentally go beyond the resources available on your system. If you request more resources than are available Nextflow will throw an error about your configuration and halt the run. To avoid that, you can use something called resource limits.</p> <p>Under process scope, in our workflow, we can define resource limits like this, which takes an array, and we can specify the maximum memory CPUs and time which are available on this system.</p> <p>Setting high values here doesn't increase the amount of resources which are requested. We're still going to be using one gigabyte in our requests, but it means that if any of these requests get to 750, they'll hit that ceiling and nothing more than that will be requested, which means that Nextflow will continue to run and won't crash because of unavailable resources.</p> <p>So this is a nice safeguard to use, especially if you're using dynamic logic with your resource allocation.</p> <p>The other situation where this is really useful is if you're using pipelines which are public and not controlled by you. They might come with configuration defaults, and Nextflow will automatically take the right approach of thresholding any resource requests to run on your system.</p> <p>Okay, great. We've talked about software. We've talked about resource allocation, and we've described different scopes of config, both for all processes and specific processes.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#3-use-a-parameter-file-to-store-workflow-parameters","title":"3. Use a parameter file to store workflow parameters","text":"<p>Okay, next we're going to turn our attention to parameters. We can define parameters in the config file just as we did before in the Nextflow script. So params dot greeting equals hello or or use params scope and set foo equals bar.</p> <p>And that's great for setting defaults for your workflow. However, when you're running pipelines, it can be nice to specify parameters in a JSON or a YAML file.</p> <p>Using a file like this is much better than specifying command line options with dash dash. As when you run a workflow, you might have to specify many parameters and it can be tedious to write them all on a single CLI and error prone. Also, it is unlikely that you'll remember all the parameters that you used, so if you code that into a file, it's easier to launch the workflow again, using the same parameters in the future.</p> <p>We've got an example file here called test params, and you can see this specifies the three parameters we've got in our workflow with three different values. Personally, I find YAML easier to write than JSON. So just to demonstrate that it works, I'm going to create a new file called Test yaml and copy these in, get rid of the quotes. And hit save.</p> <p>These JSON and YAML files can be easier to write as they're more familiar syntax. But note that these are only for parameters and they only take key value syntax like this.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#31-run-the-workflow-using-a-parameter-file","title":"3.1. Run the workflow using a parameter file","text":"<p>Let's try it out. Do same command as before. Get rid of the report and I'm going to do dash params file test params yaml.</p> <p>No, this is a core Nextflow option, so it's a single hyphen.</p> <p>Okay. It ran the workflow and it used the parameters in that YAML file instead of me specifying them all on the command line. Might seem like overkill just for this simple example, but you can imagine if you have 10 or 20 different parameters, it can be a pain to type in manually, and this is just much easier to edit in a code editor and keep hold of for reproducibility sake.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#3-determine-what-executors-should-be-used-to-do-the-work","title":"3. Determine what executor(s) should be used to do the work","text":"<p>Okay. We've talked about software packaging with Docker and conda. We've talked about process resource requirements with CPUs and memory. And we talked a little bit about how to specify parameters when running workflows.</p> <p>The final parts of the configuration really is the execution, the underlying compute infrastructure itself, and this is the real jewel in the crown of Nextflow: that we can run these same workflow across multiple different compute infrastructures.</p> <p>I'm actually going to switch over to the written training material for a second. Under this part of the training, we can see a few different examples of how different executors, in this case, HPC schedulers, define the resource requirements needed to submit a job.</p> <p>So for Slurm, you have these SBATCH headers, which define dash dash mem and the CPU number. If you're using PBS, you have different headers, and if you use Grid Engine, you have different headers again.</p> <p>You can imagine it's even more different if you want to run on the cloud, be it AWS batch, Google Cloud, Azure, or more.</p> <p>Each of these underlying compute infrastructures is called an executor and Nextflow knows how to talk to all of these different executors in order to submit jobs with the correct syntax.</p> <p>The good news is you don't have to know about this. All you have to do is tell Nextflow, which executor to use.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#31-targeting-a-different-backend","title":"3.1. Targeting a different backend","text":"<p>We go back to our config file and the process we do executor, and I'm going to type local.</p> <p>Local's actually the default, if you don't specify any other executor, local is what will be used, and that just means your host system, wherever you launched Nextflow,</p> <p>I could specify instead, Slurm. And that would submit Slurm jobs, or I could say AWS batch, and that would submit jobs to AWS batch.</p> <p>You need some additional configuration in some cases, for example, running on cloud will need certain credentials, but really this is the core of it, and it can be as simple as one or two lines of config to run your workflow in a completely different compute environment.</p> <p>Even though we are running on a simple system within code spaces, I can still play around with this a bit and pretend that we're running on Slurm. If I then launch the workflow again, Nextflow run, hello config. It will fail because it won't be able to submit jobs to Slurm. But we can still go into the work directories and see what Nextflow did. So if we go to this work directory and look at Command Run. You can see at the top of this file, we now have these sbatch header lines, which tried to specify the resources needed for the Slurm job.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#4-use-profiles-to-select-preset-configurations","title":"4. Use profiles to select preset configurations","text":"<p>Okay, we're nearly there. Final part of this chapter is talking about configuration profiles. If you're running your pipeline on several different systems, it could be annoying to have all these different Nextflow config files, which you need to specify every time.</p> <p>Instead, you can encode groupings of configuration within your Nextflow config file, and switch those groups on and off by using a profile flag. Let's see how that looks.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#41-create-profiles-for-switching-between-local-development-and-execution-on-hpc","title":"4.1. Create profiles for switching between local development and execution on HPC","text":"<p>We're going to create two profiles in our example here, one for my laptop and one for a heavier HPC system. I'm going to cheat a little bit and just copy the code from the training material and put it in here.</p> <p>We have a new scope called profiles, and then we have a name for each profile, which can be anything. And within that we have configuration, which looks exactly the same as the top level config that we already wrote. So again, we have process scope. Docker scope.</p> <p>On the profile called my laptop. I'm saying to run using the local executor, so on my host system and to use Docker.</p> <p>On the university HPC profile here I'm saying to use Slurm to submit jobs, to use conda instead of Docker, and I'm specifying different resource limits, which may match for system size of a nodes on the HPC I'm using.</p> <p>By default, none of this configuration will be used when I run Nextflow, I have to specify that I want to use one of these profiles.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#42-run-the-workflow-with-a-profile","title":"4.2. Run the workflow with a profile","text":"<p>Let's do nextflow run hello config. And I'm going to do dash profile, single hyphen because it's a core Nextflow option. And then the name I gave it, which is my laptop. Nextflow should now use the block of config that was specified within that configuration profile, and apply it when it runs Nextflow. If I wanted to use the other config block, I just have to switch that profile name. Much easier to remember. Much easier to use.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#43-create-a-test-profile","title":"4.3. Create a test profile","text":"<p>Note, the profiles can have any kind of configuration, so it doesn't have to be related to your execution environment. For example, let's create a new profile here, which has a set of parameters. We can change this to tux and change to my profile, and now when we do profile test, it's going to specify these parameters, which will overwrite the parameters which are specified at the top level of the workflow.</p> <p>When you run Nextflow, you can chain multiple profiles and they'll be applied in sequence.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#44-run-the-workflow-locally-with-the-test-profile","title":"4.4. Run the workflow locally with the test profile","text":"<p>So I can take the previous command and do comma test. That will apply the, my laptop config first, and then it will apply the test config. If there's any overlap, then the profile on the right will overwrite any configuration in previous profiles. If I hit enter, let's see what happens.</p> <p>Okay, we've got a new results file here. You can see the My Profile, which I specified as one of the options. And we can also see cowpy, my profile, and sure enough, there's tux. So that's worked.</p>"},{"location":"hello_nextflow/transcripts/06_hello_config/#wrap-up","title":"Wrap up","text":"<p>Okay! Amazing. That's it. You've made it to the end of the course. You get a little bit of celebration confetti. Well done for finishing this chapter.</p> <p>Next video transcript </p>"},{"location":"hello_nextflow/transcripts/07_next_steps/","title":"Next Steps - Transcript","text":"<p>Important notes</p> <p>This page shows the transcript only. For full step-by-step instructions, return to the course material.</p>"},{"location":"hello_nextflow/transcripts/07_next_steps/#welcome","title":"Welcome","text":"<p>Congratulations, you've done it. You've completed the first Nextflow training course, called Hello Nextflow</p> <p>well done. Thank you for sticking through it and we really appreciate the time and effort that you've put into learning Nextflow, and we really hope that it'll be useful for your work.</p>"},{"location":"hello_nextflow/transcripts/07_next_steps/#next-steps","title":"Next Steps","text":"<p>For next steps, keep an eye out on the training portal: training.nextflow.io. We're putting new course material up there all the time and refreshing it as well. So you might be able to find more advanced training, or training which is specific to a field of research that you're interested in.</p> <p>Specifically, check out the Nextflow for Science page. This has a series of short courses which are standalone, and they extend what you've learned in Hello Nextflow to specific use cases.</p> <p>There's one for Genomics and also one for RNA-seq. We hope to bring more soon.</p>"},{"location":"hello_nextflow/transcripts/07_next_steps/#side-quests","title":"Side Quests","text":"<p>There are a lot of things that we could have talked about in Hello Nextflow, which would've been too much detail. Some of these things we're putting into Side Quests, which are short courses on specific topics.</p> <p>There's also the larger fundamentals training and advanced training courses, which may include content that you find interesting.</p>"},{"location":"hello_nextflow/transcripts/07_next_steps/#nf-core","title":"nf-core","text":"<p>It's been mentioned once or twice in this course, but definitely check out the nf-core project. There are over a hundred pipelines there for different types of data, so it's entirely possible that you may not need to build your own pipeline.</p> <p>There are also nearly one and a half thousand process modules where if you use the nf-core developer tooling, you can create a pipeline and import those modules in a matter of seconds.</p>"},{"location":"hello_nextflow/transcripts/07_next_steps/#seqera-platform","title":"Seqera Platform","text":"<p>Lastly, a quick plug for Seqera Platform. This is without doubt the best way to run Nextflow in practice, it's a cloud-based platform, but you attach your own compute infrastructure, be it your own cloud account on AWS, Google Batch, or Azure, or even your own HPC. The free tier is available to everyone , and if you're an academic, you can apply to our academic program for free, pro-level access.</p> <p>Seqera Platform goes beyond just a graphical interface for launching and monitoring workflows. There's also additional tooling such as Data Studios for running interactive sessions, and fundamental tooling such as Fusion, which gives faster and cheaper access to data over the cloud.</p>"},{"location":"hello_nextflow/transcripts/07_next_steps/#support-and-events","title":"Support and events","text":"<p>Remember, if you ever hit any trouble, just go to community.seqera.io. Our forum there is really active, the Nextflow community is super strong and there's nearly always people on hand ready to help out, be it for training or any thing to do with your day-to-day usage of Nextflow.</p> <p>And of course, a great next step is to join one of our community events, whether that's an nf-core hackathon or one of the Nextflow Summit events. They're great fun, and it'd be really nice to meet you there and chat about what you're using Nextflow for.</p>"},{"location":"hello_nextflow/transcripts/07_next_steps/#thank-yous","title":"Thank yous","text":"<p>I'd like to give a huge thank you to everyone who's been involved in writing this training material. I've been presenting it, but really all the hard work has been done by the training team at Seqera. Specifically Geraldine, who's put a huge amount of work into rewriting this material.</p> <p>Also, Marcel, Ken, Adam, John, others from the scientific development team and others in the community.</p>"},{"location":"hello_nextflow/transcripts/07_next_steps/#feedback-survey","title":"Feedback survey","text":"<p>Now that you've finished the course, we'd love to know what you thought of it. On training.nextflow.io, you'll find a feedback survey underneath the Hello Nextflow section.</p> <p>There's only four questions but it's really important for us. If nothing else, it tells us roughly how many people are doing the training. It also tells us whether you liked it and if you have any suggestions, please drop them in at the end. We read every single submission.</p> <p>If you spot any errors everything is open source on GitHub, so you can create an issue or make a pull request or drop us message in the forum. We'd love to hear what you thought of it and how we could improve it. Thanks again. Hope to see you soon.</p>"},{"location":"hello_nf-core/","title":"Hello nf-core","text":"<p>nf-core is a community effort to develop and maintain a curated set of scientific pipelines built using Nextflow, as well as relevant tooling and guidelines that promote open development, testing, and peer review.</p> <p></p> <p>These pipelines are designed to be modular, scalable, and portable, allowing researchers to easily adapt and execute them using their own data and compute resources. The best practices guidelines enforced by the project further ensure that the pipelines are robust, well-documented, and validated against real-world datasets. This helps to increase the reliability and reproducibility of scientific analyses and ultimately enables researchers to accelerate their scientific discoveries.</p> <p>During this training, you will be introduced to nf-core in a series of hands-on exercises.</p> <p>Additional information: You can learn more about the project's origins and governance at https://nf-co.re/about.</p> <p>Reference publication: nf-core is published in Nature Biotechnology: Nat Biotechnol 38, 276\u2013278 (2020). Nature Biotechnology. An updated preprint is available at bioRxiv.</p> <p>Let's get started! Click on the \"Open in GitHub Codespaces\" button below to launch the training environment (preferably in a separate tab), then read on while it loads.</p> <p></p>"},{"location":"hello_nf-core/#learning-objectives","title":"Learning objectives","text":"<p>You will learn to use and develop nf-core compatible modules and pipelines, and utilize nf-core tooling effectively.</p> <p>By the end of this training, you will be able to:</p> <ul> <li>Find and run nf-core pipelines</li> <li>Describe the code structure and project organization of nf-core pipelines</li> <li>Create a basic nf-core compatible pipeline from a template</li> <li>Convert basic Nextflow modules to nf-core compatible modules</li> <li>Manage inputs and parameters using nf-core tooling</li> <li>Add nf-core modules to an nf-core compatible pipeline</li> </ul>"},{"location":"hello_nf-core/#audience-prerequisites","title":"Audience &amp; prerequisites","text":"<p>This is a general-purpose training for learners who have at least basic Nextflow skills and wish to level up to using nf-core.</p> <p>Prerequisites</p> <ul> <li>A GitHub account OR a local installation as described here.</li> <li>Experience with command line and basic scripting.</li> <li>Completed Hello Nextflow or equivalent.</li> </ul>"},{"location":"hello_nf-core/00_orientation/","title":"Orientation","text":""},{"location":"hello_nf-core/00_orientation/#github-codespaces","title":"GitHub Codespaces","text":"<p>The GitHub Codespaces environment contains all the software, code and data necessary to work through this training course, so you don't need to install anything yourself. However, you do need a (free) GitHub account to log in, and you should take a few minutes to familiarize yourself with the interface.</p> <p>If you have not yet done so, please go through the Environment Setup mini-course before going any further.</p> <p>Warning</p> <p>This training is designed for nf-core tools version 3.2.1, which should be the version installed in the codespace. If you use a different version of nf-core tooling you may have difficulty following along.</p> <p>You can check what version is installed using the command<code>nf-core --version</code>.</p>"},{"location":"hello_nf-core/00_orientation/#working-directory","title":"Working directory","text":"<p>Throughout this training course, we'll be working in the <code>hello-nf-core/</code> directory.</p> <p>Change directory now by running this command in the terminal:</p> <pre><code>cd hello-nf-core/\n</code></pre> <p>Tip</p> <p>If for whatever reason you move out of this directory, you can always use the full path to return to it, assuming you're running this within the Github Codespaces training environment:</p> <pre><code>cd /workspaces/training/hello-nf-core\n</code></pre> <p>Now let's have a look at the contents of this directory.</p>"},{"location":"hello_nf-core/00_orientation/#materials-provided","title":"Materials provided","text":"<p>You can explore the contents of this directory by using the file explorer on the left-hand side of the training workspace. Alternatively, you can use the <code>tree</code> command.</p> <p>Throughout the course, we use the output of <code>tree</code> to represent directory structure and contents in a readable form, sometimes with minor modifications for clarity.</p> <p>Here we generate a table of contents to the second level down:</p> <pre><code>tree . -L 2\n</code></pre> <p>If you run this inside <code>hello-nf-core</code>, you should see the following output:</p> Directory contents<pre><code>.\n\u251c\u2500\u2500 greetings.csv\n\u251c\u2500\u2500 original-hello\n\u2502   \u251c\u2500\u2500 hello.nf\n\u2502   \u251c\u2500\u2500 modules\n\u2502   \u2514\u2500\u2500 nextflow.config\n\u2514\u2500\u2500 solutions\n    \u2514\u2500\u2500 composable-hello\n\n4 directories, 3 files\n</code></pre> <p>Here's a summary of what you should know to get started:</p> <ul> <li> <p>The <code>greetings.csv</code> file is a CSV containing some minimal columnar data we use for testing purposes.</p> </li> <li> <p>The <code>original-hello</code> directory contains a copy of the source code produced by working through the complete Hello Nextflow training series (with Docker enabled).</p> </li> <li> <p>The <code>solutions</code> directory contains the completed workflow scripts that result from each step of the course.   They are intended to be used as a reference to check your work and troubleshoot any issues.</p> </li> </ul> <p>Now, to begin the course, click on the arrow in the bottom right corner of this page.</p>"},{"location":"hello_nf-core/01_run_demo/","title":"Part 1: Run a demo pipeline","text":"<p>In this first part of the Hello nf-core training course, we show you how to find and try out an nf-core pipeline, understand how the code is organized, and recognize how it differs from plain Nextflow code as shown in Hello Nextflow.</p> <p>We are going to use a pipeline called nf-core/demo that is maintained by the nf-core project as part of its inventory of pipelines for demonstrating code structure and tool operations.</p> <p>Make sure you are in the <code>hello-nf-core/</code> directory as instructed in the Orientation.</p>"},{"location":"hello_nf-core/01_run_demo/#1-find-and-retrieve-the-nf-coredemo-pipeline","title":"1. Find and retrieve the nf-core/demo pipeline","text":"<p>Let's start by locating the nf-core/demo pipeline on the project website at nf-co.re, which centralizes all information such as: general documentation and help articles, documentation for each of the pipelines, blog posts, event announcements and so forth.</p>"},{"location":"hello_nf-core/01_run_demo/#11-find-the-pipeline-on-the-website","title":"1.1. Find the pipeline on the website","text":"<p>In your web browser, go to https://nf-co.re/pipelines/ and type <code>demo</code> in the search bar.</p> <p></p> <p>Click on the pipeline name, <code>demo</code>, to access the pipeline details page.</p> <p>Each released pipeline has a dedicated page that includes the following documentation sections:</p> <ul> <li>Introduction: An introduction and overview of the pipeline</li> <li>Usage: Descriptions of how to execute the pipeline</li> <li>Parameters: Grouped pipeline parameters with descriptions</li> <li>Output: Descriptions and examples of the expected output files</li> <li>Results: Example output files generated from the full test dataset</li> <li>Releases &amp; Statistics: Pipeline version history and statistics</li> </ul> <p>Whenever you are considering adopting a new pipeline, you should read the pipeline documentation carefully first to understand what it does and how it should be configured before attempting to run it.</p> <p>Have a look now and see if you can find out:</p> <ul> <li>which tools the pipeline will run (Check the tab: <code>Introduction</code>)</li> <li>which inputs and parameters the pipeline accepts or requires (Check the tab: <code>Parameters</code>)</li> <li>what are the outputs produced by the pipeline (Check the tab: <code>Output</code>)</li> </ul> <p>The <code>Introduction</code> tab provides an overview of the pipeline, including a visual representation (called a subway map) and a list of tools that are run as part of the pipeline.</p> <p></p> <ol> <li>Read QC (FASTQC)</li> <li>Adapter and quality trimming (SEQTK_TRIM)</li> <li>Present QC for raw reads (MULTIQC)</li> </ol> <p>The documentation also provides an example input file (see below) and an example command line.</p> <pre><code>nextflow run nf-core/demo \\\n -profile &lt;docker/singularity/.../institute&gt; \\\n --input samplesheet.csv \\\n --outdir &lt;OUTDIR&gt;\n</code></pre> <p>You'll notice that the example command does NOT specify a workflow file, just the reference to the pipeline repository, <code>nf-core/demo</code>.</p> <p>When invoked this way, Nextflow will assume that the code is organized in a certain way. Let's retrieve the code so we can examine this structure.</p>"},{"location":"hello_nf-core/01_run_demo/#12-retrieve-the-pipeline-code","title":"1.2. Retrieve the pipeline code","text":"<p>Once we've determined the pipeline appears to be suitable for our purposes, we're going to want to try it out. Fortunately Nextflow makes it easy to retrieve pipeline from correctly-formatted repositories without having to download anything manually.</p> <p>Return to your terminal and run the following:</p> <pre><code>nextflow pull nf-core/demo\n</code></pre> <p>Nextflow will <code>pull</code> the pipeline code, meaning it will download the full repository to your local drive.</p> Output<pre><code>Checking nf-core/demo ...\n downloaded from https://github.com/nf-core/demo.git - revision: 04060b4644 [master]\n</code></pre> <p>To be clear, you can do this with any Nextflow pipeline that is appropriately set up in GitHub, not just nf-core pipelines. However nf-core is the largest open-source collection of Nextflow pipelines.</p> <p>You can get Nextflow to give you a list of what pipelines you have retrieved in this way:</p> <pre><code>nextflow list\n</code></pre> Output<pre><code>nf-core/demo\n</code></pre> <p>You'll notice that the files are not in your current work directory. By default, they are saved to <code>$NXF_HOME/assets</code>.</p> <pre><code>tree -L 2 $NXF_HOME/assets/\n</code></pre> Output<pre><code>/workspaces/.nextflow/assets/\n\u2514\u2500\u2500 nf-core\n    \u2514\u2500\u2500 demo\n</code></pre> <p>Note</p> <p>The full path may differ on your system if you're not using our training environment.</p> <p>The location of the downloaded source code is intentionally 'out of the way' on the principle that these pipelines should be used more like libraries than code that you would directly interact with.</p> <p>However, for the purposes of this training, we'd like to be able to poke around and see what's in there. So to make that easier, let's create a symbolic link to that location from our current working directory.</p> <pre><code>ln -s $NXF_HOME/assets pipelines\n</code></pre> <p>This creates a shortcut that makes it easier to explore the code we just downloaded.</p> <pre><code>tree -L 2 pipelines\n</code></pre> Output<pre><code>pipelines\n\u2514\u2500\u2500 nf-core\n    \u2514\u2500\u2500 demo\n</code></pre> <p>Now we can more easily peek into the source code as needed.</p> <p>But first, let's try running our first nf-core pipeline!</p>"},{"location":"hello_nf-core/01_run_demo/#takeaway","title":"Takeaway","text":"<p>You now know how to find a pipeline via the nf-core website and retrieve a local copy of the source code.</p>"},{"location":"hello_nf-core/01_run_demo/#whats-next","title":"What's next?","text":"<p>Learn how to try out an nf-core pipeline with minimal effort.</p>"},{"location":"hello_nf-core/01_run_demo/#2-try-out-the-pipeline-with-its-test-profile","title":"2. Try out the pipeline with its test profile","text":"<p>Conveniently, every nf-core pipeline comes with a test profile. This is a minimal set of configuration settings for the pipeline to run using a small test dataset hosted in the nf-core/test-datasets repository. It's a great way to quickly try out a pipeline at small scale.</p>"},{"location":"hello_nf-core/01_run_demo/#21-examine-the-test-profile","title":"2.1. Examine the test profile","text":"<p>It's good practice to check what a pipeline's test profile specifies before running it. The <code>test</code> profile for <code>nf-core/demo</code> is shown below:</p> conf/test.config<pre><code>/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    Nextflow config file for running minimal tests\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    Defines input files and everything required to run a fast and simple pipeline test.\n\n    Use as follows:\n        nextflow run nf-core/demo -profile test,&lt;docker/singularity&gt; --outdir &lt;OUTDIR&gt;\n\n----------------------------------------------------------------------------------------\n*/\n\nprocess {\n    resourceLimits = [\n        cpus: 4,\n        memory: '15.GB',\n        time: '1.h'\n    ]\n}\n\nparams {\n    config_profile_name        = 'Test profile'\n    config_profile_description = 'Minimal test dataset to check pipeline function'\n\n    // Input data\n    input  = 'https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/samplesheet/samplesheet_test_illumina_amplicon.csv'\n\n}\n</code></pre> <p>This tells us that the <code>nf-core/demo</code> test profile already specifies the input parameter, so you don't have to provide any input yourself. However, the <code>outdir</code> parameter is not included in the test profile, so we will have to add it to the execution command using the <code>--outdir</code> flag.</p>"},{"location":"hello_nf-core/01_run_demo/#22-run-the-pipeline","title":"2.2. Run the pipeline","text":"<p>Our examination of the test profile above told us what pipeline argument(s) we need to specify: just <code>--outdir</code>.</p> <p>We're also going to specify <code>-profile docker,test</code>, which by nf-core convention enables the use of Docker containers, and of course, invokes the test profile.</p> <p>Let's try it!</p> <pre><code>nextflow run nf-core/demo -profile docker,test --outdir demo-results\n</code></pre> <p>Here's the console output from the pipeline:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `https://github.com/nf-core/demo` [maniac_jones] DSL2 - revision: 04060b4644 [master]\n\n\n------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/demo 1.0.1\n------------------------------------------------------\nInput/output options\n  input                     : https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/samplesheet/samplesheet_test_illumina_amplicon.csv\n  outdir                    : results\n\nInstitutional config options\n  config_profile_name       : Test profile\n  config_profile_description: Minimal test dataset to check pipeline function\n\nCore Nextflow options\n  revision                  : master\n  runName                   : maniac_jones\n  containerEngine           : docker\n  launchDir                 : /workspaces/training/side-quests/nf-core/nf-core-demo\n  workDir                   : /workspaces/training/side-quests/nf-core/nf-core-demo/work\n  projectDir                : /workspaces/.nextflow/assets/nf-core/demo\n  userName                  : gitpod\n  profile                   : docker,test\n  configFiles               :\n\n!! Only displaying parameters that differ from the pipeline defaults !!\n------------------------------------------------------* The pipeline\n  https://doi.org/10.5281/zenodo.12192442\n\n* The nf-core framework\n    https://doi.org/10.1038/s41587-020-0439-x\n\n* Software dependencies\n    https://github.com/nf-core/demo/blob/master/CITATIONS.md\n\nexecutor &gt;  local (7)\n[3c/a00024] NFC\u2026_DEMO:DEMO:FASTQC (SAMPLE2_PE) | 3 of 3 \u2714\n[94/d1d602] NFC\u2026O:DEMO:SEQTK_TRIM (SAMPLE2_PE) | 3 of 3 \u2714\n[ab/460670] NFCORE_DEMO:DEMO:MULTIQC           | 1 of 1 \u2714\n-[nf-core/demo] Pipeline completed successfully-\nCompleted at: 05-Mar-2025 09:46:21\nDuration    : 1m 54s\nCPU hours   : (a few seconds)\nSucceeded   : 7\n</code></pre> <p>You see that there is more console output than when you run a basic Netxflow pipeline. There's a header that includes a summary of the pipeline's version, inputs and outputs, and a few elements of configuration.</p> <p>Moving on to the execution output, let's have a look at the lines that tell us what processes were run:</p> Output (subset)<pre><code>[3c/a00024] NFC\u2026_DEMO:DEMO:FASTQC (SAMPLE2_PE) | 3 of 3 \u2714\n[94/d1d602] NFC\u2026O:DEMO:SEQTK_TRIM (SAMPLE2_PE) | 3 of 3 \u2714\n[ab/460670] NFCORE_DEMO:DEMO:MULTIQC           | 1 of 1 \u2714\n</code></pre> <p>This tells us that three processes were run, corresponding to the three tools shown in the pipeline documentation page on the nf-core website: FASTQC, SEQTK_TRIM and MULTIQC.</p> <p>Note</p> <p>The full process names as shown here, such as <code>NFCORE_DEMO:DEMO:MULTIQC</code>, are longer than what you may have seen in the introductory Hello Nextflow material. These includes the names of their parent workflows and reflect the modularity of the pipeline code. We will go into more detail about that shortly.</p>"},{"location":"hello_nf-core/01_run_demo/#23-examine-the-pipelines-outputs","title":"2.3. Examine the pipeline's outputs","text":"<p>Finally, let's have a look at the <code>demo-results</code> directory produced by the pipeline.</p> <pre><code>tree -L 2 demo-results\n</code></pre> Output<pre><code>demo-results/\n\u251c\u2500\u2500 fastqc\n\u2502   \u251c\u2500\u2500 SAMPLE1_PE\n\u2502   \u251c\u2500\u2500 SAMPLE2_PE\n\u2502   \u2514\u2500\u2500 SAMPLE3_SE\n\u251c\u2500\u2500 fq\n\u2502   \u251c\u2500\u2500 SAMPLE1_PE\n\u2502   \u251c\u2500\u2500 SAMPLE2_PE\n\u2502   \u2514\u2500\u2500 SAMPLE3_SE\n\u251c\u2500\u2500 multiqc\n\u2502   \u251c\u2500\u2500 multiqc_data\n\u2502   \u251c\u2500\u2500 multiqc_plots\n\u2502   \u2514\u2500\u2500 multiqc_report.html\n\u2514\u2500\u2500 pipeline_info\n    \u251c\u2500\u2500 execution_report_2025-03-05_09-44-26.html\n    \u251c\u2500\u2500 execution_timeline_2025-03-05_09-44-26.html\n    \u251c\u2500\u2500 execution_trace_2025-03-05_09-44-26.txt\n    \u251c\u2500\u2500 nf_core_pipeline_software_mqc_versions.yml\n    \u251c\u2500\u2500 params_2025-03-05_09-44-29.json\n    \u2514\u2500\u2500 pipeline_dag_2025-03-05_09-44-26.html\n</code></pre> <p>If you're curious about the specifics what that all means, check out the nf-core/demo pipeline documentation page.</p> <p>At this stage, what's important to observe is that the results are organized by module, and there is additionally a directory called <code>pipeline_info</code> containing various timestamped reports about the pipeline execution. This is standard for nf-core pipelines.</p> <p>Congratulations! You have just run your first nf-core pipeline.</p>"},{"location":"hello_nf-core/01_run_demo/#takeaway_1","title":"Takeaway","text":"<p>You know how to run an nf-core pipeline using its built-in test profile.</p>"},{"location":"hello_nf-core/01_run_demo/#whats-next_1","title":"What's next?","text":"<p>Learn how the pipeline code is organized.</p>"},{"location":"hello_nf-core/01_run_demo/#3-examine-the-pipeline-code-structure","title":"3. Examine the pipeline code structure","text":"<p>The nf-core project enforces strong guidelines for how pipelines are structured, and how the code is organized, configured and documented.</p> <p>Let's have a look at how the pipeline code is organized in the <code>nf-core/demo</code> repository (using the <code>pipelines</code> symlink we created earlier). You can either use <code>tree</code> or use the file explorer in your IDE.</p> <pre><code>tree -L 1 pipelines/nf-core/demo\n</code></pre> Output (top-level only)<pre><code>pipelines/nf-core/demo\n\u251c\u2500\u2500 assets\n\u251c\u2500\u2500 CHANGELOG.md\n\u251c\u2500\u2500 CITATIONS.md\n\u251c\u2500\u2500 CODE_OF_CONDUCT.md\n\u251c\u2500\u2500 conf\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 main.nf\n\u251c\u2500\u2500 modules\n\u251c\u2500\u2500 modules.json\n\u251c\u2500\u2500 nextflow_schema.json\n\u251c\u2500\u2500 nextflow.config\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 subworkflows\n\u251c\u2500\u2500 tower.yml\n\u2514\u2500\u2500 workflows\n</code></pre> <p>There's a lot going on in there, so we'll tackle this in stages. We're going to look at the following categories:</p> <ol> <li>Pipeline code components (<code>main.nf</code>, <code>workflows</code>, <code>subworkflows</code>, <code>modules</code>)</li> <li>Configuration, parameters and inputs</li> <li>Documentation and related assets</li> </ol> <p>Let's start with the code proper, though note that for now, we're going to focus on how everything is organized, without looking at the actual code just yet.</p>"},{"location":"hello_nf-core/01_run_demo/#31-pipeline-code-components","title":"3.1. Pipeline code components","text":"<p>The pipeline code organization follows a modular structure that is designed to maximize code reuse.</p> <p>Note</p> <p>We won't go over the actual code for how these modular components are connected, because there is some additional complexity associated with the use of subworkflows that can be confusing, and understanding that is not necessary at this stage of the training. For now, we're going to focus on the logic of this modular organization.</p>"},{"location":"hello_nf-core/01_run_demo/#311-overall-organization-and-mainnf-script","title":"3.1.1. Overall organization and <code>main.nf</code> script","text":"<p>At the top level, there is the <code>main.nf</code> script, which is the entrypoint Nextflow starts from when we execute <code>nextflow run nf-core/demo</code>. That means when you run <code>nextflow run nf-core/demo</code> to run the pipeline, Nextflow automatically finds and executes the <code>main.nf</code> script, and everything else will flow from there.</p> <p>In practice, the <code>main.nf</code> script calls the actual workflow of interest, stored inside the <code>workflows</code> folder, called <code>demo.nf</code>. It also calls a few 'housekeeping' subworkflows that we're going to ignore for now.</p> <pre><code>tree pipelines/nf-core/demo/workflows\n</code></pre> Output<pre><code>pipelines/nf-core/demo/workflows\n\u2514\u2500\u2500 demo.nf\n</code></pre> <p>The <code>demo.nf</code> workflow itself calls out to various script components, namely, modules and subworkflows, stored in the corresponding <code>modules</code> and <code>subworkflows</code> folders.</p> <ul> <li>Module: A wrapper around a single process.</li> <li>Subworkflow: A mini workflow that calls two or more modules and is designed to be called by another workflow.</li> </ul> <p>Here's an overview of the nested structure of a workflow composed of subworkflows and modules:</p> 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 InputsMODULE_1SUBWORKFLOW_1OutputsMODULE_4MODULE_2MODULE_3WORKFLOWSUBWORKFLOW_1MODULE_2modules.configtool argumentspublishingoutput namesbase.configcompute resourceserror strategiesprocess MODULE_2 {    label \"process low\"     input:    tuple val(meta), path(fasta)     output:    tuple val(meta), path(fai)     when:    task.ext.when == null || task.ext.when     script:    def args = task.ext.args ?: ''    \"\"\"    my-function $args -i $fasta -o $fai    \"\"\" } <p>Not all workflows use subworkflows to organize their modules, but this is a very common pattern that makes it possible to reuse chunks of code across different pipelines in a way that is flexible while minimizing maintenance burden.</p> <p>Within this structure, <code>modules</code> and <code>subworkflows</code> are further organized into <code>local</code> and <code>nf-core</code> folders. The <code>nf-core</code> folder is for components that have come from the nf-core GitHub repository, while the <code>local</code> folder is for components that have been developed independently. Usually these are operations that very specific to that pipeline.</p> <p>Let's take a peek into those directories.</p>"},{"location":"hello_nf-core/01_run_demo/#312-modules","title":"3.1.2. Modules","text":"<p>The modules are where the process code lives, as described in Part 4 of the Hello Nextflow training course.</p> <p>In the nf-core project, modules are organized using a nested structure that refers to toolkit and tool names. The module code file describing the process is always called <code>main.nf</code>, and is accompanied by tests and <code>.yml</code> files.</p> <pre><code>tree -L 4 pipelines/nf-core/demo/modules\n</code></pre> Output<pre><code>pipelines/nf-core/demo/modules\n\u2514\u2500\u2500 nf-core\n    \u251c\u2500\u2500 fastqc\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 environment.yml\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.nf\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta.yml\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 multiqc\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 environment.yml\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.nf\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta.yml\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 tests\n    \u2514\u2500\u2500 seqtk\n        \u2514\u2500\u2500 trim\n            \u251c\u2500\u2500 environment.yml\n            \u251c\u2500\u2500 main.nf\n            \u251c\u2500\u2500 meta.yml\n            \u2514\u2500\u2500 tests\n</code></pre> <p>Here you see that the <code>fastqc</code> and <code>multiqc</code> modules sit at the top level within the <code>nf-core</code> modules, whereas the <code>trim</code> module sits under the toolkit that it belongs to, <code>seqtk</code>. In this case there are no <code>local</code> modules.</p>"},{"location":"hello_nf-core/01_run_demo/#313-subworkflows","title":"3.1.3. Subworkflows","text":"<p>As noted above, subworkflows function as wrappers that call two or more modules.</p> <p>In an nf-core pipeline, the subworkflows are divided into <code>local</code> and <code>nf-core</code> directories, and each subworkflow has its own nested directory structure with its own <code>main.nf</code> script.</p> <pre><code>tree -L 4 pipelines/nf-core/demo/subworkflows\n</code></pre> Output<pre><code>pipelines/nf-core/demo/subworkflows\n\u251c\u2500\u2500 local\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 utils_nfcore_demo_pipeline\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 main.nf\n\u2514\u2500\u2500 nf-core\n    \u251c\u2500\u2500 utils_nextflow_pipeline\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.nf\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta.yml\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 utils_nfcore_pipeline\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.nf\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta.yml\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 tests\n    \u2514\u2500\u2500 utils_nfschema_plugin\n        \u251c\u2500\u2500 main.nf\n        \u251c\u2500\u2500 meta.yml\n        \u2514\u2500\u2500 tests\n</code></pre> <p>In the case of the <code>nf-core/demo</code> pipeline, the subworkflows involved are all 'utility' or housekeeping subworkflows, as denoted by the <code>utils_</code> prefix in their names. These subworkflows are what produces the fancy nf-core header in the console output, among other accessory functions.</p> <p>Other pipelines may also use subworkflows as part of the main workflow of interest.</p> <p>Note</p> <p>If you would like to learn how to compose workflows with subworkflows, see the Workflows of Workflows Side Quest (also known as 'the WoW side quest').</p>"},{"location":"hello_nf-core/01_run_demo/#32-configuration","title":"3.2. Configuration","text":"<p>The nf-core project applies guidelines for pipeline configuration that aim to build on Nextflow's flexible customization options in a way that provides greater consistency and maintainability across pipelines.</p> <p>The central configuration file <code>nextflow.config</code> is used to set default values for parameters and other configuration options. The majority of these configuration options are applied by default while others (e.g., software dependency profiles) are included as optional profiles.</p> <p>There are several additional configuration files that are stored in the <code>conf</code> folder and which can be added to the configuration by default or optionally as profiles:</p> <ul> <li><code>base.config</code>: A 'blank slate' config file, appropriate for general use on most high-performance computing. environments. This defines broad bins of resource usage, for example, which are convenient to apply to modules.</li> <li><code>modules.config</code>: Additional module directives and arguments.</li> <li><code>test.config</code>: A profile to run the pipeline with minimal test data, which we used when we ran the demo pipeline in the previous section (code shown there).</li> <li><code>test_full.config</code>: A profile to run the pipeline with a full-sized test dataset.</li> </ul>"},{"location":"hello_nf-core/01_run_demo/#33-documentation-and-related-assets","title":"3.3. Documentation and related assets","text":"<p>At the top level, you can find a README file with summary information, as well as accessory files that summarize project information such as licensing, contribution guidelines, citation and code of conduct.</p> <p>Detailed pipeline documentation is located in the <code>docs</code> directory. This content is used to generate the web pages on the nf-core website.</p> <p>In addition to these human-readable documents, there are two JSON files that provide useful machine-readable information describing parameters and input requirements, <code>nextflow_schema.json</code> and <code>assets/schema_input.json</code>.</p> <p>The <code>nextflow_schema.json</code> is a file used to store information about the pipeline parameters including type, description and help text in a machine readable format. The schema is used for various purposes, including automated parameter validation, help text generation, and interactive parameter form rendering in UI interfaces.</p> assets/nextflow_schema.json (not showing full file)<pre><code>{\n    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n    \"$id\": \"https://raw.githubusercontent.com/nf-core/demo/master/nextflow_schema.json\",\n    \"title\": \"nf-core/demo pipeline parameters\",\n    \"description\": \"An nf-core demo pipeline\",\n    \"type\": \"object\",\n    \"$defs\": {\n        \"input_output_options\": {\n            \"title\": \"Input/output options\",\n            \"type\": \"object\",\n            \"fa_icon\": \"fas fa-terminal\",\n            \"description\": \"Define where the pipeline should find input data and save output data.\",\n            \"required\": [\"input\", \"outdir\"],\n            \"properties\": {\n                \"input\": {\n                    \"type\": \"string\",\n                    \"format\": \"file-path\",\n                    \"exists\": true,\n                    \"schema\": \"assets/schema_input.json\",\n                    \"mimetype\": \"text/csv\",\n                    \"pattern\": \"^\\\\S+\\\\.csv$\",\n                    \"description\": \"Path to comma-separated file containing information about the samples in the experiment.\",\n                    \"help_text\": \"You will need to create a design file with information about the samples in your experiment before running the pipeline. Use this parameter to specify its location. It has to be a comma-separated file with 3 columns, and a header row. See [usage docs](https://nf-co.re/demo/usage#samplesheet-input).\",\n                    \"fa_icon\": \"fas fa-file-csv\"\n                },\n                \"outdir\": {\n                    \"type\": \"string\",\n                    \"format\": \"directory-path\",\n                    \"description\": \"The output directory where the results will be saved. You have to use absolute paths to storage on Cloud infrastructure.\",\n                    \"fa_icon\": \"fas fa-folder-open\"\n                },\n                \"email\": {\n                    \"type\": \"string\",\n                    \"description\": \"Email address for completion summary.\",\n                    \"fa_icon\": \"fas fa-envelope\",\n                    \"help_text\": \"Set this parameter to your e-mail address to get a summary e-mail with details of the run sent to you when the workflow exits. If set in your user config file (`~/.nextflow/config`) then you don't need to specify this on the command line for every run.\",\n                    \"pattern\": \"^([a-zA-Z0-9_\\\\-\\\\.]+)@([a-zA-Z0-9_\\\\-\\\\.]+)\\\\.([a-zA-Z]{2,5})$\"\n                },\n                \"multiqc_title\": {\n                    \"type\": \"string\",\n                    \"description\": \"MultiQC report title. Printed as page header, used for filename if not otherwise specified.\",\n                    \"fa_icon\": \"fas fa-file-signature\"\n                }\n            }\n        },\n(truncated)\n</code></pre> <p>The <code>schema_input.json</code> is a file used to define the input samplesheet structure. Each column can have a type, pattern, description and help text in a machine readable format.</p> assets/schema_input.json<pre><code>{\n  \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n  \"$id\": \"https://raw.githubusercontent.com/nf-core/demo/master/assets/schema_input.json\",\n  \"title\": \"nf-core/demo pipeline - params.input schema\",\n  \"description\": \"Schema for the file provided with params.input\",\n  \"type\": \"array\",\n  \"items\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"sample\": {\n        \"type\": \"string\",\n        \"pattern\": \"^\\\\S+$\",\n        \"errorMessage\": \"Sample name must be provided and cannot contain spaces\",\n        \"meta\": [\"id\"]\n      },\n      \"fastq_1\": {\n        \"type\": \"string\",\n        \"format\": \"file-path\",\n        \"exists\": true,\n        \"pattern\": \"^\\\\S+\\\\.f(ast)?q\\\\.gz$\",\n        \"errorMessage\": \"FastQ file for reads 1 must be provided, cannot contain spaces and must have extension '.fq.gz' or '.fastq.gz'\"\n      },\n      \"fastq_2\": {\n        \"type\": \"string\",\n        \"format\": \"file-path\",\n        \"exists\": true,\n        \"pattern\": \"^\\\\S+\\\\.f(ast)?q\\\\.gz$\",\n        \"errorMessage\": \"FastQ file for reads 2 cannot contain spaces and must have extension '.fq.gz' or '.fastq.gz'\"\n      }\n    },\n    \"required\": [\"sample\", \"fastq_1\"]\n  }\n}\n</code></pre> <p>The schema is used for various purposes, including automated validation, and providing helpful error messages.</p> <p>An example samplesheet is provided under the <code>assets</code> directory:</p> assets/samplesheet.csv<pre><code>sample,fastq_1,fastq_2\nSAMPLE_PAIRED_END,/path/to/fastq/files/AEG588A1_S1_L002_R1_001.fastq.gz,/path/to/fastq/files/AEG588A1_S1_L002_R2_001.fastq.gz\nSAMPLE_SINGLE_END,/path/to/fastq/files/AEG588A4_S4_L003_R1_001.fastq.gz,\n</code></pre> <p>Note</p> <p>The paths in this example samplesheet are not real. For paths to real data files, you should look in the test profiles, which link to data in the <code>nf-core/test-datasets</code> repository.</p> <p>In general, it's considered good practice to link out to example data rather than include it in the pipeline code repository, unless the example data is of trivial size (as is the case for the <code>greetings.csv</code> in the Hello Nextflow training series).</p>"},{"location":"hello_nf-core/01_run_demo/#takeaway_2","title":"Takeaway","text":"<p>You know what are the main components of an nf-core pipeline and how the code is organized, what are the main elements of configuration, and what are some additional sources of information that can be useful.</p>"},{"location":"hello_nf-core/01_run_demo/#whats-next_2","title":"What's next?","text":"<p>Take a break! That was a lot. When you're feeling refreshed and ready, move on to the next section to apply what you've learned to write an nf-core compatible pipeline.</p>"},{"location":"hello_nf-core/02_rewrite_hello/","title":"Part 2: Rewrite Hello for nf-core","text":"<p>In this second part of the Hello nf-core training course, we show you how to create an nf-core compatible version of the pipeline produced by the Hello Nextflow course.</p> <p>You'll have noticed in the first section of the training that nf-core pipelines follow a fairly elaborate structure with a lot of accessory files. Creating all that from scratch would be very tedious, so the nf-core community has developed tooling to do it from a template instead, to bootstrap the process.</p> <p>We are going to show you how to use this tooling to create a pipeline scaffold, then adapt existing 'regular' pipeline code onto the nf-core scaffold.</p> <p>Note</p> <p>The nf-core-tools package is pre-installed for you in our training environment. If you are using a different environment, you need to check whether the package is installed (run <code>nf-core --help</code> in your terminal) and if not, install it as described here: https://nf-co.re/docs/nf-core-tools/installation.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#1-create-a-new-pipeline-project","title":"1. Create a new pipeline project","text":"<p>First, we create the scaffold for the new pipeline.</p> <p>Note</p> <p>Make sure you are in the <code>hello_nf-core</code> directory in your terminal.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#11-run-the-template-based-pipeline-creation-tool","title":"1.1. Run the template-based pipeline creation tool","text":"<p>Let's start by creating a new pipeline with the <code>nf-core pipelines create</code> command. This will create a new pipeline scaffold using the nf-core base template, customized with a pipeline name, description, and author.</p> <pre><code>nf-core pipelines create\n</code></pre> <p>Running this command will open a Text User Interface (TUI) for pipeline creation:</p> <p>This TUI will ask you to provide basic information about your pipeline and will provide you with a choice of features to include or exclude in your pipeline scaffold.</p> <ul> <li>On the welcome screen, click Let's go!.</li> <li>On the <code>Choose pipeline type</code> screen, click Custom.</li> <li>Enter your pipeline details as follows (replacing <code>&lt; YOUR NAME &gt;</code> with your own name), then click Next.</li> </ul> <pre><code>[ ] GitHub organisation: core\n[ ] Workflow name: hello\n[ ] A short description of your pipeline: A basic nf-core style version of Hello Nextflow\n[ ] Name of the main author(s): &lt; YOUR NAME &gt;\n</code></pre> <ul> <li>On the Template features screen, set <code>Toggle all features</code> to off, then selectively enable the following. Check your selections and click Continue.</li> </ul> <pre><code>[ ] Add configuration files\n[ ] Use nf-core components\n[ ] Use nf-schema\n[ ] Add documentation\n[ ] Add testing profiles\n</code></pre> <ul> <li>On the <code>Final details</code> screen, click Finish. Wait for the pipeline to be created, then click Continue.</li> <li>On the Create GitHub repository screen, click Finish without creating a repo. This will display instructions for creating a GitHub repository later. Ignore these and click Close.</li> </ul> <p>Once the TUI closes, you should see the following console output.</p> Output<pre><code>                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.1 - https://nf-co.re\n\n\nINFO     Launching interactive nf-core pipeline creation tool.\n</code></pre> <p>There is no explicit confirmation in the console output that the pipeline creation worked, but you should see a new directory called <code>core-hello</code>.</p> <p>View the contents of the new directory to see how much work you saved yourself by using the template.</p> <pre><code>tree core-hello\n</code></pre> Output<pre><code>core-hello/\n\u251c\u2500\u2500 assets\n\u2502   \u251c\u2500\u2500 samplesheet.csv\n\u2502   \u2514\u2500\u2500 schema_input.json\n\u251c\u2500\u2500 conf\n\u2502   \u251c\u2500\u2500 base.config\n\u2502   \u251c\u2500\u2500 modules.config\n\u2502   \u251c\u2500\u2500 test.config\n\u2502   \u2514\u2500\u2500 test_full.config\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 output.md\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 usage.md\n\u251c\u2500\u2500 main.nf\n\u251c\u2500\u2500 modules.json\n\u251c\u2500\u2500 nextflow.config\n\u251c\u2500\u2500 nextflow_schema.json\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 subworkflows\n\u2502   \u251c\u2500\u2500 local\n\u2502   \u2502   \u2514\u2500\u2500 utils_nfcore_hello_pipeline\n\u2502   \u2502       \u2514\u2500\u2500 main.nf\n\u2502   \u2514\u2500\u2500 nf-core\n\u2502       \u251c\u2500\u2500 utils_nextflow_pipeline\n\u2502       \u2502   \u251c\u2500\u2500 main.nf\n\u2502       \u2502   \u251c\u2500\u2500 meta.yml\n\u2502       \u2502   \u2514\u2500\u2500 tests\n\u2502       \u2502       \u251c\u2500\u2500 main.function.nf.test\n\u2502       \u2502       \u251c\u2500\u2500 main.function.nf.test.snap\n\u2502       \u2502       \u251c\u2500\u2500 main.workflow.nf.test\n\u2502       \u2502       \u251c\u2500\u2500 nextflow.config\n\u2502       \u2502       \u2514\u2500\u2500 tags.yml\n\u2502       \u251c\u2500\u2500 utils_nfcore_pipeline\n\u2502       \u2502   \u251c\u2500\u2500 main.nf\n\u2502       \u2502   \u251c\u2500\u2500 meta.yml\n\u2502       \u2502   \u2514\u2500\u2500 tests\n\u2502       \u2502       \u251c\u2500\u2500 main.function.nf.test\n\u2502       \u2502       \u251c\u2500\u2500 main.function.nf.test.snap\n\u2502       \u2502       \u251c\u2500\u2500 main.workflow.nf.test\n\u2502       \u2502       \u251c\u2500\u2500 main.workflow.nf.test.snap\n\u2502       \u2502       \u251c\u2500\u2500 nextflow.config\n\u2502       \u2502       \u2514\u2500\u2500 tags.yml\n\u2502       \u2514\u2500\u2500 utils_nfschema_plugin\n\u2502           \u251c\u2500\u2500 main.nf\n\u2502           \u251c\u2500\u2500 meta.yml\n\u2502           \u2514\u2500\u2500 tests\n\u2502               \u251c\u2500\u2500 main.nf.test\n\u2502               \u251c\u2500\u2500 nextflow.config\n\u2502               \u2514\u2500\u2500 nextflow_schema.json\n\u2514\u2500\u2500 workflows\n    \u2514\u2500\u2500 hello.nf\n\n14 directories, 36 files\n</code></pre> <p>That's a lot of files!</p> <p>Don't worry too much right now about what they all are; we are going to walk through the important parts together in the course of this training.</p> <p>Note</p> <p>One important difference compared to the <code>nf-core/demo</code> pipeline we examined in the first part of this training is that there is no <code>modules</code> directory. This is because we didn't include any of the default nf-core modules.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#12-test-that-the-scaffold-is-functional","title":"1.2. Test that the scaffold is functional","text":"<p>Believe it or not, even though you haven't yet added any modules to make it do real work, the pipeline scaffold can actually be run using the test profile, the same way we ran the <code>nf-core/demo</code> pipeline.</p> <pre><code>nextflow run ./core-hello -profile docker,test --outdir core-hello-results\n</code></pre> Output<pre><code> N E X T F L O W   ~  version 24.10.4\n\nLaunching `core-hello/main.nf` [special_ride] DSL2 - revision: c31b966b36\n\nDownloading plugin nf-schema@2.2.0\nInput/output options\n  input                     : https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/samplesheet/samplesheet_test_illumina_amplicon.csv\n  outdir                    : core-hello-results\n\nInstitutional config options\n  config_profile_name       : Test profile\n  config_profile_description: Minimal test dataset to check pipeline function\n\nGeneric options\n  trace_report_suffix       : 2025-05-14_10-01-18\n\nCore Nextflow options\n  runName                   : special_ride\n  containerEngine           : docker\n  launchDir                 : /workspaces/training/hello-nf-core\n  workDir                   : /workspaces/training/hello-nf-core/work\n  projectDir                : /workspaces/training/hello-nf-core/core-hello\n  userName                  : root\n  profile                   : docker,test\n  configFiles               :\n\n!! Only displaying parameters that differ from the pipeline defaults !!\n------------------------------------------------------\n-[core/hello] Pipeline completed successfully\n</code></pre> <p>This shows you that all the basic wiring is in place. You can take a look at the reports in the <code>pipeline_info</code> directory to see what was run; not much at all!</p> <p>Note</p> <p>The nf-core pipeline template includes an example samplesheet, but at time of writing it is very domain-specific. Future work will aim to produce something more generic.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#13-examine-the-placeholder-workflow","title":"1.3. Examine the placeholder workflow","text":"<p>If you look inside the <code>main.nf</code> file, you'll see it imports a workflow called <code>HELLO</code> from <code>workflows/hello</code>. This is a placeholder workflow for our workflow of interest, with some nf-core functionality already in place.</p> core-hello/workflows/hello.nf<pre><code>/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    IMPORT MODULES / SUBWORKFLOWS / FUNCTIONS\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\ninclude { paramsSummaryMap       } from 'plugin/nf-schema'\ninclude { softwareVersionsToYAML } from '../subworkflows/nf-core/utils_nfcore_pipeline'\n\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    RUN MAIN WORKFLOW\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n\nworkflow HELLO {\n\n    take:\n    ch_samplesheet // channel: samplesheet read in from --input\n    main:\n\n    ch_versions = Channel.empty()\n\n    //\n    // Collate and save software versions\n    //\n    softwareVersionsToYAML(ch_versions)\n        .collectFile(\n            storeDir: \"${params.outdir}/pipeline_info\",\n            name:  'hello_software_'  + 'versions.yml',\n            sort: true,\n            newLine: true\n        ).set { ch_collated_versions }\n\n\n    emit:\n    versions       = ch_versions                 // channel: [ path(versions.yml) ]\n\n}\n\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    THE END\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n</code></pre> <p>Compared to a basic Nextflow workflow like the one developed in Hello Nextflow, you'll notice a few things that are new here (highlighted lines above):</p> <ul> <li>The workflow block has a name</li> <li>Workflow inputs are declared using the <code>take:</code> keyword and the channel construction is moved up to the parent workflow</li> <li>Workflow content is placed inside a <code>main:</code> block</li> <li>Outputs are declared using the <code>emit:</code> keyword</li> </ul> <p>These are optional features of Nextflow that make the workflow composable, meaning that it can be called from within another workflow.</p> <p>We are going to need to plug the relevant logic from our workflow of interest into that structure. The first step for that is to make our original workflow composable.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#takeaway","title":"Takeaway","text":"<p>You now know how to create a pipeline scaffold using nf-core tools.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#whats-next","title":"What's next?","text":"<p>Learn how to make a simple workflow composable as a prelude to making it nf-core compatible.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#2-make-the-original-hello-nextflow-workflow-composable","title":"2. Make the original Hello Nextflow workflow composable","text":"<p>We provide you with a clean, fully functional copy of the completed Hello Nextflow workflow in the directory <code>original-hello</code> along with its modules and the default CSV file it expects to use as input.</p> <pre><code>tree original-hello/\n</code></pre> Output<pre><code>original-hello/\n\u251c\u2500\u2500 hello.nf\n\u251c\u2500\u2500 modules\n\u2502   \u251c\u2500\u2500 collectGreetings.nf\n\u2502   \u251c\u2500\u2500 convertToUpper.nf\n\u2502   \u251c\u2500\u2500 cowpy.nf\n\u2502   \u2514\u2500\u2500 sayHello.nf\n\u2514\u2500\u2500 nextflow.config\n</code></pre> <p>Feel free to run it to satisfy yourself that it works:</p> <pre><code>nextflow run original-hello/hello.nf\n</code></pre> Output<pre><code> N E X T F L O W   ~  version 24.10.4\n\nLaunching `original-hello/hello.nf` [goofy_babbage] DSL2 - revision: e9e72441e9\n\nexecutor &gt;  local (8)\n[a4/081cec] sayHello (1)       | 3 of 3 \u2714\n[e7/7e9058] convertToUpper (3) | 3 of 3 \u2714\n[0c/17263b] collectGreetings   | 1 of 1 \u2714\n[94/542280] cowpy              | 1 of 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>Open the <code>hello.nf</code> workflow file to inspect the code, which is shown in full below (not counting the processes, which are in modules):</p> original-hello/hello.nf<pre><code>#!/usr/bin/env nextflow\n\n/*\n* Pipeline parameters\n*/\nparams.greeting = 'greetings.csv'\nparams.batch = 'test-batch'\nparams.character = 'turkey'\n\n// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\ninclude { convertToUpper } from './modules/convertToUpper.nf'\ninclude { collectGreetings } from './modules/collectGreetings.nf'\ninclude { cowpy } from './modules/cowpy.nf'\n\nworkflow {\n\n  // create a channel for inputs from a CSV file\n  greeting_ch = Channel.fromPath(params.greeting)\n                      .splitCsv()\n                      .map { line -&gt; line[0] }\n\n  // emit a greeting\n  sayHello(greeting_ch)\n\n  // convert the greeting to uppercase\n  convertToUpper(sayHello.out)\n\n  // collect all the greetings into one file\n  collectGreetings(convertToUpper.out.collect(), params.batch)\n\n  // emit a message about the size of the batch\n  collectGreetings.out.count.view { \"There were $it greetings in this batch\" }\n\n  // generate ASCII art of the greetings with cowpy\n  cowpy(collectGreetings.out.outfile, params.character)\n}\n</code></pre> <p>Let's walk through the necessary changes one by one.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#21-name-the-workflow","title":"2.1. Name the workflow","text":"<p>First, let's give the workflow a name so we can refer to it from a parent workflow.</p> AfterBefore original-hello/hello.nf<pre><code>workflow HELLO {\n</code></pre> original-hello/hello.nf<pre><code>workflow {\n</code></pre> <p>The same conventions apply to workflow names as to module names.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#22-replace-channel-construction-with-take","title":"2.2. Replace channel construction with <code>take</code>","text":"<p>Now, replace the channel construction with a simple <code>take</code> statement declaring expected inputs.</p> AfterBefore original-hello/hello.nf<pre><code>    take:\n    // channel of greetings\n    greeting_ch\n</code></pre> original-hello/hello.nf<pre><code>    // create a channel for inputs from a CSV file\n    greeting_ch = Channel.fromPath(params.greeting)\n                        .splitCsv()\n                        .map { line -&gt; line[0] }\n</code></pre> <p>This leaves the details of how the inputs are provided up to the parent workflow.</p> <p>As part of this change, you should also delete the line <code>params.greeting = 'greetings.csv'</code> from the block of parameter definitions (line 6). That will also be left to the parent workflow to declare.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#23-preface-workflow-operations-with-main-statement","title":"2.3. Preface workflow operations with <code>main</code> statement","text":"<p>Next, add a <code>main</code> statement before the rest of the operations called in the body of the workflow.</p> AfterBefore original-hello/hello.nf<pre><code>    main:\n\n    // emit a greeting\n    sayHello(greeting_ch)\n\n    // convert the greeting to uppercase\n    convertToUpper(sayHello.out)\n\n    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect(), params.batch)\n\n    // emit a message about the size of the batch\n    collectGreetings.out.count.view { \"There were $it greetings in this batch\" }\n\n    // generate ASCII art of the greetings with cowpy\n    cowpy(collectGreetings.out.outfile, params.character)\n</code></pre> original-hello/hello.nf<pre><code>    // emit a greeting\n    sayHello(greeting_ch)\n\n    // convert the greeting to uppercase\n    convertToUpper(sayHello.out)\n\n    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect(), params.batch)\n\n    // emit a message about the size of the batch\n    collectGreetings.out.count.view { \"There were $it greetings in this batch\" }\n\n    // generate ASCII art of the greetings with cowpy\n    cowpy(collectGreetings.out.outfile, params.character)\n</code></pre> <p>This basically says 'this is what this workflow does'.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#24-add-emit-statement","title":"2.4. Add <code>emit</code> statement","text":"<p>Finally, add an <code>emit</code> statement declaring what are the final outputs of the workflow.</p> original-hello/hello.nf<pre><code>    emit:\n    cowpy_hellos = cowpy.out\n</code></pre> <p>This is a net new addition to the code compared to the original workflow.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#25-recap-of-the-completed-changes","title":"2.5. Recap of the completed changes","text":"<p>If you've done all the changes as described, your workflow should now look like this:</p> original-hello/hello.nf<pre><code>#!/usr/bin/env nextflow\n\n/*\n* Pipeline parameters\n*/\nparams.batch = 'test-batch'\nparams.character = 'turkey'\n\n// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\ninclude { convertToUpper } from './modules/convertToUpper.nf'\ninclude { collectGreetings } from './modules/collectGreetings.nf'\ninclude { cowpy } from './modules/cowpy.nf'\n\nworkflow HELLO {\n\n    take:\n    // channel of greetings\n    greeting_ch\n\n    main:\n\n    // emit a greeting\n    sayHello(greeting_ch)\n\n    // convert the greeting to uppercase\n    convertToUpper(sayHello.out)\n\n    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect(), params.batch)\n\n    // emit a message about the size of the batch\n    collectGreetings.out.count.view { \"There were $it greetings in this batch\" }\n\n    // generate ASCII art of the greetings with cowpy\n    cowpy(collectGreetings.out.outfile, params.character)\n\n    emit:\n    cowpy_hellos = cowpy.out\n}\n</code></pre> <p>This describes everything Nextflow needs EXCEPT what to feed into the input channel. That is going to be defined in the parent workflow, also called the entrypoint workflow.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#26-make-a-dummy-entrypoint-workflow","title":"2.6. Make a dummy entrypoint workflow","text":"<p>We can make a dummy entrypoint workflow to test the composable workflow without yet having to deal with the rest of the complexity of the nf-core pipeline scaffold.</p> <p>Create a blank file named <code>main.nf</code> in the same<code>original-hello</code> directory.</p> <pre><code>touch original-hello/main.nf\n</code></pre> <p>Copy the following code into the <code>main.nf</code> file.</p> original-hello/main.nf<pre><code>#!/usr/bin/env nextflow\n\n// import the workflow code from the hello.nf file\ninclude { HELLO } from './hello.nf'\n\n// declare input parameter\nparams.greeting = 'greetings.csv'\n\nworkflow {\n  // create a channel for inputs from a CSV file\n  greeting_ch = Channel.fromPath(params.greeting)\n                      .splitCsv()\n                      .map { line -&gt; line[0] }\n\n  // call the imported workflow on the channel of greetings\n  HELLO(greeting_ch)\n\n  // view the outputs emitted by the workflow\n  HELLO.out.view { \"Output: $it\" }\n}\n</code></pre> <p>There are two important observations to make here:</p> <ul> <li>The syntax for calling the imported workflow (line 16) is essentially the same as the syntax for calling modules.</li> <li>Everything that is related to pulling the inputs into the workflow (input parameter and channel construction) is now declared in this parent workflow.</li> </ul> <p>Note</p> <p>Naming the entrypoint workflow file <code>main.nf</code> is a convention, not a requirement.</p> <p>If you follow this convention, you can omit specifying the workflow file name in your <code>nextflow run</code> command. Nextflow will automatically look for a file named <code>main.nf</code> in the execution directory.</p> <p>However, you can name the entrypoint workflow file something else if you prefer. In that case, be sure to specify the workflow file name in your <code>nextflow run</code> command.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#27-test-that-the-workflow-runs","title":"2.7. Test that the workflow runs","text":"<p>We finally have all the pieces we need to verify that the composable workflow works.</p> <pre><code>nextflow run ./original-hello\n</code></pre> <p>Note</p> <p>Here you see the advantage of using the <code>main.nf</code> naming convention. If we had named the entrypoint workflow <code>something_else.nf</code>, we would have had to do <code>nextflow run original-hello/something_else.nf</code>.</p> <p>If you made all the changes correctly, this should run to completion.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.4\n\nLaunching `original-hello/main.nf` [friendly_wright] DSL2 - revision: 1ecd2d9c0a\n\nexecutor &gt;  local (8)\n[24/c6c0d8] HELLO:sayHello (3)       | 3 of 3 \u2714\n[dc/721042] HELLO:convertToUpper (3) | 3 of 3 \u2714\n[48/5ab2df] HELLO:collectGreetings   | 1 of 1 \u2714\n[e3/693b7e] HELLO:cowpy              | 1 of 1 \u2714\nThere were 3 greetings in this batch\nOutput: /workspaces/training/hello-nf-core/work/e3/693b7e48dc119d0c54543e0634c2e7/cowpy-COLLECTED-test-batch-output.txt\n</code></pre> <p>This means we've successfully upgraded our HELLO workflow to be composable.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#takeaway_1","title":"Takeaway","text":"<p>You know how to make a workflow composable by giving it a name and adding <code>take</code>, <code>main</code> and <code>emit</code> statements, and how to call it from an entrypoint workflow.</p> <p>Note</p> <p>If you're interested in digging deeper into options for composing workflows of workflows, check out the Workflow of Workflows (a.k.a. WoW) side quest.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#whats-next_1","title":"What's next?","text":"<p>Learn how to graft a basic composable workflow onto the nf-core scaffold.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#3-fit-the-updated-workflow-logic-into-the-placeholder-workflow","title":"3. Fit the updated workflow logic into the placeholder workflow","text":"<p>This is the current content of the <code>HELLO</code> workflow in <code>core-hello/workflows/hello.nf</code>. Overall this code does very little aside from some housekeeping that has to do with capturing the version of any software tools that get run in the pipeline.</p> <p>We need to add the relevant code from the version of the original workflow that we made composable.</p> core-hello/workflows/hello.nf<pre><code>/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    IMPORT MODULES / SUBWORKFLOWS / FUNCTIONS\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\ninclude { paramsSummaryMap       } from 'plugin/nf-schema'\ninclude { softwareVersionsToYAML } from '../subworkflows/nf-core/utils_nfcore_pipeline'\n\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    RUN MAIN WORKFLOW\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n\nworkflow HELLO {\n\n    take:\n    ch_samplesheet // channel: samplesheet read in from --input\n    main:\n\n    ch_versions = Channel.empty()\n\n    //\n    // Collate and save software versions\n    //\n    softwareVersionsToYAML(ch_versions)\n        .collectFile(\n            storeDir: \"${params.outdir}/pipeline_info\",\n            name:  'hello_software_'  + 'versions.yml',\n            sort: true,\n            newLine: true\n        ).set { ch_collated_versions }\n\n\n    emit:\n    versions       = ch_versions                 // channel: [ path(versions.yml) ]\n\n}\n\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    THE END\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n</code></pre> <p>We're going to tackle this in the following stages:</p> <ol> <li>Copy over the modules and set up module imports</li> <li>Leave the <code>take</code> declaration as is</li> <li>Add the workflow logic to the <code>main</code> block</li> <li>Update the <code>emit</code> block</li> </ol> <p>Note</p> <p>We're going to ignore the version capture for this first pass and will look at how to wire that up in a later section.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#31-copy-the-modules-and-set-up-module-imports","title":"3.1. Copy the modules and set up module imports","text":"<p>In the original workflow, the four processes are stored in modules, so we need to copy those over to this new project (into a new <code>local</code> directory) and add import statements to the workflow file.</p> <p>First let's copy the module files over:</p> <pre><code>mkdir -p core-hello/modules/local/\ncp original-hello/modules/* core-hello/modules/local/.\n</code></pre> <p>You should now see the directory of modules listed under <code>core-hello/</code>.</p> <pre><code>tree core-hello/modules\n</code></pre> Output<pre><code>core-hello/modules\n\u2514\u2500\u2500 local\n    \u251c\u2500\u2500 collectGreetings.nf\n    \u251c\u2500\u2500 convertToUpper.nf\n    \u251c\u2500\u2500 cowpy.nf\n    \u2514\u2500\u2500 sayHello.nf\n</code></pre> <p>Now let's set up the module import statements.</p> <p>These were the import statements in the <code>original-hello/hello.nf</code> workflow:</p> original-hello/hello.nf<pre><code>// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\ninclude { convertToUpper } from './modules/convertToUpper.nf'\ninclude { collectGreetings } from './modules/collectGreetings.nf'\ninclude { cowpy } from './modules/cowpy.nf'\n</code></pre> <p>Open the <code>core-hello/workflows/hello.nf</code> file and transpose those import statements into it as shown below.</p> AfterBefore core-hello/workflows/hello.nf<pre><code>/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    IMPORT MODULES / SUBWORKFLOWS / FUNCTIONS\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\ninclude { paramsSummaryMap       } from 'plugin/nf-schema'\ninclude { softwareVersionsToYAML } from '../subworkflows/nf-core/utils_nfcore_pipeline'\ninclude { sayHello               } from '../modules/local/sayHello.nf'\ninclude { convertToUpper         } from '../modules/local/convertToUpper.nf'\ninclude { collectGreetings       } from '../modules/local/collectGreetings.nf'\ninclude { cowpy                  } from '../modules/local/cowpy.nf'\n</code></pre> core-hello/workflows/hello.nf<pre><code>/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    IMPORT MODULES / SUBWORKFLOWS / FUNCTIONS\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\ninclude { paramsSummaryMap       } from 'plugin/nf-schema'\ninclude { softwareVersionsToYAML } from '../subworkflows/nf-core/utils_nfcore_pipeline'\n</code></pre> <p>Two more interesting observations here:</p> <ul> <li>We've adapted the formatting of the import statements to follow the nf-core style convention.</li> <li>We've updated the relative paths to the modules to reflect that they're now stored at a different level of nesting.</li> </ul>"},{"location":"hello_nf-core/02_rewrite_hello/#32-leave-the-take-declaration-as-is","title":"3.2. Leave the <code>take</code> declaration as is","text":"<p>The nf-core project has a lot of prebuilt functionality around the concept of the samplesheet, which is typically a CSV file containing columnar data. Since that is essentially what our <code>greetings.csv</code> file is, we'll keep the current <code>take</code> declaration as is, and simply update the name of the input channel in the next step.</p> core-hello/workflows/hello.nf<pre><code>    take:\n    ch_samplesheet // channel: samplesheet read in from --input\n</code></pre> <p>The input handling will be done upstream of this workflow (not in this code file).</p>"},{"location":"hello_nf-core/02_rewrite_hello/#33-add-the-workflow-logic-to-the-main-block","title":"3.3. Add the workflow logic to the <code>main</code> block","text":"<p>Now that our modules are available to the workflow, we can plug the workflow logic into the <code>main</code> block.</p> <p>As a reminder, this is the relevant code in the original workflow, which didn't change much when we made it composable (we just added the <code>main:</code> line):</p> original-hello/hello.nf<pre><code>    main:\n\n    // emit a greeting\n    sayHello(greeting_ch)\n\n    // convert the greeting to uppercase\n    convertToUpper(sayHello.out)\n\n    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect(), params.batch)\n\n    // emit a message about the size of the batch\n    collectGreetings.out.count.view { \"There were $it greetings in this batch\" }\n\n    // generate ASCII art of the greetings with cowpy\n    cowpy(collectGreetings.out.outfile, params.character)\n</code></pre> <p>We need to copy this code into the new version of the workflow (minus the <code>main:</code> keyword which is already there).</p> <p>There is already some code in there that has to do with capturing the versions of the tools that get run by the workflow. We're going to leave that alone for now (we'll deal with the tool versions later) and simply insert our code right after the <code>main:</code> line.</p> AfterBefore core-hello/workflows/hello.nf<pre><code>    main:\n\n    // emit a greeting\n    sayHello(greeting_ch)\n\n    // convert the greeting to uppercase\n    convertToUpper(sayHello.out)\n\n    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect(), params.batch)\n\n    // emit a message about the size of the batch\n    collectGreetings.out.count.view { \"There were $it greetings in this batch\" }\n\n    // generate ASCII art of the greetings with cowpy\n    cowpy(collectGreetings.out.outfile, params.character)\n\n    ch_versions = Channel.empty()\n\n    //\n    // Collate and save software versions\n    //\n    softwareVersionsToYAML(ch_versions)\n        .collectFile(\n            storeDir: \"${params.outdir}/pipeline_info\",\n            name:  'hello_software_'  + 'versions.yml',\n            sort: true,\n            newLine: true\n        ).set { ch_collated_versions }\n</code></pre> core-hello/workflows/hello.nf<pre><code>    main:\n\n    ch_versions = Channel.empty()\n\n    //\n    // Collate and save software versions\n    //\n    softwareVersionsToYAML(ch_versions)\n        .collectFile(\n            storeDir: \"${params.outdir}/pipeline_info\",\n            name:  'hello_software_'  + 'versions.yml',\n            sort: true,\n            newLine: true\n        ).set { ch_collated_versions }\n</code></pre> <p>This looks great, but we still need to update the name of the channel we're passing to the <code>sayHello()</code> process from <code>greeting_ch</code> to <code>ch_samplesheet</code> (see highlighted lines), to match what is written under the <code>take:</code> keyword.</p> AfterBefore core-hello/workflows/hello.nf<pre><code>    // emit a greeting (updated to use the nf-core convention for samplesheets)\n    sayHello(ch_samplesheet)\n</code></pre> core-hello/workflows/hello.nf<pre><code>    // emit a greeting\n    sayHello(greeting_ch)\n</code></pre> <p>Now the workflow logic is correctly wired up.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#34-update-the-emit-block","title":"3.4. Update the <code>emit</code> block","text":"<p>Finally, we need to update the <code>emit</code> block to include the declaration of the workflow's final outputs.</p> AfterBefore core-hello/workflows/hello.nf<pre><code>    emit:\n    cowpy_hellos   = cowpy.out\n    versions       = ch_versions                 // channel: [ path(versions.yml) ]\n</code></pre> core-hello/workflows/hello.nf<pre><code>    emit:\n    versions       = ch_versions                 // channel: [ path(versions.yml) ]\n</code></pre> <p>This concludes the modifications we need to make to the HELLO workflow itself.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#takeaway_2","title":"Takeaway","text":"<p>You know how to fit the core pieces of a composable workflow into an nf-core placeholder workflow.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#whats-next_2","title":"What's next?","text":"<p>Learn how to adapt how the inputs are handle in the nf-core pipeline scaffold.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#4-adapt-the-input-handling","title":"4. Adapt the input handling","text":"<p>Now that the HELLO workflow is ready to go, we need to adapt how the inputs are handled to make sure our <code>greetings.csv</code> will be handled appropriately.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#41-identify-where-inputs-are-handled","title":"4.1. Identify where inputs are handled","text":"<p>The first step is to figure out where the input handling is done.</p> <p>You may recall that when we rewrote the Hello Nextflow workflow to be composable, we moved the input parameter declaration up one level, in the <code>main.nf</code> entrypoint workflow. So let's have a look at the top level <code>main.nf</code> entrypoint workflow that was created as part of the pipeline scaffold:</p> core-hello/main.nf<pre><code>#!/usr/bin/env nextflow\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    core/hello\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    Github : https://github.com/core/hello\n----------------------------------------------------------------------------------------\n*/\n\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    IMPORT FUNCTIONS / MODULES / SUBWORKFLOWS / WORKFLOWS\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n\ninclude { HELLO  } from './workflows/hello'\ninclude { PIPELINE_INITIALISATION } from './subworkflows/local/utils_nfcore_hello_pipeline'\ninclude { PIPELINE_COMPLETION     } from './subworkflows/local/utils_nfcore_hello_pipeline'\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    NAMED WORKFLOWS FOR PIPELINE\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n\n//\n// WORKFLOW: Run main analysis pipeline depending on type of input\n//\nworkflow CORE_HELLO {\n\n    take:\n    samplesheet // channel: samplesheet read in from --input\n\n    main:\n\n    //\n    // WORKFLOW: Run pipeline\n    //\n    HELLO (\n        samplesheet\n    )\n}\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    RUN MAIN WORKFLOW\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n\nworkflow {\n\n    main:\n    //\n    // SUBWORKFLOW: Run initialisation tasks\n    //\n    PIPELINE_INITIALISATION (\n        params.version,\n        params.validate_params,\n        params.monochrome_logs,\n        args,\n        params.outdir,\n        params.input\n    )\n\n    //\n    // WORKFLOW: Run main workflow\n    //\n    CORE_HELLO (\n        PIPELINE_INITIALISATION.out.samplesheet\n    )\n    //\n    // SUBWORKFLOW: Run completion tasks\n    //\n    PIPELINE_COMPLETION (\n        params.outdir,\n        params.monochrome_logs,\n    )\n}\n\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    THE END\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n</code></pre> <p>The nf-core project makes heavy use of nested subworkflows, so this bit can be a little confusing on first approach.</p> <p>What matters here is that there are two workflows defined:</p> <ul> <li><code>CORE_HELLO</code> is a thin wrapper for running the HELLO workflow we just finished adapting in <code>core-hello/workflows/hello.nf</code>.</li> <li>An unnamed workflow that calls <code>CORE_HELLO</code> as well as two other subworkflows, <code>PIPELINE_INITIALISATION</code> and <code>PIPELINE_COMPLETION</code>.</li> </ul> <p>Importantly, we cannot find any code constructing an input channel at this level, only references to a samplesheet provided via the <code>--input</code> parameter.</p> <p>A bit of poking around reveals that the input handling is done by the <code>PIPELINE_INITIALISATION</code> subworkflow, appropriately enough.</p> <p>If we open up <code>core-hello/subworkflows/local/utils_nfcore_hello_pipeline/main.nf</code> and scroll down, we come to this chunk of code:</p> core-hello/subworkflows/local/utils_nfcore_hello_pipeline/main.nf<pre><code>    //\n    // Create channel from input file provided through params.input\n    //\n\n    Channel\n        .fromList(samplesheetToList(params.input, \"${projectDir}/assets/schema_input.json\"))\n        .map {\n            meta, fastq_1, fastq_2 -&gt;\n                if (!fastq_2) {\n                    return [ meta.id, meta + [ single_end:true ], [ fastq_1 ] ]\n                } else {\n                    return [ meta.id, meta + [ single_end:false ], [ fastq_1, fastq_2 ] ]\n                }\n        }\n        .groupTuple()\n        .map { samplesheet -&gt;\n            validateInputSamplesheet(samplesheet)\n        }\n        .map {\n            meta, fastqs -&gt;\n                return [ meta, fastqs.flatten() ]\n        }\n        .set { ch_samplesheet }\n\n    emit:\n    samplesheet = ch_samplesheet\n    versions    = ch_versions\n</code></pre> <p>This is the channel factory that parses the samplesheet and passes it on in a form that is ready to be consumed by the HELLO workflow. It is quite complex because it does a lot of parsing and validation work.</p> <p>Note</p> <p>The syntax above is a little different from what we've used previously, but basically this:</p> <pre><code>Channel.&lt;...&gt;.set { ch_samplesheet }\n</code></pre> <p>is equivalent to this:</p> <pre><code>ch_samplesheet = Channel.&lt;...&gt;\n</code></pre>"},{"location":"hello_nf-core/02_rewrite_hello/#42-replace-the-templated-input-channel-code","title":"4.2. Replace the templated input channel code","text":"<p>The good news is that our pipeline's needs are much simpler, so we can replace all of that by the channel construction code we developed in the original Hello Nextflow workflow.</p> <p>As a reminder, this is what the channel construction looked like (as seen in the solutions directory):</p> solutions/composable-hello/main.nf<pre><code>    // create a channel for inputs from a CSV file\n    greeting_ch = Channel.fromPath(params.greeting)\n                        .splitCsv()\n                        .map { line -&gt; line[0] }\n</code></pre> <p>So we just need to plug that into the initialisation workflow, with minor changes: we update the channel name from <code>greeting_ch</code> to <code>ch_samplesheet</code>, and the parameter name from <code>params.greeting</code> to <code>params.input</code> (see highlighted line).</p> AfterBefore core-hello/subworkflows/local/utils_nfcore_hello_pipeline/main.nf<pre><code>    //\n    // Create channel from input file provided through params.input\n    //\n    ch_samplesheet = Channel.fromPath(params.input)\n                        .splitCsv()\n                        .map { line -&gt; line[0] }\n\n    emit:\n    samplesheet = ch_samplesheet\n    versions    = ch_versions\n</code></pre> core-hello/subworkflows/local/utils_nfcore_hello_pipeline/main.nf<pre><code>    //\n    // Create channel from input file provided through params.input\n    //\n\n    Channel\n        .fromList(samplesheetToList(params.input, \"${projectDir}/assets/schema_input.json\"))\n        .map {\n            meta, fastq_1, fastq_2 -&gt;\n                if (!fastq_2) {\n                    return [ meta.id, meta + [ single_end:true ], [ fastq_1 ] ]\n                } else {\n                    return [ meta.id, meta + [ single_end:false ], [ fastq_1, fastq_2 ] ]\n                }\n        }\n        .groupTuple()\n        .map { samplesheet -&gt;\n            validateInputSamplesheet(samplesheet)\n        }\n        .map {\n            meta, fastqs -&gt;\n                return [ meta, fastqs.flatten() ]\n        }\n        .set { ch_samplesheet }\n\n    emit:\n    samplesheet = ch_samplesheet\n    versions    = ch_versions\n</code></pre> <p>That completes the changes we need to make the input processing work.</p> <p>In its current form, this won't let us take advantage of nf-core's built-in capabilities for schema validation, but we can add that in later. For now, we're focused on keeping it as simple as possible to get to something we can run successfully on test data.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#43-update-the-test-profile","title":"4.3. Update the test profile","text":"<p>Speaking of test data and parameters, let's update the test profile for this pipeline to use the <code>greetings.csv</code> mini-samplesheet instead of the example samplesheet provided in the template.</p> <p>Under <code>core-hello/config</code>, we find two templated test profiles: <code>test.config</code> and <code>test_full.config</code>, which are meant to test a small data sample and a full-size one. Given the purpose of our pipeline, there's not really a point to setting up a full-size test profile, so feel free to ignore or delete <code>test_full.config</code>. We're going to focus on setting up <code>test.config</code> to run on our <code>greetings.csv</code> file with a few default parameters.</p> <p>First we need to copy the <code>greetings.csv</code> file to an appropriate place in our pipeline project. Typically small test files are stored in the <code>assets</code> directory, so let's copy the file over from our working directory.</p> <pre><code>cp greetings.csv core-hello/assets/.\n</code></pre> <p>Now we can update the <code>test.config</code> file as follows:</p> AfterBefore core-hello/conf/test.config<pre><code>    params {\n        config_profile_name        = 'Test profile'\n        config_profile_description = 'Minimal test dataset to check pipeline function'\n\n        // Input data\n        input  = 'core-hello/assets/greetings.csv'\n\n        // Other parameters\n        batch     = 'test'\n        character = 'tux'\n    }\n</code></pre> core-hello/config/test.config<pre><code>    params {\n        config_profile_name        = 'Test profile'\n        config_profile_description = 'Minimal test dataset to check pipeline function'\n\n        // Input data\n        // TODO nf-core: Specify the paths to your test data on nf-core/test-datasets\n        // TODO nf-core: Give any required params for the test so that command line flags are not needed\n        input  = params.pipelines_testdata_base_path + 'viralrecon/samplesheet/samplesheet_test_illumina_amplicon.csv'\n    }\n</code></pre> <p>And while we're at it, let's lower the default resource limitations:</p> AfterBefore core-hello/config/test.config<pre><code>process {\n    resourceLimits = [\n        cpus: 2,\n        memory: '4.GB',\n        time: '1.h'\n    ]\n}\n</code></pre> core-hello/config/test.config<pre><code>process {\n    resourceLimits = [\n        cpus: 4,\n        memory: '15.GB',\n        time: '1.h'\n    ]\n}\n</code></pre> <p>This completes the code modifications we need to do.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#44-run-the-pipeline-with-the-test-profile","title":"4.4. Run the pipeline with the test profile","text":"<p>That was a lot, but we can finally try running the pipeline! Note that we have to add <code>--validate_params false</code> to the command line because we didn't set up the validation yet (that will come later).</p> <pre><code>nextflow run core-hello --outdir core-hello-results -profile test,docker --validate_params false\n</code></pre> <p>If you've done all of the modifications correctly, it should run to completion.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.4\n\nLaunching `core-hello/main.nf` [agitated_noyce] DSL2 - revision: c31b966b36\n\nInput/output options\n  input                     : core-hello/assets/greetings.csv\n  outdir                    : core-hello-results\n\nInstitutional config options\n  config_profile_name       : Test profile\n  config_profile_description: Minimal test dataset to check pipeline function\n\nGeneric options\n  validate_params           : false\n  trace_report_suffix       : 2025-05-14_11-10-22\n\nCore Nextflow options\n  runName                   : agitated_noyce\n  containerEngine           : docker\n  launchDir                 : /workspaces/training/hello-nf-core\n  workDir                   : /workspaces/training/hello-nf-core/work\n  projectDir                : /workspaces/training/hello-nf-core/core-hello\n  userName                  : root\n  profile                   : test,docker\n  configFiles               :\n\n!! Only displaying parameters that differ from the pipeline defaults !!\n------------------------------------------------------\nexecutor &gt;  local (8)\n[d6/b59dca] CORE_HELLO:HELLO:sayHello (1)       | 3 of 3 \u2714\n[0b/42f9a1] CORE_HELLO:HELLO:convertToUpper (2) | 3 of 3 \u2714\n[73/bec621] CORE_HELLO:HELLO:collectGreetings   | 1 of 1 \u2714\n[3f/e0a67a] CORE_HELLO:HELLO:cowpy              | 1 of 1 \u2714\n-[core/hello] Pipeline completed successfully-\n</code></pre> <p>As you can see, this produced the typical nf-core summary at the start thanks to the initialisation subworkflow, and the lines for each module now show the full PIPELINE:WORKFLOW:module names.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#45-find-the-pipeline-outputs","title":"4.5. Find the pipeline outputs","text":"<p>The question now is: where are the outputs of the pipeline? And the answer is quite interesting: there are now two different places to look for the results.</p> <p>We didn't change anything to the modules themselves, so the outputs handled by module-level <code>publishDir</code> directives are still going to a <code>results</code> directory as specified in the original pipeline.</p> <pre><code>tree results\n</code></pre> Output<pre><code>results\n\u251c\u2500\u2500 Bonjour-output.txt\n\u251c\u2500\u2500 COLLECTED-test-batch-output.txt\n\u251c\u2500\u2500 COLLECTED-test-output.txt\n\u251c\u2500\u2500 cowpy-COLLECTED-test-batch-output.txt\n\u251c\u2500\u2500 cowpy-COLLECTED-test-output.txt\n\u251c\u2500\u2500 Hello-output.txt\n\u251c\u2500\u2500 Hol\u00e0-output.txt\n\u251c\u2500\u2500 UPPER-Bonjour-output.txt\n\u251c\u2500\u2500 UPPER-Hello-output.txt\n\u2514\u2500\u2500 UPPER-Hol\u00e0-output.txt\n</code></pre> <p>Anything that is hooked up to the nf-core template code gets put into a directory generated automatically, called <code>core-hello-results/</code>. This includes the various reports produced by the nf-core utility subworkflows, which you can find under <code>core-hello-results/pipeline_info</code>.</p> <pre><code>tree core-hello-results\n</code></pre> Output<pre><code>core-hello-results\n\u2514\u2500\u2500 pipeline_info\n    \u251c\u2500\u2500 execution_report_2025-06-03_18-22-28.html\n    \u251c\u2500\u2500 execution_report_2025-06-03_20-11-39.html\n    \u251c\u2500\u2500 execution_timeline_2025-06-03_18-22-28.html\n    \u251c\u2500\u2500 execution_timeline_2025-06-03_20-11-39.html\n    \u251c\u2500\u2500 execution_trace_2025-06-03_18-22-28.txt\n    \u251c\u2500\u2500 execution_trace_2025-06-03_20-10-11.txt\n    \u251c\u2500\u2500 execution_trace_2025-06-03_20-11-39.txt\n    \u251c\u2500\u2500 hello_software_versions.yml\n    \u251c\u2500\u2500 params_2025-06-03_18-22-32.json\n    \u251c\u2500\u2500 params_2025-06-03_20-10-15.json\n    \u251c\u2500\u2500 params_2025-06-03_20-11-43.json\n    \u251c\u2500\u2500 pipeline_dag_2025-06-03_18-22-28.html\n    \u2514\u2500\u2500 pipeline_dag_2025-06-03_20-11-39.html\n</code></pre> <p>In our case, we didn't explicitly mark anything else as an output, so there's nothing else there.</p> <p>And there it is! It may seem like a lot of work to accomplish the same result as the original pipeline, but you do get all those lovely reports generated automatically, and you now have a solid foundation for taking advantage of additional features of nf-core, including input validation and some neat metadata handling capabilities that we'll cover in a later section.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#takeaway_3","title":"Takeaway","text":"<p>You know how to convert a regular Nextflow pipeline into an nf-core style pipeline using the nf-core template. As part of that, you learned how to make a workflow composable, and identify the most common elements of the nf-core template that need to be adapted when developing a custom nf-core style pipeline.</p>"},{"location":"hello_nf-core/02_rewrite_hello/#whats-next_3","title":"What's next?","text":"<p>Take a big break, that was hard work! Your brain deserves to chill out and you could probably use some hydration and a bit of stretching. When you're ready, move on to the next section to learn how to add an nf-core module to an existing nf-core style pipeline. (COMING SOON)</p>"},{"location":"hello_nf-core/03_add_module/","title":"Part 3: Add an existing nf-core module","text":"<p>In this third part of the Hello nf-core training course, we show you how to add an existing nf-core module to your pipeline.</p> <p>TODO: THIS IS A WIP AND SHOULD NOT BE ADDED TO THE NAVIGATION</p> <p>(NOT SUBJECT TO REVIEW)</p>"},{"location":"hello_nf-core/03_add_module/#1-find-catcat-to-replace-collectgreetings","title":"1. Find cat/cat to replace collectGreetings","text":"<p>TODO: instructions</p>"},{"location":"hello_nf-core/03_add_module/#takeaway","title":"Takeaway","text":"<p>You now know how to [...].</p>"},{"location":"hello_nf-core/03_add_module/#whats-next","title":"What's next?","text":"<p>Find out [...].</p>"},{"location":"hello_nf-core/03_add_module/#2-install-and-import-the-module","title":"2. Install and import the module","text":"<p>TODO: instructions</p>"},{"location":"hello_nf-core/03_add_module/#takeaway_1","title":"Takeaway","text":"<p>You know how to [...].</p>"},{"location":"hello_nf-core/03_add_module/#whats-next_1","title":"What's next?","text":"<p>Learn how to [...].</p>"},{"location":"hello_nf-core/03_add_module/#3-wire-up-the-module-to-the-workflow","title":"3. Wire up the module to the workflow","text":"<p>TODO: instructions</p>"},{"location":"hello_nf-core/03_add_module/#takeaway_2","title":"Takeaway","text":"<p>You know how to [...].</p>"},{"location":"hello_nf-core/03_add_module/#whats-next_2","title":"What's next?","text":"<p>[...].</p>"},{"location":"hello_nf-core/03_add_module/#4-anything-else","title":"4. [anything else?]","text":"<p>TODO: instructions</p>"},{"location":"hello_nf-core/03_add_module/#takeaway_3","title":"Takeaway","text":"<p>You know how to [...].</p>"},{"location":"hello_nf-core/03_add_module/#whats-next_3","title":"What's next?","text":"<p>[...].</p>"},{"location":"hello_nf-core/04_input_validation/","title":"Part 4: Input validation","text":"<p>In this fourth part of the Hello nf-core training course, we show you how to use the nf-schema plugin to validate inputs.</p> <p>TODO: THIS IS A WIP AND SHOULD NOT BE ADDED TO THE NAVIGATION</p> <p>(NOT SUBJECT TO REVIEW)</p>"},{"location":"hello_nf-core/04_input_validation/#1-add-a-schema","title":"1. Add a schema","text":"<p>TODO: instructions</p>"},{"location":"hello_nf-core/04_input_validation/#takeaway","title":"Takeaway","text":"<p>You now know how to [...].</p>"},{"location":"hello_nf-core/04_input_validation/#whats-next","title":"What's next?","text":"<p>Find out [...].</p>"},{"location":"hello_nf-core/04_input_validation/#2-set-up-the-validation","title":"2. Set up the validation","text":"<p>TODO: instructions</p>"},{"location":"hello_nf-core/04_input_validation/#takeaway_1","title":"Takeaway","text":"<p>You know how to [...].</p>"},{"location":"hello_nf-core/04_input_validation/#whats-next_1","title":"What's next?","text":"<p>Learn how to [...].</p>"},{"location":"hello_nf-core/04_input_validation/#3-something-with-metamap","title":"3. Something with metamap?","text":"<p>TODO: instructions</p>"},{"location":"hello_nf-core/04_input_validation/#takeaway_2","title":"Takeaway","text":"<p>You know how to [...].</p>"},{"location":"hello_nf-core/04_input_validation/#whats-next_2","title":"What's next?","text":"<p>[...].</p>"},{"location":"hello_nf-core/04_input_validation/#4-anything-else","title":"4. [anything else?]","text":"<p>TODO: instructions</p>"},{"location":"hello_nf-core/04_input_validation/#takeaway_3","title":"Takeaway","text":"<p>You know how to [...].</p>"},{"location":"hello_nf-core/04_input_validation/#whats-next_3","title":"What's next?","text":"<p>[...].</p>"},{"location":"hello_nf-core/next_steps/","title":"Next Steps","text":"<p>Congrats again on completing the Hello nf-core training course and thank you for completing our survey!</p>"},{"location":"hello_nf-core/next_steps/#1-top-3-ways-to-level-up-your-nextflow-skills","title":"1. Top 3 ways to level up your Nextflow skills","text":"<p>Here are our top three recommendations for what to do next based on the course you just completed.</p>"},{"location":"hello_nf-core/next_steps/#11-get-involved-with-the-nf-core-community","title":"1.1. Get involved with the nf-core community","text":"<p>The nf-core community is a welcoming, collaborative space where you can keep learning, connect with others, and make meaningful contributions. Join the nf-core Slack workspace to ask questions, share ideas, and stay updated. Whether you attend a hackathon, help improve a pipeline, or just chat with fellow community members, getting involved is a great way to grow your skills and be part of the future of open bioinformatics.</p>"},{"location":"hello_nf-core/next_steps/#12-apply-nextflow-to-a-scientific-analysis-use-case","title":"1.2. Apply Nextflow to a scientific analysis use case","text":"<p>Check out the Nextflow for Science page for a list of short standalone courses that demonstrate how to apply the basic concepts and mechanisms presented in Hello Nextflow to common scientific analysis use cases.</p> <p>If you don't see your domain represented by a relatable use case, let us know in the Community forum so we can add it to our development list.</p>"},{"location":"hello_nf-core/next_steps/#13-master-more-advanced-nextflow-features","title":"1.3. Master more advanced Nextflow features","text":"<p>In the Hello courses, we keep the level of technical complexity low on purpose to avoid overloading you with information you don't need in order to get started with Nextflow. As you move forward with your work, you're going to want to learn how to use the full feature set and power of Nextflow.</p> <p>To that end, we are currently working on a collection of Side Quests, which are meant to be short standalone courses that go deep into specific topics like testing, metadata handling, using conditional statements and the differences between working on HPC vs. cloud.</p> <p>For any topics that's not covered there yet, browse the Fundamentals Training and Advanced Training to find training materials about the topics that interest you.</p>"},{"location":"hello_nf-core/next_steps/#2-check-out-seqera-platform","title":"2. Check out Seqera Platform","text":"<p>Seqera Platform is the best way to run Nextflow in practice.</p> <p>It is a cloud-based platform developed by the creators of Nextflow that you can connect to your own compute infrastructure (whether local, HPC or cloud) to make it much easier to launch and manage your workflows, as well as manage your data and run analyses interactively in a cloud environment.</p> <p>The Free Tier is available for free use by everyone (with usage quotas). Qualifying academics can get free Pro-level access (no usage limitations) through the Academic Program.</p> <p>Have a look at the Seqera Platform tutorials to see if this might be useful to you.</p>"},{"location":"hello_nf-core/next_steps/#thats-it-for-now","title":"That's it for now!","text":"<p>Good luck in your Nextflow journey and don't hesitate to let us know in the Community forum what else we could do to help.</p>"},{"location":"hello_nf-core/survey/","title":"Feedback survey","text":"<p>Before you move on, please complete this short 4-question survey to rate the training, share any feedback you may have about your experience, and let us know what else we could do to help you in your Nextflow journey.</p> <p>This should take you less than a minute to complete. Thank you for helping us improve our training materials for everyone!</p>"},{"location":"nextflow_run/","title":"Nextflow Run","text":"<p>Hello! You are now on the path to running reproducible and scalable scientific workflows using Nextflow.</p> <p>The rise of big data has made it increasingly necessary to be able to analyze and perform experiments on large datasets in a portable and reproducible manner. Parallelization and distributed computing are the best ways to tackle this challenge, but the tools commonly available to computational scientists often lack good support for these techniques, or they provide a model that fits poorly with the needs of computational scientists. Nextflow was particularly created to address these challenges.</p> <p>During this training, you will be introduced to Nextflow in a series of complementary hands-on tutorials.</p> <p>Let's get started!</p> <p></p>"},{"location":"nextflow_run/#learning-objectives","title":"Learning objectives","text":"<p>In this workshop, you will learn foundational concepts and skills for configuring and running Nextflow pipelines without treating them like a black box.</p> <p>By the end of this workshop you will be able to:</p> <ul> <li>Launch a Nextflow workflow locally</li> <li>Find and interpret outputs (results) and log files generated by Nextflow</li> <li>Troubleshoot basic issues</li> <li>Identify the main components of a Nextflow workflow and explain at a general level how they relate to what the workflow does</li> <li>Configure and manage the execution of Nextflow workflows</li> </ul>"},{"location":"nextflow_run/#audience-prerequisites","title":"Audience &amp; prerequisites","text":"<p>This is a workshop for those who are completely new to Nextflow. Some basic familiarity with the command line, and common file formats is assumed.</p> <p>Prerequisites</p> <ul> <li>A GitHub account</li> <li>Basic familiarity with command line</li> </ul>"},{"location":"nextflow_run/00_orientation/","title":"Orientation","text":"<p>This orientation assumes you have already opened the training environment by clicking on the \"Open in GitHub Codespaces\" button. If not, please do so now, ideally in a second browser window or tab so you can refer back to these instructions.</p> <p></p>"},{"location":"nextflow_run/00_orientation/#github-codespaces","title":"GitHub Codespaces","text":"<p>The GitHub Codespaces environment contains all the software, code and data necessary to work through this training course, so you don't need to install anything yourself. However, you do need a (free) GitHub account to log in, and you should take a few minutes to familiarize yourself with the interface.</p> <p>If you have not yet done so, please go through the Environment Setup mini-course before going any further.</p>"},{"location":"nextflow_run/00_orientation/#working-directory","title":"Working directory","text":"<p>Throughout this training course, we'll be working in the <code>nextflow-run/</code> directory.</p> <p>Change directory now by running this command in the terminal:</p> <pre><code>cd nextflow-run/\n</code></pre> <p>Tip</p> <p>If for whatever reason you move out of this directory, you can always use the full path to return to it, assuming you're running this within the GitHub Codespaces training environment:</p> <pre><code>cd /workspaces/training/nextflow-run\n</code></pre> <p>Now let's have a look at the contents of this directory.</p>"},{"location":"nextflow_run/00_orientation/#materials-provided","title":"Materials provided","text":"<p>You can explore the contents of this directory by using the file explorer on the left-hand side of the training workspace. Alternatively, you can use the <code>tree</code> command.</p> <p>Throughout the course, we use the output of <code>tree</code> to represent directory structure and contents in a readable form, sometimes with minor modifications for clarity.</p> <p>Here we generate a table of contents to the second level down:</p> <pre><code>tree . -L 2\n</code></pre> <p>If you run this inside <code>nextflow-run</code>, you should see the following output:</p> Directory contents<pre><code>.\n\u251c\u2500\u2500 1-hello.nf\n\u251c\u2500\u2500 2a-inputs.nf\n\u251c\u2500\u2500 2b-multistep.nf\n\u251c\u2500\u2500 2c-modules.nf\n\u251c\u2500\u2500 2d-container.nf\n\u251c\u2500\u2500 3-main.nf\n\u251c\u2500\u2500 modules\n\u2502   \u251c\u2500\u2500 collectGreetings.nf\n\u2502   \u251c\u2500\u2500 convertToUpper.nf\n\u2502   \u251c\u2500\u2500 cowpy.nf\n\u2502   \u2514\u2500\u2500 sayHello.nf\n\u251c\u2500\u2500 nextflow.config\n\u2514\u2500\u2500 test-params.yaml\n\n1 directory, 12 files\n</code></pre> <p>Here's a summary of what you should know to get started:</p> <ul> <li> <p>The <code>.nf</code> files are workflow scripts that are numbered based on what part of the course they're used in.</p> </li> <li> <p>The file <code>nextflow.config</code> is a configuration file that sets minimal environment properties.   You can ignore it for now.</p> </li> <li> <p>The file <code>greetings.csv</code> contains input data we'll use in most of the course. It is described in Part 2, when we introduce it for the first time.</p> </li> <li> <p>The file <code>test-params.yaml</code> is a file we'll use in Part 3. You can ignore it for now.</p> </li> </ul> <p>Now, to begin the course, click on the arrow in the bottom right corner of this page.</p>"},{"location":"nextflow_run/01_basics/","title":"Part 1: Run basic operations","text":"<p>In this first part of the Nextflow Run training course, we ease into the topic with a very basic domain-agnostic Hello World example, which we'll use to demonstrate essential operations and point out the corresponding Nextflow code components.</p> <p>Tip</p> <p>A \"Hello World!\" is a minimalist example that is meant to demonstrate the basic syntax and structure of a programming language or software framework. The example typically consists of printing the phrase \"Hello, World!\" to the output device, such as the console or terminal, or writing it to a file.</p>"},{"location":"nextflow_run/01_basics/#0-warmup-run-hello-world-directly","title":"0. Warmup: Run Hello World directly","text":"<p>Let's demonstrate this with a simple command that we run directly in the terminal, to show what it does before we wrap it in Nextflow.</p> <p>Tip</p> <p>Remember that you should now be inside the <code>nextflow-run/</code> directory as described in the Orientation.</p>"},{"location":"nextflow_run/01_basics/#01-make-the-terminal-say-hello","title":"0.1. Make the terminal say hello","text":"<pre><code>echo 'Hello World!'\n</code></pre> <p>This outputs the text 'Hello World' to the terminal.</p> Output<pre><code>Hello World!\n</code></pre>"},{"location":"nextflow_run/01_basics/#02-now-make-it-write-the-text-output-to-a-file","title":"0.2. Now make it write the text output to a file","text":"<pre><code>echo 'Hello World!' &gt; output.txt\n</code></pre> <p>This does not output anything to the terminal.</p> Output<pre><code>\n</code></pre>"},{"location":"nextflow_run/01_basics/#03-show-the-file-contents","title":"0.3. Show the file contents","text":"<pre><code>cat output.txt\n</code></pre> <p>The text 'Hello World' is now in the output file we specified.</p> output.txt<pre><code>Hello World!\n</code></pre> <p>Tip</p> <p>In the training environment, you can also find the output file in the file explorer, and view its contents by clicking on it.</p>"},{"location":"nextflow_run/01_basics/#takeaway","title":"Takeaway","text":"<p>You now know how to run a simple command in the terminal that outputs some text, and optionally, how to make it write the output to a file.</p>"},{"location":"nextflow_run/01_basics/#whats-next","title":"What's next?","text":"<p>Find out what it takes to run a Nextflow workflow that achieves the same result.</p>"},{"location":"nextflow_run/01_basics/#1-run-the-workflow","title":"1. Run the workflow","text":"<p>We provide you with a workflow script named <code>1-hello.nf</code> that takes an input via a command-line argument named <code>--greeting</code> and produces a text file containing that greeting. We're not going to look at the code yet; first let's see what it looks like to run it.</p>"},{"location":"nextflow_run/01_basics/#11-launch-the-workflow-and-monitor-execution","title":"1.1. Launch the workflow and monitor execution","text":"<p>In the terminal, run the following command:</p> <pre><code>nextflow run 1-hello.nf --greeting 'Hello World!'\n</code></pre> <p>You console output should look something like this:</p> Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `1-hello.nf` [goofy_torvalds] DSL2 - revision: c33d41f479\n\nexecutor &gt;  local (1)\n[a3/7be2fa] sayHello | 1 of 1 \u2714\n</code></pre> <p>Congratulations, you just ran your first Nextflow workflow!</p> <p>The most important output here is the last line (line 6):</p> Output<pre><code>[a3/7be2fa] sayHello | 1 of 1 \u2714\n</code></pre> <p>This tells us that the <code>sayHello</code> process was successfully executed once (<code>1 of 1 \u2714</code>).</p> <p>That's great, but you may be wondering: where is the output?</p>"},{"location":"nextflow_run/01_basics/#12-find-the-output-file-in-the-results-directory","title":"1.2. Find the output file in the <code>results</code> directory","text":"<p>This workflow is configured to publish its output to a directory called <code>results</code>. If you look at your current directory, you will see that when you ran the workflow, Nextflow created a new directory called <code>results</code>, which contains a file called <code>output.txt</code>.</p> results/<pre><code>results\n\u2514\u2500\u2500 output.txt\n</code></pre> <p>Open the file; the contents should match the string you specified on the command line.</p> File contents results/output.txt<pre><code>Hello World!\n</code></pre> <p>That's great, our workflow did what it was supposed to do!</p> <p>However, be aware that the 'published' result is a copy (or in some cases a symlink) of the actual output produced by Nextflow when it executed the workflow.</p> <p>So now, we are going to peek under the hood to see where Nextflow actually executed the work.</p> <p>Warning</p> <p>Not all workflows will be set up to publish outputs to a results directory, and/or the directory name may be different. A little further in this section, we will show you how to find out where this behavior is specified.</p>"},{"location":"nextflow_run/01_basics/#13-find-the-original-output-and-logs-in-the-work-directory","title":"1.3. Find the original output and logs in the <code>work/</code> directory","text":"<p>When you run a workflow, Nextflow creates a distinct 'task directory' for every single invocation of each process in the workflow (=every step in the pipeline). For each one, it will stage the necessary inputs, execute the relevant instruction(s) and write outputs and log files within that one directory, which is named automatically using a hash in order to make it unique.</p> <p>All of these task directories will live under a directory called <code>work</code> within your current directory (where you're running the command).</p> <p>That may sound confusing, so let's see what that looks like in practice.</p> <p>Going back to the console output for the workflow we ran earlier, we had this line:</p> Excerpt of command output<pre><code>[a3/7be2fa] sayHello | 1 of 1 \u2714\n</code></pre> <p>See how the line starts with <code>[a3/7be2fa]</code>? That is a truncated form of the task directory path for that one process call, and tells you where to find the output of the <code>sayHello</code> process call within the <code>work/</code> directory path.</p> <p>You can find the full path by typing the following command (replacing <code>a3/7be2fa</code> with what you see in your own terminal) and pressing the tab key to autocomplete the path or adding an asterisk:</p> <pre><code>ls work/a3/7be2fa*\n</code></pre> <p>This should yield the full path directory path: <code>work/a3/7be2fa7be2fad5e71e5f49998f795677fd68</code></p> <p>Let's take a look at what's in there.</p> <p>Tip</p> <p>If you browse the contents of the task subdirectory in the VSCode file explorer, you'll see all the files right away. However, the log files are set to be invisible in the terminal, so if you want to use <code>ls</code> or <code>tree</code> to view them, you'll need to set the relevant option for displaying invisible files.</p> <pre><code>tree -a work\n</code></pre> <p>The exact subdirectory names will be different on your system.</p> Directory contents work/<pre><code>work\n\u2514\u2500\u2500 a3\n    \u2514\u2500\u2500 7be2fad5e71e5f49998f795677fd68\n        \u251c\u2500\u2500 .command.begin\n        \u251c\u2500\u2500 .command.err\n        \u251c\u2500\u2500 .command.log\n        \u251c\u2500\u2500 .command.out\n        \u251c\u2500\u2500 .command.run\n        \u251c\u2500\u2500 .command.sh\n        \u251c\u2500\u2500 .exitcode\n        \u2514\u2500\u2500 output.txt\n</code></pre> <p>You should immediately recognize the <code>output.txt</code> file, which is in fact the original output of the <code>sayHello</code> process that got published to the <code>results</code> directory. If you open it, you will find the <code>Hello World!</code> greeting again.</p> File contents work/a3/7be2fa7be2fad5e71e5f49998f795677fd68/output.txt<pre><code>Hello World!\n</code></pre> <p>So what about all those other files?</p> <p>These are the helper and log files that Nextflow wrote as part of the task execution:</p> <ul> <li><code>.command.begin</code>: Sentinel file created as soon as the task is launched.</li> <li><code>.command.err</code>: Error messages (<code>stderr</code>) emitted by the process call</li> <li><code>.command.log</code>: Complete log output emitted by the process call</li> <li><code>.command.out</code>: Regular output (<code>stdout</code>) by the process call</li> <li><code>.command.run</code>: Full script run by Nextflow to execute the process call</li> <li><code>.command.sh</code>: The command that was actually run by the process call</li> <li><code>.exitcode</code>: The exit code resulting from the command</li> </ul> <p>The <code>.command.sh</code> file is especially useful because it shows you the main command Nextflow executed not including all the bookkeeping and task/environment setup.</p> File contents work/a3/7be2fa7be2fad5e71e5f49998f795677fd68/command.sh<pre><code>#!/bin/bash -ue\necho 'Hello World!' &gt; output.txt\n</code></pre> <p>So this confirms that the workflow composed the same command we ran directly on the command-line earlier.</p> <p>Tip</p> <p>When something goes wrong and you need to troubleshoot what happened, it can be useful to look at the <code>command.sh</code> script to check exactly what command Nextflow composed based on the workflow instructions, variable interpolation and so on.</p>"},{"location":"nextflow_run/01_basics/#14-optional-exercise-re-run-with-different-greetings","title":"1.4. Optional exercise: re-run with different greetings","text":"<p>Try re-running the workflow a few times with different values for the <code>--greeting</code> argument, then look at both the contents of the <code>results/</code> directory and the task directories.</p> <p>Observe how the outputs and logs of isolated task directories are preserved, whereas the contents of the <code>results</code> directory are overwritten by the output of subsequent executions.</p>"},{"location":"nextflow_run/01_basics/#takeaway_1","title":"Takeaway","text":"<p>You know how to run a simple Nextflow script, monitor its execution and find its outputs.</p>"},{"location":"nextflow_run/01_basics/#whats-next_1","title":"What's next?","text":"<p>Learn how to read a basic Nextflow script and identify how its components relate to its functionality.</p>"},{"location":"nextflow_run/01_basics/#2-examine-the-hello-world-workflow-starter-script","title":"2. Examine the Hello World workflow starter script","text":"<p>What we did there was basically treating the workflow script like a black box. Now that we've seen what it does, let's open the box and look inside.</p> <p>The goal here is not to memorize the syntax of Nextflow code, but to form some basic intuition of what are the main components and how they are organized.</p>"},{"location":"nextflow_run/01_basics/#21-examine-the-overall-code-structure","title":"2.1. Examine the overall code structure","text":"<p>Let's open the <code>1-hello.nf</code> script in the editor pane.</p> Code 1-hello.nf<pre><code>#!/usr/bin/env nextflow\n\n/*\n * Use echo to print a greeting to a file\n */\nprocess sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        val greeting\n\n    output:\n        path 'output.txt'\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // emit a greeting\n    sayHello(params.greeting)\n}\n</code></pre> <p>A Nextflow script involves two main types of core components: one or more processes, and the workflow itself. Each process describes what operation(s) the corresponding step in the pipeline should accomplish, while the workflow describes the dataflow logic that connects the various steps.</p> <p>Let's take a closer look at the process block first, then we'll look at the workflow block.</p>"},{"location":"nextflow_run/01_basics/#22-the-process-definition","title":"2.2. The <code>process</code> definition","text":"<p>The first block of code describes a process. The process definition starts with the keyword <code>process</code>, followed by the process name and finally the process body delimited by curly braces. The process body must contain a script block which specifies the command to run, which can be anything you would be able to run in a command line terminal.</p> <p>Here we have a process called <code>sayHello</code> that takes an input variable called <code>greeting</code> and writes its output to a file named <code>output.txt</code>.</p> Code 1-hello.nf<pre><code>/*\n * Use echo to print a greeting to a file\n */\nprocess sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        val greeting\n\n    output:\n        path 'output.txt'\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; output.txt\n    \"\"\"\n}\n</code></pre> <p>This is a very minimal process definition that just contains an <code>input</code> definition, an <code>output</code> definition and the <code>script</code> to execute.</p> <p>The <code>input</code> definition includes the <code>val</code> qualifier, which tells Nextflow to expect a value of some kind (can be a string, a number, whatever).</p> <p>The <code>output</code> definition includes the <code>path</code> qualifier, which tells Nextflow this should be handled as a path (includes both directory paths and files).</p> <p>Tip</p> <p>The output definition does not determine what output will be created. It simply declares where to find the expected output file(s), so that Nextflow can look for it once execution is complete.</p> <p>This is necessary for verifying that the command was executed successfully and for passing the output to downstream processes if needed. Output produced that doesn't match what is declared in the output block will not be passed to downstream processes.</p> <p>In a real-world pipeline, a process usually contains additional information such as process directives, which we'll introduce in a little bit.</p>"},{"location":"nextflow_run/01_basics/#23-the-workflow-definition","title":"2.3. The <code>workflow</code> definition","text":"<p>The second block of code describes the workflow itself. The workflow definition starts with the keyword <code>workflow</code>, followed by an optional name, then the workflow body delimited by curly braces.</p> <p>Here we have a workflow that consists of one call to the <code>sayHello</code> process, which takes an input, <code>params.greeting</code>, which holds the value we gave to the <code>--greeting</code> parameter.</p> 1-hello.nf<pre><code>workflow {\n\n    // emit a greeting\n    sayHello(params.greeting)\n}\n</code></pre> <p>This is a very minimal workflow definition. In a real-world pipeline, the workflow typically contains multiple calls to processes connected by channels, and there may be default values set up for the variable inputs.</p> <p>We'll look into that in Part 2 of the course.</p>"},{"location":"nextflow_run/01_basics/#24-the-params-system-of-command-line-parameters","title":"2.4. The <code>params</code> system of command-line parameters","text":"<p>The <code>params.greeting</code> we provide to the <code>sayHello()</code> process call is a neat bit of Nextflow code and is worth spending an extra minute on.</p> <p>As mentioned above, that's how we pass the value of the <code>--greeting</code> command-line parameter to the <code>sayHello()</code> process call. In fact, simply declaring <code>params.someParameterName</code> will enable us to give the workflow a parameter named <code>--someParameterName</code> from the command-line.</p> <p>Tip</p> <p>These workflow parameters declared using the <code>params</code> system always take two dashes (<code>--</code>). This distinguishes them from Nextflow-level parameters, which only take one dash (<code>-</code>).</p>"},{"location":"nextflow_run/01_basics/#takeaway_2","title":"Takeaway","text":"<p>You now know how a simple Nextflow workflow is structured, and how the basic components relate to its functionality.</p>"},{"location":"nextflow_run/01_basics/#whats-next_2","title":"What's next?","text":"<p>Learn to manage your workflow executions conveniently.</p>"},{"location":"nextflow_run/01_basics/#3-manage-workflow-executions","title":"3. Manage workflow executions","text":"<p>Knowing how to launch workflows and retrieve outputs is great, but you'll quickly find there are a few other aspects of workflow management that will make your life easier.</p> <p>Here we show you how to take advantage of the <code>resume</code> feature for when you need to re-launch the same workflow, how to inspect the execution logs with <code>nextflow log</code>, and how to delete older work directories with <code>nextflow clean</code>.</p>"},{"location":"nextflow_run/01_basics/#31-re-launch-a-workflow-with-resume","title":"3.1. Re-launch a workflow with <code>-resume</code>","text":"<p>Sometimes, you're going to want to re-run a pipeline that you've already launched previously without redoing any work that was already completed successfully.</p> <p>Nextflow has an option called <code>-resume</code> that allows you to do this. Specifically, in this mode, any processes that have already been run with the exact same code, settings and inputs will be skipped. This means Nextflow will only run processes that you've added or modified since the last run, or to which you're providing new settings or inputs.</p> <p>There are two key advantages to doing this:</p> <ul> <li>If you're in the middle of developing a pipeline, you can iterate more rapidly since you only have to run the process(es) you're actively working on in order to test your changes.</li> <li>If you're running a pipeline in production and something goes wrong, in many cases you can fix the issue and relaunch the pipeline, and it will resume running from the point of failure, which can save you a lot of time and compute.</li> </ul> <p>To use it, simply add <code>-resume</code> to your command and run it:</p> <pre><code>nextflow run 1-hello.nf --greeting 'Hello World!' -resume\n</code></pre> <p>The console output should look similar.</p> Command output <pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `1-hello.nf` [tiny_noyce] DSL2 - revision: c33d41f479\n\n[a3/7be2fa] process &gt; sayHello [100%] 1 of 1, cached: 1 \u2714\n</code></pre> <p>Look for the <code>cached:</code> bit that has been added in the process status line (line 5), which means that Nextflow has recognized that it has already done this work and simply reused the result from the previous successful run.</p> <p>You can also see that the work subdirectory hash is the same as in the previous run. Nextflow is literally pointing you to the previous execution and saying \"I already did that over there.\"</p> <p>Tip</p> <p>When your re-run a pipeline with <code>resume</code>, Nextflow does not overwrite any files written to a <code>publishDir</code> directory by any process call that was previously run successfully.</p>"},{"location":"nextflow_run/01_basics/#32-inspect-the-log-of-past-executions","title":"3.2. Inspect the log of past executions","text":"<p>Whenever you launch a nextflow workflow, a line gets written to a log file called <code>history</code>, under a hidden directory called <code>.nextflow</code> in the current working directory.</p> File contents .nextflow/history<pre><code>2025-07-04 19:27:09\t1.8s\twise_watson\tOK\t3539118582ccde68dde471cc2c66295c\ta02c9c46-c3c7-4085-9139-d1b9b5b194c8\tnextflow run 1-hello.nf --greeting 'Hello World'\n2025-07-04 19:27:20\t2.9s\tspontaneous_blackwell\tOK\t3539118582ccde68dde471cc2c66295c\t59a5db23-d83c-4c02-a54e-37ddb73a337e\tnextflow run 1-hello.nf --greeting Bonjour\n2025-07-04 19:27:31\t1.8s\tgigantic_yonath\tOK\t3539118582ccde68dde471cc2c66295c\t5acaa83a-6ad6-4509-bebc-cb25d5d7ddd0\tnextflow run 1-hello.nf --greeting 'Dobry den'\n2025-07-04 19:27:45\t2.4s\tbackstabbing_swartz\tOK\t3539118582ccde68dde471cc2c66295c\t5f4b3269-5b53-404a-956c-cac915fbb74e\tnextflow run 1-hello.nf --greeting Konnichiwa\n2025-07-04 19:27:57\t2.1s\tgoofy_wilson\tOK\t3539118582ccde68dde471cc2c66295c\t5f4b3269-5b53-404a-956c-cac915fbb74e\tnextflow run 1-hello.nf --greeting Konnichiwa -resume\n</code></pre> <p>This file gives you the timestamp, run name, status, revision ID, session ID and full command line for every Nextflow run that has been launched from within the current working directory.</p> <p>A more convenient way to access this information is to use the <code>nextflow log</code> command.</p> <pre><code>nextflow log\n</code></pre> <p>This will output the contents of the log file to the terminal, augmented with a header line.</p> Command output <pre><code>TIMESTAMP               DURATION        RUN NAME                STATUS  REVISION ID     SESSION ID                              COMMAND\n2025-07-04 19:27:09     1.8s            wise_watson             OK       3539118582     a02c9c46-c3c7-4085-9139-d1b9b5b194c8    nextflow run 1-hello.nf --greeting 'Hello World'\n2025-07-04 19:27:20     2.9s            spontaneous_blackwell   OK       3539118582     59a5db23-d83c-4c02-a54e-37ddb73a337e    nextflow run 1-hello.nf --greeting Bonjour\n2025-07-04 19:27:31     1.8s            gigantic_yonath         OK       3539118582     5acaa83a-6ad6-4509-bebc-cb25d5d7ddd0    nextflow run 1-hello.nf --greeting 'Dobry den'\n2025-07-04 19:27:45     2.4s            backstabbing_swartz     OK       3539118582     5f4b3269-5b53-404a-956c-cac915fbb74e    nextflow run 1-hello.nf --greeting Konnichiwa\n2025-07-04 19:27:57     2.1s            goofy_wilson            OK       3539118582     5f4b3269-5b53-404a-956c-cac915fbb74e    nextflow run 1-hello.nf --greeting Konnichiwa -resume\n</code></pre> <p>You'll notice that the session ID changes whenever you run a new <code>nextflow run</code> command, EXCEPT if you're using the <code>-resume</code> option. In that case, the session ID stays the same.</p> <p>Nextflow uses the session ID to group run caching information under the <code>cache</code> directory, also located under <code>.nextflow</code>.</p>"},{"location":"nextflow_run/01_basics/#33-delete-older-work-directories","title":"3.3. Delete older work directories","text":"<p>During the development process, you'll typically run your draft pipelines a large number of times, which can lead to an accumulation of very many files across many subdirectories. Since the subdirectories are named randomly, it is difficult to tell from their names what are older vs. more recent runs.</p> <p>Nextflow includes a convenient <code>clean</code> subcommand that can automatically delete the work subdirectories for past runs that you no longer care about, with several options to control what will be deleted.</p> <p>Here we show you an example that deletes all subdirectories from runs before a given run, specified using its run name. The run name is the machine-generated two-part string shown in square brackets in the <code>Launching (...)</code> console output line, which we also saw recorded in the Nextflow log that we looked at earlier.</p> <p>You can use the Nextflow log to look up a run based on its timestamp and/or command line.</p> <p>Once we have that, first we try the <code>nextflow clean</code> command using the dry run flag <code>-n</code> to check what will be deleted:</p> <pre><code>nextflow clean -before backstabbing_swartz -n\n</code></pre> <p>The output will have different task directory names and may have a different number of lines, but it should look similar to the example given below.</p> Command output Output<pre><code>Would remove /workspaces/training/hello-nextflow/work/eb/1a5de36637b475afd88fca7f79e024\nWould remove /workspaces/training/hello-nextflow/work/6b/19b0e002ea13486d3a0344c336c1d0\nWould remove /workspaces/training/hello-nextflow/work/45/9a6dd7ab771f93003d040956282883\n</code></pre> <p>If you don't see any lines output, you either did not provide a valid run name or there are no past runs to delete.</p> <p>If the output looks as expected and you want to proceed with the deletion, re-run the command with the <code>-f</code> flag instead of <code>-n</code>:</p> <pre><code>nextflow clean -before backstabbing_swartz -f\n</code></pre> <p>The output should be similar to before, but now saying 'Removed' instead of 'Would remove'.</p> Command output Output<pre><code>Removed /workspaces/training/hello-nextflow/work/eb/1a5de36637b475afd88fca7f79e024\nRemoved /workspaces/training/hello-nextflow/work/6b/19b0e002ea13486d3a0344c336c1d0\nRemoved /workspaces/training/hello-nextflow/work/45/9a6dd7ab771f93003d040956282883\n</code></pre> <p>Warning</p> <p>Deleting work subdirectories from past runs removes them from Nextflow's cache and deletes any outputs that were stored in those directories. That means it breaks Nextflow's ability to resume execution without re-running the corresponding processes.</p> <p>You are responsible for saving any outputs that you care about or plan to rely on! If you're using the <code>publishDir</code> directive for that purpose, make sure to use the <code>copy</code> mode, not the <code>symlink</code> mode.</p>"},{"location":"nextflow_run/01_basics/#takeaway_3","title":"Takeaway","text":"<p>You know how to relaunch a pipeline without repeating steps that were already run in an identical way, inspect the execution log, and use the <code>nextflow clean</code> command to clean up old work directories.</p>"},{"location":"nextflow_run/01_basics/#whats-next_3","title":"What's next?","text":"<p>Take a little break! You've just absorbed the building blocks of Nextflow syntax and basic usage instructions.</p> <p>In the next section of this training, we're going to look at four successively more realistic versions of the Hello World pipeline that will demonstrate how Nextflow allows you to process multiple inputs efficiently, run workflows composed of multiple steps connected together, leverage modular code components, and utilize containers for greater reproducibility and portability.</p>"},{"location":"nextflow_run/02_pipeline/","title":"Part 2: Run pipelines","text":"<p>In Part 1 of this course (Run Basic Operations), we started with an example workflow that had only minimal features in order to keep the code complexity low. For example, <code>1-hello.nf</code> used a command-line parameter (<code>--greeting</code>) to provide a single value at a time.</p> <p>However, most real-world pipelines use more sophisticated features in order to enable efficient processing of large amounts of data at scale, and apply multiple processing steps chained together by sometimes complex logic.</p> <p>In this part of the training, we demonstrate key features of real-world pipelines by trying out expanded versions of the original Hello World pipeline.</p>"},{"location":"nextflow_run/02_pipeline/#1-processing-input-data-from-a-file","title":"1. Processing input data from a file","text":"<p>In a real-world pipeline, we typically want to process multiple data points (or data series) contained in one or more input files. And wherever possible, we want to run the processing of independent data in parallel, to shorten the time spent waiting for analysis.</p> <p>To enable this efficiently, Nextflow uses a system of queues called channels.</p> <p>To demonstrate this, we've prepared a a CSV file called <code>greetings.csv</code> that contains several input greetings, mimicking the kind of columnar data you might want to process in a real data analysis.</p> CSV file contents greetings.csv<pre><code>Hello,English,123\nBonjour,French,456\nHol\u00e0,Spanish,789\n</code></pre>  Note that the numbers are not meaningful, they are just there for illustrative purposes.   <p>And we've written an improved version of the original workflow, now called <code>2a-inputs.nf</code>, that will read in the CSV file, extract the greetings and write each of them to a separate file.</p> sayHello*-output.txtHelloBonjourHol\u00e0Hello,English,123 Bonjour,French,456Hola,Spanish,789greetings.csvHello-output.txtBonjour-output.txtHol\u00e0-output.txt <p>Let's run the workflow first, and we'll take a look at the relevant Nextflow code afterward.</p>"},{"location":"nextflow_run/02_pipeline/#11-run-the-workflow","title":"1.1. Run the workflow","text":"<p>Run the following command in your terminal.</p> <pre><code>nextflow run 2a-inputs.nf --input greetings.csv\n</code></pre> <p>This should run without error.</p> Command output <pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `2a-inputs.nf` [mighty_sammet] DSL2 - revision: 29fb5352b3\n\nexecutor &gt;  local (3)\n[8e/0eb066] sayHello (2) [100%] 3 of 3 \u2714\n</code></pre> <p>Excitingly, this seems to indicate that '3 of 3' calls were made for the process, which is encouraging, since there were three rows of data in the CSV we provided as input. This suggests the <code>sayHello()</code> process was called three times, once on each input row.</p>"},{"location":"nextflow_run/02_pipeline/#12-find-the-outputs-in-the-results-directory","title":"1.2. Find the outputs in the <code>results</code> directory","text":"<p>Let's look at the 'results' directory to see if our workflow is still writing a copy of our outputs there.</p> Directory contents results/<pre><code>results\n\u251c\u2500\u2500 Bonjour-output.txt\n\u251c\u2500\u2500 Hello-output.txt\n\u2514\u2500\u2500 Hol\u00e0-output.txt\n</code></pre> <p>Yes! We see three output files with different names, conveniently enough. (Spoiler: we changed the workflow to name the files differently.)</p> <p>If you haven't deleted the <code>results</code> folder when running Part 1 of this training, you'll see the <code>output.txt</code> file in there too.</p> <p>You can open each of them to satisfy yourself that they contain the appropriate greeting string.</p> File contents results/Hello-output.txt<pre><code>Hello\n</code></pre> results/Bonjour-output.txt<pre><code>Bonjour\n</code></pre> results/Hol\u00e0-output.txt<pre><code>Hol\u00e0\n</code></pre> <p>This confirms each greeting in the input file has been processed appropriately.</p>"},{"location":"nextflow_run/02_pipeline/#13-find-the-original-outputs-and-logs","title":"1.3. Find the original outputs and logs","text":"<p>You may have noticed that the console output above referred to only one task directory. Does that mean all three calls to <code>sayHello()</code> were executed within that one task directory?</p> <p>Let's have a look inside that <code>8e/0eb066</code> task directory:</p> Directory contents 8e/0eb066<pre><code>work/8e/0eb066071cdb4123906b7b4ea8b047/\n\u2514\u2500\u2500 Bonjour-output.txt\n</code></pre> <p>No! We only find the output corresponding to one of the greetings (as well as the accessory files if we enable display of hidden files).</p> <p>So what's going on here?</p> <p>By default, the ANSI logging system writes the status information for all calls to the same process on the same line. As a result, it only showed us one of the three task directory paths (<code>8e/0eb066</code>) in the console output. There are two others that are not listed there.</p> <p>We can modify the logging behavior to see the full list of process calls by adding the <code>-ansi-log false</code> to the command as follows:</p> <pre><code>nextflow run 2a-inputs.nf --input greetings.csv -ansi-log false\n</code></pre> <p>This time we see all three process runs and their associated work subdirectories listed in the output.</p> Command output <pre><code>N E X T F L O W  ~  version 25.04.3\nLaunching `2a-inputs.nf` [pedantic_hamilton] DSL2 - revision: 6bbc42e49f\n[ab/1a8ece] Submitted process &gt; sayHello (1)\n[0d/2cae24] Submitted process &gt; sayHello (2)\n[b5/0df1d6] Submitted process &gt; sayHello (3)\n</code></pre>  Notice that the way the status is reported is a bit different between the two logging modes. In the condensed mode, Nextflow reports whether calls were completed successfully or not. In this expanded mode, it only reports that they were submitted.   <p>This confirms that the <code>sayHello()</code> process gets called three times, and a separate task directory is created for each one.</p> <p>If we look inside each of the task directories listed there, we can verify that each one corresponds to one of the greetings.</p> Directory contents ab/1a8ece<pre><code>work/ab/1a8ece307e53f03fce689dde904b64/\n\u2514\u2500\u2500 Hello-output.txt\n</code></pre> 0d/2cae24<pre><code>work/0d/2cae2481a53593bc607077c80c9466/\n\u2514\u2500\u2500 Bonjour-output.txt\n</code></pre> b5/0df1d6<pre><code>work/b5/0df1d642353269909c2ce23fc2a8fa/\n\u2514\u2500\u2500 Hol\u00e0-output.txt\n</code></pre> <p>This confirms that each process call is executed in isolation from all the others. That has many advantages, including avoiding collisions if the process produces any intermediate files with non-unique names.</p> <p>Tip</p> <p>For a complex workflow, or a large number of inputs, having the full list output to the terminal might get a bit overwhelming, so you might prefer not to use <code>-ansi-log false</code> in those cases.</p>"},{"location":"nextflow_run/02_pipeline/#14-examine-the-code","title":"1.4. Examine the code","text":"<p>So this version of the workflow is capable of reading in a CSV file of inputs, processing the inputs separately, and naming the outputs uniquely.</p> <p>Let's take a look at what makes that possible in the workflow code. Once again, we're not aiming to memorize code syntax, but to identify signature components of the workflow that provide important functionality.</p> Code 2a-inputs.nf<pre><code>#!/usr/bin/env nextflow\n\n/*\n * Use echo to print 'Hello World!' to a file\n */\nprocess sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        val greeting\n\n    output:\n        path \"${greeting}-output.txt\"\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; '$greeting-output.txt'\n    \"\"\"\n}\n\nworkflow {\n\n    // create a channel for inputs from a CSV file\n    greeting_ch = Channel.fromPath(params.input)\n                        .splitCsv()\n                        .map { line -&gt; line[0] }\n\n    // emit a greeting\n    sayHello(greeting_ch)\n}\n</code></pre>"},{"location":"nextflow_run/02_pipeline/#141-loading-the-input-data-from-the-csv","title":"1.4.1. Loading the input data from the CSV","text":"<p>This is the most interesting part: how did we switch from taking a single value from the command-line, to taking a CSV file, parsing it and processing the individual greetings it contains?</p> <p>In Nextflow, we do that with a channel: a construct designed to handle inputs efficiently and shuttle them from one step to another in multi-step workflows, while providing built-in parallelism and many additional benefits.</p> <p>Let's break it down.</p> 2a-inputs.nf<pre><code>workflow {\n\n    // create a channel for inputs from a CSV file\n    greeting_ch = Channel.fromPath(params.input)\n                        .splitCsv()\n                        .map { line -&gt; line[0] }\n</code></pre> <p>This is where the magic happens, starting at line 25. Here's what that line means in plain English:</p> <ul> <li><code>Channel</code> creates a channel, i.e. a queue that will hold the data</li> <li><code>.fromPath</code> specifies the data source is a filepath</li> <li><code>(params.input)</code> specifies the filepath is provided by <code>--input</code> on the command line</li> </ul> <p>In other words, that line tells Nextflow: take the filepath given with <code>--input</code> and get ready to treat its contents as input data.</p> <p>Then the next two lines apply operators that do the actual parsing of the file and loading of the data into the appropriate data structure:</p> <ul> <li><code>.splitCsv()</code> tells Nextflow to parse the CSV file into an array representing rows and columns</li> <li><code>.map { line -&gt; line[0] }</code> tells Nextflow to take only the element in the first column from each row</li> </ul> <p>So in practice, starting from the following CSV file:</p> greetings.csv<pre><code>Hello,English,123\nBonjour,French,456\nHol\u00e0,Spanish,789\n</code></pre> <p>We have transformed that into an array that looks like this:</p> Array contents<pre><code>[[Hello,English,123],[Bonjour,French,456],[Hol\u00e0,Spanish,789]]\n</code></pre> <p>And then we've taken the first element from each of the three rows and loaded them into a Nextflow channel that now contains: <code>Hello</code>, <code>Bonjour</code>, and <code>Hol\u00e0</code>.</p> <p>The result of this very short snippet of code is a channel called <code>greeting_ch</code> loaded with the three individual greetings from the CSV file, ready for processing.</p>"},{"location":"nextflow_run/02_pipeline/#142-call-the-process-on-each-greeting","title":"1.4.2. Call the process on each greeting","text":"<p>Next, in the last line of the workflow block, we provide the loaded <code>greeting_ch</code> channel as input to the <code>sayHello()</code> process.</p> 2a-inputs.nf<pre><code>    sayHello(greeting_ch)\n}\n</code></pre> <p>This tells Nextflow to run the process individually on each element in the channel, i.e. on each greeting.</p> <p>And because Nextflow is smart like that, it will run these process calls in parallel if possible, depending on the available computing infrastructure.</p> <p>That is how you can achieve efficient and scalable processing of a lot of data (many samples, or data points, whatever is your unit of research) with comparatively very little code.</p>"},{"location":"nextflow_run/02_pipeline/#143-ensure-the-outputs-are-uniquely-named","title":"1.4.3. Ensure the outputs are uniquely named","text":"<p>Finally, it's worth taking a quick look at how we get the output files to be named uniquely.</p> 2a-inputs.nf<pre><code>    output:\n        path \"${greeting}-output.txt\"\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; '$greeting-output.txt'\n    \"\"\"\n</code></pre> <p>You see that, compared to the version of this process in <code>1-hello.nf</code>, the output declaration and the relevant bit of the command have changed to include the greeting value in the output file name.</p> <p>This is one way to ensure that the output file names won't collide when they get published to the common <code>results</code> directory.</p> <p>And that's the only change we've had to make inside the process declaration.</p>"},{"location":"nextflow_run/02_pipeline/#takeaway","title":"Takeaway","text":"<p>You understand at a basic level how channels and operators enable us to process multiple inputs efficiently.</p>"},{"location":"nextflow_run/02_pipeline/#whats-next","title":"What's next?","text":"<p>Discover how multi-step workflows are constructed and how they operate.</p>"},{"location":"nextflow_run/02_pipeline/#2-running-multi-step-workflows","title":"2. Running multi-step workflows","text":"<p>Most real-world workflows involve more than one step. Let's build on what we just learned about channels, and look at how Nextflow uses channels and operators to connect processes together in a multi-step workflow.</p> <p>To that end, we provide you with an example workflow that chains together three separate steps and demonstrates the following:</p> <ol> <li>Making data flow from one process to the next</li> <li>Collecting outputs from multiple process calls into a single process call</li> </ol> <p>Specifically, we made an expanded version of the workflow called <code>2b-multistep.nf</code> that takes each input greeting, converts it to uppercase, then collects all the uppercased greetings into a single output file.</p> sayHello*-output.txtconvertToUpperUPPER-*collectGreetingsCOLLECTED-output.txtHELLOBONJOURHOL\u00e0Hello,English,123 Bonjour,French,456Hol\u00e0,Spanish,789greetings.csvHELLOBONJOURHOL\u00e0UPPER-Hello-output.txtUPPER-Bonjour-output.txtUPPER-Hol\u00e0-output.txt <p>As previously, we'll run the workflow first then look at the code to see what's changed.</p>"},{"location":"nextflow_run/02_pipeline/#21-run-the-workflow","title":"2.1. Run the workflow","text":"<p>Run the following command in your terminal:</p> <pre><code>nextflow run 2b-multistep.nf --input greetings.csv\n</code></pre> <p>Once again this should run successfully.</p> Command output <pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `2b-multistep.nf` [soggy_franklin] DSL2 - revision: bc8e1b2726\n\n[d6/cdf466] sayHello (1)       | 3 of 3 \u2714\n[99/79394f] convertToUpper (2) | 3 of 3 \u2714\n[1e/83586c] collectGreetings   | 1 of 1 \u2714\n</code></pre> <p>You see that as promised, multiple steps were run as part of the workflow; the first two (<code>sayHello</code> and <code>convertToUpper</code>) were presumably run on each individual greeting, and the third (<code>collectGreetings</code>) will have been run only once, on the outputs of all three of the <code>convertToUpper</code> calls.</p>"},{"location":"nextflow_run/02_pipeline/#22-find-the-outputs","title":"2.2. Find the outputs","text":"<p>Let's verify that that is in fact what happened by taking a look in the <code>results</code> directory.</p> Directory contents Directory contents<pre><code>results\n\u251c\u2500\u2500 Bonjour-output.txt\n\u251c\u2500\u2500 COLLECTED-output.txt\n\u251c\u2500\u2500 Hello-output.txt\n\u251c\u2500\u2500 Hol\u00e0-output.txt\n\u251c\u2500\u2500 UPPER-Bonjour-output.txt\n\u251c\u2500\u2500 UPPER-Hello-output.txt\n\u2514\u2500\u2500 UPPER-Hol\u00e0-output.txt\n</code></pre> <p>Look at the file names and check their contents to confirm that they are what you expect; for example:</p> bash<pre><code>cat results/COLLECTED-output.txt\n</code></pre> Command output <pre><code>HELLO\nBONJOUR\nHOL\u00e0\n</code></pre> <p>That is the expected final result of our multi-step pipeline.</p>"},{"location":"nextflow_run/02_pipeline/#23-examine-the-code","title":"2.3. Examine the code","text":"<p>Let's look at the code and see what we can tie back to what we just observed.</p> Code 2a-inputs.nf<pre><code>#!/usr/bin/env nextflow\n\n/*\n * Use echo to print 'Hello World!' to a file\n */\nprocess sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        val greeting\n\n    output:\n        path \"${greeting}-output.txt\"\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; '$greeting-output.txt'\n    \"\"\"\n}\n\n/*\n * Use a text replacement tool to convert the greeting to uppercase\n */\nprocess convertToUpper {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        path input_file\n\n    output:\n        path \"UPPER-${input_file}\"\n\n    script:\n    \"\"\"\n    cat '$input_file' | tr '[a-z]' '[A-Z]' &gt; 'UPPER-${input_file}'\n    \"\"\"\n}\n\n/*\n * Collect uppercase greetings into a single output file\n */\nprocess collectGreetings {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        path input_files\n\n    output:\n        path \"COLLECTED-output.txt\"\n\n    script:\n    \"\"\"\n    cat ${input_files} &gt; 'COLLECTED-output.txt'\n    \"\"\"\n}\n\nworkflow {\n\n    // create a channel for inputs from a CSV file\n    greeting_ch = Channel.fromPath(params.input)\n                        .splitCsv()\n                        .map { line -&gt; line[0] }\n\n    // emit a greeting\n    sayHello(greeting_ch)\n\n    // convert the greeting to uppercase\n    convertToUpper(sayHello.out)\n\n    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect())\n}\n</code></pre> <p>The most obvious difference compared to the previous version of the workflow is that now there are multiple process definitions, and correspondingly, several process calls in the workflow block.</p>"},{"location":"nextflow_run/02_pipeline/#231-multiple-process-definitions","title":"2.3.1. Multiple process definitions","text":"<p>In addition to the original <code>sayHello</code> process, we now also have <code>convertToUpper</code> and <code>collectGreetings</code>, which match the names of the processes we saw in the console output.</p> <p>All three are structured in the same way and follow roughly the same logic. We won't go into that in detail, but it shows how a process can be given additional parameters and emit multiple outputs.</p>"},{"location":"nextflow_run/02_pipeline/#232-processes-are-connected-via-channels","title":"2.3.2. Processes are connected via channels","text":"<p>The really interesting thing to look at here is how the process calls are chained together in the workflow block.</p> Code 2a-inputs.nf<pre><code>workflow {\n\n    // create a channel for inputs from a CSV file\n    greeting_ch = Channel.fromPath(params.input)\n                        .splitCsv()\n                        .map { line -&gt; line[0] }\n\n    // emit a greeting\n    sayHello(greeting_ch)\n\n    // convert the greeting to uppercase\n    convertToUpper(sayHello.out)\n\n    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect())\n}\n</code></pre> <p>You can see that the first process call, <code>sayHello(greeting_ch)</code>, is unchanged.</p> <p>Then the next process call, to <code>convertToUpper</code>, refers to the output of <code>sayHello</code> as <code>sayHello.out</code>.</p> 2a-inputs.nf<pre><code>    // convert the greeting to uppercase\n    convertToUpper(sayHello.out)\n</code></pre> <p>This tells Nextflow to provide <code>sayHello.out</code>, which represents the channel output by <code>sayHello()</code>, as an input to <code>convertToUpper</code>.</p> <p>That is, at its simplest, how we shuttle data from one step to the next in Nextflow.</p> <p>Finally, the third call, <code>collectGreetings</code>, is doing the same thing, with a twist:</p> 2a-inputs.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect())\n</code></pre> <p>This one is a bit more complicated and deserves its own discussion.</p>"},{"location":"nextflow_run/02_pipeline/#233-operators-provide-additional-wiring-options","title":"2.3.3. Operators provide additional wiring options","text":"<p>What we're seeing in <code>convertToUpper.out.collect()</code> is the use of another operator (like <code>splitCsv</code> and <code>map</code> in the previous section), called <code>collect</code>.</p> <p>This operator is used to collect the outputs from multiple calls to the same process (as when we run <code>sayHello</code> on multiple greetings independently) and package them into a single channel element.</p> <p>This allows us to take all the separate uppercased greetings produced by the second step of the workflow and feed them all together to a single call in the third step of the pipeline.</p> collectGreetingsCOLLECTED-output.txtHELLOBONJOURHOL\u00e0UPPER-Hello-output.txtUPPER-Bonjour-output.txtUPPER-Hol\u00e0-output.txtWITH THE collect() OPERATOR <p>If we didn't apply <code>collect()</code> to the output of <code>convertToUpper()</code> before feeding it to <code>collectGreetings()</code>, Nextflow would simply run <code>collectGreetings()</code> independently on each greeting, which would not achieve our goal.</p> UPPER-Hello-output.txtUPPER-Bonjour-output.txtUPPER-Hol\u00e0-output.txtcollectGreetingscollectGreetingscollectGreetingsCOLLECTED-output.txtCOLLECTED-output.txtCOLLECTED-output.txtHELLOBONJOURHOL\u00e0WITHOUT THE collect() OPERATOR <p>There are many other operators available to apply transformations to the contents of channels between process calls.</p> <p>This gives pipeline developers a lot of flexibility for customizing the flow logic of their pipeline. The downside is that it can sometimes make it harder to decipher what the pipeline is doing.</p>"},{"location":"nextflow_run/02_pipeline/#24-use-the-graph-preview","title":"2.4. Use the graph preview","text":"<p>One very helpful tool for understanding what a pipeline does, if it's not adequately documented, is the graph preview functionality available in VSCode thanks to the Nextflow extension. You can see this in the training environment by clicking on the small <code>DAG preview</code> link displayed just above the workflow block in any Nextflow script.</p> <p></p> <p>This does not show operators, but it does give a useful representation of how process calls are connected and what are their inputs.</p>"},{"location":"nextflow_run/02_pipeline/#takeaway_1","title":"Takeaway","text":"<p>You understand at a basic level how multi-step workflows are constructed using channels and operators and how they operate.</p>"},{"location":"nextflow_run/02_pipeline/#whats-next_1","title":"What's next?","text":"<p>Learn how Nextflow pipelines can be modularized to promote code reuse and maintainability.</p>"},{"location":"nextflow_run/02_pipeline/#3-running-modularized-pipelines","title":"3. Running modularized pipelines","text":"<p>So far, all the workflows we've looked at have consisted of one single workflow file containing all the relevant code.</p> <p>However, real-world pipelines typically benefit from being modularized, meaning that the code is split into different files. This can make their development and maintenance more efficient and sustainable.</p> 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 InputsMODULE_1SUBWORKFLOW_1OutputsMODULE_4MODULE_2MODULE_3WORKFLOWSUBWORKFLOW_1MODULE_2modules.configtool argumentspublishingoutput namesbase.configcompute resourceserror strategiesprocess MODULE_2 {    label \"process low\"     input:    tuple val(meta), path(fasta)     output:    tuple val(meta), path(fai)     when:    task.ext.when == null || task.ext.when     script:    def args = task.ext.args ?: ''    \"\"\"    my-function $args -i $fasta -o $fai    \"\"\" } <p>Here we are going to demonstrate the most common form of code modularity in Nextflow, which is the use of modules.</p> <p>In Nextflow, a module is a single process definition that is encapsulated by itself in a standalone code file. To use a module in a workflow, you just add a single-line import statement to your workflow code file; then you can integrate the process into the workflow the same way you normally would.</p> <p>Putting processes into individual modules makes it possible to reuse process definitions in multiple workflows without producing multiple copies of the code. This makes the code more shareable, flexible and maintainable.</p> <p>We have of course once again prepared a suitable workflow for demonstration purposes, called <code>2c-modules.nf</code>, along with a set of modules located in the <code>modules/</code> directory.</p> Directory contents modules/<pre><code>modules/\n\u251c\u2500\u2500 collectGreetings.nf\n\u251c\u2500\u2500 convertToUpper.nf\n\u251c\u2500\u2500 cowpy.nf\n\u2514\u2500\u2500 sayHello.nf\n</code></pre> <p>You see there are four Nextflow files, each named after one of the processes. You can ignore the <code>cowpy.nf</code> file for now; we'll get to that one later.</p>"},{"location":"nextflow_run/02_pipeline/#31-examine-the-code","title":"3.1. Examine the code","text":"<p>This time we're going to look at the code first, so let's open each of the files listed above (not shown here).</p> <p>We see that the code for the processes and workflow logic are exactly the same as in the previous version of the workflow. However, the process code is now located in the modules instead of being in the main workflow file, and there are now import statements in the workflow file telling Nextflow to pull them in at runtime.</p> hello-modules.nf<pre><code>// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\ninclude { convertToUpper } from './modules/convertToUpper.nf'\ninclude { collectGreetings } from './modules/collectGreetings.nf'\n\nworkflow {\n</code></pre> <p>You can look inside one of the modules to satisfy yourself that the process definition is unchanged; it's literally just been copy-pasted into a standalone file.</p> Example: sayHello process module modules/sayHello.nf<pre><code>#!/usr/bin/env nextflow\n\n/*\n * Use echo to print 'Hello World!' to a file\n */\nprocess sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        val greeting\n\n    output:\n        path \"${greeting}-output.txt\"\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; '$greeting-output.txt'\n    \"\"\"\n}\n</code></pre> <p>So let's see what it looks like to run this new version.</p>"},{"location":"nextflow_run/02_pipeline/#32-run-the-workflow","title":"3.2. Run the workflow","text":"<p>Run this command in your terminal, with the <code>-resume</code> flag:</p> <pre><code>nextflow run 2c-modules.nf --input greetings.csv -resume\n</code></pre> <p>Once again this should run successfully.</p> Command output <pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `2c-modules.nf` [soggy_franklin] DSL2 - revision: bc8e1b2726\n\n[j6/cdfa66] sayHello (1)       | 3 of 3, cached: \u2714\n[95/79484f] convertToUpper (2) | 3 of 3, cached: \u2714\n[5e/4358gc] collectGreetings   | 1 of 1, cached: \u2714\n</code></pre> <p>You'll notice that the process executions all cached successfully, meaning that Nextflow recognized that it has already done the requested work, even though the code has been split up and the main workflow file has been renamed.</p> <p>None of that matters to Nextflow; what matters is the job script that is generated once all the code has been pulled together and evaluated.</p> <p>Tip</p> <p>It is also possible to encapsulate a section of a workflow as a 'subworkflow' that can be imported into a larger pipeline, but that is outside the scope of this course.</p> <p>You can learn more about developing composable workflows in the Side Quest on Workflows of Workflows.</p>"},{"location":"nextflow_run/02_pipeline/#takeaway_2","title":"Takeaway","text":"<p>You know how processes can be stored in standalone modules to promote code reuse and improve maintainability.</p>"},{"location":"nextflow_run/02_pipeline/#whats-next_2","title":"What's next?","text":"<p>Learn to use containers for managing software dependencies.</p>"},{"location":"nextflow_run/02_pipeline/#4-using-containerized-software","title":"4. Using containerized software","text":"<p>So far the workflows we've been using as examples just needed to run very basic text processing operations using UNIX tools available in our environment.</p> <p>However, real-world pipelines typically require specialized tools and packages that are not included by default in most environments. Usually, you'd need to install these tools, manage their dependencies, and resolve any conflicts.</p> <p>That is all very tedious and annoying. A much better way to address this problem is to use containers.</p> <p>A container is a lightweight, standalone, executable unit of software created from a container image that includes everything needed to run an application including code, system libraries and settings.</p> <p>Tip</p> <p>We teach this using the technology Docker, but Nextflow supports several other container technologies as well.</p>"},{"location":"nextflow_run/02_pipeline/#41-use-a-container-directly","title":"4.1. Use a container directly","text":"<p>First, let's try interacting with a container directly. This will help solidify your understanding of what containers are before we start using them in Nextflow.</p>"},{"location":"nextflow_run/02_pipeline/#411-pull-the-container-image","title":"4.1.1. Pull the container image","text":"<p>To use a container, you usually download or \"pull\" a container image from a container registry, and then run the container image to create a container instance.</p> <p>The general syntax is as follows:</p> Syntax<pre><code>docker pull '&lt;container&gt;'\n</code></pre> <ul> <li><code>docker pull</code> is the instruction to the container system to pull a container image from a repository.</li> <li><code>'&lt;container&gt;'</code> is the URI address of the container image.</li> </ul> <p>As an example, let's pull a container image that contains cowpy, a python implementation of a tool called <code>cowsay</code> that generates ASCII art to display arbitrary text inputs in a fun way.</p> <p>There are various repositories where you can find published containers. We used the Seqera Containers service to generate this Docker container image from the <code>cowpy</code> Conda package: <code>'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'</code>.</p> <p>Run the complete pull command:</p> <pre><code>docker pull 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n</code></pre> <p>This tells the system to download the image specified.</p> Command output <pre><code>Unable to find image 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273' locally\n131d6a1b707a8e65: Pulling from library/cowpy\ndafa2b0c44d2: Pull complete\ndec6b097362e: Pull complete\nf88da01cff0b: Pull complete\n4f4fb700ef54: Pull complete\n92dc97a3ef36: Pull complete\n403f74b0f85e: Pull complete\n10b8c00c10a5: Pull complete\n17dc7ea432cc: Pull complete\nbb36d6c3110d: Pull complete\n0ea1a16bbe82: Pull complete\n030a47592a0a: Pull complete\n622dd7f15040: Pull complete\n895fb5d0f4df: Pull complete\nDigest: sha256:fa50498b32534d83e0a89bb21fec0c47cc03933ac95c6b6587df82aaa9d68db3\nStatus: Downloaded newer image for community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273\ncommunity.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273\n</code></pre> <p>Once the download is complete, you have a local copy of the container image.</p>"},{"location":"nextflow_run/02_pipeline/#412-spin-up-the-container","title":"4.1.2. Spin up the container","text":"<p>Containers can be run as a one-off command, but you can also use them interactively, which gives you a shell prompt inside the container and allows you to play with the command.</p> <p>The general syntax is as follows:</p> Syntax<pre><code>docker run --rm '&lt;container&gt;' [tool command]\n</code></pre> <ul> <li><code>docker run --rm '&lt;container&gt;'</code> is the instruction to the container system to spin up a container instance from a container image and execute a command in it.</li> <li><code>--rm</code> tells the system to shut down the container instance after the command has completed.</li> </ul> <p>Fully assembled, the container execution command looks like this:</p> <pre><code>docker run --rm -it 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n</code></pre> <p>Run that command, and you should see your prompt change to something like <code>(base) root@b645838b3314:/tmp#</code>, which indicates that you are now inside the container.</p> <p>You can verify this by running <code>ls</code> to list directory contents:</p> <pre><code>ls /\n</code></pre> Command output <pre><code>bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\n</code></pre> <p>You observe see that the filesystem inside the container is different from the filesystem on your host system.</p> <p>Tip</p> <p>When you run a container, it is isolated from the host system by default. This means that the container can't access any files on the host system unless you explicitly allow it to do so by specifying that you want to mount a volume as part of the <code>docker run</code> command using the following syntax:</p> Syntax<pre><code>-v &lt;outside_path&gt;:&lt;inside_path&gt;\n</code></pre> <p>This effectively establishes a tunnel through the container wall that you can use to access that part of your filesystem.</p>"},{"location":"nextflow_run/02_pipeline/#413-run-the-cowpy-tool","title":"4.1.3. Run the <code>cowpy</code> tool","text":"<p>From inside the container, you can run the <code>cowpy</code> command directly.</p> <pre><code>cowpy \"Hello Containers\"\n</code></pre> <p>This produces ASCII art of the default cow character (or 'cowacter') with a speech bubble containing the text we specified.</p> Command output <pre><code> ______________________________________________________\n&lt; Hello Containers &gt;\n ------------------------------------------------------\n     \\   ^__^\n      \\  (oo)\\_______\n         (__)\\       )\\/\\\n           ||----w |\n           ||     ||\n</code></pre> <p>Now that you have tested the basic usage, you can try giving it some parameters. For example, the tool documentation says we can set the character with <code>-c</code>.</p> <pre><code>cowpy \"Hello Containers\" -c tux\n</code></pre> <p>This time the ASCII art output shows the Linux penguin, Tux, because we specified the <code>-c tux</code> parameter.</p> Command output <pre><code> __________________\n&lt; Hello Containers &gt;\n ------------------\n   \\\n    \\\n        .--.\n       |o_o |\n       |:_/ |\n      //   \\ \\\n     (|     | )\n    /'\\_   _/`\\\n    \\___)=(___/\n</code></pre> <p>Since you're inside the container, you can run the cowpy command as many times as you like, varying the input parameters, without having to worry about install any libraries on your system itself.</p> <p>Tip</p> <p>Use the '-c' flag to pick a different character, including: <code>beavis</code>, <code>cheese</code>, <code>daemon</code>, <code>dragonandcow</code>, <code>ghostbusters</code>, <code>kitty</code>, <code>moose</code>, <code>milk</code>, <code>stegosaurus</code>, <code>turkey</code>, <code>turtle</code>, <code>tux</code></p> <p>Feel free to play around with this. When you're done, exit the container using the <code>exit</code> command:</p> <pre><code>exit\n</code></pre> <p>You will find yourself back in your normal shell.</p>"},{"location":"nextflow_run/02_pipeline/#42-use-a-container-in-a-workflow","title":"4.2. Use a container in a workflow","text":"<p>When we run a pipeline, we want to be able to tell Nextflow what container to use at each step, and importantly, we want it to handle all that work we just did: pull the container, spin it up, run the command and tear the container down when it's done.</p> <p>Good news: that's exactly what Nextflow is going to do for us. We just need to specify a container for each process.</p> <p>To demonstrate how this work, we made another version of our workflow that runs <code>cowpy</code> on the file of collected greetings produced in the third step.</p> COLLECTED-output.txtHELLOBONJOURHOL\u00e0cowPycowpy-COLLECTED-output.txt ________/ HOL\u00e0    \\| HELLO   |\\ BONJOUR / --------     \\   ^__^      \\  (oo)\\_______         (__)\\       )\\/\\           ||----w |           ||     || <p>This should output a file containing the ASCII art with the three greetings in the speech bubble.</p>"},{"location":"nextflow_run/02_pipeline/#421-examine-the-code","title":"4.2.1. Examine the code","text":"<p>The workflow is very similar to the previous one, plus the extra step to run `cowpy. The differences are highlighted in the code snippet below.</p> Code 2d-container.nf<pre><code>#!/usr/bin/env nextflow\n\n// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\ninclude { convertToUpper } from './modules/convertToUpper.nf'\ninclude { collectGreetings } from './modules/collectGreetings.nf'\ninclude { cowpy } from './modules/cowpy.nf'\n\nworkflow {\n\n    // create a channel for inputs from a CSV file\n    greeting_ch = Channel.fromPath(params.input)\n                        .splitCsv()\n                        .map { line -&gt; line[0] }\n\n    // emit a greeting\n    sayHello(greeting_ch)\n\n    // convert the greeting to uppercase\n    convertToUpper(sayHello.out)\n\n    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect())\n\n    // generate ASCII art with cowpy\n    cowpy(collectGreetings.out, params.character)\n}\n</code></pre> <p>You see that this workflow imports a <code>cowpy</code> process from a module file, and calls it on the output of the <code>collectGreetings()</code> call.</p> modules/cowpy.nf<pre><code>cowpy(collectGreetings.out, params.character)\n</code></pre> <p>The <code>cowpy</code> process, which wraps the cowpy command to generate ASCII art, is defined in the <code>cowpy.nf</code> module.</p> Code modules/cowpy.nf<pre><code>#!/usr/bin/env nextflow\n\n// Generate ASCII art with cowpy\nprocess cowpy {\n\n    container 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        path input_file\n        val character\n\n    output:\n        path \"cowpy-${input_file}\"\n\n    script:\n    \"\"\"\n    cat $input_file | cowpy -c \"$character\" &gt; cowpy-${input_file}\n    \"\"\"\n\n}\n</code></pre> <p>The <code>cowpy</code> process requires two inputs: the path to an input file containing the text to put in the speech bubble (<code>input_file</code>), and a value for the character variable.</p> <p>Importantly, it also includes the line <code>container 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'</code>, which points to the container URI we used earlier.</p>"},{"location":"nextflow_run/02_pipeline/#422-check-that-docker-is-enabled-in-the-configuration","title":"4.2.2. Check that Docker is enabled in the configuration","text":"<p>We're going to slightly anticipate Part 3 of this training course by introducing the topic of configuration.</p> <p>One of the main ways Nextflow offers for configuring workflow execution is to use a <code>nextflow.config</code> file. When such a file is present in the current directory, Nextflow will automatically load it in and apply any configuration it contains.</p> <p>To that end, we include a <code>nextflow.config</code> file with a single line of code that enables Docker.</p> nextflow.config<pre><code>docker.enabled = true\n</code></pre> <p>You can check that this is indeed set correctly either by opening the file, or by running the <code>nextflow config</code> command in the terminal.</p> <pre><code>nextflow config\n</code></pre> Command output nextflow.config<pre><code>docker {\n   enabled = true\n}\n</code></pre> <p>That tells Nextflow to use Docker for any process that specifies a compatible container.</p> <p>Tip</p> <p>It is possible to enable Docker execution from the command-line, on a per-run basis, using the <code>-with-docker &lt;container&gt;</code> parameter. However, that only allows us to specify one container for the entire workflow, whereas the approach we just showed you allows us to specify a different container per process. This is better for modularity, code maintenance and reproducibility.</p>"},{"location":"nextflow_run/02_pipeline/#423-run-the-workflow","title":"4.2.3. Run the workflow","text":"<p>Let's run the workflow with the <code>-resume</code> flag, and specify that we want the character to be the turkey.</p> <pre><code>nextflow run 2d-container.nf --input greetings.csv --character turkey -resume\n</code></pre> <p>This should work without error.</p> Command output <pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `2d-container.nf` [elegant_brattain] DSL2 - revision: 028a841db1\n\nexecutor &gt;  local (1)\n[95/fa0bac] sayHello (3)       | 3 of 3, cached: 3 \u2714\n[92/32533f] convertToUpper (3) | 3 of 3, cached: 3 \u2714\n[aa/e697a2] collectGreetings   | 1 of 1, cached: 1 \u2714\n[7f/caf718] cowpy              | 1 of 1 \u2714\n</code></pre> <p>The first three steps cached since we've already run them before, but the <code>cowpy</code> process is new so that actually gets run.</p> <p>You can find the output of the <code>cowpy</code> step in the <code>results</code> directory.</p> Output file contents results/cowpy-COLLECTED-output.txt<pre><code> _________\n/ HOL\u00e0    \\\n| HELLO   |\n\\ BONJOUR /\n ---------\n  \\                                  ,+*^^*+___+++_\n   \\                           ,*^^^^              )\n    \\                       _+*                     ^**+_\n     \\                    +^       _ _++*+_+++_,         )\n              _+^^*+_    (     ,+*^ ^          \\+_        )\n             {       )  (    ,(    ,_+--+--,      ^)      ^\\\n            { (\\@)    } f   ,(  ,+-^ __*_*_  ^^\\_   ^\\       )\n           {:;-/    (_+*-+^^^^^+*+*&lt;_ _++_)_    )    )      /\n          ( /  (    (        ,___    ^*+_+* )   &lt;    &lt;      \\\n           U _/     )    *--&lt;  ) ^\\-----++__)   )    )       )\n            (      )  _(^)^^))  )  )\\^^^^^))^*+/    /       /\n          (      /  (_))_^)) )  )  ))^^^^^))^^^)__/     +^^\n         (     ,/    (^))^))  )  ) ))^^^^^^^))^^)       _)\n          *+__+*       (_))^)  ) ) ))^^^^^^))^^^^^)____*^\n          \\             \\_)^)_)) ))^^^^^^^^^^))^^^^)\n           (_             ^\\__^^^^^^^^^^^^))^^^^^^^)\n             ^\\___            ^\\__^^^^^^))^^^^^^^^)\\\\\n                  ^^^^^\\uuu/^^\\uuu/^^^^\\^\\^\\^\\^\\^\\^\\^\\\n                     ___) &gt;____) &gt;___   ^\\_\\_\\_\\_\\_\\_\\)\n                    ^^^//\\\\_^^//\\\\_^       ^(\\_\\_\\_\\)\n                      ^^^ ^^ ^^^ ^\n</code></pre>  What a beautiful turkey!   <p>You see that the character is saying all the greetings, since it ran on the file of collected uppercased greetings.</p> <p>More to the point, we were able to run this as part of our pipeline without having to do a proper installation of cowpy and all its dependencies. And we can now share the pipeline with collaborators and have them run it on their infrastructure without them needing to install anything either, aside from Docker or one of its alternatives (such as Singularity/Apptainer) as mentioned above.</p>"},{"location":"nextflow_run/02_pipeline/#424-inspect-how-nextflow-launched-the-containerized-task","title":"4.2.4. Inspect how Nextflow launched the containerized task","text":"<p>Let's take a look at the <code>.command.run</code> file inside the task directory where the <code>cowpy</code> call was executed. This file contains all the commands Nextflow ran on your behalf in the course of executing the pipeline.</p> <p>Open the <code>.command.run</code> file and search for <code>nxf_launch</code> to find the launch command Nextflow used.</p> Partial file contents work/7f/caf7189fca6c56ba627b75749edcb3/.command.run<pre><code>nxf_launch() {\n    docker run -i --cpu-shares 1024 -e \"NXF_TASK_WORKDIR\" -v /workspaces/training/hello-nextflow/work:/workspaces/training/hello-nextflow/work -w \"$NXF_TASK_WORKDIR\" --name $NXF_BOXID community.wave.seqera.io/library/pip_cowpy:131d6a1b707a8e65 /bin/bash -ue /workspaces/training/nextflow-run/work/7f/caf7189fca6c56ba627b75749edcb3/.command.sh\n}\n</code></pre> <p>This launch command shows that Nextflow is using a very similar <code>docker run</code> command to launch the process call as we did when we ran it manually. It also mounts the corresponding work subdirectory into the container, sets the working directory inside the container accordingly, and runs our templated bash script in the <code>.command.sh</code> file.</p> <p>This confirms that all the hard work we had to do manually in the previous section is now done for us by Nextflow!</p>"},{"location":"nextflow_run/02_pipeline/#takeaway_3","title":"Takeaway","text":"<p>You understand what role containers play in managing software tool versions and ensuring reproducibility.</p> <p>More generally, you have a basic understanding of what are the core components of real-world Nextflow pipelines and how they are organized. You know the fundamentals of how Nextflow can process multiple inputs efficiently, run workflows composed of multiple steps connected together, leverage modular code components, and utilize containers for greater reproducibility and portability.</p>"},{"location":"nextflow_run/02_pipeline/#whats-next_3","title":"What's next?","text":"<p>Take another break! That was a big pile of information about how Nextflow pipelines work. In the next section of this training, we're going to delve deeper into the topic of configuration. You will learn how to configure the execution of your pipeline to fit your infrastructure as well as manage configuration of inputs and parameters.</p>"},{"location":"nextflow_run/03_config/","title":"Part 3: Configuration","text":"<p>This section will explore how to manage the configuration of a Nextflow pipeline in order to customize its behavior, adapt it to different environments, and optimize resource usage without altering a single line of the workflow code itself.</p> <p>There are multiple ways to do this; here we are going to use the simplest and most common configuration file mechanism, the <code>nextflow.config</code> file. As noted previously, whenever there is a file named <code>nextflow.config</code> in the current directory, Nextflow will automatically load configuration from it.</p> <p>Tip</p> <p>Anything you put into the <code>nextflow.config</code> can be overridden at runtime by providing the relevant process directives or parameters and values on the command line, or by importing another configuration file, according to the order of precedence described here.</p> <p>In this part of the training, we're going to use the <code>nextflow.config</code> file to demonstrate essential components of Nextflow configuration such as process directives, executors, profiles, and parameter files. By learning to utilize these configuration options effectively, you can enhance the flexibility, scalability, and performance of your pipelines.</p> <p>To exercise these elements of configuration, we're going to be running a fresh copy of the workflow we last ran at the end of Part 2 of this training course, renamed <code>3-main.nf</code>.</p>"},{"location":"nextflow_run/03_config/#1-determine-what-software-packaging-technology-to-use","title":"1. Determine what software packaging technology to use","text":"<p>The first step toward adapting your workflow configuration to your compute environment is specifying where the software packages that will get run in each step are going to be coming from. Are they already installed in the local compute environment? Do we need to retrieve images and run them via a container system? Or do we need to retrieve Conda packages and build a local Conda environment?</p> <p>For most of this training course so far, we just used locally installed software in our workflow. Then in the last section of Part 2, we introduced Docker containers and the <code>nextflow.config</code> file, which we used to enable the use of Docker containers.</p> <p>Now let's see how we can configure an alternative software packaging option via the <code>nextflow.config</code> file.</p>"},{"location":"nextflow_run/03_config/#11-disable-docker-and-enable-conda-in-the-config-file","title":"1.1. Disable Docker and enable Conda in the config file","text":"<p>Let's pretend we're working on an HPC cluster and the admin doesn't allow the use of Docker for security reasons.</p> <p>Fortunately for us, Nextflow supports multiple other container technologies such as including Singularity/Apptainer (which is more widely used on HPC), and software package managers such as Conda.</p> <p>We can change our configuration file to use Conda instead of Docker. To do so, we switch the value of <code>docker.enabled</code> to <code>false</code>, and add a directive enabling the use of Conda:</p> AfterBefore nextflow.config<pre><code>docker.enabled = false\nconda.enabled = true\n</code></pre> nextflow.config<pre><code>docker.enabled = true\n</code></pre> <p>This will allow Nextflow to create and utilize Conda environments for processes that have Conda packages specified. Which means we now need to add one of those to the <code>cowpy</code> process definition!</p>"},{"location":"nextflow_run/03_config/#12-specify-a-conda-package-in-the-process-definition","title":"1.2. Specify a Conda package in the process definition","text":"<p>We've already retrieved the URI for a Conda package containing the <code>cowpy</code> tool: <code>conda-forge::cowpy==1.1.5</code></p> <p>Tip</p> <p>There are a few different ways to get the URI for a given conda package. We recommend using the Seqera Containers search query, which will give you a URI that you can copy and paste, even if you're not planning to create a container from it.</p> <p>Now we add the URI to the <code>cowpy</code> process definition using the <code>conda</code> directive:</p> AfterBefore modules/cowpy.nf<pre><code>process cowpy {\n\n    container 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n    conda 'conda-forge::cowpy==1.1.5'\n\n    publishDir 'results', mode: 'copy'\n</code></pre> modules/cowpy.nf<pre><code>process cowpy {\n\n    container 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n\n    publishDir 'results', mode: 'copy'\n</code></pre> <p>To be clear, we're not replacing the <code>container</code> directive, we're adding an alternative option.</p>"},{"location":"nextflow_run/03_config/#13-run-the-workflow-to-verify-that-it-can-use-conda","title":"1.3. Run the workflow to verify that it can use Conda","text":"<p>Let's try it out.</p> <pre><code>nextflow run 3-main.nf --input greetings.csv --character turkey\n</code></pre> <p>This should work without error.</p> Command output Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `3-main.nf` [trusting_lovelace] DSL2 - revision: 028a841db1\n\nexecutor &gt;  local (8)\n[ee/4ca1f2] sayHello (3)       | 3 of 3 \u2714\n[20/2596a7] convertToUpper (1) | 3 of 3 \u2714\n[b3/e15de5] collectGreetings   | 1 of 1 \u2714\n[c5/af5f88] cowpy              | 1 of 1 \u2714\n</code></pre> <p>Behind the scenes, Nextflow has retrieved the Conda packages and created the environment, which normally takes a bit of work; so it's nice that we don't have to do any of that ourselves!</p> <p>Tip</p> <p>This runs quickly because the <code>cowpy</code> package is quite small, but if you're working with large packages, it may take a bit longer than usual the first time, and you might see the console output stay 'stuck' for a minute or so before completing. This is normal and is due to the extra work Nextflow does the first time you use a new package.</p> <p>From our standpoint, it looks like it works exactly the same as running with Docker, even though on the backend the mechanics are a bit different.</p> <p>This means we're all set to run with Conda environments if needed.</p> <p>Tip</p> <p>Since these directives are assigned per process, it is possible 'mix and match', i.e. configure some of the processes in your workflow to run with Docker and others with Conda, for example, if the compute infrastructure you are using supports both. In that case, you would enable both Docker and Conda in your configuration file. If both are available for a given process, Nextflow will prioritize containers.</p> <p>And as noted earlier, Nextflow supports multiple other software packaging and container technologies, so you are not limited to just those two.</p>"},{"location":"nextflow_run/03_config/#takeaway","title":"Takeaway","text":"<p>You know how to configure which software package each process should use, and how to switch between technologies.</p>"},{"location":"nextflow_run/03_config/#whats-next","title":"What's next?","text":"<p>Learn how to specify what executor Nextflow should use to actually do the work.</p>"},{"location":"nextflow_run/03_config/#2-specify-what-executor-should-be-used-to-do-the-work","title":"2. Specify what executor should be used to do the work","text":"<p>Until now, we have been running our pipeline with the local executor. This executes each task on the machine that Nextflow is running on. When Nextflow begins, it looks at the available CPUs and memory. If the resources of the tasks ready to run exceed the available resources, Nextflow will hold the last tasks back from execution until one or more of the earlier tasks have finished, freeing up the necessary resources.</p> <p>For very large workloads, you may discover that your local machine is a bottleneck, either because you have a single task that requires more resources than you have available, or because you have so many tasks that waiting for a single machine to run them would take too long. The local executor is convenient and efficient, but is limited to that single machine. Nextflow supports many different execution backends, including HPC schedulers (Slurm, LSF, SGE, PBS, Moab, OAR, Bridge, HTCondor and others) as well as cloud execution backends such (AWS Batch, Google Cloud Batch, Azure Batch, Kubernetes and more).</p>"},{"location":"nextflow_run/03_config/#21-targeting-a-different-backend","title":"2.1. Targeting a different backend","text":"<p>The choice of executor is set by a process directive called <code>executor</code>. By default it is set to <code>local</code>, so the following configuration is implied:</p> Built-in configuration<pre><code>process {\n    executor = 'local'\n}\n</code></pre> <p>To set the executor to target a different backend, simply specify the executor you want using similar syntax as described above for resource allocations (see documentation for all options).</p> nextflow.config<pre><code>process {\n    executor = 'slurm'\n}\n</code></pre> <p>Warning</p> <p>We can't actually test this in the training environment because it's not set up to connect to an HPC.</p>"},{"location":"nextflow_run/03_config/#22-dealing-with-backend-specific-syntax-for-execution-parameters","title":"2.2. Dealing with backend-specific syntax for execution parameters","text":"<p>Most high-performance computing platforms allow (and sometimes require) that you specify certain parameters such as resource allocation requests and limitations (for e.g. number of CPUs and memory) and name of the job queue to use.</p> <p>Unfortunately, each of these systems uses different technologies, syntaxes and configurations for defining how a job should be defined and submitted to the relevant scheduler.</p> <p>For example, a job requiring 8 CPUs and 4GB of RAM to be executed on the queue \"my-science-work\" needs to be expressed in the following ways depending on the backend:</p> Config for SLURM / submit using `sbatch` <pre><code>#SBATCH -o /path/to/my/task/directory/my-task-1.log\n#SBATCH --no-requeue\n#SBATCH -c 8\n#SBATCH --mem 4096M\n#SBATCH -p my-science-work\n</code></pre> Config for PBS / submit using `qsub` <pre><code>#PBS -o /path/to/my/task/directory/my-task-1.log\n#PBS -j oe\n#PBS -q my-science-work\n#PBS -l nodes=1:ppn=5\n#PBS -l mem=4gb\n</code></pre> Config for SGE / submit using `qsub` <pre><code>#$ -o /path/to/my/task/directory/my-task-1.log\n#$ -j y\n#$ -terse\n#$ -notify\n#$ -q my-science-work\n#$ -l slots=5\n#$ -l h_rss=4096M,mem_free=4096M\n</code></pre> <p>Fortunately, Nextflow simplifies all of this. It provides a standardized syntax so that you can specify the relevant properties such as <code>cpus</code>, <code>memory</code> and <code>queue</code> (see documentation for other properties) just once. Then, at runtime, Nextflow will use those settings to generate the appropriate backend-specific scripts based on the executor setting.</p> <p>We'll cover that standardized syntax in the next section.</p>"},{"location":"nextflow_run/03_config/#takeaway_1","title":"Takeaway","text":"<p>You now know how to change the executor to use different kinds of computing infrastructure.</p>"},{"location":"nextflow_run/03_config/#whats-next_1","title":"What's next?","text":"<p>Learn how to evaluate and express resource allocations and limitations in Nextflow.</p>"},{"location":"nextflow_run/03_config/#3-allocate-compute-resources-with-process-directives","title":"3. Allocate compute resources with process directives","text":"<p>As noted above, high-performance computing system generally allow or require you to specify request allocations and set limitations for compute resources such as the number of CPUs and memory to use.</p> <p>By default, Nextflow will use a single CPU and 2GB of memory for each process. The corresponding process directives are called <code>cpus</code> and <code>memory</code>, so the following configuration is implied:</p> Built-in configuration<pre><code>process {\n    cpus = 1\n    memory = 2.GB\n}\n</code></pre> <p>You can modify these values, either for all processes or for specific named processes, using additional process directives in your configuration file. Nextflow will translate them into the appropriate instructions for the chosen executor.</p> <p>But how do you know what values to use?</p>"},{"location":"nextflow_run/03_config/#31-run-the-workflow-to-generate-a-resource-utilization-report","title":"3.1. Run the workflow to generate a resource utilization report","text":"<p>If you don't know up front how much CPU and memory your processes are likely to need, you can do some resource profiling, meaning you run the workflow with some default allocations, record how much each process used, and from there, estimate how to adjust the base allocations.</p> <p>Conveniently, Nextflow includes built-in tools for doing this, and will happily generate a report for you on request.</p> <p>To do so, add <code>-with-report &lt;filename&gt;.html</code> to your command line.</p> <pre><code>nextflow run 3-main.nf -with-report report-config-1.html\n</code></pre> <p>The report is an html file, which you can download and open in your browser. You can also right click it in the file explorer on the left and click on <code>Show preview</code> in order to view it in the training environment.</p> <p>Take a few minutes to look through the report and see if you can identify some opportunities for adjusting resources. Make sure to click on the tabs that show the utilization results as a percentage of what was allocated. There is some documentation describing all the available features.</p>"},{"location":"nextflow_run/03_config/#32-set-resource-allocations-for-all-processes","title":"3.2. Set resource allocations for all processes","text":"<p>The profiling shows that the processes in our training workflow are very lightweight, so let's reduce the default memory allocation to 1GB per process.</p> <p>Add the following to your <code>nextflow.config</code> file:</p> nextflow.config<pre><code>process {\n    memory = 1.GB\n}\n</code></pre>"},{"location":"nextflow_run/03_config/#33-set-resource-allocations-for-an-individual-process","title":"3.3. Set resource allocations for an individual process","text":"<p>At the same time, we're going to pretend that the <code>cowpy</code> process requires more resources than the others, just so we can demonstrate how to adjust allocations for an individual process.</p> AfterBefore nextflow.config<pre><code>process {\n    memory = 1.GB\n    withName: 'cowpy' {\n        memory = 2.GB\n        cpus = 2\n    }\n}\n</code></pre> nextflow.config<pre><code>process {\n    memory = 1.GB\n}\n</code></pre> <p>With this configuration, all processes will request 1GB of memory and a single CPU (the implied default), except the <code>cowpy</code> process, which will request 2GB and 2 CPUs.</p> <p>Tip</p> <p>If you have a machine with few CPUs and you allocate a high number per process, you might see process calls getting queued behind each other. This is because Nextflow ensures we don't request more CPUs than are available.</p> <p>You could then run the workflow again, supplying a different filename for the profiling report, and compare performance before and after the configuration changes. You may not notice any real difference since this is such a small workload, but this is the approach you would use to analyze the performance and resource requirements of a real-world workflow.</p> <p>It is very useful when your processes have different resource requirements. It empowers you to right-size the resource allocations you set up for each process based on actual data, not guesswork.</p> <p>Tip</p> <p>This is just a tiny taster of what you can do to optimize your use of resources. Nextflow itself has some really neat dynamic retry logic built in to retry jobs that fail due to resource limitations. Additionally, the Seqera Platform offers AI-driven tooling for optimizing your resource allocations automatically.</p>"},{"location":"nextflow_run/03_config/#34-add-resource-limits","title":"3.4. Add resource limits","text":"<p>Depending on what computing executor and compute infrastructure you're using, there may be some constraints on what you can (or must) allocate. For example, your cluster may require you to stay within certain limits.</p> <p>You can use the <code>resourceLimits</code> directive to set the relevant limitations. The syntax looks like this when it's by itself in a process block:</p> Syntax example<pre><code>process {\n    resourceLimits = [\n        memory: 750.GB,\n        cpus: 200,\n        time: 30.d\n    ]\n}\n</code></pre> <p>Nextflow will translate these values into the appropriate instructions depending on the executor that you specified.</p> <p>As previously, we can't demonstrate this in action since we don't have access to relevant infrastructure in the training environment. However, if you were to set the executor to <code>slurm</code>, try running the workflow with resource allocations that exceed these limits, then look up the <code>sbatch</code> command in the <code>.command.run</code> script file (which will be generated even though the run is doomed to fail), you would see that the requests that would get sent to the executor are capped at the values specified by <code>resourceLimits</code>.</p> <p>Tip</p> <p>The nf-core project has compiled a collection of configuration files shared by various institutions around the world, covering a wide range of HPC and cloud executors.</p> <p>Those shared configs are valuable both for people who work there and can therefore just utilize their institution's configuration out of the box, and as a model for people who are looking to develop a configuration for their own infrastructure.</p>"},{"location":"nextflow_run/03_config/#takeaway_2","title":"Takeaway","text":"<p>You know how to generate a profiling report to assess resource utilization and how to modify resource allocations for all processes and/or for individual processes, as well as set resource limitations for running on HPC.</p>"},{"location":"nextflow_run/03_config/#whats-next_2","title":"What's next?","text":"<p>Learn how to manage workflow parameters.</p>"},{"location":"nextflow_run/03_config/#4-manage-workflow-parameters","title":"4. Manage workflow parameters","text":"<p>So far we've been looking at configuration from the technical point of view of the compute infrastructure. Now let's consider another aspect of workflow configuration that is very important for reproducibility: the configuration of the workflow parameters.</p> <p>Currently, our workflow is set up to accept a couple of parameter values via the command-line. This is fine for a simple workflow with very few parameters that need to be set for a given run. However, many real-world workflows will have many more parameters that may be run-specific, and putting all of them in the command line would be tedious and error-prone.</p>"},{"location":"nextflow_run/03_config/#41-specify-default-parameter-values","title":"4.1. Specify default parameter values","text":"<p>It is possible to specify default values in the workflow script itself; for example you may see something like this in the main body of the workflow:</p> Syntax example<pre><code>params.input = 'greetings.csv'\nparams.character = 'turkey'\n</code></pre> <p>The same syntax can also be used to store parameter defaults in the <code>nextflow.config</code> file. Let's try that out.</p> <p>Open the <code>nextflow.config</code> file and add the following lines to it:</p> nextflow.config<pre><code>/*\n * Pipeline parameters\n */\nparams.input = 'greetings.csv'\nparams.character = 'turkey'\n</code></pre> Code (full file) nextflow.config<pre><code>docker.enabled = false\nconda.enabled = true\n\nprocess {\n    memory = 1.GB\n    withName: 'cowpy' {\n        memory = 2.GB\n        cpus = 2\n    }\n}\n\n/*\n * Pipeline parameters\n */\nparams.input = 'greetings.csv'\nparams.character = 'turkey'\n</code></pre> <p>Now you can run the workflow without specifying the parameters on the command line.</p> <pre><code>nextflow run 3-main.nf\n</code></pre> <p>This will produce the same output, but is more convenient to type, especially when the workflow requires multiple parameters.</p> Command output <pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `3-main.nf` [wise_mahavira] DSL2 - revision: 356df0818d\n\nexecutor &gt;  local (8)\n[2e/d12fcb] sayHello (2)       [100%] 3 of 3 \u2714\n[a0/5799b6] convertToUpper (3) [100%] 3 of 3 \u2714\n[db/d3bbb6] collectGreetings   [100%] 1 of 1 \u2714\n[a9/f75d13] cowpy              [100%] 1 of 1 \u2714\n</code></pre> <p>The final output file should contain the turkey character saying the greetings.</p> File contents results/cowpy-COLLECTED-output.txt<pre><code> _________\n/ HELLO   \\\n| BONJOUR |\n\\ HOL\u00e0    /\n ---------\n  \\                                  ,+*^^*+___+++_\n   \\                           ,*^^^^              )\n    \\                       _+*                     ^**+_\n     \\                    +^       _ _++*+_+++_,         )\n              _+^^*+_    (     ,+*^ ^          \\+_        )\n             {       )  (    ,(    ,_+--+--,      ^)      ^\\\n            { (\\@)    } f   ,(  ,+-^ __*_*_  ^^\\_   ^\\       )\n           {:;-/    (_+*-+^^^^^+*+*&lt;_ _++_)_    )    )      /\n          ( /  (    (        ,___    ^*+_+* )   &lt;    &lt;      \\\n           U _/     )    *--&lt;  ) ^\\-----++__)   )    )       )\n            (      )  _(^)^^))  )  )\\^^^^^))^*+/    /       /\n          (      /  (_))_^)) )  )  ))^^^^^))^^^)__/     +^^\n         (     ,/    (^))^))  )  ) ))^^^^^^^))^^)       _)\n          *+__+*       (_))^)  ) ) ))^^^^^^))^^^^^)____*^\n          \\             \\_)^)_)) ))^^^^^^^^^^))^^^^)\n           (_             ^\\__^^^^^^^^^^^^))^^^^^^^)\n             ^\\___            ^\\__^^^^^^))^^^^^^^^)\\\\\n                  ^^^^^\\uuu/^^\\uuu/^^^^\\^\\^\\^\\^\\^\\^\\^\\\n                     ___) &gt;____) &gt;___   ^\\_\\_\\_\\_\\_\\_\\)\n                    ^^^//\\\\_^^//\\\\_^       ^(\\_\\_\\_\\)\n                      ^^^ ^^ ^^^ ^\n</code></pre> <p>You can override those defaults by providing parameter values on the command line, or by providing them through another source of configuration information.</p>"},{"location":"nextflow_run/03_config/#42-override-defaults-with-a-run-specific-config-file","title":"4.2. Override defaults with a run-specific config file","text":"<p>You may want to override those defaults without having to either specify parameters on the command line, or modify the original script file.</p> <p>A clean way to do this is to create a new <code>nextflow.config</code> file in a run-specific working directory.</p> <p>Let's start by creating a new directory:</p> <pre><code>mkdir -p tux-run\ncd tux-run\n</code></pre> <p>Then, create a blank configuration file in that directory:</p> <pre><code>touch nextflow.config\n</code></pre> <p>Now open the new file and add the parameters you want to customize:</p> tux-run/nextflow.config<pre><code>params.input = '../greetings.csv'\nparams.character = 'tux'\n</code></pre> <p>Note that the path to the input file must reflect the directory structure.</p> <p>We can now run our pipeline from within our new working directory:</p> <pre><code>nextflow run ../3-main.nf\n</code></pre> <p>This will create a new set of directories under <code>tux-run/</code> including <code>tux-run/work/</code> and <code>tux-run/results/</code>.</p> Command output <pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `../3-main.nf` [trusting_escher] DSL2 - revision: 356df0818d\n\nexecutor &gt;  local (8)\n[59/b66913] sayHello (2)       [100%] 3 of 3 \u2714\n[ad/f06364] convertToUpper (3) [100%] 3 of 3 \u2714\n[10/714895] collectGreetings   [100%] 1 of 1 \u2714\n[88/3ece98] cowpy              [100%] 1 of 1 \u2714\n</code></pre> <p>In this run, Nextflow combines the <code>nextflow.config</code> in our current directory with the <code>nextflow.config</code> in the root directory of the pipeline, and thereby overrides the default character (turkey) with the tux character.</p> <p>The final output file should contain the tux character saying the greetings.</p> File contents results/cowpy-COLLECTED-output.txt<pre><code> _________\n/ HELLO   \\\n| BONJOUR |\n\\ HOL\u00e0    /\n ---------\n   \\\n    \\\n        .--.\n       |o_o |\n       |:_/ |\n      //   \\ \\\n     (|     | )\n    /'\\_   _/`\\\n    \\___)=(___/\n</code></pre> <p>That's it!</p> <p>Make sure to change back to the previous directory before moving to the next section.</p> <pre><code>cd ..\n</code></pre> <p>Now let's look at another useful way to set parameter values.</p>"},{"location":"nextflow_run/03_config/#43-specify-parameters-using-a-parameter-file","title":"4.3. Specify parameters using a parameter file","text":"<p>Nextflow also allows us to specify parameters via a parameter file in either YAML or JSON format. This makes it very convenient to manage and distribute alternative sets of default values, for example, as well as run-specific parameter values.</p> <p>We provide an example YAML parameter file in the current directory, called <code>test-params.yaml</code>, which contains a key-value pair for each of the inputs our workflow expects.</p> File contents test-params.yaml<pre><code>input: \"greetings.csv\"\ncharacter: \"stegosaurus\"\n</code></pre> <p>To run the workflow with this parameter file, simply add <code>-params-file &lt;filename&gt;</code> to the base command.</p> <pre><code>nextflow run 3-main.nf -params-file test-params.yaml\n</code></pre> <p>This should run without error.</p> Command output Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `3-main.nf` [disturbed_sammet] DSL2 - revision: ede9037d02\n\nexecutor &gt;  local (8)\n[f0/35723c] sayHello (2)       | 3 of 3 \u2714\n[40/3efd1a] convertToUpper (3) | 3 of 3 \u2714\n[17/e97d32] collectGreetings   | 1 of 1 \u2714\n[98/c6b57b] cowpy              | 1 of 1 \u2714\n</code></pre> <p>The final output file should contain the stegosaurus character saying the greetings.</p> File contents results/cowpy-COLLECTED-output.txt<pre><code>_________\n/ HELLO   \\\n| HOL\u00e0    |\n\\ BONJOUR /\n ---------\n\\                             .       .\n \\                           / `.   .' \"\n  \\                  .---.  &lt;    &gt; &lt;    &gt;  .---.\n   \\                 |    \\  \\ - ~ ~ - /  /    |\n         _____          ..-~             ~-..-~\n        |     |   \\~~~\\.'                    `./~~~/\n       ---------   \\__/                        \\__/\n      .'  O    \\     /               /       \\  \"\n     (_____,    `._.'               |         }  \\/~~~/\n      `----.          /       }     |        /    \\__/\n            `-.      |       /      |       /      `. ,~~|\n                ~-.__|      /_ - ~ ^|      /- _      `..-'\n                     |     /        |     /     ~-.     `-. _  _  _\n                     |_____|        |_____|         ~ - . _ _ _ _ _&gt;\n</code></pre> <p>Using a parameter file may seem like overkill when you only have a few parameters to specify, but some pipelines expect dozens of parameters. In those cases, using a parameter file will allow us to provide parameter values at runtime without having to type massive command lines and without modifying the workflow script. It also makes it easier to distribute sets of parameters to collaborators.</p>"},{"location":"nextflow_run/03_config/#takeaway_3","title":"Takeaway","text":"<p>You know how to manage parameter defaults and override them at runtime using command-line arguments or a parameter file. There are a few more options but these are the ones you are most likely to encounter.</p>"},{"location":"nextflow_run/03_config/#whats-next_3","title":"What's next?","text":"<p>Learn how to bring it all together by using profiles to switch between alternative configurations more conveniently.</p>"},{"location":"nextflow_run/03_config/#5-use-profiles-to-select-preset-configurations","title":"5. Use profiles to select preset configurations","text":"<p>You may want to switch between alternative settings depending on what computing infrastructure you're using. For example, you might want to develop and run small-scale tests locally on your laptop, then run full-scale workloads on HPC or cloud.</p> <p>This applies to workflow parameters too: you may have different sets of reference files or groups of settings that you want to swap out depending on the data you're analyzing (e.g. mouse vs human data etc).</p> <p>Nextflow lets you set up profiles that describe different configurations, which you can then select at runtime using a command-line argument, rather than having to modify the configuration file itself.</p>"},{"location":"nextflow_run/03_config/#51-create-profiles-for-switching-between-local-development-and-execution-on-hpc","title":"5.1. Create profiles for switching between local development and execution on HPC","text":"<p>Let's set up two alternative profiles; one for running small scale loads on a regular computer, where we'll use Docker containers, and one for running on a university HPC with a Slurm scheduler, where we'll use Conda packages.</p> <p>Add the following to your <code>nextflow.config</code> file:</p> nextflow.config<pre><code>profiles {\n    my_laptop {\n        process.executor = 'local'\n        docker.enabled = true\n    }\n    univ_hpc {\n        process.executor = 'slurm'\n        conda.enabled = true\n        process.resourceLimits = [\n            memory: 750.GB,\n            cpus: 200,\n            time: 30.d\n        ]\n    }\n}\n</code></pre> <p>You see that for the university HPC, we're also specifying resource limitations.</p>"},{"location":"nextflow_run/03_config/#52-run-the-workflow-with-a-profile","title":"5.2. Run the workflow with a profile","text":"<p>To specify a profile in our Nextflow command line, we use the <code>-profile</code> argument.</p> <p>Let's try running the workflow with the <code>my_laptop</code> configuration.</p> <pre><code>nextflow run 3-main.nf -profile my_laptop\n</code></pre> <p>This should run without error and produce the same results as previously.</p> Command output <pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `3-main.nf` [gigantic_brazil] DSL2 - revision: ede9037d02\n\nexecutor &gt;  local (8)\n[58/da9437] sayHello (3)       | 3 of 3 \u2714\n[35/9cbe77] convertToUpper (2) | 3 of 3 \u2714\n[67/857d05] collectGreetings   | 1 of 1 \u2714\n[37/7b51b5] cowpy              | 1 of 1 \u2714\n</code></pre> <p>As you can see, this allows us to toggle between configurations very conveniently at runtime.</p> <p>Warning</p> <p>The <code>univ_hpc</code> profile will not run properly in the training environment since we do not have access to a Slurm scheduler.</p> <p>If in the future we find other elements of configuration that are always co-occurring with these, we can simply add them to the corresponding profile(s). We can also create additional profiles if there are other elements of configuration that we want to group together.</p>"},{"location":"nextflow_run/03_config/#53-create-a-test-profile","title":"5.3. Create a test profile","text":"<p>As noted above, profiles are not only for infrastructure configuration. We can also use them to swap out sets of default values for workflow parameters, or to make it easier for ourselves and for others to try out the workflow without having to gather appropriate input values themselves.</p> <p>Let's take the example of creating a test profile to make it easy to test the workflow with minimal effort.</p> <p>The syntax for expressing default values is the same as when writing them into the workflow file itself, except we wrap them in a block named <code>test</code>:</p> Syntax example<pre><code>    test {\n        params.&lt;parameter1&gt;\n        params.&lt;parameter2&gt;\n        ...\n    }\n</code></pre> <p>If we add a test profile for our workflow, the <code>profiles</code> block becomes:</p> nextflow.config<pre><code>profiles {\n    my_laptop {\n        process.executor = 'local'\n        docker.enabled = true\n    }\n    univ_hpc {\n        process.executor = 'slurm'\n        conda.enabled = true\n        process.resourceLimits = [\n            memory: 750.GB,\n            cpus: 200,\n            time: 30.d\n        ]\n    }\n    test {\n        params.input = 'greetings.csv'\n        params.character = 'turtle'\n    }\n}\n</code></pre> <p>Just like for technical configuration profiles, you can set up multiple different profiles specifying workflow parameters under any arbitrary name you like.</p>"},{"location":"nextflow_run/03_config/#54-run-the-workflow-locally-with-the-test-profile","title":"5.4. Run the workflow locally with the test profile","text":"<p>Conveniently, profiles are not mutually exclusive, so we can specify multiple profiles in our command line using the following syntax <code>-profile &lt;profile1&gt;,&lt;profile2&gt;</code> (for any number of profiles).</p> <p>Tip</p> <p>If you combine profiles that set values for the same elements of configuration and are described in the same configuration file, Nextflow will resolve the conflict by using whichever value it read in last (i.e. whatever comes later in the file).</p> <p>Let's try adding the test profile to our previous command:</p> <pre><code>nextflow run 3-main.nf -profile my_laptop,test\n</code></pre> <p>This should run without error.</p> Command output <pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `3-main.nf` [gigantic_brazil] DSL2 - revision: ede9037d02\n\nexecutor &gt;  local (8)\n[58/da9437] sayHello (3)       | 3 of 3 \u2714\n[35/9cbe77] convertToUpper (2) | 3 of 3 \u2714\n[67/857d05] collectGreetings   | 1 of 1 \u2714\n[37/7b51b5] cowpy              | 1 of 1 \u2714\n</code></pre> <p>The final output file should contain the turtle character saying the greetings.</p> File contents results/cowpy-COLLECTED-output.txt<pre><code> _________\n/ BONJOUR \\\n| HOL\u00e0    |\n\\ HELLO   /\n ---------\n    \\                                  ___-------___\n     \\                             _-~~             ~~-_\n      \\                         _-~                    /~-_\n             /^\\__/^\\         /~  \\                   /    \\\n           /|  O|| O|        /      \\_______________/        \\\n          | |___||__|      /       /                \\          \\\n          |          \\    /      /                    \\          \\\n          |   (_______) /______/                        \\_________ \\\n          |         / /         \\                      /            \\\n           \\         \\^\\\\         \\                  /               \\     /\n             \\         ||           \\______________/      _-_       //\\__//\n               \\       ||------_-~~-_ ------------- \\ --/~   ~\\    || __/\n                 ~-----||====/~     |==================|       |/~~~~~\n                  (_(__/  ./     /                    \\_\\      \\.\n                         (_(___/                         \\_____)_)\n</code></pre> <p>This means that as long as we distribute any test data files with the workflow code, anyone can quickly try out the workflow without having to supply their own inputs via the command line or a parameter file.</p> <p>Tip</p> <p>You can even point to URLs for larger files that are stored externally. Nextflow will download them automatically as long as there is an open connection.</p>"},{"location":"nextflow_run/03_config/#55-use-nextflow-config-to-see-the-resolved-configuration","title":"5.5. Use <code>nextflow config</code> to see the resolved configuration","text":"<p>As noted above, sometimes the same parameter can be set to different values in profiles that you want to combine. And more generally, there are numerous places where elements of configuration can be stored, and sometimes the same properties can be set to different values in different places.</p> <p>Nextflow applies a set order of precedence to resolve any conflicts, but that can be tricky to determine yourself. And even if nothing is conflicting, it can be tedious to look up all the possible places where things could be configured.</p> <p>Fortunately, Nextflow includes a convenient utility tool called <code>config</code> that can automate that whole process for you.</p> <p>The <code>config</code> tool will explore all the contents in your current working directory, hoover up any configuration files, and produce the fully resolved configuration that Nextflow would use to run the workflow. This allows you to find out what settings will be used without having to launch anything.</p>"},{"location":"nextflow_run/03_config/#551-resolve-the-default-configuration","title":"5.5.1. Resolve the default configuration","text":"<p>Run this command to resolve the configuration that would be applied by default.</p> <pre><code>nextflow config\n</code></pre> Command output <pre><code>docker {\n   enabled = false\n}\n\nconda {\n   enabled = true\n}\n\nprocess {\n   memory = '1 GB'\n   withName:cowpy {\n      memory = '2 GB'\n      cpus = 2\n   }\n}\n\nparams {\n   input = 'greetings.csv'\n   character = 'turkey'\n}\n</code></pre>"},{"location":"nextflow_run/03_config/#552-resolve-the-configuration-with-specific-settings-activated","title":"5.5.2. Resolve the configuration with specific settings activated","text":"<p>If you provide command-line parameters, e.g. enabling one or more profiles or loading a parameter file, the command will additionally take those into account.</p> <pre><code>nextflow config -profile my_laptop,test\n</code></pre> Command output <pre><code>docker {\n   enabled = true\n}\n\nconda {\n   enabled = true\n}\n\nprocess {\n   memory = '1 GB'\n   withName:cowpy {\n      memory = '2 GB'\n      cpus = 2\n   }\n   executor = 'local'\n}\n\nparams {\n   input = 'greetings.csv'\n   character = 'turtle'\n}\n</code></pre> <p>This gets especially useful for complex projects that involve multiple layers of configuration.</p>"},{"location":"nextflow_run/03_config/#takeaway_4","title":"Takeaway","text":"<p>You know how to use profiles to select a preset configuration at runtime with minimal hassle. More generally, you know how to configure your workflow executions to suit different compute platforms and enhance the reproducibility of your analyses.</p>"},{"location":"nextflow_run/03_config/#whats-next_4","title":"What's next?","text":"<p>Give yourself a big pat on the back! You know everything you need to know to get started running and managing Nextflow pipelines.</p> <p>That concludes this course, but if you're eager to keep learning, we have two main recommendations:</p> <ul> <li>If you want to dig deeper into developing your own pipelines, have a look at Hello Nextflow, a course for beginners that covers the same general progression as this one but goes into much more detail about channels and operators.</li> <li>If you would like to continue learning how to run Nextflow pipelines without going deeper into the code, have a look at the first part of Hello nf-core, which introduces the tooling for finding and running pipelines from the hugely popular nf-core project.</li> </ul> <p>Have fun!</p>"},{"location":"nextflow_run/04_nf-core/","title":"Part 3: Run nf-core","text":"<ol> <li>Meet the nf-core style Hello World</li> <li>Run it (with test profile) and interpret the console output</li> <li>Locate the outputs (results)</li> <li>Find a 'real' nf-core pipeline</li> <li>Run nf-core/demo</li> </ol>"},{"location":"nextflow_run/05_seqera/","title":"Part 4: Run on Seqera","text":"<p>So far we've been running Nextflow workflows on our local machine using the command line interface. In this section, we'll introduce you to Seqera Platform, a powerful cloud-based platform for running, monitoring, and sharing Nextflow workflows.</p> <p>Tip</p> <p>Sign up to try Seqera for free or request a demo for deployments in your own on-premise or cloud environment.</p> <p>You'll learn how to use Seqera Platform within Nextflow CLI, the Seqera Platform GUI, and the API.</p> <p>You can use Seqera Platform via either the CLI, through the online GUI or through the API.</p>"},{"location":"nextflow_run/seqera/01_run_with_cli/","title":"01 run with cli","text":""},{"location":"nextflow_run/seqera/01_run_with_cli/#1-use-seqera-platform-to-capture-and-monitor-nextflow-jobs-launched-from-the-cli","title":"1. Use Seqera Platform to capture and monitor Nextflow jobs launched from the CLI","text":"<p>We'll start by using the Nextflow CLI to launch a pipeline and monitor it in Seqera Platform. Start by logging into the Seqera Platform.</p> <p>Nextflow Tower</p> <p>Seqera Platform was previously known as Nextflow Tower. You'll still see references to the previous name in environment variables and CLI option names.</p>"},{"location":"nextflow_run/seqera/01_run_with_cli/#11-set-up-your-seqera-platform-token-by-exporting-it-to-your-environment","title":"1.1. Set up your Seqera Platform token by exporting it to your environment","text":"<p>Follow these steps to set up your token:</p> <ol> <li> <p>Create a new token by clicking on the Settings drop-down menu:</p> <p></p> </li> <li> <p>Name your token:</p> <p></p> </li> <li> <p>Save your token safely:</p> <p></p> </li> <li> <p>To make your token available to the Nextflow CLI, export it on the command line:</p> <p>Open a terminal and type:</p> <pre><code> export TOWER_ACCESS_TOKEN=eyxxxxxxxxxxxxxxxQ1ZTE=\n</code></pre> <p>Where <code>eyxxxxxxxxxxxxxxxQ1ZTE=</code> is the token you have just created.</p> <p>Security Note</p> <p>Keep your token secure and do not share it with others. Add a Space before the <code>export</code> command above to prevent your token from being saved in your shell history.</p> </li> </ol>"},{"location":"nextflow_run/seqera/01_run_with_cli/#12-run-nextflow-cli-with-seqera-platform-visualizing-and-capturing-logs","title":"1.2. Run Nextflow CLI with Seqera Platform visualizing and capturing logs","text":"<p>Run a Nextflow workflow with the addition of the <code>-with-tower</code> command:</p> <pre><code>nextflow run nextflow-io/hello -with-tower\n</code></pre> <p>You will see output similar to the following:</p> Output<pre><code> N E X T F L O W   ~  version 24.04.4\n\nLaunching `https://github.com/nextflow-io/hello` [evil_engelbart] DSL2 - revision: afff16a9b4 [master]\n\nDownloading plugin nf-tower@1.9.1\nMonitor the execution with Seqera Platform using this URL: https://cloud.seqera.io/user/kenbrewer/watch/5Gs0qqV9Y9rguE\nexecutor &gt;  local (4)\n[80/810411] process &gt; sayHello (1) [100%] 4 of 4 \u2714\nCiao world!\n\nBonjour world!\n\nHola world!\n\nHello world!\n</code></pre> <p>Hold Ctrl or Cmd and click on the link to open it in your browser. You'll see the Seqera Platform interface with the job finished and the logs captured.</p> <p></p> <p>You will see and be able to monitor your Nextflow jobs in Seqera Platform.</p>"},{"location":"nextflow_run/seqera/01_run_with_cli/#13-set-up-seqera-platform-in-nextflow-configuration","title":"1.3. Set up Seqera Platform in Nextflow configuration","text":"<p>Doing that token setup regularly can become tedious, so let's set this configuration for all our pipeline runs with the global Nextflow configuration file located at <code>$HOME/.nextflow/config</code>.</p> <p>Before we set the configuration, we need to permanently store the token from our environment in Nextflow using a Nextflow secret:</p> <pre><code>nextflow secrets set tower_access_token \"$TOWER_ACCESS_TOKEN\"\n</code></pre> <p>Make sure your token was saved using:</p> <pre><code>nextflow secrets get tower_access_token\n</code></pre> <p>Next, open the Nextflow configuration file located at <code>$HOME/.nextflow/config</code>:</p> <pre><code>code $HOME/.nextflow/config\n</code></pre> <p>Then add the following block of configuration:</p> $HOME/.nextflow/config<pre><code>tower {\n    enabled = true\n    endpoint = \"https://api.cloud.seqera.io\"\n    accessToken = secrets.tower_access_token\n}\n</code></pre> <p>Endpoint</p> <p>The <code>endpoint</code> is the URL of the Seqera Platform API. If your institution is running a private instance of Seqera Platform, you should change this to the appropriate URL.</p> <p>Run your Nextflow workflows as before, but without the <code>-with-tower</code> command:</p> <pre><code>nextflow run nextflow-io/hello\n</code></pre> <p>You will see the following output:</p> Output<pre><code> N E X T F L O W   ~  version 24.04.4\n\nLaunching `https://github.com/nextflow-io/hello` [fabulous_euclid] DSL2 - revision: afff16a9b4 [master]\n\nMonitor the execution with Seqera Platform using this URL: https://cloud.seqera.io/user/kenbrewer/watch/KYjRktIlOuxrh\nexecutor &gt;  local (4)\n[71/eaa915] process &gt; sayHello (3) [100%] 4 of 4 \u2714\nCiao world!\n\nBonjour world!\n\nHola world!\n\nHello world!\n</code></pre> <p>Note that we are logging to Seqera Platform even though we did not use the <code>-with-tower</code> command!</p>"},{"location":"nextflow_run/seqera/01_run_with_cli/#14-use-seqera-platform-to-explore-the-resolved-configuration-of-a-nextflow-pipeline","title":"1.4. Use Seqera Platform to explore the resolved configuration of a Nextflow pipeline","text":"<p>Click on the link provided in the output to open the Seqera Platform for your run, then click on the <code>Configuration</code> tab. If you ran your pipeline from the <code>hello_nextflow</code> directory, you'll see something like this:</p> <p></p> <p>Notice that configuration for our pipeline run is being run pulled from three separate files:</p> <ul> <li><code>/home/gitpod/.nextflow/config</code> - This is the global configuration file we just added.</li> <li><code>/home/gitpod/.nextflow/assets/nextflow-io/hello/nextflow.config</code> - This is the <code>nextflow.config</code> file from the <code>nextflow-io/hello</code> repository.</li> <li><code>/workspaces/training/nf-training/hello-nextflow/nextflow.config</code> - This is the <code>nextflow.config</code> file from our current working directory.</li> </ul> <p>Nextflow resolves these configurations at runtime with a specific order of precedence. The general rule, however, is that more specific configurations override less specific ones, and config/params specified on the CLI will override defaults in the config files.</p> <p>Helpfully, Seqera Platform shows us the final output of this configuration resolution process which can be very useful for debugging!</p>"},{"location":"nextflow_run/seqera/01_run_with_cli/#takeaway","title":"Takeaway","text":"<p>You have learned how to:</p> <ul> <li>Set up your Seqera Platform token by exporting it to your environment.</li> <li>Run Nextflow CLI with Seqera Platform visualizing and capturing logs.</li> <li>Set up Seqera Platform logging by default.</li> <li>Use Seqera Platform to explore the resolved configuration of a Nextflow pipeline.</li> </ul>"},{"location":"nextflow_run/seqera/01_run_with_cli/#whats-next","title":"What's next?","text":"<p>Learn how to launch Nextflow pipelines from Seqera Platform using the Launchpad feature.</p>"},{"location":"nextflow_run/seqera/02_run_with_launchpad/","title":"02 run with launchpad","text":""},{"location":"nextflow_run/seqera/02_run_with_launchpad/#2-using-seqera-platform-launchpad-to-run-nextflow-workflows","title":"2 Using Seqera Platform Launchpad to run Nextflow workflows","text":"<p>So far we've been running Nextflow workflows on our local machine using the command line interface but sending the logs to Seqera Platform for monitoring and visualization. Next we want to start using Seqera Platform to launch Nextflow workflows on our behalf.</p> <p>Community Showcase</p> <p>Having a compute environment capable of running Nextflow workflows configured in Seqera Platform is normally a prerequisite for this task. But we want to see how it works before we put in that effort, so we'll start by launching a job in community/showcase workspace which has a compute environment already set up.</p> <p>Trainer Tip</p> <p>Launch a test run of the nf-core/rnaseq pipeline in the community/showcase workspace prior to starting this session, so you'll have a recent run for participants to inspect.</p>"},{"location":"nextflow_run/seqera/02_run_with_launchpad/#21-navigate-to-the-communityshowcase-workspace","title":"2.1. Navigate to the community/showcase workspace","text":"<p>Seqera Platform has a concept of organizations and workspaces which are used to organize and share pipelines, compute environments, data, credentials, and more. The <code>community/showcase</code> workspace is a public workspace where you can see some example pipelines and compute environments. Each user has an allotted amount of free compute to use in this workspace.</p> <p>Click on your username in the top left corner of the screen to bring up the list of organizations and workspaces you have access to. Select the <code>community/showcase</code> workspace.</p>"},{"location":"nextflow_run/seqera/02_run_with_launchpad/#22-launch-a-test-run-of-nf-corernaseq-pipeline","title":"2.2. Launch a test run of nf-core/rnaseq pipeline","text":"<p>In the <code>community/showcase</code> workspace, you will see a list of pipelines that have been set up by the workspace owner for you to run. Follow these steps to launch a test run of a pipeline:</p> <p></p> <ol> <li>Find the <code>nf-core-rnaseq</code> pipeline in the list of pipelines.</li> <li>Click on the <code>Launch</code> button to bring up the launch form.</li> <li>Change the \"Workflow run name\" to \"-rnaseq-test\". <li>Click \"Next\" to bring up the parameters form.</li> <li>Find the <code>trimmer</code> parameter and change it to <code>fastp</code>.</li> <li>Click on \"Next\" to inspect the advanced configuration.</li> <li>Click \"Launch\" to start the pipeline!</li> <p>Tip</p> <p>In the advanced configuration, you'll see a section named \"Pre-run script\" with a script similar to the following:</p> <pre><code>export NXF_FILE_ROOT=s3://nf-tower-bucket/scratch/$TOWER_WORKFLOW_ID\n</code></pre> <p>This is what ensures that everyone's pipeline will write to a unique location in cloud storage despite all having the <code>outdir</code> parameter set to <code>./results</code>.</p>"},{"location":"nextflow_run/seqera/02_run_with_launchpad/#23-monitor-the-pipeline-run","title":"2.3. Monitor the pipeline run","text":"<p>After launching the pipeline, you will be taken to the pipeline run page where you can monitor the progress of the pipeline. It may take some time for the pipeline to start running while AWS Batch spins up the needed resources, so go to the \"Runs\" tab above the pipeline and open a recent completed (or failed) run by one of your \"teammates\" in the community.</p>"},{"location":"nextflow_run/seqera/02_run_with_launchpad/#24-inspect-a-pipeline-run","title":"2.4. Inspect a pipeline run","text":"<p>Scroll down to find the list of tasks that were executed in the pipeline run. For example, by searching for <code>fastq</code> we can find the task <code>NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_FASTP:FASTP (WT_REP2)</code> that was executed as part of the pipeline.</p> <p>Click on the task to see the task details:</p> <p></p> <ol> <li> <p>Find the following details on the \"About\" page for the the task you're inspecting:</p> </li> <li> <p> How long did the task script run (not including scheduling time)?</p> </li> <li> How many CPUs were allocated to the task?</li> <li> What was the virtual machine type that the task ran on?</li> <li> <p> What was the estimated cost of the task?</p> </li> <li> <p>Explore the Execution Log tab. What information is available here?</p> </li> <li> <p>Explore the Data Explorer tab. Note that the work directory structure we've seen during local runs is replicated here in cloud storage!</p> </li> </ol>"},{"location":"nextflow_run/seqera/02_run_with_launchpad/#takeaway","title":"Takeaway","text":"<p>You have learned how to:</p> <ul> <li>Switch between organizations and workspaces in Seqera Platform.</li> <li>Launch a Nextflow pipeline that ran in the cloud using Seqera Platform.</li> <li>Monitor the progress of the pipeline run.</li> <li>Inspect the details of a task that was executed as part of the pipeline.</li> </ul>"},{"location":"nextflow_run/seqera/02_run_with_launchpad/#next-steps","title":"Next steps","text":"<p>In the next section, we will learn how to set up a compute environment in Seqera Platform to run our own Nextflow workflows.</p>"},{"location":"nf4_science/","title":"Nextflow for Science","text":"<p>These are courses that demonstrate how to apply the concepts and components presented in the Hello Nextflow beginner course to specific scientific use cases. Each course consists of a series of training modules that are designed to help learners build up their skills progressively.</p> <p>Nextflow for Genomics</p> <p> Learn to develop a pipeline for genomics in Nextflow.</p> <p>This is a course for researchers who wish to learn how to develop their own genomics pipelines. The course uses a variant calling use case to demonstrate how to develop a simple but functional genomics pipeline.</p> <p>Start the Nextflow for Genomics training </p> <p>Nextflow for RNAseq</p> <p> Learn to develop a pipeline for RNAseq data processing in Nextflow.</p> <p>This is a course for researchers who wish to learn how to develop their own RNAseq pipelines. The course uses a bulk RNAseq processing use case to demonstrate how to develop a simple but functional RNAseq pipeline.</p> <p>Start the Nextflow for RNAseq training </p> <p>Let us know what other domains and use cases you'd like to see covered here by posting in the Training section of the community forum.</p>"},{"location":"nf4_science/genomics/","title":"Nextflow for Genomics","text":"<p>This training course is intended for researchers in genomics and related fields who are interested in developing or customizing data analysis pipelines. It builds on the Hello Nextflow beginner training and demonstrates how to use Nextflow in the specific context of the genomics domain.</p> <p>Specifically, this course demonstrates how to implement a simple variant calling pipeline with GATK (Genome Analysis Toolkit), a widely used software package for analyzing high-throughput sequencing data.</p> <p>Let's get started! Click on the \"Open in GitHub Codespaces\" button below to launch the training environment (preferably in a separate tab), then read on while it loads.</p> <p></p>"},{"location":"nf4_science/genomics/#learning-objectives","title":"Learning objectives","text":"<p>By working through this course, you will learn how to apply foundational Nextflow concepts and tooling to a typical genomics use case.</p> <p>By the end of this workshop you will be able to:</p> <ul> <li>Write a linear workflow to apply variant calling to a single sample</li> <li>Handle accessory files such as index files and reference genome resources appropriately</li> <li>Leverage Nextflow's dataflow paradigm to parallelize per-sample variant calling</li> <li>Implement multi-sample variant calling using relevant channel operators</li> <li>Configure pipeline execution and manage and optimize resource allocations</li> <li>Implement per-step and end-to-end pipeline tests that handle genomics-specific idiosyncrasies appropriately</li> </ul>"},{"location":"nf4_science/genomics/#prerequisites","title":"Prerequisites","text":"<p>The course assumes some minimal familiarity with the following:</p> <ul> <li>Tools and file formats commonly used in this scientific domain</li> <li>Experience with the command line</li> <li>Foundational Nextflow concepts and tooling covered in the Hello Nextflow beginner training.</li> </ul> <p>For technical requirements and environment setup, see the Environment Setup mini-course.</p>"},{"location":"nf4_science/genomics/00_orientation/","title":"Orientation","text":"<p>The training environment contains all the software, code and data necessary to work through this training course, so you don't need to install anything yourself. However, you do need a (free) account to log in, and you should take a few minutes to familiarize yourself with the interface.</p> <p>If you have not yet done so, please follow this link before going any further.</p>"},{"location":"nf4_science/genomics/00_orientation/#materials-provided","title":"Materials provided","text":"<p>Throughout this training course, we'll be working in the <code>nf4-science/genomics/</code> directory, which you need to move into when you open the training workspace. This directory contains all the code files, test data and accessory files you will need.</p> <p>Feel free to explore the contents of this directory; the easiest way to do so is to use the file explorer on the left-hand side of the training workspace in the VSCode interface. Alternatively, you can use the <code>tree</code> command. Throughout the course, we use the output of <code>tree</code> to represent directory structure and contents in a readable form, sometimes with minor modifications for clarity.</p> <p>Here we generate a table of contents to the second level down:</p> <pre><code>tree . -L 2\n</code></pre> <p>If you run this inside <code>nf4-science/genomics</code>, you should see the following output:</p> Directory contents<pre><code>.\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 bam\n\u2502   \u251c\u2500\u2500 ref\n\u2502   \u251c\u2500\u2500 sample_bams.txt\n\u2502   \u2514\u2500\u2500 samplesheet.csv\n\u251c\u2500\u2500 genomics-1.nf\n\u251c\u2500\u2500 genomics-2.nf\n\u251c\u2500\u2500 genomics-3.nf\n\u251c\u2500\u2500 genomics-4.nf\n\u251c\u2500\u2500 nextflow.config\n\u2514\u2500\u2500 solutions\n    \u251c\u2500\u2500 modules\n    \u251c\u2500\u2500 nf-test.config\n    \u2514\u2500\u2500 tests\n\n6 directories, 8 files\n</code></pre> <p>Note</p> <p>Don't worry if this seems like a lot; we'll go through the relevant pieces at each step of the course. This is just meant to give you an overview.</p> <p>Here's a summary of what you should know to get started:</p> <ul> <li> <p>The <code>.nf</code> files are workflow scripts that are named based on what part of the course they're used in.</p> </li> <li> <p>The file <code>nextflow.config</code> is a configuration file that sets minimal environment properties.   You can ignore it for now.</p> </li> <li> <p>The <code>data</code> directory contains input data and related resources, described later in the course.</p> </li> </ul> <p>Completed workflows (solutions) will be added in the near future.</p> <p>Tip</p> <p>If for whatever reason you move out of this directory, you can always run this command to return to it:</p> <pre><code>cd /workspaces/training/nf4-science/genomics\n</code></pre> <p>Now, to begin the course, click on the arrow in the bottom right corner of this page.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/","title":"Part 1: Per-sample variant calling","text":"<p>In the first part of this course, we show you how to build a simple variant calling pipeline that applies GATK variant calling to individual sequencing samples.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#method-overview","title":"Method overview","text":"<p>Variant calling is a genomic analysis method that aims to identify variations in a genome sequence relative to a reference genome. Here we are going to use tools and methods designed for calling short variants, i.e. SNPs and indels.</p> <p></p> <p>A full variant calling pipeline typically involves a lot of steps, including mapping to the reference (sometime referred to as genome alignment) and variant filtering and prioritization. For simplicity, in this part of the course we are going to focus on just the variant calling part.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#dataset","title":"Dataset","text":"<p>We provide the following data and related resources:</p> <ul> <li>A reference genome consisting of a small region of the human chromosome 20 (from hg19/b37) and its accessory files (index and sequence dictionary).</li> <li>Three whole genome sequencing samples corresponding to a family trio (mother, father and son), which have been subset to a small slice of data on chromosome 20 to keep the file sizes small.   This is Illumina short-read sequencing data that have already been mapped to the reference genome, provided in BAM format (Binary Alignment Map, a compressed version of SAM, Sequence Alignment Map).</li> <li>A list of genomic intervals, i.e. coordinates on the genome where our samples have data suitable for calling variants, provided in BED format.</li> </ul>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#workflow","title":"Workflow","text":"<p>In this part of the course, we're going to develop a workflow that does the following:</p> <ol> <li>Generate an index file for each BAM input file using Samtools</li> <li>Run the GATK HaplotypeCaller on each BAM input file to generate per-sample variant calls in VCF (Variant Call Format)</li> </ol> BAMSamtools indexBAM indexIntervalsReference+ index &amp; dictGATK HaplotypeCallerVCF + index <p>Note</p> <p>Index files are a common feature of bioinformatics file formats; they contain information about the structure of the main file that allows tools like GATK to access a subset of the data without having to read through the whole file. This is important because of how big these files can get.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#0-warmup-test-the-samtools-and-gatk-commands-interactively","title":"0. Warmup: Test the Samtools and GATK commands interactively","text":"<p>First we want to try out the commands manually before we attempt to wrap them in a workflow. The tools we need (Samtools and GATK) are not installed in the GitHub Codespaces environment, so we'll use them via containers (see Hello Containers).</p> <p>Note</p> <p>Make sure you're in the <code>nf4-science/genomics</code> directory so that the last part of the path shown when you type <code>pwd</code> is <code>genomics</code>.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#01-index-a-bam-input-file-with-samtools","title":"0.1. Index a BAM input file with Samtools","text":"<p>We're going to pull down a Samtools container, spin it up interactively and run the <code>samtools index</code> command on one of the BAM files.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#011-pull-the-samtools-container","title":"0.1.1. Pull the Samtools container","text":"<pre><code>docker pull community.wave.seqera.io/library/samtools:1.20--b5dfbd93de237464\n</code></pre>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#012-spin-up-the-samtools-container-interactively","title":"0.1.2. Spin up the Samtools container interactively","text":"<pre><code>docker run -it -v ./data:/data community.wave.seqera.io/library/samtools:1.20--b5dfbd93de237464\n</code></pre>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#013-run-the-indexing-command","title":"0.1.3. Run the indexing command","text":"<p>The Samtools documentation gives us the command line to run to index a BAM file.</p> <p>We only need to provide the input file; the tool will automatically generate a name for the output by appending <code>.bai</code> to the input filename.</p> <pre><code>samtools index /data/bam/reads_mother.bam\n</code></pre> <p>This should complete immediately, and you should now see a file called <code>reads_mother.bam.bai</code> in the same directory as the original BAM input file.</p> Directory contents<pre><code>data/bam/\n\u251c\u2500\u2500 reads_father.bam\n\u251c\u2500\u2500 reads_mother.bam\n\u251c\u2500\u2500 reads_mother.bam.bai\n\u2514\u2500\u2500 reads_son.bam\n</code></pre>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#014-exit-the-samtools-container","title":"0.1.4. Exit the Samtools container","text":"<pre><code>exit\n</code></pre>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#02-call-variants-with-gatk-haplotypecaller","title":"0.2. Call variants with GATK HaplotypeCaller","text":"<p>We're going to pull down a GATK container, spin it up interactively and run the <code>gatk HaplotypeCaller</code> command on the BAM file we just indexed.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#021-pull-the-gatk-container","title":"0.2.1. Pull the GATK container","text":"<pre><code>docker pull community.wave.seqera.io/library/gatk4:4.5.0.0--730ee8817e436867\n</code></pre>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#022-spin-up-the-gatk-container-interactively","title":"0.2.2. Spin up the GATK container interactively","text":"<pre><code>docker run -it -v ./data:/data community.wave.seqera.io/library/gatk4:4.5.0.0--730ee8817e436867\n</code></pre>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#023-run-the-variant-calling-command","title":"0.2.3. Run the variant calling command","text":"<p>The GATK documentation gives us the command line to run to perform variant calling on a BAM file.</p> <p>We need to provide the BAM input file (<code>-I</code>) as well as the reference genome (<code>-R</code>), a name for the output file (<code>-O</code>) and a list of genomic intervals to analyze (<code>-L</code>).</p> <p>However, we don't need to specify the path to the index file; the tool will automatically look for it in the same directory, based on the established naming and co-location convention. The same applies to the reference genome's accessory files (index and sequence dictionary files, <code>*.fai</code> and <code>*.dict</code>).</p> <pre><code>gatk HaplotypeCaller \\\n        -R /data/ref/ref.fasta \\\n        -I /data/bam/reads_mother.bam \\\n        -O reads_mother.vcf \\\n        -L /data/ref/intervals.bed\n</code></pre> <p>The output file <code>reads_mother.vcf</code> is created inside your working directory in the container, so you won't see it in the VS Code file explorer unless you change the output file path. However, it's a small test file, so you can <code>cat</code> it to open it and view the contents. If you scroll all the way up to the start of the file, you'll find a header composed of many lines of metadata, followed by a list of variant calls, one per line.</p> reads_mother.vcf<pre><code>#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\treads_mother\n20_10037292_10066351\t3480\t.\tC\tCT\t503.03\t.\tAC=2;AF=1.00;AN=2;DP=23;ExcessHet=0.0000;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=27.95;SOR=1.179\tGT:AD:DP:GQ:PL\t1/1:0,18:18:54:517,54,0\n20_10037292_10066351\t3520\t.\tAT\tA\t609.03\t.\tAC=2;AF=1.00;AN=2;DP=18;ExcessHet=0.0000;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=33.83;SOR=0.693\tGT:AD:DP:GQ:PL\t1/1:0,18:18:54:623,54,0\n20_10037292_10066351\t3529\t.\tT\tA\t155.64\t.\tAC=1;AF=0.500;AN=2;BaseQRankSum=-0.544;DP=21;ExcessHet=0.0000;FS=1.871;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=7.78;ReadPosRankSum=-1.158;SOR=1.034\tGT:AD:DP:GQ:PL\t0/1:12,8:20:99:163,0,328\n</code></pre> <p>Each line describes a possible variant identified in the sample's sequencing data. For guidance on interpreting VCF format, see this helpful article.</p> <p>The output VCF file is accompanied by an index file called <code>reads_mother.vcf.idx</code> that was automatically created by GATK. It has the same function as the BAM index file, to allow tools to seek and retrieve subsets of data without loading in the entire file.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#024-exit-the-gatk-container","title":"0.2.4. Exit the GATK container","text":"<pre><code>exit\n</code></pre>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#takeaway","title":"Takeaway","text":"<p>You know how to test the Samtools indexing and GATK variant calling commands in their respective containers.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#whats-next","title":"What's next?","text":"<p>Learn how to wrap those same commands into a two-step workflow that uses containers to execute the work.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#1-write-a-single-stage-workflow-that-runs-samtools-index-on-a-bam-file","title":"1. Write a single-stage workflow that runs Samtools index on a BAM file","text":"<p>We provide you with a workflow file, <code>genomics-1.nf</code>, that outlines the main parts of the workflow. It's not functional; its purpose is just to serve as a skeleton that you'll use to write the actual workflow.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#11-define-the-indexing-process","title":"1.1. Define the indexing process","text":"<p>Let's start by writing a process, which we'll call <code>SAMTOOLS_INDEX</code>, describing the indexing operation.</p> genomics-1.nf<pre><code>/*\n * Generate BAM index file\n */\nprocess SAMTOOLS_INDEX {\n\n    container 'community.wave.seqera.io/library/samtools:1.20--b5dfbd93de237464'\n\n    publishDir params.outdir, mode: 'symlink'\n\n    input:\n        path input_bam\n\n    output:\n        path \"${input_bam}.bai\"\n\n    script:\n    \"\"\"\n    samtools index '$input_bam'\n    \"\"\"\n}\n</code></pre> <p>You should recognize all the pieces from what you learned in Part 1 &amp; Part 2 of this training series; the only notable change is that this time we're using <code>mode: symlink</code> for the <code>publishDir</code> directive, and we're using a parameter to define the <code>publishDir</code>.</p> <p>Note</p> <p>Even though the data files we're using here are very small, in genomics they can get very large. For the purposes of demonstration in the teaching environment, we're using the 'symlink' publishing mode to avoid unnecessary file copies. You shouldn't do this in your final workflows, since you'll lose results when you clean up your <code>work</code> directory.</p> <p>This process is going to require us to pass in a file path via the <code>input_bam</code> input, so let's set that up next.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#12-add-an-input-and-output-parameter-declaration","title":"1.2. Add an input and output parameter declaration","text":"<p>At the top of the file, under the <code>Pipeline parameters</code> section, we declare a CLI parameter called <code>reads_bam</code> and give it a default value. That way, we can be lazy and not specify the input when we type the command to launch the pipeline (for development purposes). We're also going to set <code>params.outdir</code> with a default value for the output directory.</p> genomics-1.nf<pre><code>/*\n * Pipeline parameters\n */\n\n// Primary input\nparams.reads_bam = \"${projectDir}/data/bam/reads_mother.bam\"\nparams.outdir    = \"results_genomics\"\n</code></pre> <p>Now we have a process ready, as well as a parameter to give it an input to run on, so let's wire those things up together.</p> <p>Note</p> <p><code>${projectDir}</code> is a built-in Nextflow variable that points to the directory where the current Nextflow workflow script (<code>genomics-1.nf</code>) is located.</p> <p>This makes it easy to reference files, data directories, and other resources included in the workflow repository without hardcoding absolute paths.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#13-add-workflow-block-to-run-samtools_index","title":"1.3. Add workflow block to run SAMTOOLS_INDEX","text":"<p>In the <code>workflow</code> block, we need to set up a channel to feed the input to the <code>SAMTOOLS_INDEX</code> process; then we can call the process itself to run on the contents of that channel.</p> genomics-1.nf<pre><code>workflow {\n\n    // Create input channel (single file via CLI parameter)\n    reads_ch = Channel.fromPath(params.reads_bam)\n\n    // Create index file for input BAM file\n    SAMTOOLS_INDEX(reads_ch)\n}\n</code></pre> <p>You'll notice we're using the same <code>.fromPath</code> channel factory as we used in Hello Channels. Indeed, we're doing something very similar. The difference is that we're telling Nextflow to just load the file path itself into the channel as an input element, rather than reading in its contents.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#14-run-the-workflow-to-verify-that-the-indexing-step-works","title":"1.4. Run the workflow to verify that the indexing step works","text":"<p>Let's run the workflow! As a reminder, we don't need to specify an input in the command line because we set up a default value for the input when we declared the input parameter.</p> <pre><code>nextflow run genomics-1.nf\n</code></pre> <p>The command should produce something like this:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\n \u2503 Launching `genomics-1.nf` [reverent_sinoussi] DSL2 - revision: 41d43ad7fe\n\nexecutor &gt;  local (1)\n[2a/e69536] SAMTOOLS_INDEX (1) | 1 of 1 \u2714\n</code></pre> <p>You can check that the index file has been generated correctly by looking in the work directory or in the directory set up with <code>publishDir</code>.</p> Directory contents<pre><code>work/2a/e695367b2f60df09cf826b07192dc3\n\u251c\u2500\u2500 reads_mother.bam -&gt; /workspaces/training/nf4-science/genomics/data/bam/reads_mother.bam\n\u2514\u2500\u2500 reads_mother.bam.bai\n</code></pre> Directory contents<pre><code>results_genomics/\n\u2514\u2500\u2500 reads_mother.bam.bai\n</code></pre> <p>There it is!</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#takeaway_1","title":"Takeaway","text":"<p>You know how to wrap a genomics tool in a single-step Nextflow workflow and have it run using a container.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#whats-next_1","title":"What's next?","text":"<p>Add a second step that consumes the output of the first.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#2-add-a-second-process-to-run-gatk-haplotypecaller-on-the-indexed-bam-file","title":"2. Add a second process to run GATK HaplotypeCaller on the indexed BAM file","text":"<p>Now that we have an index for our input file, we can move on to setting up the variant calling step, which is the interesting part of the workflow.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#21-define-the-variant-calling-process","title":"2.1. Define the variant calling process","text":"<p>Let's write a process, which we'll call <code>GATK_HAPLOTYPECALLER</code>, describing the variant calling operation.</p> genomics-1.nf<pre><code>/*\n * Call variants with GATK HaplotypeCaller\n */\nprocess GATK_HAPLOTYPECALLER {\n\n    container \"community.wave.seqera.io/library/gatk4:4.5.0.0--730ee8817e436867\"\n\n    publishDir params.outdir, mode: 'symlink'\n\n    input:\n        path input_bam\n        path input_bam_index\n        path ref_fasta\n        path ref_index\n        path ref_dict\n        path interval_list\n\n    output:\n        path \"${input_bam}.vcf\"     , emit: vcf\n        path \"${input_bam}.vcf.idx\" , emit: idx\n\n    script:\n    \"\"\"\n    gatk HaplotypeCaller \\\n        -R ${ref_fasta} \\\n        -I ${input_bam} \\\n        -O ${input_bam}.vcf \\\n        -L ${interval_list}\n    \"\"\"\n}\n</code></pre> <p>You'll notice that we've introduced some new syntax here (<code>emit:</code>) to uniquely name each of our output channels, and the reasons for this will become clear soon.</p> <p>This command takes quite a few more inputs, because GATK needs more information to perform the analysis compared to a simple indexing job. But you'll note that there are even more inputs defined in the inputs block than are listed in the GATK command. Why is that?</p> <p>Note</p> <p>The GATK knows to look for the BAM index file and the reference genome's accessory files because it is aware of the conventions surrounding those files. However, Nextflow is designed to be domain-agnostic and doesn't know anything about bioinformatics file format requirements.</p> <p>We need to tell Nextflow explicitly that it has to stage those files in the working directory at runtime; otherwise it won't do it, and GATK will (correctly) throw an error about the index files being missing.</p> <p>Similarly, we have to list the output VCF's index file (the <code>\"${input_bam}.vcf.idx\"</code> file) explicitly so that Nextflow will know to keep track of that file in case it's needed in subsequent steps.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#22-add-definitions-for-accessory-inputs","title":"2.2. Add definitions for accessory inputs","text":"<p>Since our new process expects a handful of additional files to be provided, we set up some CLI parameters for them under the <code>Pipeline parameters</code> section, along with some default values (same reasons as before).</p> genomics-1.nf<pre><code>// Accessory files\nparams.reference        = \"${projectDir}/data/ref/ref.fasta\"\nparams.reference_index  = \"${projectDir}/data/ref/ref.fasta.fai\"\nparams.reference_dict   = \"${projectDir}/data/ref/ref.dict\"\nparams.intervals        = \"${projectDir}/data/ref/intervals.bed\"\n</code></pre>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#23-create-variables-to-hold-the-accessory-file-paths","title":"2.3. Create variables to hold the accessory file paths","text":"<p>While main data inputs are streamed dynamically through channels, there are two approaches for handling accessory files. The recommended approach is to create explicit channels, which makes data flow clearer and more consistent. Alternatively, the file() function to create variables can be used for simpler cases, particularly when you need to reference the same file in multiple processes - though be aware this still creates channels implicitly.</p> <p>Add this to the workflow block (after the <code>reads_ch</code> creation):</p> genomics-1.nf<pre><code>// Load the file paths for the accessory files (reference and intervals)\nref_file        = file(params.reference)\nref_index_file  = file(params.reference_index)\nref_dict_file   = file(params.reference_dict)\nintervals_file  = file(params.intervals)\n</code></pre> <p>This will make the accessory file paths available for providing as input to any processes that need them.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#24-add-a-call-to-the-workflow-block-to-run-gatk_haplotypecaller","title":"2.4. Add a call to the workflow block to run GATK_HAPLOTYPECALLER","text":"<p>Now that we've got our second process set up and all the inputs and accessory files are ready and available, we can add a call to the <code>GATK_HAPLOTYPECALLER</code> process in the workflow body.</p> genomics-1.nf<pre><code>// Call variants from the indexed BAM file\nGATK_HAPLOTYPECALLER(\n    reads_ch,\n    SAMTOOLS_INDEX.out,\n    ref_file,\n    ref_index_file,\n    ref_dict_file,\n    intervals_file\n)\n</code></pre> <p>You should recognize the <code>*.out</code> syntax from Part 1 of this training series; we are telling Nextflow to take the channel output by <code>SAMTOOLS_INDEX</code> and plugging that into the <code>GATK_HAPLOTYPECALLER</code> process call.</p> <p>Note</p> <p>You'll notice that the inputs are provided in the exact same order in the call to the process as they are listed in the input block of the process. In Nextflow, inputs are positional, meaning you must follow the same order; and of course there have to be the same number of elements.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#25-run-the-workflow-to-verify-that-the-variant-calling-step-works","title":"2.5. Run the workflow to verify that the variant calling step works","text":"<p>Let's run the expanded workflow with <code>-resume</code> so that we don't have to run the indexing step again.</p> <pre><code>nextflow run genomics-1.nf -resume\n</code></pre> <p>Now if we look at the console output, we see the two processes listed:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\n \u2503 Launching `genomics-1.nf` [grave_volta] DSL2 - revision: 4790abc96a\n\nexecutor &gt;  local (1)\n[2a/e69536] SAMTOOLS_INDEX (1)       | 1 of 1, cached: 1 \u2714\n[53/e18e98] GATK_HAPLOTYPECALLER (1) | 1 of 1 \u2714\n</code></pre> <p>The first process was skipped thanks to the caching, as expected, whereas the second process was run since it's brand new.</p> <p>You'll find the output file <code>reads_mother.bam.vcf</code> in the results directory, as well its index file (<code>*.vcf.idx</code>). Both are symbolic links to the original files in the work directory where the process call was executed.</p> Directory contents<pre><code>results_genomics/\n\u251c\u2500\u2500 reads_mother.bam.bai\n\u251c\u2500\u2500 reads_mother.bam.vcf -&gt; /workspaces/training/nf4-science/genomics/work/53/e18e987d56c47f59b7dd268649ec01/reads_mother.bam.vcf\n\u2514\u2500\u2500 reads_mother.bam.vcf.idx -&gt; /workspaces/training/nf4-science/genomics/work/53/e18e987d56c47f59b7dd268649ec01/reads_mother.bam.vcf.idx\n</code></pre> <p>If you open the VCF file, you should see the same contents as in the file you generated by running the GATK command directly in the container.</p> reads_mother.bam.vcf<pre><code>#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\treads_mother\n20_10037292_10066351\t3480\t.\tC\tCT\t503.03\t.\tAC=2;AF=1.00;AN=2;DP=23;ExcessHet=0.0000;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=27.95;SOR=1.179\tGT:AD:DP:GQ:PL\t1/1:0,18:18:54:517,54,0\n20_10037292_10066351\t3520\t.\tAT\tA\t609.03\t.\tAC=2;AF=1.00;AN=2;DP=18;ExcessHet=0.0000;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=33.83;SOR=0.693\tGT:AD:DP:GQ:PL\t1/1:0,18:18:54:623,54,0\n20_10037292_10066351\t3529\t.\tT\tA\t155.64\t.\tAC=1;AF=0.500;AN=2;BaseQRankSum=-0.544;DP=21;ExcessHet=0.0000;FS=1.871;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=7.78;ReadPosRankSum=-1.158;SOR=1.034\tGT:AD:DP:GQ:PL\t0/1:12,8:20:99:163,0,328\n</code></pre> <p>This is the output we care about generating for each sample in our study.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#takeaway_2","title":"Takeaway","text":"<p>You know how to make a very basic two-step workflow that does real analysis work and is capable of dealing with genomics file format idiosyncrasies like the accessory files.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#whats-next_2","title":"What's next?","text":"<p>Make the workflow handle multiple samples in bulk.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#3-adapt-the-workflow-to-run-on-a-batch-of-samples","title":"3. Adapt the workflow to run on a batch of samples","text":"<p>It's all well and good to have a workflow that can automate processing on a single sample, but what if you have 1000 samples? Do you need to write a bash script that loops through all your samples?</p> <p>No, thank goodness! Just make a minor tweak to the code and Nextflow will handle that for you too.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#31-turn-the-input-parameter-declaration-into-an-array-listing-the-three-samples","title":"3.1. Turn the input parameter declaration into an array listing the three samples","text":"<p>Let's turn that default file path in the input BAM file declaration into an array listing file paths for our three test samples, up under the <code>Pipeline parameters</code> section.</p> <p>Before:</p> genomics-1.nf<pre><code>// Primary input\nparams.reads_bam = \"${projectDir}/data/bam/reads_mother.bam\"\n</code></pre> <p>After:</p> genomics-1.nf<pre><code>// Primary input (array of three samples)\nparams.reads_bam = [\n    \"${projectDir}/data/bam/reads_mother.bam\",\n    \"${projectDir}/data/bam/reads_father.bam\",\n    \"${projectDir}/data/bam/reads_son.bam\"\n]\n</code></pre> <p>And that's actually all we need to do, because the channel factory we use in the workflow body (<code>.fromPath</code>) is just as happy to accept multiple file paths to load into the input channel as it was to load a single one.</p> <p>Note</p> <p>Normally, you wouldn't want to hardcode the list of samples into your workflow file, but we're doing that here to keep things simple. We'll present more elegant ways for handling inputs later in this training series.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#32-run-the-workflow-to-verify-that-it-runs-on-all-three-samples","title":"3.2. Run the workflow to verify that it runs on all three samples","text":"<p>Let's try running the workflow now that the plumbing is set up to run on all three test samples.</p> <pre><code>nextflow run genomics-1.nf -resume\n</code></pre> <p>Funny thing: this might work, OR it might fail. If your workflow run succeeded, run it again until you get an error like this:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\n \u2503 Launching `genomics-1.nf` [loving_pasteur] DSL2 - revision: d2a8e63076\n\nexecutor &gt;  local (4)\n[01/eea165] SAMTOOLS_INDEX (2)       | 3 of 3, cached: 1 \u2714\n[a5/fa9fd0] GATK_HAPLOTYPECALLER (3) | 1 of 3, cached: 1\nERROR ~ Error executing process &gt; 'GATK_HAPLOTYPECALLER (2)'\n\nCaused by:\n  Process `GATK_HAPLOTYPECALLER (2)` terminated with an error exit status (2)\n\nCommand executed:\n\n  gatk HaplotypeCaller         -R ref.fasta         -I reads_father.bam         -O reads_father.bam.vcf         -L intervals.bed         -ERC GVCF\n\nCommand exit status:\n  2\n\nCommand error:\n</code></pre> <p>Further down, buried in the GATK command error output, there will be a line like this:</p> Output<pre><code>A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.\n</code></pre> <p>Well, that's weird, considering we explicitly indexed the BAM files in the first step of the workflow. Could there be something wrong with the plumbing?</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#321-check-the-work-directories-for-the-relevant-calls","title":"3.2.1. Check the work directories for the relevant calls","text":"<p>Let's take a look inside the work directory for the failed <code>GATK_HAPLOTYPECALLER</code> process call listed in the console output.</p> Directory contents<pre><code>work/a5/fa9fd0994b6beede5fb9ea073596c2\n\u251c\u2500\u2500 intervals.bed -&gt; /workspaces/training/nf4-science/genomics/data/ref/intervals.bed\n\u251c\u2500\u2500 reads_father.bam.bai -&gt; /workspaces/training/nf4-science/genomics/work/01/eea16597bd6e810fb4cf89e60f8c2d/reads_father.bam.bai\n\u251c\u2500\u2500 reads_son.bam -&gt; /workspaces/training/nf4-science/genomics/data/bam/reads_son.bam\n\u251c\u2500\u2500 reads_son.bam.vcf\n\u251c\u2500\u2500 reads_son.bam.vcf.idx\n\u251c\u2500\u2500 ref.dict -&gt; /workspaces/training/nf4-science/genomics/data/ref/ref.dict\n\u251c\u2500\u2500 ref.fasta -&gt; /workspaces/training/nf4-science/genomics/data/ref/ref.fasta\n\u2514\u2500\u2500 ref.fasta.fai -&gt; /workspaces/training/nf4-science/genomics/data/ref/ref.fasta.fai\n</code></pre> <p>Pay particular attention to the names of the BAM file and the BAM index that are listed in this directory: <code>reads_son.bam</code> and <code>reads_father.bam.bai</code>.</p> <p>What the heck? Nextflow has staged an index file in this process call's work directory, but it's the wrong one. How could this have happened?</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#321-use-the-view-operator-to-inspect-channel-contents","title":"3.2.1. Use the view() operator to inspect channel contents","text":"<p>Add these two lines in the workflow body before the <code>GATK_HAPLOTYPER</code> process call:</p> genomics-1.nf<pre><code>    // temporary diagnostics\n    reads_ch.view()\n    SAMTOOLS_INDEX.out.view()\n</code></pre> <p>Then run the workflow command again.</p> <pre><code>nextflow run genomics-1.nf\n</code></pre> <p>You may need to run it several times for it to fail again. This error will not reproduce consistently because it is dependent on some variability in the execution times of the individual process calls.</p> <p>This is what the output of the two <code>.view()</code> calls we added looks like for a failed run:</p> Output<pre><code>/workspaces/training/nf4-science/genomics/data/bam/reads_mother.bam\n/workspaces/training/nf4-science/genomics/data/bam/reads_father.bam\n/workspaces/training/nf4-science/genomics/data/bam/reads_son.bam\n/workspaces/training/nf4-science/genomics/work/9c/53492e3518447b75363e1cd951be4b/reads_father.bam.bai\n/workspaces/training/nf4-science/genomics/work/cc/37894fffdf6cc84c3b0b47f9b536b7/reads_son.bam.bai\n/workspaces/training/nf4-science/genomics/work/4d/dff681a3d137ba7d9866e3d9307bd0/reads_mother.bam.bai\n</code></pre> <p>The first three lines correspond to the input channel and the second, to the output channel. You can see that the BAM files and index files for the three samples are not listed in the same order!</p> <p>Note</p> <p>When you call a Nextflow process on a channel containing multiple elements, Nextflow will try to parallelize execution as much as possible, and will collect outputs in whatever order they become available. The consequence is that the corresponding outputs may be collected in a different order than the original inputs were fed in.</p> <p>As currently written, our workflow script assumes that the index files will come out of the indexing step listed in the same mother/father/son order as the inputs were given. But that is not guaranteed to be the case, which is why sometimes (though not always) the wrong files get paired up in the second step.</p> <p>To fix this, we need to make sure the BAM files and their index files travel together through the channels.</p> <p>Tip</p> <p>The <code>view()</code> statements in the workflow code don't do anything, so it's not a problem to leave them in. However they will clutter up your console output, so we recommend removing them when you're done troubleshooting the issue.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#33-change-the-output-of-the-samtools_index-process-into-a-tuple-that-keeps-the-input-file-and-its-index-together","title":"3.3. Change the output of the SAMTOOLS_INDEX process into a tuple that keeps the input file and its index together","text":"<p>The simplest way to ensure a BAM file and its index stay closely associated is to package them together into a tuple coming out of the index task.</p> <p>Note</p> <p>A tuple is a finite, ordered list of elements that is commonly used for returning multiple values from a function. Tuples are particularly useful for passing multiple inputs or outputs between processes while preserving their association and order.</p> <p>First, let's change the output of the <code>SAMTOOLS_INDEX</code> process to include the BAM file in its output declaration.</p> <p>Before:</p> genomics-1.nf<pre><code>output:\n    path \"${input_bam}.bai\"\n</code></pre> <p>After:</p> genomics-1.nf<pre><code>output:\n    tuple path(input_bam), path(\"${input_bam}.bai\")\n</code></pre> <p>This way, each index file will be tightly coupled with its original BAM file, and the overall output of the indexing step will be a single channel containing pairs of files.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#34-change-the-input-to-the-gatk_haplotypecaller-process-to-be-a-tuple","title":"3.4. Change the input to the GATK_HAPLOTYPECALLER process to be a tuple","text":"<p>Since we've changed the 'shape' of the output of the first process in the workflow, we need to update the input definition of the second process to match.</p> <p>Specifically, where we previously declared two separate input paths in the input block of the <code>GATK_HAPLOTYPECALLER</code> process, we now declare a single input matching the structure of the tuple emitted by <code>SAMTOOLS_INDEX</code>.</p> <p>Before:</p> genomics-1.nf<pre><code>input:\n    path input_bam\n    path input_bam_index\n</code></pre> <p>After:</p> genomics-1.nf<pre><code>input:\n    tuple path(input_bam), path(input_bam_index)\n</code></pre> <p>Of course, since we've now changed the shape of the inputs that <code>GATK_HAPLOTYPECALLER</code> expects, we need to update the process call accordingly in the workflow body.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#35-update-the-call-to-gatk_haplotypecaller-in-the-workflow-block","title":"3.5. Update the call to GATK_HAPLOTYPECALLER in the workflow block","text":"<p>We no longer need to provide the original <code>reads_ch</code> to the <code>GATK_HAPLOTYPECALLER</code> process, since the BAM file is now bundled (in the form of a symlink) into the channel output by <code>SAMTOOLS_INDEX</code>.</p> <p>As a result, we can simply delete that line.</p> <p>Before:</p> genomics-1.nf<pre><code>GATK_HAPLOTYPECALLER(\n    reads_ch,\n    SAMTOOLS_INDEX.out,\n</code></pre> <p>After:</p> genomics-1.nf<pre><code>GATK_HAPLOTYPECALLER(\n    SAMTOOLS_INDEX.out,\n</code></pre> <p>That is all the re-wiring that is necessary to solve the index mismatch problem.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#36-run-the-workflow-to-verify-it-works-correctly-on-all-three-samples-every-time","title":"3.6. Run the workflow to verify it works correctly on all three samples every time","text":"<p>Of course, the proof is in the pudding, so let's run the workflow again a few times to make sure this will work reliably going forward.</p> <pre><code>nextflow run genomics-1.nf\n</code></pre> <p>This time (and every time) everything should run correctly:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\n \u2503 Launching `genomics-1.nf` [special_goldstine] DSL2 - revision: 4cbbf6ea3e\n\nexecutor &gt;  local (6)\n[d6/10c2c4] SAMTOOLS_INDEX (1)       | 3 of 3 \u2714\n[88/1783aa] GATK_HAPLOTYPECALLER (2) | 3 of 3 \u2714\n</code></pre> <p>If you'd like, you can use <code>.view()</code> again to peek at what the contents of the <code>SAMTOOLS_INDEX</code> output channel looks like:</p> genomics-1.nf<pre><code>SAMTOOLS_INDEX.out.view()\n</code></pre> <p>You'll see the channel contains the three expected tuples (file paths truncated for readability).</p> Output<pre><code>[.../4c/e16099*/reads_son.bam, .../4c/e16099*/reads_son.bam.bai]\n[.../42/e70b8b*/reads_father.bam, .../42/e70b8b*/reads_father.bam.bai]\n[.../18/23b4bb*/reads_mother.bam, .../18/23b4bb*/reads_mother.bam.bai]\n</code></pre> <p>That will be much safer, going forward.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#takeaway_3","title":"Takeaway","text":"<p>You know how to make your workflow run on multiple samples (independently).</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#whats-next_3","title":"What's next?","text":"<p>Make it easier to handle samples in bulk.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#4-make-the-workflow-accept-a-text-file-containing-a-batch-of-input-files","title":"4. Make the workflow accept a text file containing a batch of input files","text":"<p>A very common way to provide multiple data input files to a workflow is to do it with a text file containing the file paths. It can be as simple as a text file listing one file path per line and nothing else, or the file can contain additional metadata, in which case it's often called a samplesheet.</p> <p>Here we are going to show you how to do the simple case.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#41-examine-the-provided-text-file-listing-the-input-file-paths","title":"4.1. Examine the provided text file listing the input file paths","text":"<p>We already made a text file listing the input file paths, called <code>sample_bams.txt</code>, which you can find in the <code>data/</code> directory.</p> sample_bams.txt<pre><code>/workspaces/training/nf4-science/genomics/data/bam/reads_mother.bam\n/workspaces/training/nf4-science/genomics/data/bam/reads_father.bam\n/workspaces/training/nf4-science/genomics/data/bam/reads_son.bam\n</code></pre> <p>As you can see, we listed one file path per line, and they are absolute paths.</p> <p>Note</p> <p>The files we are using here are just on your GitHub Codespaces's local filesystem, but we could also point to files in cloud storage.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#42-update-the-parameter-default","title":"4.2. Update the parameter default","text":"<p>Let's switch the default value for our <code>reads_bam</code> input parameter to point to the <code>sample_bams.txt</code> file.</p> <p>Before:</p> genomics-1.nf<pre><code>// Primary input\nparams.reads_bam = [\n    \"${projectDir}/data/bam/reads_mother.bam\",\n    \"${projectDir}/data/bam/reads_father.bam\",\n    \"${projectDir}/data/bam/reads_son.bam\"\n]\n</code></pre> <p>After:</p> genomics-1.nf<pre><code>// Primary input (file of input files, one per line)\nparams.reads_bam = \"${projectDir}/data/sample_bams.txt\"\n</code></pre> <p>This way we can continue to be lazy, but the list of files no longer lives in the workflow code itself, which is a big step in the right direction.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#43-update-the-channel-factory-to-read-lines-from-a-file","title":"4.3. Update the channel factory to read lines from a file","text":"<p>Currently, our input channel factory treats any files we give it as the data inputs we want to feed to the indexing process. Since we're now giving it a file that lists input file paths, we need to change its behavior to parse the file and treat the file paths it contains as the data inputs.</p> <p>Fortunately we can do that very simply, just by adding the <code>.splitText()</code> operator to the channel construction step.</p> <p>Before:</p> genomics-1.nf<pre><code>// Create input channel (single file via CLI parameter)\nreads_ch = Channel.fromPath(params.reads_bam)\n</code></pre> <p>After:</p> genomics-1.nf<pre><code>// Create input channel from a text file listing input file paths\nreads_ch = Channel.fromPath(params.reads_bam).splitText()\n</code></pre> <p>Tip</p> <p>This is another great opportunity to use the <code>.view()</code> operator to look at what the channel contents look like before and after applying an operator.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#44-run-the-workflow-to-verify-that-it-works-correctly","title":"4.4. Run the workflow to verify that it works correctly","text":"<p>Let's run the workflow one more time.</p> <pre><code>nextflow run genomics-1.nf -resume\n</code></pre> <p>This should produce the same result as before, right?</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\n \u2503 Launching `genomics-1.nf` [sick_albattani] DSL2 - revision: 46d84642f6\n\n[18/23b4bb] SAMTOOLS_INDEX (1)       | 3 of 3, cached: 3 \u2714\n[12/f727bb] GATK_HAPLOTYPECALLER (3) | 3 of 3, cached: 3 \u2714\n</code></pre> <p>Yes! In fact, Nextflow correctly detects that the process calls are exactly the same, and doesn't even bother re-running everything, since we were running with <code>-resume</code>.</p> <p>And that's it! Our simple variant calling workflow has all the basic features we wanted.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#takeaway_4","title":"Takeaway","text":"<p>You know how to make a multi-step linear workflow to index a BAM file and apply per-sample variant calling using GATK.</p> <p>More generally, you've learned how to use essential Nextflow components and logic to build a simple genomics pipeline that does real work, taking into account the idiosyncrasies of genomics file formats and tool requirements.</p>"},{"location":"nf4_science/genomics/01_per_sample_variant_calling/#whats-next_4","title":"What's next?","text":"<p>Celebrate your success and take an extra long break!</p> <p>In the next part of this course, you'll learn how to use a few additional Nextflow features (including more channel operators) to apply joint variant calling to the data.</p>"},{"location":"nf4_science/genomics/02_joint_calling/","title":"Part 2: Joint calling on a cohort","text":"<p>In the first part of this course, you built a variant calling pipeline that was completely linear and processed each sample's data independently of the others. However, in a real genomics use case, you'll typically need to look at the variant calls of multiple samples together.</p> <p>In this second part, we show you how to use channels and channel operators to implement joint variant calling with GATK, building on the pipeline from Part 1.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#method-overview","title":"Method overview","text":"<p>The GATK variant calling method we used in first part of this course simply generated variant calls per sample. That's fine if you only want to look at the variants from each sample in isolation, but that yields limited information. It's often more interesting to look at how variant calls differ across multiple samples, and to do so, GATK offers an alternative method called joint variant calling, which we demonstrate here.</p> <p>Joint variant calling involves generating a special kind of variant output called GVCF (for Genomic VCF) for each sample, then combining the GVCF data from all the samples and finally, running a 'joint genotyping' statistical analysis.</p> <p></p> <p>What's special about a sample's GVCF is that it contains records summarizing sequence data statistics about all positions in the targeted area of the genome, not just the positions where the program found evidence of variation. This is critical for the joint genotyping calculation (further reading).</p> <p>The GVCF is produced by GATK HaplotypeCaller, the same tool we used in Part 1, with an additional parameter (<code>-ERC GVCF</code>). Combining the GVCFs is done with GATK GenomicsDBImport, which combines the per-sample calls into a data store (analogous to a database), then the actual 'joint genotyping' analysis is done with GATK GenotypeGVCFs.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#workflow","title":"Workflow","text":"<p>So to recap, in this part of the course, we're going to develop a workflow that does the following:</p> BAMSamtools indexBAM indexIntervalsReference+ index &amp; dictGATK HaplotypeCallerGVCF + indexx multiple samplesGVCF + indexGVCF + indexGVCF + indexGenomicsDBvariant storeGATK GenomicsDBImportGATK GenotypeGVCFsJoint-calledVCFGVCF mode <ol> <li>Generate an index file for each BAM input file using Samtools</li> <li>Run the GATK HaplotypeCaller on each BAM input file to generate a GVCF of per-sample genomic variant calls</li> <li>Collect all the GVCFs and combine them into a GenomicsDB data store</li> <li>Run joint genotyping on the combined GVCF data store to produce a cohort-level VCF</li> </ol> <p>We'll apply this to the same dataset as in Part 1.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#0-warmup-run-samtools-and-gatk-directly","title":"0. Warmup: Run Samtools and GATK directly","text":"<p>Just like previously, we want to try out the commands manually before we attempt to wrap them in a workflow.</p> <p>Note</p> <p>Make sure you're in the correct working directory:  <code>cd /workspaces/training/nf4-science/genomics</code></p>"},{"location":"nf4_science/genomics/02_joint_calling/#01-index-a-bam-input-file-with-samtools","title":"0.1. Index a BAM input file with Samtools","text":"<p>This first step is the same as in Part 1, so it should feel very familiar, but this time we need to do it for all three samples.</p> <p>Note</p> <p>We've technically already generated index files for the three samples through our pipeline, so we could go fish those out of the results directory. However, it's cleaner to just redo this manually, and it'll only take a minute.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#011-spin-up-the-samtools-container-interactively","title":"0.1.1. Spin up the Samtools container interactively","text":"<pre><code>docker run -it -v ./data:/data community.wave.seqera.io/library/samtools:1.20--b5dfbd93de237464\n</code></pre>"},{"location":"nf4_science/genomics/02_joint_calling/#012-run-the-indexing-command-for-the-three-samples","title":"0.1.2. Run the indexing command for the three samples","text":"<pre><code>samtools index /data/bam/reads_mother.bam\nsamtools index /data/bam/reads_father.bam\nsamtools index /data/bam/reads_son.bam\n</code></pre> <p>Just like previously, this should produce the index files in the same directory as the corresponding BAM files.</p> Directory contents<pre><code>data/bam/\n\u251c\u2500\u2500 reads_father.bam\n\u251c\u2500\u2500 reads_father.bam.bai\n\u251c\u2500\u2500 reads_mother.bam\n\u251c\u2500\u2500 reads_mother.bam.bai\n\u251c\u2500\u2500 reads_son.bam\n\u2514\u2500\u2500 reads_son.bam.bai\n</code></pre> <p>Now that we have index files for all three samples, we can proceed to generating the GVCFs for each of them.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#013-exit-the-samtools-container","title":"0.1.3. Exit the Samtools container","text":"<pre><code>exit\n</code></pre>"},{"location":"nf4_science/genomics/02_joint_calling/#02-call-variants-with-gatk-haplotypecaller-in-gvcf-mode","title":"0.2. Call variants with GATK HaplotypeCaller in GVCF mode","text":"<p>This second step is very similar to what we did Part 1: Hello Genomics, but we are now going to run GATK in 'GVCF mode'.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#021-spin-up-the-gatk-container-interactively","title":"0.2.1. Spin up the GATK container interactively","text":"<pre><code>docker run -it -v ./data:/data community.wave.seqera.io/library/gatk4:4.5.0.0--730ee8817e436867\n</code></pre>"},{"location":"nf4_science/genomics/02_joint_calling/#022-run-the-variant-calling-command-with-the-gvcf-option","title":"0.2.2. Run the variant calling command with the GVCF option","text":"<p>In order to produce a genomic VCF (GVCF), we add the <code>-ERC GVCF</code> option to the base command, which switches on the HaplotypeCaller's GVCF mode.</p> <p>We also change the file extension for the output file from <code>.vcf</code> to <code>.g.vcf</code>. This is technically not a requirement, but it is a strongly recommended convention.</p> <pre><code>gatk HaplotypeCaller \\\n        -R /data/ref/ref.fasta \\\n        -I /data/bam/reads_mother.bam \\\n        -O reads_mother.g.vcf \\\n        -L /data/ref/intervals.bed \\\n        -ERC GVCF\n</code></pre> <p>This creates the GVCF output file <code>reads_mother.g.vcf</code> in the current working directory in the container.</p> <p>If you <code>cat</code> it to view the contents, you'll see it's much longer than the equivalent VCF we generated in Part 1. You can't even scroll up to the start of the file, and most of the lines look quite different from what we saw in the VCF in Part 1.</p> Output<pre><code>20_10037292_10066351    14714   .       T       &lt;NON_REF&gt;       .       .       END=14718       GT:DP:GQ:MIN_DP:PL       0/0:37:99:37:0,99,1192\n20_10037292_10066351    14719   .       T       &lt;NON_REF&gt;       .       .       END=14719       GT:DP:GQ:MIN_DP:PL       0/0:36:82:36:0,82,1087\n20_10037292_10066351    14720   .       T       &lt;NON_REF&gt;       .       .       END=14737       GT:DP:GQ:MIN_DP:PL       0/0:42:99:37:0,100,1160\n</code></pre> <p>These represent non-variant regions where the variant caller found no evidence of variation, so it captured some statistics describing its level of confidence in the absence of variation. This makes it possible to distinguish between two very different case figures: (1) there is good quality data showing that the sample is homozygous-reference, and (2) there is not enough good data available to make a determination either way.</p> <p>In a GVCF, there are typically lots of such non-variant lines, with a smaller number of variant records sprinkled among them. Try running <code>head -176</code> on the GVCF to load in just the first 176 lines of the file to find an actual variant call.</p> Output<pre><code>20_10037292_10066351    3479    .       T       &lt;NON_REF&gt;       .       .       END=3479        GT:DP:GQ:MIN_DP:PL       0/0:34:36:34:0,36,906\n20_10037292_10066351    3480    .       C       CT,&lt;NON_REF&gt;    503.03  .       DP=23;ExcessHet=0.0000;MLEAC=2,0;MLEAF=1.00,0.00;RAW_MQandDP=82800,23    GT:AD:DP:GQ:PL:SB       1/1:0,18,0:18:54:517,54,0,517,54,517:0,0,7,11\n20_10037292_10066351    3481    .       T       &lt;NON_REF&gt;       .       .       END=3481        GT:DP:GQ:MIN_DP:PL       0/0:21:51:21:0,51,765\n</code></pre> <p>The second line shows the first variant record in the file, which corresponds to the first variant in the VCF file we looked at in Part 1.</p> <p>Just like the original VCF was, the output GVCF file is also accompanied by an index file, called <code>reads_mother.g.vcf.idx</code>.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#023-repeat-the-process-on-the-other-two-samples","title":"0.2.3. Repeat the process on the other two samples","text":"<p>In order to test the joint genotyping step, we need GVCFs for all three samples, so let's generate those manually now.</p> <pre><code>gatk HaplotypeCaller \\\n        -R /data/ref/ref.fasta \\\n        -I /data/bam/reads_father.bam \\\n        -O reads_father.g.vcf \\\n        -L /data/ref/intervals.bed \\\n        -ERC GVCF\n</code></pre> <pre><code>gatk HaplotypeCaller \\\n        -R /data/ref/ref.fasta \\\n        -I /data/bam/reads_son.bam \\\n        -O reads_son.g.vcf \\\n        -L /data/ref/intervals.bed \\\n        -ERC GVCF\n</code></pre> <p>Once this completes, you should have three files ending in <code>.g.vcf</code> in your work directory (one per sample) and their respective index files ending in <code>.g.vcf.idx</code>.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#03-run-joint-genotyping","title":"0.3. Run joint genotyping","text":"<p>Now that we have all the GVCFs, we can finally try out the joint genotyping approach to generating variant calls for a cohort of samples. As a reminder, it's a two-step method that consists of combining the data from all the GVCFs into a data store, then running the joint genotyping analysis proper to generate the final VCF of joint-called variants.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#031-combine-all-the-per-sample-gvcfs","title":"0.3.1. Combine all the per-sample GVCFs","text":"<p>This first step uses another GATK tool, called GenomicsDBImport, to combine the data from all the GVCFs into a GenomicsDB data store.</p> <pre><code>gatk GenomicsDBImport \\\n    -V reads_mother.g.vcf \\\n    -V reads_father.g.vcf \\\n    -V reads_son.g.vcf \\\n    -L /data/ref/intervals.bed \\\n    --genomicsdb-workspace-path family_trio_gdb\n</code></pre> <p>The output of this step is effectively a directory containing a set of further nested directories holding the combined variant data in the form of multiple different files. You can poke around it but you'll quickly see this data store format is not intended to be read directly by humans.</p> <p>Note</p> <p>GATK includes tools that make it possible to inspect and extract variant call data from the data store as needed.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#032-run-the-joint-genotyping-analysis-proper","title":"0.3.2. Run the joint genotyping analysis proper","text":"<p>This second step uses yet another GATK tool, called GenotypeGVCFs, to recalculate variant statistics and individual genotypes in light of the data available across all samples in the cohort.</p> <pre><code>gatk GenotypeGVCFs \\\n    -R /data/ref/ref.fasta \\\n    -V gendb://family_trio_gdb \\\n    -O family_trio.vcf\n</code></pre> <p>This creates the VCF output file <code>family_trio.vcf</code> in the current working directory in the container. It's another reasonably small file so you can <code>cat</code> this file to view its contents, and scroll up to find the first few variant lines.</p> family_trio.vcf<pre><code>#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  reads_father    reads_mother    reads_son\n20_10037292_10066351    3480    .       C       CT      1625.89 .       AC=5;AF=0.833;AN=6;BaseQRankSum=0.220;DP=85;ExcessHet=0.0000;FS=2.476;MLEAC=5;MLEAF=0.833;MQ=60.00;MQRankSum=0.00;QD=21.68;ReadPosRankSum=-1.147e+00;SOR=0.487    GT:AD:DP:GQ:PL  0/1:15,16:31:99:367,0,375       1/1:0,18:18:54:517,54,0 1/1:0,26:26:78:756,78,0\n20_10037292_10066351    3520    .       AT      A       1678.89 .       AC=5;AF=0.833;AN=6;BaseQRankSum=1.03;DP=80;ExcessHet=0.0000;FS=2.290;MLEAC=5;MLEAF=0.833;MQ=60.00;MQRankSum=0.00;QD=22.39;ReadPosRankSum=0.701;SOR=0.730 GT:AD:DP:GQ:PL   0/1:18,13:31:99:296,0,424       1/1:0,18:18:54:623,54,0 1/1:0,26:26:78:774,78,0\n20_10037292_10066351    3529    .       T       A       154.29  .       AC=1;AF=0.167;AN=6;BaseQRankSum=-5.440e-01;DP=104;ExcessHet=0.0000;FS=1.871;MLEAC=1;MLEAF=0.167;MQ=60.00;MQRankSum=0.00;QD=7.71;ReadPosRankSum=-1.158e+00;SOR=1.034       GT:AD:DP:GQ:PL  0/0:44,0:44:99:0,112,1347       0/1:12,8:20:99:163,0,328        0/0:39,0:39:99:0,105,1194\n</code></pre> <p>This looks more like the original VCF we generated in Part 1, except this time we have genotype-level information for all three samples. The last three columns in the file are the genotype blocks for the samples, listed in alphabetical order.</p> <p>If we look at the genotypes called for our test family trio for the very first variant, we see that the father is heterozygous-variant (<code>0/1</code>), and the mother and son are both homozygous-variant (<code>1/1</code>).</p> <p>That is ultimately the information we're looking to extract from the dataset! So let's go wrap all this into a Nextflow workflow so we can do this at scale.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#033-exit-the-gatk-container","title":"0.3.3. Exit the GATK container","text":"<pre><code>exit\n</code></pre>"},{"location":"nf4_science/genomics/02_joint_calling/#takeaway","title":"Takeaway","text":"<p>You know how to run the individual commands involved in joint variant calling in the terminal to verify that they will produce the information you want.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#whats-next","title":"What's next?","text":"<p>Wrap these commands into an actual pipeline.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#1-modify-the-per-sample-variant-calling-step-to-produce-a-gvcf","title":"1. Modify the per-sample variant calling step to produce a GVCF","text":"<p>The good news is that we don't need to start all over, since we already wrote a workflow that does some of this work in Part 1. However, that pipeline produces VCF files, whereas now we want GVCF files in order to do the joint genotyping. So we need to start by switching on the GVCF variant calling mode and updating the output file extension.</p> <p>Note</p> <p>For convenience, we are going to work with a fresh copy of the GATK workflow as it stands at the end of Part 1, but under a different name: <code>genomics-2.nf</code>.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#11-tell-haplotypecaller-to-emit-a-gvcf-and-update-the-output-extension","title":"1.1. Tell HaplotypeCaller to emit a GVCF and update the output extension","text":"<p>Let's open the <code>genomics-2.nf</code> file in the code editor. It should look very familiar, but feel free to run it if you want to satisfy yourself that it runs as expected.</p> <p>We're going to start by making two changes:</p> <ul> <li>Add the <code>-ERC GVCF</code> parameter to the GATK HaplotypeCaller command;</li> <li>Update the output file path to use the corresponding <code>.g.vcf</code> extension, as per GATK convention.</li> </ul> <p>Make sure you add a backslash (<code>\\</code>) at the end of the previous line when you add <code>-ERC GVCF</code>.</p> <p>Before:</p> genomics-2.nf<pre><code>    \"\"\"\n    gatk HaplotypeCaller \\\n        -R ${ref_fasta} \\\n        -I ${input_bam} \\\n        -O ${input_bam}.vcf \\\n        -L ${interval_list}\n    \"\"\"\n</code></pre> <p>After:</p> genomics-2.nf<pre><code>    \"\"\"\n    gatk HaplotypeCaller \\\n        -R ${ref_fasta} \\\n        -I ${input_bam} \\\n        -O ${input_bam}.g.vcf \\\n        -L ${interval_list} \\\n        -ERC GVCF\n    \"\"\"\n</code></pre> <p>And that's all it takes to switch HaplotypeCaller to generating GVCFs instead of VCFs, right?</p>"},{"location":"nf4_science/genomics/02_joint_calling/#12-run-the-pipeline-to-verify-that-you-can-generate-gvcfs","title":"1.2. Run the pipeline to verify that you can generate GVCFs","text":"<p>The Nextflow execution command is the same as before, save for the workflow filename itself. Make sure to update that appropriately.</p> <pre><code>nextflow run genomics-2.nf\n</code></pre> <p>And the output is... all red! Oh no.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\n \u2503 Launching `genomics-2.nf` [nice_gates] DSL2 - revision: 43c7de9890\n\nexecutor &gt;  local (6)\n[a1/0b5d00] SAMTOOLS_INDEX (3)       | 3 of 3 \u2714\n[89/4d0c70] GATK_HAPLOTYPECALLER (3) | 0 of 3\nERROR ~ Error executing process &gt; 'GATK_HAPLOTYPECALLER (2)'\n\nCaused by:\n  Missing output file(s) `reads_mother.bam.vcf` expected by process `GATK_HAPLOTYPECALLER (2)`\n\nCommand executed:\n\n  gatk HaplotypeCaller         -R ref.fasta         -I reads_mother.bam         -O reads_mother.bam.g.vcf         -L intervals.bed         -ERC GVCF\n</code></pre> <p>The command that was executed is correct, so we were right that that was enough to change the GATK tool's behavior. But look at that line about the missing output file. Notice anything?</p> <p>That's right, we forgot to tell Nextflow to expect a new file name. Oops.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#12-update-the-output-file-extension-in-the-process-outputs-block-too","title":"1.2. Update the output file extension in the process outputs block too","text":"<p>Because it's not enough to just change the file extension in the tool command itself, you also have to tell Nextflow that the expected output filename has changed.</p> <p>Before:</p> genomics-2.nf<pre><code>    output:\n        path \"${input_bam}.vcf\"     , emit: vcf\n        path \"${input_bam}.vcf.idx\" , emit: idx\n</code></pre> <p>After:</p> genomics-2.nf<pre><code>    output:\n        path \"${input_bam}.g.vcf\"     , emit: vcf\n        path \"${input_bam}.g.vcf.idx\" , emit: idx\n</code></pre>"},{"location":"nf4_science/genomics/02_joint_calling/#13-run-the-pipeline-again","title":"1.3. Run the pipeline again","text":"<p>Let's run it with <code>-resume</code> this time.</p> <pre><code>nextflow run genomics-2.nf -resume\n</code></pre> <p>Ah, this time it works.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\n \u2503 Launching `genomics-2.nf` [elated_carlsson] DSL2 - revision: 6a5786a6fa\n\nexecutor &gt;  local (3)\n[47/f7fac1] SAMTOOLS_INDEX (2)       | 3 of 3, cached: 3 \u2714\n[ce/096ac6] GATK_HAPLOTYPECALLER (1) | 3 of 3 \u2714\n</code></pre> <p>The Nextflow output itself doesn't look any different (compared to a successful run in normal VCF mode), but now we can find the <code>.g.vcf</code> files and their respective index files, for all three samples, in the <code>results_genomics</code> directory.</p> Directory contents (symlinks truncated)<pre><code>results_genomics/\n\u251c\u2500\u2500 reads_father.bam -&gt; */47/f7fac1*/reads_father.bam\n\u251c\u2500\u2500 reads_father.bam.bai -&gt; */47/f7fac1*/reads_father.bam.bai\n\u251c\u2500\u2500 reads_father.bam.g.vcf -&gt; */cb/ad7430*/reads_father.bam.g.vcf\n\u251c\u2500\u2500 reads_father.bam.g.vcf.idx -&gt; */cb/ad7430*/reads_father.bam.g.vcf.idx\n\u251c\u2500\u2500 reads_mother.bam -&gt; */a2/56a3a8*/reads_mother.bam\n\u251c\u2500\u2500 reads_mother.bam.bai -&gt; */a2/56a3a8*/reads_mother.bam.bai\n\u251c\u2500\u2500 reads_mother.bam.g.vcf -&gt; */ce/096ac6*/reads_mother.bam.g.vcf\n\u251c\u2500\u2500 reads_mother.bam.g.vcf.idx -&gt; */ce/096ac6*/reads_mother.bam.g.vcf.idx\n\u251c\u2500\u2500 reads_son.bam -&gt; */a1/0b5d00*/reads_son.bam\n\u251c\u2500\u2500 reads_son.bam.bai -&gt; */a1/0b5d00*/reads_son.bam.bai\n\u251c\u2500\u2500 reads_son.bam.g.vcf -&gt; */c2/6b6563*/reads_son.bam.g.vcf\n\u2514\u2500\u2500 reads_son.bam.g.vcf.idx -&gt; */c2/6b6563*/reads_son.bam.g.vcf.idx\n</code></pre> <p>If you open one of the GVCF files and scroll through it, you can verify that GATK HaplotypeCaller produced GVCF files as requested.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#takeaway_1","title":"Takeaway","text":"<p>Okay, this one was minimal in terms of Nextflow learning... But it was a nice opportunity to reiterate the importance of the process output block!</p>"},{"location":"nf4_science/genomics/02_joint_calling/#whats-next_1","title":"What's next?","text":"<p>Learn to collect the contents of a channel and pass them on to the next process as a single input.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#2-collect-and-combine-the-gvcf-data-across-all-samples","title":"2. Collect and combine the GVCF data across all samples","text":"<p>We now need to combine the data from all the per-sample GVCFs into a form that supports the joint genotyping analysis we want to do.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#21-define-the-process-that-will-combine-the-gvcfs","title":"2.1. Define the process that will combine the GVCFs","text":"<p>As a reminder of what we did earlier in the warmup section, combining the GVCFs is a job for the GATK tool GenomicsDBImport, which will produce a data store in the so-called GenomicsDB format.</p> <p>Let's write a new process to define how that's going to work, based on the command we used earlier in the warmup section.</p> genomics-2.nf<pre><code>/*\n * Combine GVCFs into GenomicsDB datastore\n */\nprocess GATK_GENOMICSDB {\n\n    container \"community.wave.seqera.io/library/gatk4:4.5.0.0--730ee8817e436867\"\n    publishDir params.outdir, mode: 'symlink'\n\n    input:\n        path all_gvcfs\n        path all_idxs\n        path interval_list\n        val cohort_name\n\n    output:\n        path \"${cohort_name}_gdb\"\n\n    script:\n    \"\"\"\n    gatk GenomicsDBImport \\\n        -V ${all_gvcfs} \\\n        -L ${interval_list} \\\n        --genomicsdb-workspace-path ${cohort_name}_gdb\n    \"\"\"\n}\n</code></pre> <p>What do you think, looks reasonable?</p> <p>Let's wire it up and see what happens.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#22-add-a-cohort_name-parameter-with-a-default-value","title":"2.2. Add a <code>cohort_name</code> parameter with a default value","text":"<p>We need to provide an arbitrary name for the cohort. Later in the training series you'll learn how to use sample metadata for this sort of thing, but for now we just declare a CLI parameter using <code>params</code> and give it a default value for convenience.</p> genomics-2.nf<pre><code>// Base name for final output file\nparams.cohort_name = \"family_trio\"\n</code></pre>"},{"location":"nf4_science/genomics/02_joint_calling/#23-gather-the-outputs-of-gatk_haplotypecaller-across-samples","title":"2.3. Gather the outputs of GATK_HAPLOTYPECALLER across samples","text":"<p>If we were to just plug the output channel from the <code>GATK_HAPLOTYPECALLER</code> process as is, Nextflow would call the process on each sample GVCF separately. However, we want to bundle all three GVCFs (and their index files) in such a way that Nextflow hands all of them together to a single process call.</p> <p>Good news: we can do that using the <code>collect()</code> channel operator. Let's add the following lines to the <code>workflow</code> body, right after the call to GATK_HAPLOTYPECALLER:</p> genomics-2.nf<pre><code>// Collect variant calling outputs across samples\nall_gvcfs_ch = GATK_HAPLOTYPECALLER.out.vcf.collect()\nall_idxs_ch = GATK_HAPLOTYPECALLER.out.idx.collect()\n</code></pre> <p>Does that seem a bit complicated? Let's break this down and translate it into plain language.</p> <ol> <li>We're taking the output channel from the <code>GATK_HAPLOTYPECALLER</code> process, referred to using the <code>.out</code> property.</li> <li>Each 'element' coming out of the channel is a pair of files: the GVCF and its index file, in that order because that's the order they're listed in the process output block. Conveniently, because in the last session we named the outputs of this process (using <code>emit:</code>), we can pick out the GVCFs on one hand by adding <code>.vcf</code> and the index files on the other by adding <code>.idx</code> after the <code>.out</code> property. If we had not named those outputs, we would have had to refer to them by <code>.out[0]</code> and <code>.out[1]</code>, respectively.</li> <li>We append the <code>collect()</code> channel operator to bundle all the GVCF files together into a single element in a new channel called <code>all_gvcfs_ch</code>, and do the same with the index files to form the new channel called <code>all_idxs_ch</code>.</li> </ol> <p>Tip</p> <p>If you're having a hard time envisioning exactly what is happening here, remember that you can use the <code>view()</code> operator to inspect the contents of channels before and after applying channel operators.</p> <p>The resulting <code>all_gvcfs_ch</code> and <code>all_idxs_ch</code> channels are what we're going to plug into the <code>GATK_GENOMICSDB</code> process we just wrote.</p> <p>Note</p> <p>In case you were wondering, we collect the GVCFs and their index files separately because the GATK GenomicsDBImport command only wants to see the GVCF file paths. Fortunately, since Nextflow will stage all the files together for execution, we don't have to worry about the order of files like we did for BAMs and their index in Part 1.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#24-add-a-call-to-the-workflow-block-to-run-gatk_genomicsdb","title":"2.4. Add a call to the workflow block to run GATK_GENOMICSDB","text":"<p>We've got a process, and we've got input channels. We just need to add the process call.</p> genomics-2.nf<pre><code>// Combine GVCFs into a GenomicsDB datastore\nGATK_GENOMICSDB(\n    all_gvcfs_ch,\n    all_idxs_ch,\n    intervals_file,\n    params.cohort_name\n)\n</code></pre> <p>Ok, everything is wired up.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#25-run-the-workflow","title":"2.5. Run the workflow","text":"<p>Let's see if this works.</p> <pre><code>nextflow run genomics-2.nf -resume\n</code></pre> <p>It run fairly quickly, since we're running with <code>-resume</code>, but...</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\n \u2503 Launching `genomics-2.nf` [mad_edison] DSL2 - revision: 6aea0cfded\n\nexecutor &gt;  local (1)\n[a2/56a3a8] SAMTOOLS_INDEX (1)       | 3 of 3, cached: 3 \u2714\n[ce/096ac6] GATK_HAPLOTYPECALLER (1) | 3 of 3, cached: 3 \u2714\n[df/994a06] GATK_GENOMICSDB          | 0 of 1\nERROR ~ Error executing process &gt; 'GATK_GENOMICSDB'\n\nCaused by:\n  Process `GATK_GENOMICSDB` terminated with an error exit status (1)\n\nCommand executed:\n\n  gatk GenomicsDBImport         -V reads_father.bam.g.vcf reads_son.bam.g.vcf reads_mother.bam.g.vcf         -L intervals.bed         --genomicsdb-workspace-path family_trio_gdb\n</code></pre> <p>Ah. On the bright side, we see that Nextflow has picked up the <code>GATK_GENOMICSDB</code> process, and specifically called it just once. That suggests that the <code>collect()</code> approach worked, to a point. But, and it's a big one, the process call failed.</p> <p>When we dig into the console output above, we can see the command executed isn't correct.</p> <p>Can you spot the error? Look at this bit: <code>-V reads_father.bam.g.vcf reads_son.bam.g.vcf reads_mother.bam.g.vcf</code></p> <p>We gave <code>gatk GenomicsDBImport</code> multiple GVCF files for a single <code>-V</code> argument, but the tool expects a separate <code>-V</code> argument for each GVCF file.</p> <p>As a reminder, this was the command we ran in the container:</p> <pre><code>gatk GenomicsDBImport \\\n    -V reads_mother.g.vcf \\\n    -V reads_father.g.vcf \\\n    -V reads_son.g.vcf \\\n    -L /data/ref/intervals.bed \\\n    --genomicsdb-workspace-path family_trio_gdb\n</code></pre> <p>So that means we need to somehow transform our bundle of GVCF files into a properly formatted command string.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#26-construct-a-command-line-with-a-separate-v-argument-for-each-input-gvcf","title":"2.6. Construct a command line with a separate <code>-V</code> argument for each input GVCF","text":"<p>This is where Nextflow being based on Groovy comes in handy, because it's going to allow us to use some fairly straightforward string manipulations to construct the necessary command string.</p> <p>Specifically, using this syntax: <code>all_gvcfs.collect { gvcf -&gt; \"-V ${gvcf}\" }.join(' ')</code></p> <p>Once again, let's break it down into its components.</p> <ol> <li>First, we take the contents of the <code>all_gvcfs</code> input channel and apply <code>.collect()</code> on it (just like earlier).</li> <li>That allows us to pass each individual GVCF file path in the bundle to the closure, <code>{ gvcf -&gt; \"-V ${gvcf}\" }</code>, where <code>gvcf</code> refers to that GVCF file path.    The closure is a mini-function that we use to prepend <code>-V</code> to the file path, in the form of <code>\"-V ${gvcf}\"</code>.</li> <li>Then we use <code>.join(' ')</code> to concatenate all three strings with a single space as separator.</li> </ol> <p>With a concrete example, it looks like this:</p> <ol> <li>We have three files:</li> </ol> <p><code>[A.ext, B.ext, C.ext]</code></p> <ol> <li>The closure modifies each one to create the strings:</li> </ol> <p><code>\"-V A.ext\", \"-V B.ext\", \"-V C.ext\"</code></p> <ol> <li>The <code>.join(' ')</code> operation generates the final string:</li> </ol> <p><code>\"-V A.ext -V B.ext -V C.ext\"</code></p> <p>Once we have that string, we can assign it to a local variable, <code>gvcfs_line</code>, defined with the <code>def</code> keyword:</p> <p><code>def gvcfs_line = all_gvcfs.collect { gvcf -&gt; \"-V ${gvcf}\" }.join(' ')</code></p> <p>Ok, so we have our string manipulation thingy. Where do we put it?</p> <p>We want this to go inside the process definition somewhere, because we want to do it after we've channeled the GVCF file paths into the process. That is because Nextflow must see them as file paths in order to stage the files themselves correctly for execution.</p> <p>But where in the process can we add this?</p> <p>Fun fact: you can add arbitrary code after <code>script:</code> and before the <code>\"\"\"</code> !</p> <p>Great, let's add our string manipulation line there then, and update the <code>gatk GenomicsDBImport</code> command to use the concatenated string it produces.</p> <p>Before:</p> genomics-2.nf<pre><code>    script:\n    \"\"\"\n    gatk GenomicsDBImport \\\n        -V ${all_gvcfs} \\\n        -L ${interval_list} \\\n        --genomicsdb-workspace-path ${cohort_name}_gdb\n    \"\"\"\n</code></pre> <p>After:</p> genomics-2.nf<pre><code>    script:\n    def gvcfs_line = all_gvcfs.collect { gvcf -&gt; \"-V ${gvcf}\" }.join(' ')\n    \"\"\"\n    gatk GenomicsDBImport \\\n        ${gvcfs_line} \\\n        -L ${interval_list} \\\n        --genomicsdb-workspace-path ${cohort_name}_gdb\n    \"\"\"\n</code></pre> <p>That should be all that's needed to provide the inputs to <code>gatk GenomicsDBImport</code> correctly.</p> <p>Tip</p> <p>When you update the <code>gatk GenomicsDBImport</code> command, make sure to remove the <code>-V</code> prefix when you swap in the <code>${gvcfs_line}</code> variable.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#27-run-the-workflow-to-verify-that-it-generates-the-genomicsdb-output-as-expected","title":"2.7. Run the workflow to verify that it generates the GenomicsDB output as expected","text":"<p>Alright, let's see if that addressed the issue.</p> <pre><code>nextflow run genomics-2.nf -resume\n</code></pre> <p>Aha! It seems to be working now.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\n \u2503 Launching `genomics-2.nf` [special_noyce] DSL2 - revision: 11f7a51bbe\n\nexecutor &gt;  local (1)\n[6a/5dcf6a] SAMTOOLS_INDEX (3)       | 3 of 3, cached: 3 \u2714\n[d6/d0d060] GATK_HAPLOTYPECALLER (3) | 3 of 3, cached: 3 \u2714\n[e8/749a05] GATK_GENOMICSDB          | 1 of 1 \u2714\n</code></pre> <p>The first two steps were successfully skipped, and the third step worked like a charm this time. We find our output data store in the results directory.</p> Directory contents<pre><code>results_genomics/family_trio_gdb\n\u251c\u2500\u2500 20_10037292_10066351$12912$14737\n\u251c\u2500\u2500 20_10037292_10066351$3277$5495\n\u251c\u2500\u2500 20_10037292_10066351$7536$9859\n\u251c\u2500\u2500 callset.json\n\u251c\u2500\u2500 __tiledb_workspace.tdb\n\u251c\u2500\u2500 vcfheader.vcf\n\u2514\u2500\u2500 vidmap.json\n</code></pre> <p>By the way, we didn't have to do anything special to handle the output being a directory instead of a single file. Isn't that nice?</p>"},{"location":"nf4_science/genomics/02_joint_calling/#takeaway_2","title":"Takeaway","text":"<p>Now you know how to collect outputs from a channel and bundle them as a single input to another process. You also know how to construct a command line to provide inputs to a given tool with the appropriate syntax.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#whats-next_2","title":"What's next?","text":"<p>Learn how to add a second command to the same process.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#3-run-the-joint-genotyping-step-as-part-of-the-same-process","title":"3. Run the joint genotyping step as part of the same process","text":"<p>Now that we have the combined genomic variant calls, we can run the joint genotyping tool, which will produce the final output that we actually care about: the VCF of cohort-level variant calls.</p> <p>For logistical reasons, we decide to include the joint genotyping inside the same process.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#31-rename-the-process-from-gatk_genomicsdb-to-gatk_jointgenotyping","title":"3.1. Rename the process from GATK_GENOMICSDB to GATK_JOINTGENOTYPING","text":"<p>Since the process will be running more than one tool, we change its name to refer to the overall operation rather than a single tool name.</p> <p>Before:</p> genomics-2.nf<pre><code>/*\n * Combine GVCFs into GenomicsDB datastore\n */\nprocess GATK_GENOMICSDB {\n</code></pre> <p>After:</p> genomics-2.nf<pre><code>/*\n * Combine GVCFs into GenomicsDB datastore and run joint genotyping to produce cohort-level calls\n */\nprocess GATK_JOINTGENOTYPING {\n</code></pre> <p>Remember to keep your process names as descriptive as possible, to maximize readability for your colleagues \u2014and your future self!</p>"},{"location":"nf4_science/genomics/02_joint_calling/#32-add-the-joint-genotyping-command-to-the-gatk_jointgenotyping-process","title":"3.2. Add the joint genotyping command to the GATK_JOINTGENOTYPING process","text":"<p>Simply add the second command after the first one inside the script section.</p> <p>Before:</p> genomics-2.nf<pre><code>    \"\"\"\n    gatk GenomicsDBImport \\\n        ${gvcfs_line} \\\n        -L ${interval_list} \\\n        --genomicsdb-workspace-path ${cohort_name}_gdb\n    \"\"\"\n</code></pre> <p>After:</p> genomics-2.nf<pre><code>    \"\"\"\n    gatk GenomicsDBImport \\\n        ${gvcfs_line} \\\n        -L ${interval_list} \\\n        --genomicsdb-workspace-path ${cohort_name}_gdb\n\n    gatk GenotypeGVCFs \\\n        -R ${ref_fasta} \\\n        -V gendb://${cohort_name}_gdb \\\n        -L ${interval_list} \\\n        -O ${cohort_name}.joint.vcf\n    \"\"\"\n</code></pre> <p>The two commands will be run in serial, in the same way that they would if we were to run them manually in the terminal.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#33-add-the-reference-genome-files-to-the-gatk_jointgenotyping-process-input-definitions","title":"3.3. Add the reference genome files to the GATK_JOINTGENOTYPING process input definitions","text":"<p>The second command requires the reference genome files, so we need to add those to the process inputs.</p> <p>Before:</p> genomics-2.nf<pre><code>input:\n    path all_gvcfs\n    path all_idxs\n    path interval_list\n    val cohort_name\n</code></pre> <p>After:</p> genomics-2.nf<pre><code>input:\n    path all_gvcfs\n    path all_idxs\n    path interval_list\n    val cohort_name\n    path ref_fasta\n    path ref_index\n    path ref_dict\n</code></pre> <p>It may seem annoying to type these out, but remember, you only type them once, and then you can run the workflow a million times. Worth it?</p>"},{"location":"nf4_science/genomics/02_joint_calling/#34-update-the-process-output-definition-to-emit-the-vcf-of-cohort-level-variant-calls","title":"3.4. Update the process output definition to emit the VCF of cohort-level variant calls","text":"<p>We don't really care about saving the GenomicsDB datastore, which is just an intermediate format that only exists for logistical reasons, so we can just remove it from the output block if we want.</p> <p>The output we're actually interested in is the VCF produced by the joint genotyping command.</p> <p>Before:</p> genomics-2.nf<pre><code>output:\n    path \"${cohort_name}_gdb\"\n</code></pre> <p>After:</p> genomics-2.nf<pre><code>output:\n    path \"${cohort_name}.joint.vcf\"     , emit: vcf\n    path \"${cohort_name}.joint.vcf.idx\" , emit: idx\n</code></pre> <p>We're almost done!</p>"},{"location":"nf4_science/genomics/02_joint_calling/#35-update-the-process-call-from-gatk_genomicsdb-to-gatk_jointgenotyping","title":"3.5. Update the process call from GATK_GENOMICSDB to GATK_JOINTGENOTYPING","text":"<p>Let's not forget to rename the process call in the workflow body from GATK_GENOMICSDB to GATK_JOINTGENOTYPING. And while we're at it, we should also add the reference genome files as inputs, since we need to provide them to the joint genotyping tool.</p> <p>Before:</p> genomics-2.nf<pre><code>// Combine GVCFs into a GenomicsDB data store\nGATK_GENOMICSDB(\n    all_gvcfs_ch,\n    all_idxs_ch,\n    intervals_file,\n    params.cohort_name\n)\n</code></pre> <p>After:</p> genomics-2.nf<pre><code>// Combine GVCFs into a GenomicsDB data store and apply joint genotyping\nGATK_JOINTGENOTYPING(\n    all_gvcfs_ch,\n    all_idxs_ch,\n    intervals_file,\n    params.cohort_name,\n    ref_file,\n    ref_index_file,\n    ref_dict_file\n)\n</code></pre> <p>Now everything should be completely wired up.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#36-run-the-workflow","title":"3.6. Run the workflow","text":"<p>Finally, we can run the modified workflow...</p> <pre><code>nextflow run genomics-2.nf -resume\n</code></pre> <p>And it works!</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\n \u2503 Launching `genomics-2.nf` [modest_gilbert] DSL2 - revision: 4f49922223\n\nexecutor &gt;  local (1)\n[6a/5dcf6a] SAMTOOLS_INDEX (3)       | 3 of 3, cached: 3 \u2714\n[fe/6c9ad4] GATK_HAPLOTYPECALLER (1) | 3 of 3, cached: 3 \u2714\n[e2/a8d95f] GATK_JOINTGENOTYPING     | 1 of 1 \u2714\n</code></pre> <p>You'll find the final output file, <code>family_trio.joint.vcf</code> (and its file index), in the results directory. If you're the skeptical type, you can click on it to open it and verify that the workflow has generated the same variant calls that you obtained by running the tools manually at the start of this section.</p> family_trio.joint.vcf<pre><code>#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\treads_father\treads_mother\treads_son\n20_10037292_10066351\t3480\t.\tC\tCT\t1625.89\t.\tAC=5;AF=0.833;AN=6;BaseQRankSum=0.220;DP=85;ExcessHet=0.0000;FS=2.476;MLEAC=5;MLEAF=0.833;MQ=60.00;MQRankSum=0.00;QD=21.68;ReadPosRankSum=-1.147e+00;SOR=0.487\tGT:AD:DP:GQ:PL\t0/1:15,16:31:99:367,0,375\t1/1:0,18:18:54:517,54,0\t1/1:0,26:26:78:756,78,0\n20_10037292_10066351\t3520\t.\tAT\tA\t1678.89\t.\tAC=5;AF=0.833;AN=6;BaseQRankSum=1.03;DP=80;ExcessHet=0.0000;FS=2.290;MLEAC=5;MLEAF=0.833;MQ=60.00;MQRankSum=0.00;QD=22.39;ReadPosRankSum=0.701;SOR=0.730\tGT:AD:DP:GQ:PL\t0/1:18,13:31:99:296,0,424\t1/1:0,18:18:54:623,54,0\t1/1:0,26:26:78:774,78,0\n20_10037292_10066351\t3529\t.\tT\tA\t154.29\t.\tAC=1;AF=0.167;AN=6;BaseQRankSum=-5.440e-01;DP=104;ExcessHet=0.0000;FS=1.871;MLEAC=1;MLEAF=0.167;MQ=60.00;MQRankSum=0.00;QD=7.71;ReadPosRankSum=-1.158e+00;SOR=1.034\tGT:AD:DP:GQ:PL\t0/0:44,0:44:99:0,112,1347\t0/1:12,8:20:99:163,0,328\t0/0:39,0:39:99:0,105,1194\n</code></pre> <p>You now have an automated, fully reproducible joint variant calling workflow!</p> <p>Note</p> <p>Keep in mind the data files we gave you cover only a tiny portion of chromosome 20. The real size of a variant callset would be counted in millions of variants. That's why we use only tiny subsets of data for training purposes!</p>"},{"location":"nf4_science/genomics/02_joint_calling/#takeaway_3","title":"Takeaway","text":"<p>You know how to use some common operators as well as Groovy closures to control the flow of data in your workflow.</p>"},{"location":"nf4_science/genomics/02_joint_calling/#whats-next_3","title":"What's next?","text":"<p>Celebrate your success and take an extra super mega long break! This was tough and you deserve it.</p> <p>When you're ready to move on, have a look at our training portal to browse available training courses and select your next step.</p> <p>Good luck!</p>"},{"location":"nf4_science/genomics/03_configuration/","title":"Part 5: Resource profiling and optimization","text":"<p>THIS IS A PLACEHOLDER</p> <p>Note</p> <p>This training module is under redevelopment.</p> <p>TODO</p>"},{"location":"nf4_science/genomics/03_configuration/#43-run-the-workflow-to-generate-a-resource-utilization-report","title":"4.3. Run the workflow to generate a resource utilization report","text":"<p>To have Nextflow generate the report automatically, simply add <code>-with-report &lt;filename&gt;.html</code> to your command line.</p> <pre><code>nextflow run main.nf -profile my_laptop -with-report report-config-1.html\n</code></pre> <p>The report is an html file, which you can download and open in your browser. You can also right click it in the file explorer on the left and click on <code>Show preview</code> in order to view it in VS Code.</p> <p>Take a few minutes to look through the report and see if you can identify some opportunities for adjusting resources. Make sure to click on the tabs that show the utilization results as a percentage of what was allocated. There is some documentation describing all the available features.</p> <p>One observation is that the <code>GATK_JOINTGENOTYPING</code> seems to be very hungry for CPU, which makes sense since it performs a lot of complex calculations. So we could try boosting that and see if it cuts down on runtime.</p> <p>However, we seem to have overshot the mark with the memory allocations; all processes are only using a fraction of what we're giving them. We should dial that back down and save some resources.</p>"},{"location":"nf4_science/genomics/03_configuration/#44-adjust-resource-allocations-for-a-specific-process","title":"4.4. Adjust resource allocations for a specific process","text":"<p>We can specify resource allocations for a given process using the <code>withName</code> process selector. The syntax looks like this when it's by itself in a process block:</p> Syntax<pre><code>process {\n    withName: 'GATK_JOINTGENOTYPING' {\n        cpus = 4\n    }\n}\n</code></pre> <p>Let's add that to the existing process block in the <code>nextflow.config</code> file.</p> nextflow.config<pre><code>process {\n    // defaults for all processes\n    cpus = 2\n    memory = 2.GB\n    // allocations for a specific process\n    withName: 'GATK_JOINTGENOTYPING' {\n        cpus = 4\n    }\n}\n</code></pre> <p>With that specified, the default settings will apply to all processes except the <code>GATK_JOINTGENOTYPING</code> process, which is a special snowflake that gets a lot more CPU. Hopefully that should have an effect.</p>"},{"location":"nf4_science/genomics/03_configuration/#45-run-again-with-the-modified-configuration","title":"4.5. Run again with the modified configuration","text":"<p>Let's run the workflow again with the modified configuration and with the reporting flag turned on, but notice we're giving the report a different name so we can differentiate them.</p> <pre><code>nextflow run main.nf -profile my_laptop -with-report report-config-2.html\n</code></pre> <p>Once again, you probably won't notice a substantial difference in runtime, because this is such a small workload and the tools spend more time in ancillary tasks than in performing the 'real' work.</p> <p>However, the second report shows that our resource utilization is more balanced now.</p> <p>As you can see, this approach is useful when your processes have different resource requirements. It empowers you to right-size the resource allocations you set up for each process based on actual data, not guesswork.</p> <p>Note</p> <p>This is just a tiny taster of what you can do to optimize your use of resources. Nextflow itself has some really neat dynamic retry logic built in to retry jobs that fail due to resource limitations. Additionally, the Seqera Platform offers AI-driven tooling for optimizing your resource allocations automatically as well.</p> <p>We'll cover both of those approaches in an upcoming part of this training course.</p> <p>That being said, there may be some constraints on what you can (or must) allocate depending on what computing executor and compute infrastructure you're using. For example, your cluster may require you to stay within certain limits that don't apply when you're running elsewhere.</p>"},{"location":"nf4_science/genomics/03_modules/","title":"Part 3: Moving code into modules","text":"<p>In the first part of this course, you built a variant calling pipeline that was completely linear and processed each sample's data independently of the others.</p> <p>In the second part, we showed you how to use channels and channel operators to implement joint variant calling with GATK, building on the pipeline from Part 1.</p> <p>In this part, we'll show you how to convert the code in that workflow into modules. To follow this part of the training, you should have completed Part 1 and Part 2, as well as Hello Modules, which covers the basics of modules.</p>"},{"location":"nf4_science/genomics/03_modules/#0-warmup","title":"0. Warmup","text":"<p>When we started developing our workflow, we put everything in one single code file. Now it's time to tackle modularizing our code, i.e. extracting the process definitions into modules.</p> <p>We're going to start with the same workflow as in Part 2, which we've provided for you in the file <code>genomics-3.nf</code>.</p> <p>Note</p> <p>Make sure you're in the correct working directory:  <code>cd /workspaces/training/nf4-science/genomics</code></p> <p>Let's try running that now.</p> <pre><code>nextflow run genomics-3.nf -resume\n</code></pre> <p>And it works!</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `genomics-3.nf` [gloomy_poincare] DSL2 - revision: 43203316e0\n\nexecutor &gt;  local (7)\n[18/89dfa4] SAMTOOLS_INDEX (1)       | 3 of 3 \u2714\n[30/b2522b] GATK_HAPLOTYPECALLER (2) | 3 of 3 \u2714\n[a8/d2c189] GATK_JOINTGENOTYPING     | 1 of 1 \u2714\n</code></pre> <p>Like previously, there will now be a <code>work</code> directory and a <code>results_genomics</code> directory inside your project directory.</p>"},{"location":"nf4_science/genomics/03_modules/#takeaway","title":"Takeaway","text":"<p>You're ready to start modularizing your workflow.</p>"},{"location":"nf4_science/genomics/03_modules/#whats-next","title":"What's next?","text":"<p>Move the Genomics workflow's processes into modules.</p>"},{"location":"nf4_science/genomics/03_modules/#1-move-processes-into-modules","title":"1. Move processes into modules","text":"<p>As you learned in Hello Modules, you can create a module simply by copying the process definition into its own file, in any directory, and you can name that file anything you want.</p> <p>For reasons that will become clear later (in particular when we come to testing), in this training we'll follow the convention of naming the file <code>main.nf</code>, and placing it in a directory structure named after the tool kit and the command.</p>"},{"location":"nf4_science/genomics/03_modules/#11-create-a-module-for-the-samtools_index-process","title":"1.1. Create a module for the <code>SAMTOOLS_INDEX</code> process","text":"<p>In the case of the <code>SAMTOOLS_INDEX</code> process, 'samtools' is the toolkit and 'index' is the command. So, we'll create a directory structure <code>modules/samtools/index</code> and put the <code>SAMTOOLS_INDEX</code> process definition in the <code>main.nf</code> file inside that directory.</p> <pre><code>mkdir -p modules/samtools/index\ntouch modules/samtools/index/main.nf\n</code></pre> <p>Open the <code>main.nf</code> file and copy the <code>SAMTOOLS_INDEX</code> process definition into it, so you end up with something like this:</p> modules/samtools/index/main.nf<pre><code>#!/usr/bin/env nextflow\n\n/*\n * Generate BAM index file\n */\nprocess SAMTOOLS_INDEX {\n\n    container 'community.wave.seqera.io/library/samtools:1.20--b5dfbd93de237464'\n\n    publishDir params.outdir, mode: 'symlink'\n\n    input:\n        path input_bam\n\n    output:\n        tuple path(input_bam), path(\"${input_bam}.bai\")\n\n    script:\n    \"\"\"\n    samtools index '$input_bam'\n    \"\"\"\n}\n</code></pre> <p>Then, remove the <code>SAMTOOLS_INDEX</code> process definition from <code>genomics-3.nf</code>, and add an import declaration for the module before the next process definition, like this:</p> <p>Before:</p> tests/main.nf.test<pre><code>/*\n * Call variants with GATK HaplotypeCaller\n */\nprocess GATK_HAPLOTYPECALLER {\n</code></pre> <p>After:</p> genomics-3.nf<pre><code>// Include modules\ninclude { SAMTOOLS_INDEX } from './modules/samtools/index/main.nf'\n\n/*\n * Call variants with GATK HaplotypeCaller\n */\nprocess GATK_HAPLOTYPECALLER {\n</code></pre> <p>You can now run the workflow again, and it should still work the same way as before. If you supply the <code>-resume</code> flag, no new tasks should even need to be run:</p> <pre><code>nextflow run genomics-3.nf -resume\n</code></pre> Re-used Output after moving SAMTOOLS_INDEX to a module<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `genomics-3.nf` [ridiculous_jones] DSL2 - revision: c5a13e17a1\n\n[cf/289c2d] SAMTOOLS_INDEX (2)       | 3 of 3, cached: 3 \u2714\n[30/b2522b] GATK_HAPLOTYPECALLER (1) | 3 of 3, cached: 3 \u2714\n[a8/d2c189] GATK_JOINTGENOTYPING     | 1 of 1, cached: 1 \u2714\n</code></pre>"},{"location":"nf4_science/genomics/03_modules/#12-create-a-modules-for-the-gatk_haplotypecaller-and-gatk_jointgenotyping-processes","title":"1.2. Create a modules for the <code>GATK_HAPLOTYPECALLER</code> and <code>GATK_JOINTGENOTYPING</code> processes","text":"<p>Repeat the same steps for the remaining processes. You'll need to create a directory for each process, and then create a <code>main.nf</code> file inside that directory, removing the process definition from the workflow's <code>main.nf</code> file and adding an import declaration for the module. Once you're done, check that your modules directory structure is correct by running:</p> <pre><code>tree modules/\n</code></pre> Directory structure<pre><code>modules/\n\u251c\u2500\u2500 gatk\n\u2502   \u251c\u2500\u2500 haplotypecaller\n\u2502   \u2502   \u2514\u2500\u2500 main.nf\n\u2502   \u2514\u2500\u2500 jointgenotyping\n\u2502       \u2514\u2500\u2500 main.nf\n\u2514\u2500\u2500 samtools\n    \u2514\u2500\u2500 index\n        \u2514\u2500\u2500 main.nf\n\n5 directories, 3 files\n</code></pre> <p>You should also have something like this in the main workflow file, after the parameters section:</p> <pre><code>include { SAMTOOLS_INDEX } from './modules/samtools/index/main.nf'\ninclude { GATK_HAPLOTYPECALLER } from './modules/gatk/haplotypecaller/main.nf'\ninclude { GATK_JOINTGENOTYPING } from './modules/gatk/jointgenotyping/main.nf'\n\nworkflow {\n</code></pre>"},{"location":"nf4_science/genomics/03_modules/#takeaway_1","title":"Takeaway","text":"<p>You've practiced modularizing a workflow, with the genomics workflow as an example.</p>"},{"location":"nf4_science/genomics/03_modules/#whats-next_1","title":"What's next?","text":"<p>Test the modularised workflow.</p>"},{"location":"nf4_science/genomics/03_modules/#2-test-the-modularised-workflow","title":"2. Test the modularised workflow","text":"<p>Let's try running that now.</p> <pre><code>nextflow run genomics-3.nf -resume\n</code></pre> <p>And it works!</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `genomics-3.nf` [gloomy_poincare] DSL2 - revision: 43203316e0\n\nexecutor &gt;  local (7)\n[18/89dfa4] SAMTOOLS_INDEX (1)       | 3 of 3 \u2714\n[30/b2522b] GATK_HAPLOTYPECALLER (2) | 3 of 3 \u2714\n[a8/d2c189] GATK_JOINTGENOTYPING     | 1 of 1 \u2714\n</code></pre> <p>Yep, everything still works, including the resumability of the pipeline.</p>"},{"location":"nf4_science/genomics/03_modules/#takeaway_2","title":"Takeaway","text":"<p>You've practiced modularizing a workflow, and you've seen that it still works the same way as before.</p>"},{"location":"nf4_science/genomics/03_modules/#3-summary","title":"3. Summary","text":"<p>So, once again (assuming you followed Hello Modules), you've done all this work and absolutely nothing has changed to how the pipeline works! This is a good thing, because it means that you've modularised your workflow without impacting its function. Importantly, you've laid a foundation for doing things that will make your code more modular and easier to maintain- for example, you can now add tests to your pipeline using the nf-test framework. This is what we'll be looking at in the next part of this course.</p>"},{"location":"nf4_science/genomics/04_testing/","title":"Part 4: Adding tests","text":"<p>In the first part of this course, you built a variant calling pipeline that was completely linear and processed each sample's data independently of the others.</p> <p>In the second part, we showed you how to use channels and channel operators to implement joint variant calling with GATK.</p> <p>In the third part, we modularized the pipeline.</p> <p>In this part of the training, we're going to show you how to use nf-test, a testing framework that integrates well with Nextflow and makes it straightforward to add both module-level and workflow-level tests to your pipeline. To follow this part of the training, you should have completed Part 1, Part 2, and Part 3, as well as the nf-test side quest, which covers the basics of nf-test, and why testing is important.</p>"},{"location":"nf4_science/genomics/04_testing/#0-warmup","title":"0. Warmup","text":"<p>Note</p> <p>Make sure you're in the correct working directory:  <code>cd /workspaces/training/nf4-science/genomics</code></p> <p>If you worked through the previous parts of this training course, you should have a working version of the genomics pipeline, with a modules directory structure like:</p> Directory structure<pre><code>modules/\n\u251c\u2500\u2500 gatk\n\u2502   \u251c\u2500\u2500 haplotypecaller\n\u2502   \u2502   \u2514\u2500\u2500 main.nf\n\u2502   \u2514\u2500\u2500 jointgenotyping\n\u2502       \u2514\u2500\u2500 main.nf\n\u2514\u2500\u2500 samtools\n    \u2514\u2500\u2500 index\n        \u2514\u2500\u2500 main.nf\n</code></pre> <p>This modules directory can be found in the <code>solutions</code> directory if you need it.</p> <p>We're going to start with the same workflow as in Part 3, which we've provided for you in the file <code>genomics-4.nf</code>. Exactly as for the nf-test side quest, we're going to add a few different types of tests to the three processes in this pipeline, as well as a workflow-level test.</p>"},{"location":"nf4_science/genomics/04_testing/#01-check-the-workflow-runs","title":"0.1. Check the workflow runs","text":"<p>Before we start adding tests, let's make sure the workflow runs as expected.</p> <p>Let's try running that now.</p> <pre><code>nextflow run genomics-4.nf -resume\n</code></pre> <p>This should look very familiar by now if you've been working through this training course from the start.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `genomics-4.nf` [gloomy_poincare] DSL2 - revision: 43203316e0\n\nexecutor &gt;  local (7)\n[18/89dfa4] SAMTOOLS_INDEX (1)       | 3 of 3 \u2714\n[30/b2522b] GATK_HAPLOTYPECALLER (2) | 3 of 3 \u2714\n[a8/d2c189] GATK_JOINTGENOTYPING     | 1 of 1 \u2714\n</code></pre> <p>Like previously, there will now be a <code>work</code> directory and a <code>results_genomics</code> directory inside your project directory. We'll actually make use of these results later on in our testing. But from now on we're going to be using the <code>nf-test</code> package to test the pipeline.</p>"},{"location":"nf4_science/genomics/04_testing/#02-initialize-nf-test","title":"0.2. Initialize <code>nf-test</code>","text":"<p>As for the nf-test side quest, we need to initialize the <code>nf-test</code> package.</p> <pre><code>nf-test init\n</code></pre> <p>This should produce the following output:</p> <pre><code>\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\nProject configured. Configuration is stored in nf-test.config\n</code></pre> <p>It also creates a <code>tests</code> directory containing a configuration file stub.</p>"},{"location":"nf4_science/genomics/04_testing/#takeaway","title":"Takeaway","text":"<p>Now we're ready to start writing tests for our genomics pipeline.</p>"},{"location":"nf4_science/genomics/04_testing/#whats-next","title":"What's next?","text":"<p>Write basic tests that evaluate whether the process calls were successful and produced the correct outputs.</p>"},{"location":"nf4_science/genomics/04_testing/#1-test-a-process-for-success-and-matching-outputs","title":"1. Test a process for success and matching outputs","text":"<p>We'll start by testing the <code>SAMTOOLS_INDEX</code> process, which creates index files for BAM files to enable efficient random access. This is a good first test case because:</p> <ol> <li>It has a single, well-defined input (a BAM file)</li> <li>It produces a predictable output (a BAI index file)</li> <li>The output should be identical for identical inputs</li> </ol>"},{"location":"nf4_science/genomics/04_testing/#11-generate-a-test-file-stub","title":"1.1. Generate a test file stub","text":"<p>First, generate a test file stub:</p> <pre><code>nf-test generate process modules/samtools/index/main.nf\n</code></pre> <p>This creates a file in the same directory as <code>main.nf</code>, summarized in the terminal output as follows:</p> Output<pre><code>Load source file '/workspaces/training/nf4-science/genomics/modules/samtools/index/main.nf'\nWrote process test file '/workspaces/training/nf4-science/genomics/tests/modules/samtools/index/main.nf.test\n\nSUCCESS: Generated 1 test files.\n</code></pre> <p>You can navigate to the directory in the file explorer and open the file, which should contain the following code:</p> tests/modules/samtools/index/main.nf.test<pre><code>nextflow_process {\n\n    name \"Test Process SAMTOOLS_INDEX\"\n    script \"modules/samtools/index/main.nf\"\n    process \"SAMTOOLS_INDEX\"\n\n    test(\"Should run without failures\") {\n\n        when {\n            params {\n                // define parameters here. Example:\n                // outdir = \"tests/results\"\n            }\n            process {\n                \"\"\"\n                // define inputs of the process here. Example:\n                // input[0] = file(\"test-file.txt\")\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert snapshot(process.out).match()\n        }\n\n    }\n\n}\n</code></pre> <p>The starting assertions should be familiar from the nf-test side quest:</p> <ul> <li><code>assert process.success</code> states that we expect the process to run successfully and complete without any failures.</li> <li><code>snapshot(process.out).match()</code> states that we expect the result of the run to be identical to the result obtained in a previous run (if applicable).   We discuss this in more detail later.</li> </ul> <p>Using this as a starting point, we need to add the right test inputs for the samtools index process, and any parameters if applicable.</p>"},{"location":"nf4_science/genomics/04_testing/#12-move-the-test-file-and-update-the-script-path","title":"1.2. Move the test file and update the script path","text":"<p>Before we get to work on filling out the test, we need to move the file to its definitive location. Part of the reason we added a directory for each module is that we can now ship tests in a <code>tests</code> directory co-located with each module's <code>main.nf</code> file. Let's create that directory and move the test file there.</p> <pre><code>mkdir -p modules/samtools/index/tests\nmv tests/modules/samtools/index/main.nf.test modules/samtools/index/tests/\n</code></pre> <p>Now we can simplify the <code>script</code> section of the test file to a relative path:</p> <p>Before:</p> modules/samtools/index/tests/main.nf.test<pre><code>name \"Test Process SAMTOOLS_INDEX\"\nscript \"modules/samtools/index/main.nf\"\nprocess \"SAMTOOLS_INDEX\"\n</code></pre> <p>After:</p> modules/samtools/index/tests/main.nf.test<pre><code>name \"Test Process SAMTOOLS_INDEX\"\nscript \"../main.nf\"\nprocess \"SAMTOOLS_INDEX\"\n</code></pre> <p>This tells the test where to find the module's <code>main.nf</code> file, without having to specify the full path.</p>"},{"location":"nf4_science/genomics/04_testing/#13-provide-test-inputs-for-samtools_index","title":"1.3. Provide test inputs for SAMTOOLS_INDEX","text":"<p>The stub file includes a placeholder that we need to replace with an actual test input, appropriate to the input of <code>samtools index</code>. The appropriate input is a BAM file, which we have available in the <code>data/bam</code> directory.</p> <p>Before:</p> modules/samtools/index/tests/main.nf.test<pre><code>process {\n    \"\"\"\n    // define inputs of the process here. Example:\n    // input[0] = file(\"test-file.txt\")\n    \"\"\"\n}\n</code></pre> <p>After:</p> modules/samtools/index/tests/main.nf.test<pre><code>process {\n    \"\"\"\n    input[0] = file(\"${projectDir}/data/bam/reads_son.bam\")\n    \"\"\"\n}\n</code></pre>"},{"location":"nf4_science/genomics/04_testing/#14-name-the-test-based-on-functionality","title":"1.4. Name the test based on functionality","text":"<p>As we learned before, it's good practice to rename the test to something that makes sense in the context of the test.</p> <p>Before:</p> modules/samtools/index/tests/main.nf.test<pre><code>test(\"Should run without failures\") {\n</code></pre> <p>This takes an arbitrary string, so we could put anything we want. Here we choose to refer to the file name and its format:</p> <p>After:</p> modules/samtools/index/tests/main.nf.test<pre><code>test(\"Should index reads_son.bam correctly\") {\n</code></pre>"},{"location":"nf4_science/genomics/04_testing/#15-specify-test-parameters","title":"1.5. Specify test parameters","text":"<p>The <code>params</code> block in the stub file includes a placeholder for parameters:</p> <p>Before:</p> modules/samtools/index/tests/main.nf.test<pre><code>params {\n    // define parameters here. Example:\n    // outdir = \"tests/results\"\n}\n</code></pre> <p>We use it to specify a location for the results to be output, using the default suggestion:</p> <p>After:</p> modules/samtools/index/tests/main.nf.test<pre><code>params {\n    outdir = \"tests/results\"\n}\n</code></pre>"},{"location":"nf4_science/genomics/04_testing/#16-run-the-test-and-examine-the-output","title":"1.6. Run the test and examine the output","text":"<p>Run the test:</p> <pre><code>nf-test test modules/samtools/index/tests/main.nf.test\n</code></pre> <p>This should produce:</p> Output<pre><code>\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Process SAMTOOLS_INDEX\n\n  Test [e761f2e2] 'Should index reads_son.bam correctly' PASSED (11.226s)\n  Snapshots:\n    1 created [Should index reads_son.bam correctly]\n\n\nSnapshot Summary:\n  1 created\n\nSUCCESS: Executed 1 tests in 11.265s\n</code></pre> <p>As we learned previously, this verified the basic assertion about the success of the process and created a snapshot file based on the output of the process. We can see the contents of the snapshot file in the <code>tests/modules/samtools/index/tests/main.nf.test.snap</code> file:</p> tests/modules/samtools/index/tests/main.nf.test.snap<pre><code>{\n  \"Should index reads_son.bam correctly\": {\n    \"content\": [\n      {\n        \"0\": [\n          [\n            \"reads_son.bam:md5,af5956d9388ba017944bef276b71d809\",\n            \"reads_son.bam.bai:md5,a2ca7b84998218ee77eff14af8eb8ca2\"\n          ]\n        ]\n      }\n    ],\n    \"meta\": {\n      \"nf-test\": \"0.9.2\",\n      \"nextflow\": \"24.10.0\"\n    },\n    \"timestamp\": \"2025-03-03T16:59:54.195992321\"\n  }\n}\n</code></pre> <p>We can also run the test again and see that it passes, because the output is identical to the snapshot:</p> <pre><code>nf-test test modules/samtools/index/tests/main.nf.test\n</code></pre> <p>This produces:</p> Output<pre><code>\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Process SAMTOOLS_INDEX\n\n  Test [625e39ee] 'Should index reads_son.bam correctly' PASSED (11.91s)\n\n\nSUCCESS: Executed 1 tests in 11.947s\n</code></pre>"},{"location":"nf4_science/genomics/04_testing/#17-add-more-tests-to-samtools_index","title":"1.7. Add more tests to <code>SAMTOOLS_INDEX</code>","text":"<p>Sometimes it's useful to test a range of different input files to ensure we're testing for a variety of potential issues. Let's also test for the mother and father's bam files in the trio from our test data. Add the following tests to the test file:</p> <pre><code>    test(\"Should index reads_mother.bam correctly\") {\n\n        when {\n            params {\n                outdir = \"tests/results\"\n            }\n            process {\n                \"\"\"\n                input[0] = file(\"${projectDir}/data/bam/reads_mother.bam\")\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert snapshot(process.out).match()\n        }\n\n    }\n\n    test(\"Should index reads_father.bam correctly\") {\n\n        when {\n            params {\n                outdir = \"tests/results\"\n            }\n            process {\n                \"\"\"\n                input[0] = file(\"${projectDir}/data/bam/reads_father.bam\")\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert snapshot(process.out).match()\n        }\n\n    }\n</code></pre> <p>Then you can run the test again:</p> <pre><code>nf-test test modules/samtools/index/tests/main.nf.test\n</code></pre> <p>This produces:</p> modules/samtools/index/tests/main.nf.test<pre><code>\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\nWarning: every snapshot that fails during this test run is re-record.\n\nTest Process SAMTOOLS_INDEX\n\n  Test [625e39ee] 'Should index reads_son.bam correctly' PASSED (10.916s)\n  Test [a8b28f36] 'Should index reads_mother.bam correctly' PASSED (11.147s)\n  Test [c15852a1] 'Should index reads_father.bam correctly' PASSED (10.785s)\n  Snapshots:\n    2 created [Should index reads_father.bam correctly, Should index reads_mother.bam correctly]\n\n\nSnapshot Summary:\n  2 created\n\nSUCCESS: Executed 3 tests in 32.913s\n</code></pre> <p>Notice the warning, referring to the effect of the <code>--update-snapshot</code> parameter.</p> <p>Note</p> <p>Here we are using test data that we used previously to demonstrate the scientific outputs of the pipeline. If we had been planning to operate these tests in a production environment, we would have generated smaller inputs for testing purposes.</p> <p>In general it's important to keep unit tests as light as possible by using the smallest pieces of data necessary and sufficient for evaluating process functionality, otherwise the total runtime can add up quite seriously. A test suite that takes too long to run regularly is a test suite that's likely to get skipped in the interest of expediency.</p>"},{"location":"nf4_science/genomics/04_testing/#takeaway_1","title":"Takeaway","text":"<p>You've written your first module test for a genomics process, verifying that <code>SAMTOOLS_INDEX</code> correctly creates index files for different BAM files. The test suite ensures that:</p> <ol> <li>The process runs successfully</li> <li>Index files are created</li> <li>The outputs are consistent across runs</li> <li>The process works for all sample BAM files</li> </ol>"},{"location":"nf4_science/genomics/04_testing/#whats-next_1","title":"What's next?","text":"<p>Learn how to write tests for other processes in our genomics workflow, using the setup method to handle chained processes. We'll also evaluate whether outputs, specifically our VCF files, contain expected variant calls.</p>"},{"location":"nf4_science/genomics/04_testing/#2-add-tests-to-a-chained-process-and-test-for-contents","title":"2. Add tests to a chained process and test for contents","text":"<p>To test <code>GATK_HAPLOTYPECALLER</code>, we need to provide the process with the <code>SAMTOOLS_INDEX</code> output as an input. We could do that by running <code>SAMTOOLS_INDEX</code>, retrieving its outputs, and storing them with the test data for the workflow. That's actually the recommended approach for a polished pipeline, but nf-test provides an alternative approach, using the <code>setup</code> method.</p> <p>With the setup method, we can trigger the <code>SAMTOOLS_INDEX</code> process as part of the test setup, and then use its output as an input for <code>GATK_HAPLOTYPECALLER</code>. This has a cost - we're going to have to run the <code>SAMTOOLS_INDEX</code> process every time we run the test for <code>GATK_HAPLOTYPECALLER</code>- but maybe we're still developing the workflow and don't want to pre-generate test data we might have to change later. <code>SAMTOOLS_INDEX</code> process is also very quick, so maybe the benefits of pre-generating and storing its outputs are negligible. Let's see the setup method works.</p>"},{"location":"nf4_science/genomics/04_testing/#21-generate-and-place-the-test-file","title":"2.1. Generate and place the test file","text":"<p>As previously, first we generate the file stub:</p> <pre><code>nf-test generate process modules/gatk/haplotypecaller/main.nf\n</code></pre> <p>This produces the following test stub:</p> tests/modules/gatk/haplotypecaller/main.nf.test<pre><code>nextflow_process {\n\n    name \"Test Process GATK_HAPLOTYPECALLER\"\n    script \"modules/gatk/haplotypecaller/main.nf\"\n    process \"GATK_HAPLOTYPECALLER\"\n\n    test(\"Should run without failures\") {\n\n        when {\n            params {\n                // define parameters here. Example:\n                // outdir = \"tests/results\"\n            }\n            process {\n                \"\"\"\n                // define inputs of the process here. Example:\n                // input[0] = file(\"test-file.txt\")\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert snapshot(process.out).match()\n        }\n\n    }\n\n}\n</code></pre>"},{"location":"nf4_science/genomics/04_testing/#22-move-the-test-file-and-update-the-script-path","title":"2.2. Move the test file and update the script path","text":"<p>We create a directory for the test file co-located with the module's <code>main.nf</code> file:</p> <pre><code>mkdir -p modules/gatk/haplotypecaller/tests\n</code></pre> <p>And move the test stub file there:</p> <pre><code>mv tests/modules/gatk/haplotypecaller/main.nf.test modules/gatk/haplotypecaller/tests/\n</code></pre> <p>Finally, don't forget to update the script path:</p> <p>Before:</p> modules/gatk/haplotypecaller/tests/main.nf.test<pre><code>    name \"Test Process GATK_HAPLOTYPECALLER\"\n    script \"modules/gatk/haplotypecaller/main.nf\"\n    process \"GATK_HAPLOTYPECALLER\"\n</code></pre> <p>After:</p> modules/gatk/haplotypecaller/tests/main.nf.test<pre><code>    name \"Test Process GATK_HAPLOTYPECALLER\"\n    script \"../main.nf\"\n    process \"GATK_HAPLOTYPECALLER\"\n</code></pre>"},{"location":"nf4_science/genomics/04_testing/#23-provide-inputs-using-the-setup-method","title":"2.3. Provide inputs using the setup method","text":"<p>We insert a <code>setup</code> block before the <code>when</code> block, where we can trigger a run of the <code>SAMTOOLS_INDEX</code> process on one of our original input files. Also, remember as before to change the test name to something meaningful.</p> <p>Before:</p> modules/gatk/haplotypecaller/tests/main.nf.test<pre><code>test(\"Should run without failures\") {\n\n    when {\n</code></pre> <p>After:</p> modules/gatk/haplotypecaller/tests/main.nf.test<pre><code>    test(\"Should call son's haplotype correctly\") {\n\n        setup {\n            run(\"SAMTOOLS_INDEX\") {\n                script \"../../../samtools/index/main.nf\"\n                process {\n                    \"\"\"\n                    input[0] =  file(\"${projectDir}/data/bam/reads_son.bam\")\n                    \"\"\"\n                }\n            }\n        }\n\n        when {\n</code></pre> <p>Then we can refer to the output of that process in the <code>when</code> block where we specify the test inputs:</p> modules/gatk/haplotypecaller/tests/main.nf.test<pre><code>        when {\n            params {\n                outdir = \"tests/results\"\n            }\n            process {\n                \"\"\"\n                input[0] = SAMTOOLS_INDEX.out\n                input[1] = file(\"${projectDir}/data/ref/ref.fasta\")\n                input[2] = file(\"${projectDir}/data/ref/ref.fasta.fai\")\n                input[3] = file(\"${projectDir}/data/ref/ref.dict\")\n                input[4] = file(\"${projectDir}/data/ref/intervals.bed\")\n                \"\"\"\n            }\n        }\n</code></pre> <p>Make that change and run the test again:</p> <pre><code>nf-test test modules/gatk/haplotypecaller/tests/main.nf.test\n</code></pre> <p>This produces the following output:</p> Output<pre><code>\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Process GATK_HAPLOTYPECALLER\n\n  Test [30247349] 'Should call son's haplotype correctly' PASSED (20.002s)\n  Snapshots:\n    1 created [Should call son's haplotype correctly]\n\n\nSnapshot Summary:\n  1 created\n\nSUCCESS: Executed 1 tests in 20.027s\n</code></pre> <p>It also produces a snapshot file like earlier.</p>"},{"location":"nf4_science/genomics/04_testing/#24-run-again-and-observe-failure","title":"2.4. Run again and observe failure","text":"<p>Interestingly, if you run the exact same command again, this time the test will fail. This command:</p> <pre><code>nf-test test modules/gatk/haplotypecaller/tests/main.nf.test\n</code></pre> <p>produces:</p> Output<pre><code>\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Process GATK_HAPLOTYPECALLER\n\n  Test [c847bfae] 'Should call son's haplotype correctly' FAILED (19.979s)\n\n  java.lang.RuntimeException: Different Snapshot:\n  [                                                                                           [\n      {                                                                                           {\n          \"0\": [                                                                                      \"0\": [\n              \"reads_son.bam.g.vcf:md5,069316cdd4328542ffc6ae247b1dac39\"                         |                 \"reads_son.bam.g.vcf:md5,005f1a13ee39f11b0fc9bea094850eac\"\n          ],                                                                                          ],\n          \"1\": [                                                                                      \"1\": [\n              \"reads_son.bam.g.vcf.idx:md5,dc36c18f2afdc546f41e68b2687e9334\"                     |                 \"reads_son.bam.g.vcf.idx:md5,dbad4b76a4b90c158ffc9c9740764242\"\n          ],                                                                                          ],\n          \"idx\": [                                                                                    \"idx\": [\n              \"reads_son.bam.g.vcf.idx:md5,dc36c18f2afdc546f41e68b2687e9334\"                     |                 \"reads_son.bam.g.vcf.idx:md5,dbad4b76a4b90c158ffc9c9740764242\"\n          ],                                                                                          ],\n          \"vcf\": [                                                                                    \"vcf\": [\n              \"reads_son.bam.g.vcf:md5,069316cdd4328542ffc6ae247b1dac39\"                         |                 \"reads_son.bam.g.vcf:md5,005f1a13ee39f11b0fc9bea094850eac\"\n          ]                                                                                           ]\n      }                                                                                           }\n  ]                                                                                           ]\n\n  Nextflow stdout:\n\n  Nextflow stderr:\n\n\n    Obsolete snapshots can only be checked if all tests of a file are executed successful.\n\n\nFAILURE: Executed 1 tests in 20.032s (1 failed)\n</code></pre> <p>The error message tells you there were differences between the snapshots for the two runs; specifically, the md5sum values are different for the VCF files.</p> <p>Why? To make a long story short, the HaplotypeCaller tool includes a timestamp in the VCF header that is different every time (by definition). As a result, we can't just expect the files to have identical md5sums even if they have identical content in terms of the variant calls themselves.</p> <p>How do we deal with that?</p>"},{"location":"nf4_science/genomics/04_testing/#25-use-a-content-assertion-method-to-check-a-specific-variant","title":"2.5. Use a content assertion method to check a specific variant","text":"<p>One way to solve the problem is to use a different kind of assertion. In this case, we're going to check for specific content instead of asserting identity. More exactly, we'll have the tool read the lines of the VCF file and check for the existence of specific lines.</p> <p>In practice, we replace the second assertion in the <code>then</code> block as follows:</p> <p>Before:</p> modules/gatk/haplotypecaller/tests/main.nf.test<pre><code>then {\n    assert process.success\n    assert snapshot(process.out).match()\n}\n</code></pre> <p>After:</p> modules/gatk/haplotypecaller/tests/main.nf.test<pre><code>        then {\n            assert process.success\n            assert path(process.out[0][0]).readLines().contains('#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\treads_son')\n            assert path(process.out[0][0]).readLines().contains('20_10037292_10066351\t3277\t.\tG\t&lt;NON_REF&gt;\t.\t.\tEND=3282\tGT:DP:GQ:MIN_DP:PL\t0/0:25:72:24:0,72,719')\n        }\n</code></pre> <p>Here we're reading in the full content of the VCF output file and searching for a content match, which is okay to do on a small test file, but you wouldn't want to do that on a larger file. You might instead choose to read in specific lines.</p> <p>This approach does require choosing more carefully what we want to use as the 'signal' to test for. On the bright side, it can be used to test with great precision whether an analysis tool can consistently identify 'difficult' features (such as rare variants) as it undergoes further development.</p>"},{"location":"nf4_science/genomics/04_testing/#26-run-again-and-observe-success","title":"2.6. Run again and observe success","text":"<p>Once we've modified the test in this way, we can run the test multiple times, and it will consistently pass.</p> <pre><code>nf-test test modules/gatk/haplotypecaller/tests/main.nf.test\n</code></pre> <p>This produces:</p> Output<pre><code>\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Process GATK_HAPLOTYPECALLER\n\n  Test [c847bfae] 'Should call son's haplotype correctly' PASSED (19.33s)\n\n\nSUCCESS: Executed 1 tests in 19.382s\n</code></pre>"},{"location":"nf4_science/genomics/04_testing/#27-add-more-tests","title":"2.7. Add more tests","text":"<p>Add similar tests for the mother and father samples:</p> modules/gatk/haplotypecaller/tests/main.nf.test<pre><code>    test(\"Should call mother's haplotype correctly\") {\n\n        setup {\n            run(\"SAMTOOLS_INDEX\") {\n                script \"../../../samtools/index/main.nf\"\n                process {\n                    \"\"\"\n                    input[0] =  file(\"${projectDir}/data/bam/reads_mother.bam\")\n                    \"\"\"\n                }\n            }\n        }\n\n        when {\n            params {\n                outdir = \"tests/results\"\n            }\n            process {\n                \"\"\"\n                input[0] = SAMTOOLS_INDEX.out\n                input[1] = file(\"${projectDir}/data/ref/ref.fasta\")\n                input[2] = file(\"${projectDir}/data/ref/ref.fasta.fai\")\n                input[3] = file(\"${projectDir}/data/ref/ref.dict\")\n                input[4] = file(\"${projectDir}/data/ref/intervals.bed\")\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert path(process.out[0][0]).readLines().contains('#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\treads_mother')\n            assert path(process.out[0][0]).readLines().contains('20_10037292_10066351\t3277\t.\tG\t&lt;NON_REF&gt;\t.\t.\tEND=3278\tGT:DP:GQ:MIN_DP:PL\t0/0:38:99:37:0,102,1530')\n        }\n    }\n\n    test(\"Should call father's haplotype correctly\") {\n\n        setup {\n            run(\"SAMTOOLS_INDEX\") {\n                script \"../../../samtools/index/main.nf\"\n                process {\n                    \"\"\"\n                    input[0] =  file(\"${projectDir}/data/bam/reads_father.bam\")\n                    \"\"\"\n                }\n            }\n        }\n\n        when {\n            params {\n                outdir = \"tests/results\"\n            }\n            process {\n                \"\"\"\n                input[0] = SAMTOOLS_INDEX.out\n                input[1] = file(\"${projectDir}/data/ref/ref.fasta\")\n                input[2] = file(\"${projectDir}/data/ref/ref.fasta.fai\")\n                input[3] = file(\"${projectDir}/data/ref/ref.dict\")\n                input[4] = file(\"${projectDir}/data/ref/intervals.bed\")\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert path(process.out[0][0]).readLines().contains('#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\treads_father')\n            assert path(process.out[0][0]).readLines().contains('20_10037292_10066351\t3277\t.\tG\t&lt;NON_REF&gt;\t.\t.\tEND=3281\tGT:DP:GQ:MIN_DP:PL\t0/0:44:99:42:0,120,1800')\n        }\n    }\n</code></pre>"},{"location":"nf4_science/genomics/04_testing/#28-run-the-test-command","title":"2.8. Run the test command","text":"<pre><code>nf-test test modules/gatk/haplotypecaller/tests/main.nf.test\n</code></pre> <p>This produces:</p> Output<pre><code>\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Process GATK_HAPLOTYPECALLER\n\n  Test [c847bfae] 'Should call son's haplotype correctly' PASSED (19.91s)\n  Test [44494e9c] 'Should call mother's haplotype correctly' PASSED (18.606s)\n  Test [eb0d1a07] 'Should call father's haplotype correctly' PASSED (18.773s)\n\n\nSUCCESS: Executed 3 tests in 57.348s\n</code></pre> <p>That completes the basic test plan for this second step in the pipeline. On to the third and last module-level test!</p>"},{"location":"nf4_science/genomics/04_testing/#takeaway_2","title":"Takeaway","text":"<p>You've learned how to:</p> <ol> <li>Test processes that depend on outputs from other processes</li> <li>Verify specific genomic variants in VCF output files</li> <li>Handle non-deterministic outputs by checking specific content</li> <li>Test variant calling across multiple samples</li> </ol>"},{"location":"nf4_science/genomics/04_testing/#whats-next_2","title":"What's next?","text":"<p>Learn how to write tests that use pre-generated test data for the joint genotyping step.</p>"},{"location":"nf4_science/genomics/04_testing/#3-use-pre-generated-test-data","title":"3. Use pre-generated test data","text":"<p>For the joint genotyping step, we'll use a different approach - using pre-generated test data. This is often preferable for:</p> <ol> <li>Complex processes with multiple dependencies</li> <li>Processes that take a long time to run</li> <li>Processes that are part of a stable, production pipeline</li> </ol>"},{"location":"nf4_science/genomics/04_testing/#31-generate-test-data","title":"3.1. Generate test data","text":"<p>Let's inspect the results we generated at the start of this section:</p> <pre><code>tree results_genomics/\n</code></pre> Results directory contents<pre><code>results_genomics/\n\u251c\u2500\u2500 family_trio.joint.vcf\n\u251c\u2500\u2500 family_trio.joint.vcf.idx\n\u251c\u2500\u2500 reads_father.bam -&gt; /workspaces/training/nf4-science/genomics/work/42/a3bf19dbfaf1f3672b16a5d5e6a8be/reads_father.bam\n\u251c\u2500\u2500 reads_father.bam.bai -&gt; /workspaces/training/nf4-science/genomics/work/cf/289c2d264f496d60a69e3e9ba6463e/reads_father.bam.bai\n\u251c\u2500\u2500 reads_father.bam.g.vcf -&gt; /workspaces/training/nf4-science/genomics/work/30/b2522b83c63baff8c3cf75704512a2/reads_father.bam.g.vcf\n\u251c\u2500\u2500 reads_father.bam.g.vcf.idx -&gt; /workspaces/training/nf4-science/genomics/work/30/b2522b83c63baff8c3cf75704512a2/reads_father.bam.g.vcf.idx\n\u251c\u2500\u2500 reads_mother.bam -&gt; /workspaces/training/nf4-science/genomics/work/af/f31a6ade82cc0cf853c4f61c8bc473/reads_mother.bam\n\u251c\u2500\u2500 reads_mother.bam.bai -&gt; /workspaces/training/nf4-science/genomics/work/18/89dfa40a3def17e45421e54431a126/reads_mother.bam.bai\n\u251c\u2500\u2500 reads_mother.bam.g.vcf -&gt; /workspaces/training/nf4-science/genomics/work/f6/be2efa58e625d08cf8d0da1d0e9f09/reads_mother.bam.g.vcf\n\u251c\u2500\u2500 reads_mother.bam.g.vcf.idx -&gt; /workspaces/training/nf4-science/genomics/work/f6/be2efa58e625d08cf8d0da1d0e9f09/reads_mother.bam.g.vcf.idx\n\u251c\u2500\u2500 reads_son.bam -&gt; /workspaces/training/nf4-science/genomics/work/9f/9615dd553d6f13d8bec4f006ac395f/reads_son.bam\n\u251c\u2500\u2500 reads_son.bam.bai -&gt; /workspaces/training/nf4-science/genomics/work/4d/cb384a97db5687cc9daab002017c7c/reads_son.bam.bai\n\u251c\u2500\u2500 reads_son.bam.g.vcf -&gt; /workspaces/training/nf4-science/genomics/work/fe/2f22d56aa16ed45f8bc419312894f6/reads_son.bam.g.vcf\n\u2514\u2500\u2500 reads_son.bam.g.vcf.idx -&gt; /workspaces/training/nf4-science/genomics/work/fe/2f22d56aa16ed45f8bc419312894f6/reads_son.bam.g.vcf.idx\n\n0 directories, 14 files\n</code></pre> <p>Note that some of these files are symlinks to the actual files in the <code>work</code> directory.</p> <p>The joint genotyping step needs the VCF files produced by the haplotype caller steps as inputs, along with the indices. So let's copy the results we have into the <code>jointgenotyping</code> module's test directory. We'll follow symlinks to the original files.</p> <pre><code>mkdir -p modules/gatk/jointgenotyping/tests/inputs/\ncp -rL results_genomics/*.g.vcf results_genomics/*.g.vcf.idx modules/gatk/jointgenotyping/tests/inputs/\n</code></pre> <p>Now we can use these files as inputs to the test we're going to write for the joint genotyping step.</p>"},{"location":"nf4_science/genomics/04_testing/#32-generate-the-test-file-stub","title":"3.2. Generate the test file stub","text":"<p>As previously, first we generate the file stub:</p> <pre><code>nf-test generate process modules/gatk/jointgenotyping/main.nf\n</code></pre> <p>This produces the following test stub:</p> tests/modules/gatk/jointgenotyping/main.nf.test<pre><code>nextflow_process {\n\n    name \"Test Process GATK_JOINTGENOTYPING\"\n    script \"modules/gatk/jointgenotyping/main.nf\"\n    process \"GATK_JOINTGENOTYPING\"\n\n    test(\"Should run without failures\") {\n\n        when {\n            params {\n                // define parameters here. Example:\n                // outdir = \"tests/results\"\n            }\n            process {\n                \"\"\"\n                // define inputs of the process here. Example:\n                // input[0] = file(\"test-file.txt\")\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert snapshot(process.out).match()\n        }\n\n    }\n\n}\n</code></pre>"},{"location":"nf4_science/genomics/04_testing/#33-move-the-test-file-and-update-the-script-path","title":"3.3. Move the test file and update the script path","text":"<p>This time we already have a directory for tests co-located with the module's <code>main.nf</code> file, so we can move the test stub file there:</p> <pre><code>mv tests/modules/gatk/jointgenotyping/main.nf.test modules/gatk/jointgenotyping/tests/\n</code></pre> <p>And don't forget to update the script path:</p> <p>Before:</p> modules/gatk/jointgenotyping/tests/main.nf.test<pre><code>name \"Test Process GATK_JOINTGENOTYPING\"\nscript \"modules/gatk/jointgenotyping/main.nf\"\nprocess \"GATK_JOINTGENOTYPING\"\n</code></pre> <p>After:</p> modules/gatk/jointgenotyping/tests/main.nf.test<pre><code>name \"Test Process GATK_JOINTGENOTYPING\"\nscript \"../main.nf\"\nprocess \"GATK_JOINTGENOTYPING\"\n</code></pre>"},{"location":"nf4_science/genomics/04_testing/#34-provide-inputs","title":"3.4. Provide inputs","text":"<p>Fill in the inputs based on the process input definitions and rename the test accordingly:</p> modules/gatk/jointgenotyping/tests/main.nf.test<pre><code>    test(\"Should call trio's joint genotype correctly\") {\n\n        when {\n            params {\n                outdir = \"tests/results\"\n            }\n            process {\n                \"\"\"\n                input[0] = [\n                    file(\"${projectDir}/modules/gatk/jointgenotyping/tests/inputs/reads_father.bam.g.vcf\"),\n                    file(\"${projectDir}/modules/gatk/jointgenotyping/tests/inputs/reads_mother.bam.g.vcf\"),\n                    file(\"${projectDir}/modules/gatk/jointgenotyping/tests/inputs/reads_son.bam.g.vcf\")\n                ]\n                input[1] = [\n                    file(\"${projectDir}/modules/gatk/jointgenotyping/tests/inputs/reads_father.bam.g.vcf.idx\"),\n                    file(\"${projectDir}/modules/gatk/jointgenotyping/tests/inputs/reads_mother.bam.g.vcf.idx\"),\n                    file(\"${projectDir}/modules/gatk/jointgenotyping/tests/inputs/reads_son.bam.g.vcf.idx\")\n                ]\n                input[2] = file(\"${projectDir}/data/ref/intervals.bed\")\n                input[3] = \"family_trio\"\n                input[4] = file(\"${projectDir}/data/ref/ref.fasta\")\n                input[5] = file(\"${projectDir}/data/ref/ref.fasta.fai\")\n                input[6] = file(\"${projectDir}/data/ref/ref.dict\")\n                \"\"\"\n            }\n        }\n</code></pre>"},{"location":"nf4_science/genomics/04_testing/#35-use-content-assertions","title":"3.5. Use content assertions","text":"<p>The output of the joint genotyping step is another VCF file, so we're going to use a content assertion again.</p> modules/gatk/jointgenotyping/tests/main.nf.test<pre><code>    then {\n        assert process.success\n        assert path(process.out[0][0]).readLines().contains('#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\treads_father\treads_mother\treads_son')\n        assert path(process.out[0][0]).readLines().contains('20_10037292_10066351\t3480\t.\tC\tCT\t1625.89\t.\tAC=5;AF=0.833;AN=6;BaseQRankSum=0.220;DP=85;ExcessHet=0.0000;FS=2.476;MLEAC=5;MLEAF=0.833;MQ=60.00;MQRankSum=0.00;QD=21.68;ReadPosRankSum=-1.147e+00;SOR=0.487\tGT:AD:DP:GQ:PL\t0/1:15,16:31:99:367,0,375\t1/1:0,18:18:54:517,54,0\t1/1:0,26:26:78:756,78,0')\n    }\n</code></pre> <p>By checking the content of a specific variant in the output file, this test verifies that:</p> <ol> <li>The joint genotyping process runs successfully</li> <li>The output VCF contains all three samples in the correct order</li> <li>A specific variant is called correctly with:</li> <li>Accurate genotypes for each sample (0/1 for father, 1/1 for mother and son)</li> <li>Correct read depths and genotype qualities</li> <li>Population-level statistics like allele frequency (AF=0.833)</li> </ol> <p>We haven't snapshotted the whole file, but by checking a specific variant, we can be confident that the joint genotyping process is working as expected.</p>"},{"location":"nf4_science/genomics/04_testing/#36-run-the-test","title":"3.6. Run the test","text":"<pre><code>nf-test test modules/gatk/jointgenotyping/tests/main.nf.test\n</code></pre> <p>This produces:</p> Output<pre><code>\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Process GATK_JOINTGENOTYPING\n\n  Test [ac2067de] 'Should call trio's joint genotype correctly' PASSED (21.604s)\n\n\nSUCCESS: Executed 1 tests in 21.622s\n</code></pre> <p>The test passes, verifying that our joint genotyping process correctly:</p> <ol> <li>Combines individual sample VCFs</li> <li>Performs joint variant calling</li> <li>Produces a multi-sample VCF with consistent genotype calls across runs</li> </ol>"},{"location":"nf4_science/genomics/04_testing/#takeaway_3","title":"Takeaway","text":"<p>You know how to:</p> <ul> <li>Use previously generated results as inputs for tests</li> <li>Write tests using pre-generated test data</li> </ul>"},{"location":"nf4_science/genomics/04_testing/#whats-next_3","title":"What's next?","text":"<p>Add a workflow-level test to verify the entire variant calling pipeline works end-to-end.</p>"},{"location":"nf4_science/genomics/04_testing/#4-add-a-workflow-level-test","title":"4. Add a workflow-level test","text":"<p>Now we'll test the complete variant calling pipeline, from BAM files to joint genotypes. This verifies that:</p> <ol> <li>All processes work together correctly</li> <li>Data flows properly between steps</li> <li>Final variant calls are consistent</li> </ol>"},{"location":"nf4_science/genomics/04_testing/#41-generate-the-workflow-test","title":"4.1. Generate the workflow test","text":"<p>Generate a test file for the complete pipeline:</p> <pre><code>nf-test generate pipeline genomics-4.nf\n</code></pre> <p>This creates a basic test stub:</p> tests/genomics-4.nf.test<pre><code>nextflow_pipeline {\n\n    name \"Test Workflow genomics-4.nf\"\n    script \"genomics-4.nf\"\n\n    test(\"Should run without failures\") {\n\n        when {\n            params {\n                // define parameters here. Example:\n                // outdir = \"tests/results\"\n            }\n        }\n\n        then {\n            assert workflow.success\n        }\n\n    }\n\n}\n</code></pre> <p>Just correct the name to something meaningful (you'll see why this is useful shortly).</p> <p>Before:</p> tests/genomics-4.nf.test<pre><code>    test(\"Should run without failures\") {\n</code></pre> <p>After:</p> tests/genomics-4.nf.test<pre><code>    test(\"Should run the pipeline without failures\") {\n</code></pre> <p>Note</p> <p>In this case the test file can stay where <code>nf-test</code> created it.</p>"},{"location":"nf4_science/genomics/04_testing/#42-specify-input-parameters","title":"4.2. Specify input parameters","text":"<p>We still need to specify inputs, which is done slightly different at the workflow level compared to module-level tests. There are several ways of doing this, including by specifying a profile. However, a simpler way is to set up a <code>params {}</code> block in the <code>nextflow.config</code> file that <code>nf-test init</code> originally created in the <code>tests</code> directory.</p> tests/nextflow.config<pre><code>/*\n * Pipeline parameters\n */\n\nparams {\n    // Primary input (file of input files, one per line)\n    reads_bam        = \"${projectDir}/data/sample_bams.txt\"\n\n    // Output directory\n    outdir = \"results_genomics\"\n\n    // Reference genome and intervals\n    reference        = \"${projectDir}/data/ref/ref.fasta\"\n    reference_index  = \"${projectDir}/data/ref/ref.fasta.fai\"\n    reference_dict   = \"${projectDir}/data/ref/ref.dict\"\n    intervals        = \"${projectDir}/data/ref/intervals.bed\"\n\n    // Base name for final output file\n    cohort_name      = \"family_trio\"\n}\n</code></pre> <p>When we run the test, <code>nf-test</code> will pick up this configuration file and pull in the inputs accordingly.</p>"},{"location":"nf4_science/genomics/04_testing/#43-run-the-workflow-test","title":"4.3. Run the workflow test","text":"<pre><code>nf-test test tests/genomics-4.nf.test\n</code></pre> <p>This produces:</p> Output<pre><code>\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Workflow genomics-4.nf\n\n  Test [c7dbcaca] 'Should run without failures' PASSED (48.443s)\n\n\nSUCCESS: Executed 1 tests in 48.486s\n</code></pre> <p>The test passes, confirming that our complete variant calling pipeline:</p> <ol> <li>Successfully processes all samples</li> <li>Correctly chains together all steps</li> </ol>"},{"location":"nf4_science/genomics/04_testing/#44-run-all-tests","title":"4.4. Run ALL tests","text":"<p>nf-test has one more trick up it's sleeve. We can run all the tests at once! Modify the <code>nf-test.config</code> file so that nf-test looks in every directory for nf-test files. You can do this by modifying the <code>testsDir</code> parameter:</p> <p>Before:</p> nf-test.config<pre><code>config {\n\n    testsDir \"tests\"\n    workDir \".nf-test\"\n    configFile \"tests/nextflow.config\"\n    profile \"\"\n\n}\n</code></pre> <p>After:</p> nf-test.config<pre><code>config {\n\n    testsDir \".\"\n    workDir \".nf-test\"\n    configFile \"tests/nextflow.config\"\n    profile \"\"\n\n}\n</code></pre> <p>Now, we can simply run nf-test and it will run every single test in our repository:</p> <pre><code>nf-test test\n</code></pre> <p>This produces:</p> Output<pre><code>\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Process GATK_HAPLOTYPECALLER\n\n  Test [c847bfae] 'Should call son's haplotype correctly' PASSED (20.951s)\n  Test [44494e9c] 'Should call mother's haplotype correctly' PASSED (19.155s)\n  Test [eb0d1a07] 'Should call father's haplotype correctly' PASSED (21.843s)\n\nTest Process GATK_JOINTGENOTYPING\n\n  Test [ac2067de] 'Should call trio's joint genotype correctly' PASSED (22.994s)\n\nTest Process SAMTOOLS_INDEX\n\n  Test [625e39ee] 'Should index reads_son.bam correctly' PASSED (11.281s)\n  Test [a8b28f36] 'Should index reads_mother.bam correctly' PASSED (11.126s)\n  Test [c15852a1] 'Should index reads_father.bam correctly' PASSED (12.005s)\n\nTest Workflow genomics-4.nf\n\n  Test [c7dbcaca] 'Should run the pipeline without failures' PASSED (47.92s)\n\n\nSUCCESS: Executed 8 tests in 167.772s\n</code></pre> <p>7 tests in 1 command! We spent a long time configuring lots and lots of tests, but when it came to running them it was very quick and easy. You can see how useful this is when maintaining a large pipeline, which could include hundreds of different elements. We spend time writing tests once so we can save time running them many times.</p> <p>Furthermore, we can automate this! Imagine tests running every time you or a colleague tries to add new code. This is how we ensure our pipelines maintain a high standard.</p>"},{"location":"nf4_science/genomics/04_testing/#takeaway_4","title":"Takeaway","text":"<p>You now know how to write and run several kinds of tests for your genomics pipeline using nf-test. This testing framework helps ensure your variant calling workflow produces consistent, reliable results across different environments and as you make code changes.</p> <p>You've learned to test critical components like:</p> <ul> <li>The <code>SAMTOOLS_INDEX</code> process that prepares BAM files for variant calling</li> <li>The <code>GATK_HAPLOTYPECALLER</code> process that identifies variants in individual samples</li> <li>The <code>GATK_JOINTGENOTYPING</code> process that combines variant calls across a cohort</li> </ul> <p>You've also implemented different testing strategies specific to genomics data:</p> <ul> <li>Verifying that VCF files contain expected variant calls despite non-deterministic elements like timestamps</li> <li>Testing with a family trio dataset to ensure proper variant identification across related samples</li> <li>Checking for specific genomic coordinates and variant information in your output files</li> </ul> <p>These testing skills are essential for developing robust bioinformatics pipelines that can reliably process genomic data and produce accurate variant calls. As you continue working with Nextflow for genomics analysis, this testing foundation will help you maintain high-quality code that produces trustworthy scientific results.</p>"},{"location":"nf4_science/genomics/05_configuration/","title":"Part 3: Resource profiling and optimization","text":"<p>THIS IS A PLACEHOLDER</p> <p>Note</p> <p>This training module is under redevelopment.</p> <p>TODO</p>"},{"location":"nf4_science/genomics/05_configuration/#43-run-the-workflow-to-generate-a-resource-utilization-report","title":"4.3. Run the workflow to generate a resource utilization report","text":"<p>To have Nextflow generate the report automatically, simply add <code>-with-report &lt;filename&gt;.html</code> to your command line.</p> <pre><code>nextflow run main.nf -profile my_laptop -with-report report-config-1.html\n</code></pre> <p>The report is an html file, which you can download and open in your browser. You can also right click it in the file explorer on the left and click on <code>Show preview</code> in order to view it in VS Code.</p> <p>Take a few minutes to look through the report and see if you can identify some opportunities for adjusting resources. Make sure to click on the tabs that show the utilization results as a percentage of what was allocated. There is some documentation describing all the available features.</p> <p>One observation is that the <code>GATK_JOINTGENOTYPING</code> seems to be very hungry for CPU, which makes sense since it performs a lot of complex calculations. So we could try boosting that and see if it cuts down on runtime.</p> <p>However, we seem to have overshot the mark with the memory allocations; all processes are only using a fraction of what we're giving them. We should dial that back down and save some resources.</p>"},{"location":"nf4_science/genomics/05_configuration/#44-adjust-resource-allocations-for-a-specific-process","title":"4.4. Adjust resource allocations for a specific process","text":"<p>We can specify resource allocations for a given process using the <code>withName</code> process selector. The syntax looks like this when it's by itself in a process block:</p> Syntax<pre><code>process {\n    withName: 'GATK_JOINTGENOTYPING' {\n        cpus = 4\n    }\n}\n</code></pre> <p>Let's add that to the existing process block in the <code>nextflow.config</code> file.</p> nextflow.config<pre><code>process {\n    // defaults for all processes\n    cpus = 2\n    memory = 2.GB\n    // allocations for a specific process\n    withName: 'GATK_JOINTGENOTYPING' {\n        cpus = 4\n    }\n}\n</code></pre> <p>With that specified, the default settings will apply to all processes except the <code>GATK_JOINTGENOTYPING</code> process, which is a special snowflake that gets a lot more CPU. Hopefully that should have an effect.</p>"},{"location":"nf4_science/genomics/05_configuration/#45-run-again-with-the-modified-configuration","title":"4.5. Run again with the modified configuration","text":"<p>Let's run the workflow again with the modified configuration and with the reporting flag turned on, but notice we're giving the report a different name so we can differentiate them.</p> <pre><code>nextflow run main.nf -profile my_laptop -with-report report-config-2.html\n</code></pre> <p>Once again, you probably won't notice a substantial difference in runtime, because this is such a small workload and the tools spend more time in ancillary tasks than in performing the 'real' work.</p> <p>However, the second report shows that our resource utilization is more balanced now.</p> <p>As you can see, this approach is useful when your processes have different resource requirements. It empowers you to right-size the resource allocations you set up for each process based on actual data, not guesswork.</p> <p>Note</p> <p>This is just a tiny taster of what you can do to optimize your use of resources. Nextflow itself has some really neat dynamic retry logic built in to retry jobs that fail due to resource limitations. Additionally, the Seqera Platform offers AI-driven tooling for optimizing your resource allocations automatically as well.</p> <p>We'll cover both of those approaches in an upcoming part of this training course.</p> <p>That being said, there may be some constraints on what you can (or must) allocate depending on what computing executor and compute infrastructure you're using. For example, your cluster may require you to stay within certain limits that don't apply when you're running elsewhere.</p>"},{"location":"nf4_science/genomics/next_steps/","title":"Next Steps","text":"<p>Congrats again on completing the Nextflow For Genomics training course and thank you for completing our survey!</p>"},{"location":"nf4_science/genomics/next_steps/#1-top-3-ways-to-level-up-your-nextflow-skills","title":"1. Top 3 ways to level up your Nextflow skills","text":"<p>Here are our top three recommendations for what to do next based on the course you just completed.</p>"},{"location":"nf4_science/genomics/next_steps/#11-apply-nextflow-to-other-scientific-analysis-use-cases","title":"1.1. Apply Nextflow to other scientific analysis use cases","text":"<p>Check out the Nextflow for Science page for a list of other short standalone courses that demonstrate how to apply the basic concepts and mechanisms presented in Hello Nextflow to common scientific analysis use cases.</p> <p>If you don't see your domain represented by a relatable use case, let us know in the Community forum so we can add it to our development list.</p>"},{"location":"nf4_science/genomics/next_steps/#12-get-started-with-nf-core","title":"1.2. Get started with nf-core","text":"<p>nf-core is a worldwide collaborative effort to develop standardized open-source pipelines for a wide range of scientific research applications.** The project includes over 100 pipelines that are available for use out of the box and well over 1400 process modules that can be integrated into your own projects, as well as a rich set of developer tools.</p> <p>The Hello nf-core training course will introduce you to the nf-core community-curated pipelines and development framework, designed to help you write reproducible, scalable, and standardized workflows. You\u2019ll learn how to use existing nf-core pipelines, contribute to their development, and even start building your own, supported by best practices and a vibrant community. If you\u2019re ready to apply your Nextflow skills in real-world projects, this is the perfect next step.</p>"},{"location":"nf4_science/genomics/next_steps/#13-master-more-advanced-nextflow-features","title":"1.3. Master more advanced Nextflow features","text":"<p>In the Hello courses, we keep the level of technical complexity low on purpose to avoid overloading you with information you don't need in order to get started with Nextflow. As you move forward with your work, you're going to want to learn how to use the full feature set and power of Nextflow.</p> <p>To that end, we are currently working on a collection of Side Quests, which are meant to be short standalone courses that go deep into specific topics like testing, metadata handling, using conditional statements and the differences between working on HPC vs. cloud.</p> <p>For any topics that's not covered there yet, browse the Fundamentals Training and Advanced Training to find training materials about the topics that interest you.</p>"},{"location":"nf4_science/genomics/next_steps/#2-check-out-seqera-platform","title":"2. Check out Seqera Platform","text":"<p>Seqera Platform is the best way to run Nextflow in practice.</p> <p>It is a cloud-based platform developed by the creators of Nextflow that you can connect to your own compute infrastructure (whether local, HPC or cloud) to make it much easier to launch and manage your workflows, as well as manage your data and run analyses interactively in a cloud environment.</p> <p>The Free Tier is available for free use by everyone (with usage quotas). Qualifying academics can get free Pro-level access (no usage limitations) through the Academic Program.</p> <p>Have a look at the Seqera Platform tutorials to see if this might be useful to you.</p>"},{"location":"nf4_science/genomics/next_steps/#thats-it-for-now","title":"That's it for now!","text":"<p>Good luck in your Nextflow journey and don't hesitate to let us know in the Community forum what else we could do to help.</p>"},{"location":"nf4_science/genomics/survey/","title":"Feedback survey","text":"<p>Before you move on, please complete this short 4-question survey to rate the training, share any feedback you may have about your experience, and let us know what else we could do to help you in your Nextflow journey.</p> <p>This should take you less than a minute to complete. Thank you for helping us improve our training materials for everyone!</p>"},{"location":"nf4_science/rnaseq/","title":"Nextflow for RNAseq","text":"<p>This training course is intended for researchers in transcriptomics and related fields who are interested in developing or customizing data analysis pipelines. It builds on the Hello Nextflow beginner training and demonstrates how to use Nextflow in the specific context of bulk RNAseq analysis.</p> <p>Specifically, this course demonstrates how to implement a simple bulk RNAseq processing pipeline to trim adapter sequences, align the reads to a genome reference and performs quality control (QC) at several stages.</p> <p>Let's get started! Click on the \"Open in GitHub Codespaces\" button below to launch the training environment (preferably in a separate tab), then read on while it loads.</p> <p></p>"},{"location":"nf4_science/rnaseq/#learning-objectives","title":"Learning objectives","text":"<p>By working through this course, you will learn how to apply foundational Nextflow concepts and tooling to a typical RNAseq use case.</p> <p>By the end of this workshop you will be able to:</p> <ul> <li>Write a linear workflow to apply basic RNAseq processing and QC methods</li> <li>Handle domain-specific files such as FASTQ and reference genome resources appropriately</li> <li>Handle single-end and paired-end sequencing data</li> <li>Leverage Nextflow's dataflow paradigm to parallelize per-sample RNAseq processing</li> <li>Aggregate QC reports across multiple steps and samples using relevant channel operators</li> </ul>"},{"location":"nf4_science/rnaseq/#prerequisites","title":"Prerequisites","text":"<p>The course assumes some minimal familiarity with the following:</p> <ul> <li>Tools and file formats commonly used in this scientific domain</li> <li>Experience with the command line</li> <li>Foundational Nextflow concepts and tooling covered in the Hello Nextflow beginner training.</li> </ul> <p>For technical requirements and environment setup, see the Environment Setup mini-course.</p>"},{"location":"nf4_science/rnaseq/00_orientation/","title":"Orientation","text":"<p>The training environment contains all the software, code and data necessary to work through this training course, so you don't need to install anything yourself. However, you do need a (free) account to log in, and you should take a few minutes to familiarize yourself with the interface.</p> <p>If you have not yet done so, please the Environment Setup mini-course before going any further.</p>"},{"location":"nf4_science/rnaseq/00_orientation/#materials-provided","title":"Materials provided","text":"<p>Throughout this training course, we'll be working in the <code>nf4-science/rnaseq/</code> directory, which you need to move into when you open the training workspace. This directory contains all the code files, test data and accessory files you will need.</p> <p>Feel free to explore the contents of this directory; the easiest way to do so is to use the file explorer on the left-hand side of the training workspace in the VSCode interface. Alternatively, you can use the <code>tree</code> command. Throughout the course, we use the output of <code>tree</code> to represent directory structure and contents in a readable form, sometimes with minor modifications for clarity.</p> <p>Here we generate a table of contents to the second level down:</p> <pre><code>tree . -L 3\n</code></pre> <p>If you run this inside <code>nf4-science/rnaseq</code>, you should see the following output:</p> Directory contents<pre><code>.\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 genome.fa\n\u2502   \u251c\u2500\u2500 paired-end.csv\n\u2502   \u251c\u2500\u2500 reads\n\u2502   \u2502   \u251c\u2500\u2500 ENCSR000COQ1_1.fastq.gz\n\u2502   \u2502   \u251c\u2500\u2500 ENCSR000COQ1_2.fastq.gz\n\u2502   \u2502   \u251c\u2500\u2500 ENCSR000COQ2_1.fastq.gz\n\u2502   \u2502   \u251c\u2500\u2500 ENCSR000COQ2_2.fastq.gz\n\u2502   \u2502   \u251c\u2500\u2500 ENCSR000COR1_1.fastq.gz\n\u2502   \u2502   \u251c\u2500\u2500 ENCSR000COR1_2.fastq.gz\n\u2502   \u2502   \u251c\u2500\u2500 ENCSR000COR2_1.fastq.gz\n\u2502   \u2502   \u251c\u2500\u2500 ENCSR000COR2_2.fastq.gz\n\u2502   \u2502   \u251c\u2500\u2500 ENCSR000CPO1_1.fastq.gz\n\u2502   \u2502   \u251c\u2500\u2500 ENCSR000CPO1_2.fastq.gz\n\u2502   \u2502   \u251c\u2500\u2500 ENCSR000CPO2_1.fastq.gz\n\u2502   \u2502   \u2514\u2500\u2500 ENCSR000CPO2_2.fastq.gz\n\u2502   \u2514\u2500\u2500 single-end.csv\n\u251c\u2500\u2500 nextflow.config\n\u251c\u2500\u2500 rnaseq.nf\n\u2514\u2500\u2500 solutions\n    \u251c\u2500\u2500 modules\n    \u2502   \u251c\u2500\u2500 fastqc.nf\n    \u2502   \u251c\u2500\u2500 fastqc_pe.nf\n    \u2502   \u251c\u2500\u2500 hisat2_align.nf\n    \u2502   \u251c\u2500\u2500 hisat2_align_pe.nf\n    \u2502   \u251c\u2500\u2500 multiqc.nf\n    \u2502   \u251c\u2500\u2500 trim_galore.nf\n    \u2502   \u2514\u2500\u2500 trim_galore_pe.nf\n    \u251c\u2500\u2500 rnaseq-2.1.nf\n    \u251c\u2500\u2500 rnaseq-2.2.nf\n    \u251c\u2500\u2500 rnaseq-2.3.nf\n    \u251c\u2500\u2500 rnaseq-3.1.nf\n    \u251c\u2500\u2500 rnaseq-3.2.nf\n    \u2514\u2500\u2500 rnaseq_pe-3.3.nf\n</code></pre> <p>Note</p> <p>Don't worry if this seems like a lot; we'll go through the relevant pieces at each step of the course. This is just meant to give you an overview.</p> <p>Here's a summary of what you should know to get started:</p> <ul> <li> <p>The <code>rnaseq.nf</code> file is the outline if the workflow script we will work to develop.</p> </li> <li> <p>The file <code>nextflow.config</code> is a configuration file that sets minimal environment properties. You can ignore it for now.</p> </li> <li> <p>The <code>data</code> directory contains input data and related resources:</p> </li> <li> <p>A reference genome called <code>genome.fa</code> consisting of a small region of the human chromosome 20 (from hg19/b37).</p> </li> <li>RNAseq data that has been subset to a small region to keep the file sizes down, in the <code>reads/</code> directory.</li> <li> <p>CSV files listing the IDs and paths of the example data files, for processing in batches.</p> </li> <li> <p>The <code>solutions</code> directory contains the completed workflow scripts and modules that result from each step of the course.   They are intended to be used as a reference to check your work and troubleshoot any issues.   The number in the filename corresponds to the step of the relevant part of the course.</p> </li> </ul> <p>Tip</p> <p>If for whatever reason you move out of this directory, you can always run this command to return to it:</p> <pre><code>cd /workspaces/training/nf4-science/rnaseq\n</code></pre> <p>Now, to begin the course, click on the arrow in the bottom right corner of this page.</p>"},{"location":"nf4_science/rnaseq/01_method/","title":"Part 1: Method overview and manual testing","text":"<p>There are multiple valid methods for processing and analyzing bulk RNAseq data. For this course, we are following the method described here by Drs. Simon Andrews and Laura Biggins at the Babraham Institute.</p> <p>Our goal is to develop a workflow that implements the following processing steps: run initial quality control on reads in a bulk RNAseq sample, trim adapter sequences from the reads, align the reads to a reference genome, and produce a comprehensive quality control (QC) report.</p> <ul> <li>FASTQC: Perform QC on the read data before trimming using FastQC</li> <li>TRIM_GALORE: Trim adapter sequences and perform QC after trimming using Trim Galore (bundles Cutadapt and FastQC)</li> <li>HISAT2_ALIGN: Align reads to the reference genome using Hisat2</li> <li>MULTIQC: Generate a comprehensive QC report using MultiQC</li> </ul> <p>However, before we dive into writing any workflow code, we are going to try out the commands manually on some test data. The tools we need are not installed in the GitHub Codespaces environment, so we'll use them via containers (see Hello Containers).</p> <p>Note</p> <p>Make sure you're in the <code>nf4-science/rnaseq</code> directory. The last part of the path shown when you type <code>pwd</code> should be <code>rnaseq</code>.</p>"},{"location":"nf4_science/rnaseq/01_method/#1-initial-qc-and-adapter-trimming","title":"1. Initial QC and adapter trimming","text":"<p>We're going to pull a container image that has both <code>fastqc</code> and <code>trim_galore</code> installed, spin it up interactively and run the trimming and QC commands on one of the example data files.</p>"},{"location":"nf4_science/rnaseq/01_method/#11-pull-the-container","title":"1.1. Pull the container","text":"<pre><code>docker pull community.wave.seqera.io/library/trim-galore:0.6.10--1bf8ca4e1967cd18\n</code></pre> <p>This gives you the following console output as the system downloads the image:</p> Output<pre><code>0.6.10--1bf8ca4e1967cd18: Pulling from library/trim-galore\ndafa2b0c44d2: Pull complete\ndec6b097362e: Pull complete\nf88da01cff0b: Pull complete\n4f4fb700ef54: Pull complete\n92dc97a3ef36: Pull complete\n403f74b0f85e: Pull complete\n10b8c00c10a5: Pull complete\n17dc7ea432cc: Pull complete\nbb36d6c3110d: Pull complete\n0ea1a16bbe82: Pull complete\n030a47592a0a: Pull complete\n32ec762be2d0: Pull complete\nd2cb90387285: Pull complete\nDigest: sha256:4f00e7b2a09f3c8d8a9ce955120e177152fb1e56f63a2a6e186088b1250d9907\nStatus: Downloaded newer image for community.wave.seqera.io/library/trim-galore:0.6.10--1bf8ca4e1967cd18\ncommunity.wave.seqera.io/library/trim-galore:0.6.10--1bf8ca4e1967cd18\n</code></pre>"},{"location":"nf4_science/rnaseq/01_method/#12-spin-up-the-container-interactively","title":"1.2. Spin up the container interactively","text":"<pre><code>docker run -it -v ./data:/data community.wave.seqera.io/library/trim-galore:0.6.10--1bf8ca4e1967cd18\n</code></pre> <p>Your prompt will change to something like <code>(base) root@b645838b3314:/tmp#</code>, which indicates that you are now inside the container.</p> <p>The <code>-v ./data:/data</code> part of the command will enable us to access the contents of the <code>data/</code> directory from inside the container.</p> <pre><code>ls /data/reads\n</code></pre> Output<pre><code>ENCSR000COQ1_1.fastq.gz  ENCSR000COQ2_2.fastq.gz  ENCSR000COR2_1.fastq.gz  ENCSR000CPO1_2.fastq.gz\nENCSR000COQ1_2.fastq.gz  ENCSR000COR1_1.fastq.gz  ENCSR000COR2_2.fastq.gz  ENCSR000CPO2_1.fastq.gz\nENCSR000COQ2_1.fastq.gz  ENCSR000COR1_2.fastq.gz  ENCSR000CPO1_1.fastq.gz  ENCSR000CPO2_2.fastq.gzO\n</code></pre>"},{"location":"nf4_science/rnaseq/01_method/#13-run-the-first-fastqc-command","title":"1.3. Run the first <code>fastqc</code> command","text":"<p>Let's run <code>fastqc</code> to collect quality control metrics on the read data.</p> <pre><code>fastqc /data/reads/ENCSR000COQ1_1.fastq.gz\n</code></pre> <p>This should run very quickly:</p> Output<pre><code>application/gzip\nStarted analysis of ENCSR000COQ1_1.fastq.gz\nApprox 5% complete for ENCSR000COQ1_1.fastq.gz\nApprox 10% complete for ENCSR000COQ1_1.fastq.gz\nApprox 15% complete for ENCSR000COQ1_1.fastq.gz\nApprox 20% complete for ENCSR000COQ1_1.fastq.gz\nApprox 25% complete for ENCSR000COQ1_1.fastq.gz\nApprox 30% complete for ENCSR000COQ1_1.fastq.gz\nApprox 35% complete for ENCSR000COQ1_1.fastq.gz\nApprox 40% complete for ENCSR000COQ1_1.fastq.gz\nApprox 45% complete for ENCSR000COQ1_1.fastq.gz\nApprox 50% complete for ENCSR000COQ1_1.fastq.gz\nApprox 55% complete for ENCSR000COQ1_1.fastq.gz\nApprox 60% complete for ENCSR000COQ1_1.fastq.gz\nApprox 65% complete for ENCSR000COQ1_1.fastq.gz\nApprox 70% complete for ENCSR000COQ1_1.fastq.gz\nApprox 75% complete for ENCSR000COQ1_1.fastq.gz\nApprox 80% complete for ENCSR000COQ1_1.fastq.gz\nApprox 85% complete for ENCSR000COQ1_1.fastq.gz\nApprox 90% complete for ENCSR000COQ1_1.fastq.gz\nApprox 95% complete for ENCSR000COQ1_1.fastq.gz\nAnalysis complete for ENCSR000COQ1_1.fastq.gz\n</code></pre> <p>You can find the output files in the same directory as the original data:</p> <pre><code>ls /data/reads/ENCSR000COQ1_1_fastqc*\n</code></pre> Output<pre><code>/data/reads/ENCSR000COQ1_1_fastqc.html  /data/reads/ENCSR000COQ1_1_fastqc.zip\n</code></pre>"},{"location":"nf4_science/rnaseq/01_method/#14-trim-adapter-sequences-with-trim_galore","title":"1.4. Trim adapter sequences with <code>trim_galore</code>","text":"<p>Now let's run <code>trim_galore</code>, which bundles Cutadapt and FastQC, to trim the adapter sequences and collect post-trimming QC metrics.</p> <pre><code>trim_galore --fastqc /data/reads/ENCSR000COQ1_1.fastq.gz\n</code></pre> <p>The <code>--fastqc</code> flag causes the command to automatically run a QC collection step after trimming is complete.</p> <p>The output is very verbose so what follows is abbreviated.</p> Output<pre><code>Multicore support not enabled. Proceeding with single-core trimming.\nPath to Cutadapt set as: 'cutadapt' (default)\nCutadapt seems to be working fine (tested command 'cutadapt --version')\nCutadapt version: 4.9\nsingle-core operation.\nigzip command line interface 2.31.0\nigzip detected. Using igzip for decompressing\n\n&lt;...&gt;\n\nAnalysis complete for ENCSR000COQ1_1_trimmed.fq.gz\n</code></pre> <p>You can find the output files in the working directory:</p> <pre><code>ls ENCSR000COQ1_1*\n</code></pre> Output<pre><code>ENCSR000COQ1_1.fastq.gz_trimming_report.txt  ENCSR000COQ1_1_trimmed_fastqc.html\nENCSR000COQ1_1_trimmed.fq.gz                 ENCSR000COQ1_1_trimmed_fastqc.zip\n</code></pre>"},{"location":"nf4_science/rnaseq/01_method/#16-move-the-output-files-to-the-filesystem-outside-the-container","title":"1.6. Move the output files to the filesystem outside the container","text":"<p>Anything that remains inside the container will be inaccessible to future work so let's move these to a new directory.</p> <pre><code>mkdir /data/trimmed\nmv ENCSR000COQ1_1* /data/trimmed\n</code></pre>"},{"location":"nf4_science/rnaseq/01_method/#17-exit-the-container","title":"1.7. Exit the container","text":"<pre><code>exit\n</code></pre>"},{"location":"nf4_science/rnaseq/01_method/#2-align-the-reads-to-the-reference-genome","title":"2. Align the reads to the reference genome","text":"<p>We're going to pull a container image that has <code>hisat2</code> installed, spin it up interactively and run the alignment command to align the RNAseq data to a reference genome.</p>"},{"location":"nf4_science/rnaseq/01_method/#21-pull-the-hisat2-container","title":"2.1. Pull the <code>hisat2</code> container","text":"<pre><code>docker pull community.wave.seqera.io/library/hisat2_samtools:5e49f68a37dc010e\n</code></pre> Output<pre><code>Unable to find image 'community.wave.seqera.io/library/hisat2_samtools:5e49f68a37dc010e' locally\n5e49f68a37dc010e: Pulling from library/hisat2_samtools\ndafa2b0c44d2: Already exists\ndec6b097362e: Already exists\nf88da01cff0b: Already exists\n4f4fb700ef54: Already exists\n92dc97a3ef36: Already exists\n403f74b0f85e: Already exists\n10b8c00c10a5: Already exists\n17dc7ea432cc: Already exists\nbb36d6c3110d: Already exists\n0ea1a16bbe82: Already exists\n030a47592a0a: Already exists\ne74ed5dd390b: Pull complete\nabfcf0185e51: Pull complete\nDigest: sha256:29d8e1a3172a2bdde7be813f7ebec22d331388194a7c0de872b4ccca4bed8f45\nStatus: Downloaded newer image for community.wave.seqera.io/library/hisat2_samtools:5e49f68a37dc010e\n</code></pre>"},{"location":"nf4_science/rnaseq/01_method/#22-spin-up-the-hisat2-container-interactively","title":"2.2. Spin up the <code>hisat2</code> container interactively","text":"<pre><code>docker run -it -v ./data:/data community.wave.seqera.io/library/hisat2_samtools:5e49f68a37dc010e\n</code></pre> <p>The command is the same as before, with the relevant container URI swapped in.</p>"},{"location":"nf4_science/rnaseq/01_method/#23-create-the-hisat2-genome-index-files","title":"2.3. Create the Hisat2 genome index files","text":"<p>Hisat2 requires the genome reference to be provided in a very specific format, and can't just consume the <code>genome.fa</code> FASTA file that we provide, so we're going to take this opportunity to create the relevant resources.</p> <pre><code>hisat2-build /data/genome.fa genome_index\n</code></pre> <p>The output is very verbose so the following is abbreviated:</p> Output<pre><code>Settings:\n  Output files: \"genome_index.*.ht2\"\n&lt;...&gt;\nTotal time for call to driver() for forward index: 00:00:16\n</code></pre> <p>This creates multiple genome index files, which you can find in the working directory.</p> <pre><code>ls genome_index.*\n</code></pre> Output<pre><code>genome_index.1.ht2  genome_index.3.ht2  genome_index.5.ht2  genome_index.7.ht2\ngenome_index.2.ht2  genome_index.4.ht2  genome_index.6.ht2  genome_index.8.ht2\n</code></pre> <p>We'll use these in a moment, but first let's generate a gzipped tarball with these genome index files; we'll need them later and generating these is not typically something we want to do as part of a workflow.</p> <pre><code>tar -czvf /data/genome_index.tar.gz genome_index.*\n</code></pre> <p>This stores a <code>genome_index.tar.gz</code> tarball containing the genome index files in the <code>data/</code> directory on our filesystem, which wil come in handy in Part 2 of this course.</p>"},{"location":"nf4_science/rnaseq/01_method/#25-run-the-hisat2-command","title":"2.5. Run the <code>hisat2</code> command","text":"<p>Now we can run the alignment command, which performs the alignment step with <code>hisat2</code> then pipes the output to <code>samtools</code> to write the output out as a BAM file.</p> <p>The read data input is the <code>/data/trimmed/ENCSR000COQ1_1_trimmed.fq.gz</code> file we generated with <code>trim_galore</code> in the previous step.</p> <pre><code>hisat2 -x genome_index -U /data/trimmed/ENCSR000COQ1_1_trimmed.fq.gz \\\n    --new-summary --summary-file ENCSR000COQ1_1_trimmed.hisat2.log | \\\n    samtools view -bS -o ENCSR000COQ1_1_trimmed.bam\n</code></pre> <p>This runs almost instantly because it's a very small test file. At real scale this could take a lot longer.</p> Output<pre><code>HISAT2 summary stats:\n        Total reads: 27816\n                Aligned 0 time: 1550 (5.57%)\n                Aligned 1 time: 25410 (91.35%)\n                Aligned &gt;1 times: 856 (3.08%)\n        Overall alignment rate: 94.43%\n</code></pre> <p>Once again you can find the output files in the working directory:</p> <pre><code>ls ENCSR000COQ1_1*\n</code></pre> Output<pre><code>ENCSR000COQ1_1_trimmed.bam  ENCSR000COQ1_1_trimmed.hisat2.log\n</code></pre>"},{"location":"nf4_science/rnaseq/01_method/#26-move-the-output-files-to-the-filesystem-outside-the-container","title":"2.6. Move the output files to the filesystem outside the container","text":"<pre><code>mkdir /data/aligned\nmv ENCSR000COQ1_1* /data/aligned\n</code></pre>"},{"location":"nf4_science/rnaseq/01_method/#27-exit-the-container","title":"2.7. Exit the container","text":"<pre><code>exit\n</code></pre>"},{"location":"nf4_science/rnaseq/01_method/#3-generate-a-comprehensive-qc-report","title":"3. Generate a comprehensive QC report","text":"<p>We're going to pull a container image that has <code>multiqc</code> installed, spin it up interactively and run a report generation command on the before/after FastQC report files.</p>"},{"location":"nf4_science/rnaseq/01_method/#31-pull-the-multiqc-container","title":"3.1. Pull the <code>multiqc</code> container","text":"<pre><code>docker pull community.wave.seqera.io/library/pip_multiqc:ad8f247edb55897c\n</code></pre> Output<pre><code>ad8f247edb55897c: Pulling from library/pip_multiqc\ndafa2b0c44d2: Already exists\ndec6b097362e: Already exists\nf88da01cff0b: Already exists\n4f4fb700ef54: Already exists\n92dc97a3ef36: Already exists\n403f74b0f85e: Already exists\n10b8c00c10a5: Already exists\n17dc7ea432cc: Already exists\nbb36d6c3110d: Already exists\n0ea1a16bbe82: Already exists\n030a47592a0a: Already exists\n3f229294c69a: Pull complete\n5a5ad47fd84c: Pull complete\nDigest: sha256:0ebb1d9605395a7df49ad0eb366b21f46afd96a5090376b0d8941cf5294a895a\nStatus: Downloaded newer image for community.wave.seqera.io/library/pip_multiqc:ad8f247edb55897c\ncommunity.wave.seqera.io/library/pip_multiqc:ad8f247edb55897c\n</code></pre>"},{"location":"nf4_science/rnaseq/01_method/#32-spin-up-the-multiqc-container-interactively","title":"3.2. Spin up the <code>multiqc</code> container interactively","text":"<pre><code>docker run -it -v ./data:/data community.wave.seqera.io/library/pip_multiqc:ad8f247edb55897c\n</code></pre>"},{"location":"nf4_science/rnaseq/01_method/#33-run-the-multiqc-command","title":"3.3. Run the <code>multiqc</code> command","text":"<pre><code>multiqc /data/reads /data/trimmed /data/aligned -n ENCSR000COQ1_1_QC\n</code></pre> <p>MultiQC is able to search through directories for compatible QC reports and will aggregate everything it finds.</p> Output<pre><code>/// MultiQC \ud83d\udd0d v1.27.1\n\n       file_search | Search path: /data/reads\n       file_search | Search path: /data/trimmed\n       file_search | Search path: /data/aligned\n         searching | \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 20/20\n            hisat2 | Found 1 reports\n          cutadapt | Found 1 reports\n            fastqc | Found 1 reports\n     write_results | Data        : ENCSR000COQ1_1_QC_data\n     write_results | Report      : ENCSR000COQ1_1_QC.html\n           multiqc | MultiQC complete\n</code></pre> <p>Here we see the tool found all three QC reports we generated: the initial QC we did with <code>fastqc</code>, the post-trimming report from <code>cutadapt</code> (made via <code>trim_galore</code>) and the post-alignment QC produced by <code>hisat2</code>.</p> <p>The output files are once again in the working directory:</p> <pre><code>ls ENCSR000COQ1_1_QC*\n</code></pre> Output<pre><code>ENCSR000COQ1_1_QC.html\n\nENCSR000COQ1_1_QC_data:\ncutadapt_filtered_reads_plot.txt                     fastqc_top_overrepresented_sequences_table.txt\ncutadapt_trimmed_sequences_plot_3_Counts.txt         hisat2_se_plot.txt\ncutadapt_trimmed_sequences_plot_3_Obs_Exp.txt        multiqc.log\nfastqc-status-check-heatmap.txt                      multiqc_citations.txt\nfastqc_adapter_content_plot.txt                      multiqc_cutadapt.txt\nfastqc_per_base_n_content_plot.txt                   multiqc_data.json\nfastqc_per_base_sequence_quality_plot.txt            multiqc_fastqc.txt\nfastqc_per_sequence_gc_content_plot_Counts.txt       multiqc_general_stats.txt\nfastqc_per_sequence_gc_content_plot_Percentages.txt  multiqc_hisat2.txt\nfastqc_per_sequence_quality_scores_plot.txt          multiqc_software_versions.txt\nfastqc_sequence_counts_plot.txt                      multiqc_sources.txt\nfastqc_sequence_duplication_levels_plot.txt\n</code></pre>"},{"location":"nf4_science/rnaseq/01_method/#34-move-the-output-files-to-the-filesystem-outside-the-container","title":"3.4. Move the output files to the filesystem outside the container","text":"<pre><code>mkdir /data/final_qc\nmv ENCSR000COQ1_1_QC** /data/final_qc\n</code></pre>"},{"location":"nf4_science/rnaseq/01_method/#35-exit-the-container","title":"3.5. Exit the container","text":"<pre><code>exit\n</code></pre>"},{"location":"nf4_science/rnaseq/01_method/#takeaway","title":"Takeaway","text":"<p>You have tested all the individual commands interactively in the relevant containers.</p>"},{"location":"nf4_science/rnaseq/01_method/#whats-next","title":"What's next?","text":"<p>Learn how to wrap those same commands into a multi-step workflow that uses containers to execute the work.</p>"},{"location":"nf4_science/rnaseq/02_single-sample/","title":"Part 2: Single-sample implementation","text":"<p>In this part of the course, we're going to write the simplest possible workflow that wraps all the commands we ran in Part 1 to automate running them, and we'll just aim to process one sample at a time.</p> <p>We'll do this in three stages:</p> <ol> <li>Write a single-stage workflow that runs the initial QC step</li> <li>Add adapter trimming and post-trimming QC</li> <li>Add alignment to the reference genome</li> </ol>"},{"location":"nf4_science/rnaseq/02_single-sample/#1-write-a-single-stage-workflow-that-runs-the-initial-qc","title":"1. Write a single-stage workflow that runs the initial QC","text":"<p>Let's start by writing a simple workflow that runs the FastQC tool on a FASTQ file containing single-end RNAseq reads.</p> <p>We provide you with a workflow file, <code>rnaseq.nf</code>, that outlines the main parts of the workflow.</p> rnaseq.nf<pre><code>#!/usr/bin/env nextflow\n\n// Module INCLUDE statements\n\n/*\n * Pipeline parameters\n */\n\n// Primary input\n\nworkflow {\n\n    // Create input channel\n\n    // Call processes\n\n}\n</code></pre> <p>Keep in mind this workflow code is correct but it's not functional; its purpose is just to serve as a skeleton that you'll use to write the actual workflow.</p>"},{"location":"nf4_science/rnaseq/02_single-sample/#11-create-a-directory-to-store-modules","title":"1.1. Create a directory to store modules","text":"<p>We'll create standalone modules for each process to make it easier to manage and reuse them, so let's create a directory to store them.</p> <pre><code>mkdir modules\n</code></pre>"},{"location":"nf4_science/rnaseq/02_single-sample/#12-create-a-module-for-the-qc-metrics-collection-process","title":"1.2. Create a module for the QC metrics collection process","text":"<p>Let's create a module file called <code>modules/fastqc.nf</code> to house the <code>FASTQC</code> process:</p> <pre><code>touch modules/fastqc.nf\n</code></pre> <p>Open the file in the code editor and copy the following code into it:</p> modules/fastqc.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess FASTQC {\n\n    container \"community.wave.seqera.io/library/trim-galore:0.6.10--1bf8ca4e1967cd18\"\n    publishDir \"results/fastqc\", mode: 'symlink'\n\n    input:\n    path reads\n\n    output:\n    path \"${reads.simpleName}_fastqc.zip\", emit: zip\n    path \"${reads.simpleName}_fastqc.html\", emit: html\n\n    script:\n    \"\"\"\n    fastqc $reads\n    \"\"\"\n}\n</code></pre> <p>You should recognize all the pieces from what you learned in Part 1 &amp; Part 2 of this training series; the only notable change is that this time we're using <code>mode: symlink</code> for the <code>publishDir</code> directive, and we're using a parameter to define the <code>publishDir</code>.</p> <p>Note</p> <p>Even though the data files we're using here are very small, in genomics they can get very large. For the purposes of demonstration in the teaching environment, we're using the 'symlink' publishing mode to avoid unnecessary file copies. You shouldn't do this in your final workflows, since you'll lose results when you clean up your <code>work</code> directory.</p>"},{"location":"nf4_science/rnaseq/02_single-sample/#12-import-the-module-into-the-workflow-file","title":"1.2. Import the module into the workflow file","text":"<p>Add the statement <code>include { FASTQC } from './modules/fastqc.nf'</code> to the <code>rnaseq.nf</code> file:</p> rnaseq.nf<pre><code>// Module INCLUDE statements\ninclude { FASTQC } from './modules/fastqc.nf'\n</code></pre>"},{"location":"nf4_science/rnaseq/02_single-sample/#13-add-an-input-declaration","title":"1.3. Add an input declaration","text":"<p>Declare an input parameter with a default value:</p> rnaseq.nf<pre><code>// Primary input\nparams.reads = \"data/reads/ENCSR000COQ1_1.fastq.gz\"\n</code></pre>"},{"location":"nf4_science/rnaseq/02_single-sample/#14-create-an-input-channel-in-the-workflow-block","title":"1.4. Create an input channel in the workflow block","text":"<p>Use a basic <code>.fromPath()</code> channel factory to create the input channel:</p> rnaseq.nf<pre><code>workflow {\n\n    // Create input channel from a file path\n    read_ch = channel.fromPath(params.reads)\n\n    // Call processes\n\n}\n</code></pre>"},{"location":"nf4_science/rnaseq/02_single-sample/#15-call-the-fastqc-process-on-the-input-channel","title":"1.5. Call the <code>FASTQC</code> process on the input channel","text":"rnaseq.nf<pre><code>workflow {\n\n    // Create input channel from a file path\n    read_ch = channel.fromPath(params.reads)\n\n    // Initial quality control\n    FASTQC(read_ch)\n\n}\n</code></pre>"},{"location":"nf4_science/rnaseq/02_single-sample/#16-run-the-workflow-to-test-that-it-works","title":"1.6. Run the workflow to test that it works","text":"<p>We could use the <code>--reads</code> parameter to specify an input from command line, but during development we can be lazy and just use the test default we set up.</p> <pre><code>nextflow run rnaseq.nf\n</code></pre> <p>This should run very quickly if you worked through Part 1 and have already pulled the container. If you skipped it, Nextflow will pull the container for you; you don't have to do anything for it to happen, but you may need to wait up to a minute.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `rnaseq.nf` [fabulous_snyder] DSL2 - revision: 3394c725ee\n\nexecutor &gt;  local (1)\n[d6/d94c3a] FASTQC (1) [100%] 1 of 1 \u2714\n</code></pre> <p>You can find the outputs under <code>results/fastqc</code> as specified in the <code>FASTQC</code> process by the <code>publishDir</code> directive.</p> <pre><code>ls results/fastqc\n</code></pre> Output<pre><code>ENCSR000COQ1_1_fastqc.html  ENCSR000COQ1_1_fastqc.zip\n</code></pre>"},{"location":"nf4_science/rnaseq/02_single-sample/#2-add-adapter-trimming-and-post-trimming-quality-control","title":"2. Add adapter trimming and post-trimming quality control","text":"<p>We're going to use the Trim_Galore wrapper, which bundles Cutadapt for the trimming itself and FastQC for the post-trimming quality control.</p>"},{"location":"nf4_science/rnaseq/02_single-sample/#21-create-a-module-for-the-trimming-and-qc-process","title":"2.1. Create a module for the trimming and QC process","text":"<p>Let's create a module file called <code>modules/trim_galore.nf</code> to house the <code>TRIM_GALORE</code> process:</p> <pre><code>touch modules/trim_galore.nf\n</code></pre> <p>Open the file in the code editor and copy the following code into it:</p> modules/trim_galore.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess TRIM_GALORE {\n\n    container \"community.wave.seqera.io/library/trim-galore:0.6.10--1bf8ca4e1967cd18\"\n    publishDir \"results/trimming\", mode: 'symlink'\n\n    input:\n    path reads\n\n    output:\n    path \"${reads.simpleName}_trimmed.fq.gz\", emit: trimmed_reads\n    path \"${reads}_trimming_report.txt\", emit: trimming_reports\n    path \"${reads.simpleName}_trimmed_fastqc.{zip,html}\", emit: fastqc_reports\n\n    script:\n    \"\"\"\n    trim_galore --fastqc $reads\n    \"\"\"\n}\n</code></pre>"},{"location":"nf4_science/rnaseq/02_single-sample/#22-import-the-module-into-the-workflow-file","title":"2.2. Import the module into the workflow file","text":"<p>Add the statement <code>include { TRIM_GALORE } from './modules/trim_galore.nf'</code> to the <code>rnaseq.nf</code> file:</p> rnaseq.nf<pre><code>// Module INCLUDE statements\ninclude { FASTQC } from './modules/fastqc.nf'\ninclude { TRIM_GALORE } from './modules/trim_galore.nf'\n</code></pre>"},{"location":"nf4_science/rnaseq/02_single-sample/#23-call-the-process-on-the-input-channel","title":"2.3. Call the process on the input channel","text":"rnaseq.nf<pre><code>workflow {\n\n    // Create input channel from a file path\n    read_ch = channel.fromPath(params.reads)\n\n    // Initial quality control\n    FASTQC(read_ch)\n\n    // Adapter trimming and post-trimming QC\n    TRIM_GALORE(read_ch)\n}\n</code></pre>"},{"location":"nf4_science/rnaseq/02_single-sample/#24-run-the-workflow-to-test-that-it-works","title":"2.4. Run the workflow to test that it works","text":"<pre><code>nextflow run rnaseq.nf\n</code></pre> <p>This should run very quickly too, since we're runnng on such a small input file.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `rnaseq.nf` [fabulous_snyder] DSL2 - revision: 3394c725ee\n\nexecutor &gt;  local (1)\n[d6/d94c3a] FASTQC (1) [100%] 1 of 1 \u2714\n[c2/e4a9bb] TRIM_GALORE (1)  [100%] 1 of 1 \u2714\n</code></pre> <p>You can find the outputs under <code>results/trimming</code> as specified in the <code>TRIM_GALORE</code> process by the <code>publishDir</code> directive.</p> <pre><code>ls results/trimming\n</code></pre> Output<pre><code>ENCSR000COQ1_1.fastq.gz_trimming_report.txt  ENCSR000COQ1_1_trimmed_fastqc.zip\nENCSR000COQ1_1_trimmed_fastqc.html           ENCSR000COQ1_1_trimmed.fq.gz\n</code></pre>"},{"location":"nf4_science/rnaseq/02_single-sample/#3-align-the-reads-to-the-reference-genome","title":"3. Align the reads to the reference genome","text":"<p>Finally we can run the genome alignment step using Hisat2, which will also emit FastQC-style quality control metrics.</p>"},{"location":"nf4_science/rnaseq/02_single-sample/#31-create-a-module-for-the-hisat2-process","title":"3.1. Create a module for the HiSat2 process","text":"<p>Let's create a module file called <code>modules/hisat2_align.nf</code> to house the <code>HISAT2_ALIGN</code> process:</p> <pre><code>touch modules/hisat2_align.nf\n</code></pre> <p>Open the file in the code editor and copy the following code into it:</p> modules/hisat2_align.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess HISAT2_ALIGN {\n\n    container \"community.wave.seqera.io/library/hisat2_samtools:5e49f68a37dc010e\"\n    publishDir \"results/align\", mode: 'symlink'\n\n    input:\n    path reads\n    path index_zip\n\n    output:\n    path \"${reads.simpleName}.bam\", emit: bam\n    path \"${reads.simpleName}.hisat2.log\", emit: log\n\n    script:\n    \"\"\"\n    tar -xzvf $index_zip\n    hisat2 -x ${index_zip.simpleName} -U $reads \\\n        --new-summary --summary-file ${reads.simpleName}.hisat2.log | \\\n        samtools view -bS -o ${reads.simpleName}.bam\n    \"\"\"\n}\n</code></pre>"},{"location":"nf4_science/rnaseq/02_single-sample/#32-import-the-module-into-the-workflow-file","title":"3.2. Import the module into the workflow file","text":"<p>Add the statement <code>include { HISAT2_ALIGN } from './modules/hisat2_align.nf'</code> to the <code>rnaseq.nf</code> file:</p> rnaseq.nf<pre><code>// Module INCLUDE statements\ninclude { FASTQC } from './modules/fastqc.nf'\ninclude { TRIM_GALORE } from './modules/trim_galore.nf'\ninclude { HISAT2_ALIGN } from './modules/hisat2_align.nf'\n</code></pre>"},{"location":"nf4_science/rnaseq/02_single-sample/#33-add-a-parameter-declaration-to-provide-the-genome-index","title":"3.3. Add a parameter declaration to provide the genome index","text":"<p>Declare an input parameter with a default value:</p> rnaseq.nf<pre><code>/*\n * Pipeline parameters\n */\nparams.hisat2_index_zip = \"data/genome_index.tar.gz\"\n</code></pre>"},{"location":"nf4_science/rnaseq/02_single-sample/#34-call-the-hisat2_align-process-on-the-trimmed-reads-output-by-trim_galore","title":"3.4. Call the <code>HISAT2_ALIGN</code> process on the trimmed reads output by <code>TRIM_GALORE</code>","text":"<p>The trimmed reads are in the <code>TRIM_GALORE.out.trimmed_reads</code> channel output by the previous step.</p> <p>In addition, we use <code>file (params.hisat2_index_zip)</code> to provide the Hisat2 tool with the gzipped genome index tarball.</p> rnaseq.nf<pre><code>workflow {\n\n    // Create input channel from a file path\n    read_ch = channel.fromPath(params.reads)\n\n    // Initial quality control\n    FASTQC(read_ch)\n\n    // Adapter trimming and post-trimming QC\n    TRIM_GALORE(read_ch)\n\n    // Alignment to a reference genome\n    HISAT2_ALIGN(TRIM_GALORE.out.trimmed_reads, file (params.hisat2_index_zip))\n}\n</code></pre>"},{"location":"nf4_science/rnaseq/02_single-sample/#34-run-the-workflow-to-test-that-it-works","title":"3.4. Run the workflow to test that it works","text":"<pre><code>nextflow run rnaseq.nf\n</code></pre> <p>This should run very quickly too, since we're runnng on such a small input file.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `rnaseq.nf` [extravagant_khorana] DSL2 - revision: 701b41bd16\n\nexecutor &gt;  local (3)\n[e4/d15ad4] FASTQC (1)       [100%] 1 of 1 \u2714\n[c6/12b2be] TRIM_GALORE (1)  [100%] 1 of 1 \u2714\n[c6/7a9f13] HISAT2_ALIGN (1) [100%] 1 of 1 \u2714\n</code></pre> <p>You can find the outputs under <code>results/align</code> as specified in the <code>HISAT2_ALIGN</code> process by the <code>publishDir</code> directive.</p> <pre><code>ls results/align\n</code></pre> Output<pre><code>ENCSR000COQ1_1_trimmed.bam  ENCSR000COQ1_1_trimmed.hisat2.log\n</code></pre> <p>This completes the basic processing we need to apply to each sample.</p> <p>We'll add MultiQC report aggregation in Part 2, after we've made the workflow accept multiple samples at a time.</p>"},{"location":"nf4_science/rnaseq/02_single-sample/#takeaway","title":"Takeaway","text":"<p>You know how to wrap all the core steps to process single-end RNAseq samples individually.</p>"},{"location":"nf4_science/rnaseq/02_single-sample/#whats-next","title":"What's next?","text":"<p>Learn how to modify the workflow to process multiple samples in parallel, aggregate QC reports across all steps for all samples, and enable running the workflow on paired-end RNAseq data.</p>"},{"location":"nf4_science/rnaseq/03_multi-sample/","title":"Part 3: Multi-sample paired-end implementation","text":"<p>In this final part of the course, we're going to take our simple workflow to the next level by turning it into a powerful batch automation tool to handle arbitrary numbers of samples. And while we're at it, we're also going to switch it to expect paired-end data, which is more common in newer studies.</p> <p>We'll do this in three stages:</p> <ol> <li>Make the workflow accept multiple input samples and parallelize execution</li> <li>Add comprehensive QC report generation</li> <li>Switch to paired-end RNAseq data</li> </ol>"},{"location":"nf4_science/rnaseq/03_multi-sample/#1-make-the-workflow-accept-multiple-input-samples-and-parallelize-execution","title":"1. Make the workflow accept multiple input samples and parallelize execution","text":"<p>We're going to need to change how we manage the input.</p>"},{"location":"nf4_science/rnaseq/03_multi-sample/#11-change-the-primary-input-to-be-a-csv-of-file-paths-instead-of-a-single-file","title":"1.1. Change the primary input to be a CSV of file paths instead of a single file","text":"<p>We provide a CSV file containing sample IDs and FASTQ file paths in the <code>data/</code> directory. This CSV file includes a header line. Note that the FASTQ file paths are absolute paths.</p> data/single-end.csv<pre><code>sample_id,fastq_path\nENCSR000COQ1,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000COQ1_1.fastq.gz\nENCSR000COQ2,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000COQ2_1.fastq.gz\nENCSR000COR1,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000COR1_1.fastq.gz\nENCSR000COR2,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000COR2_1.fastq.gz\nENCSR000CPO1,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000CPO1_1.fastq.gz\nENCSR000CPO2,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000CPO2_1.fastq.gz\n</code></pre> <p>Let's rename the primary input parameter to <code>input_csv</code> and change the default to be the path to the <code>single-end.csv</code> file.</p> rnaseq.nf<pre><code>// Primary input\nparams.input_csv = \"data/single-end.csv\"\n</code></pre>"},{"location":"nf4_science/rnaseq/03_multi-sample/#12-update-the-input-channel-factory-to-handle-a-csv-as-input","title":"1.2. Update the input channel factory to handle a CSV as input","text":"<p>We're going to want to load the contents of the file into the channel instead of just the file path itself, so we use the <code>.splitCsv()</code> operator to parse the CSV format, then the <code>.map()</code> operator to grab the specific piece of information we want (the FASTQ file path).</p> rnaseq.nf<pre><code>    // Create input channel from the contents of a CSV file\n    read_ch = Channel.fromPath(params.input_csv)\n        .splitCsv(header:true)\n        .map { row -&gt; file(row.fastq_path) }\n</code></pre>"},{"location":"nf4_science/rnaseq/03_multi-sample/#13-run-the-workflow-to-test-that-it-works","title":"1.3. Run the workflow to test that it works","text":"<pre><code>nextflow run rnaseq.nf\n</code></pre> <p>This time we see each step gets run 6 times, on each of the 6 data files we provided.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `rnaseq.nf` [golden_curry] DSL2 - revision: 2a5ba5be1e\n\nexecutor &gt;  local (18)\n[07/3ff9c5] FASTQC (6)       [100%] 6 of 6 \u2714\n[cc/16859f] TRIM_GALORE (6)  [100%] 6 of 6 \u2714\n[68/4c27b5] HISAT2_ALIGN (6) [100%] 6 of 6 \u2714\n</code></pre> <p>That's all it took to get the workflow to run on multiple files! Nextflow handles all the parallelism for us.</p>"},{"location":"nf4_science/rnaseq/03_multi-sample/#2-aggregate-pre-processing-qc-metrics-into-a-single-multiqc-report","title":"2. Aggregate pre-processing QC metrics into a single MultiQC report","text":"<p>All this produces a lot of QC reports, and we don't want to have to dig through individual reports. This is the perfect point to put in a MultiQC report aggregation step!</p>"},{"location":"nf4_science/rnaseq/03_multi-sample/#21-create-a-module-for-the-qc-aggregation-process","title":"2.1. Create a module for the QC aggregation process","text":"<p>Let's create a module file called <code>modules/multiqc.nf</code> to house the <code>MULTIQC</code> process:</p> <pre><code>touch modules/multiqc.nf\n</code></pre> <p>Open the file in the code editor and copy the following code into it:</p> modules/multiqc.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess MULTIQC {\n\n    container \"community.wave.seqera.io/library/pip_multiqc:ad8f247edb55897c\"\n    publishDir \"results/multiqc\", mode: 'symlink'\n\n    input:\n    path '*'\n    val output_name\n\n    output:\n    path \"${output_name}.html\", emit: report\n    path \"${output_name}_data\", emit: data\n\n    script:\n    \"\"\"\n    multiqc . -n ${output_name}.html\n    \"\"\"\n}\n</code></pre>"},{"location":"nf4_science/rnaseq/03_multi-sample/#22-import-the-module-into-the-workflow-file","title":"2.2. Import the module into the workflow file","text":"<p>Add the statement <code>include { MULTIQC } from './modules/multiqc.nf'</code> to the <code>rnaseq.nf</code> file:</p> rnaseq.nf<pre><code>// Module INCLUDE statements\ninclude { FASTQC } from './modules/fastqc.nf'\ninclude { TRIM_GALORE } from './modules/trim_galore.nf'\ninclude { HISAT2_ALIGN } from './modules/hisat2_align.nf'\ninclude { MULTIQC } from './modules/multiqc.nf'\n</code></pre>"},{"location":"nf4_science/rnaseq/03_multi-sample/#23-add-a-report_id-parameter-and-give-it-a-sensible-default","title":"2.3. Add a <code>report_id</code> parameter and give it a sensible default","text":"rnaseq.nf<pre><code>/*\n * Pipeline parameters\n */\nparams.hisat2_index_zip = \"data/genome_index.tar.gz\"\nparams.report_id = \"all_single-end\"\n</code></pre>"},{"location":"nf4_science/rnaseq/03_multi-sample/#24-call-the-process-on-the-outputs-of-the-previous-steps","title":"2.4. Call the process on the outputs of the previous steps","text":"<p>We need to give the <code>MULTIQC</code> process all the QC-related outputs from previous steps.</p> <p>For that, we're going to use the <code>.mix()</code> operator, which aggregates multiple channels into a single one.</p> <p>If we had four processes called A, B, C and D with a simple <code>.out</code> channel each, the syntax would look like this: <code>A.out.mix( B.out, C.out, D.out )</code>. As you can see, you apply it to the first of the channels you want to combine (doesn't matter which) and just add all the others, separated by commas, in the parenthesis that follows.</p> <p>In the case of our workflow, we have the following outputs to aggregate:</p> <ul> <li><code>FASTQC.out.zip</code></li> <li><code>FASTQC.out.html</code></li> <li><code>TRIM_GALORE.out.trimming_reports</code></li> <li><code>TRIM_GALORE.out.fastqc_reports</code></li> <li><code>HISAT2_ALIGN.out.log</code></li> </ul> <p>So the syntax example becomes:</p> Applying .mix() in the MULTIQC call<pre><code>        FASTQC.out.zip.mix(\n        FASTQC.out.html,\n        TRIM_GALORE.out.trimming_reports,\n        TRIM_GALORE.out.fastqc_reports,\n        HISAT2_ALIGN.out.log\n        )\n</code></pre> <p>That will collect QC reports per sample. But since we want to aggregate them across all samples, we need to add the <code>collect()</code> operator in order to pull the reports for all the samples into a single call to <code>MULTIQC</code>. And we also need to give it the <code>report_id</code> parameter.</p> <p>This gives us the following:</p> The completed MULTIQC call<pre><code>    // Comprehensive QC report generation\n    MULTIQC(\n        FASTQC.out.zip.mix(\n        FASTQC.out.html,\n        TRIM_GALORE.out.trimming_reports,\n        TRIM_GALORE.out.fastqc_reports,\n        HISAT2_ALIGN.out.log\n        ).collect(),\n        params.report_id\n    )\n</code></pre> <p>In the context of the full workflow block, it ends up looking like this:</p> rnaseq.nf<pre><code>workflow {\n    // Create input channel from the contents of a CSV file\n    read_ch = Channel.fromPath(params.input_csv)\n        .splitCsv(header:true)\n        .map { row -&gt; file(row.fastq_path) }\n\n    /// Initial quality control\n    FASTQC(read_ch)\n\n    // Adapter trimming and post-trimming QC\n    TRIM_GALORE(read_ch)\n\n    // Alignment to a reference genome\n    HISAT2_ALIGN(TRIM_GALORE.out.trimmed_reads, file (params.hisat2_index_zip))\n\n    // Comprehensive QC report generation\n    MULTIQC(\n        FASTQC.out.zip.mix(\n        FASTQC.out.html,\n        TRIM_GALORE.out.trimming_reports,\n        TRIM_GALORE.out.fastqc_reports,\n        HISAT2_ALIGN.out.log\n        ).collect(),\n        params.report_id\n    )\n}\n</code></pre>"},{"location":"nf4_science/rnaseq/03_multi-sample/#25-run-the-workflow-to-test-that-it-works","title":"2.5. Run the workflow to test that it works","text":"<pre><code>nextflow run rnaseq.nf -resume\n</code></pre> <p>This time we see a single call to MULTIQC added after the cached process calls:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `rnaseq.nf` [modest_pare] DSL2 - revision: fc724d3b49\n\nexecutor &gt;  local (1)\n[07/3ff9c5] FASTQC (6)       [100%] 6 of 6, cached: 6 \u2714\n[2c/8d8e1e] TRIM_GALORE (5)  [100%] 6 of 6, cached: 6 \u2714\n[a4/7f9c44] HISAT2_ALIGN (6) [100%] 6 of 6, cached: 6 \u2714\n[56/e1f102] MULTIQC          [100%] 1 of 1 \u2714\n</code></pre> <p>You can find the outputs under <code>results/trimming</code> as specified in the <code>TRIM_GALORE</code> process by the <code>publishDir</code> directive.</p> <pre><code>tree -L 2 results/multiqc\n</code></pre> Output<pre><code>results/multiqc\n\u251c\u2500\u2500 all_single-end_data\n\u2502   \u251c\u2500\u2500 cutadapt_filtered_reads_plot.txt\n\u2502   \u251c\u2500\u2500 cutadapt_trimmed_sequences_plot_3_Counts.txt\n\u2502   \u251c\u2500\u2500 cutadapt_trimmed_sequences_plot_3_Obs_Exp.txt\n\u2502   \u251c\u2500\u2500 fastqc_adapter_content_plot.txt\n\u2502   \u251c\u2500\u2500 fastqc_overrepresented_sequences_plot.txt\n\u2502   \u251c\u2500\u2500 fastqc_per_base_n_content_plot.txt\n\u2502   \u251c\u2500\u2500 fastqc_per_base_sequence_quality_plot.txt\n\u2502   \u251c\u2500\u2500 fastqc_per_sequence_gc_content_plot_Counts.txt\n\u2502   \u251c\u2500\u2500 fastqc_per_sequence_gc_content_plot_Percentages.txt\n\u2502   \u251c\u2500\u2500 fastqc_per_sequence_quality_scores_plot.txt\n\u2502   \u251c\u2500\u2500 fastqc_sequence_counts_plot.txt\n\u2502   \u251c\u2500\u2500 fastqc_sequence_duplication_levels_plot.txt\n\u2502   \u251c\u2500\u2500 fastqc_sequence_length_distribution_plot.txt\n\u2502   \u251c\u2500\u2500 fastqc-status-check-heatmap.txt\n\u2502   \u251c\u2500\u2500 fastqc_top_overrepresented_sequences_table.txt\n\u2502   \u251c\u2500\u2500 hisat2_se_plot.txt\n\u2502   \u251c\u2500\u2500 multiqc_citations.txt\n\u2502   \u251c\u2500\u2500 multiqc_cutadapt.txt\n\u2502   \u251c\u2500\u2500 multiqc_data.json\n\u2502   \u251c\u2500\u2500 multiqc_fastqc.txt\n\u2502   \u251c\u2500\u2500 multiqc_general_stats.txt\n\u2502   \u251c\u2500\u2500 multiqc_hisat2.txt\n\u2502   \u251c\u2500\u2500 multiqc.log\n\u2502   \u251c\u2500\u2500 multiqc_software_versions.txt\n\u2502   \u2514\u2500\u2500 multiqc_sources.txt\n\u2514\u2500\u2500 all_single-end.html\n</code></pre> <p>That last <code>all_single-end.html</code> file is the full aggregated report, conveniently packaged into one easy to browse HTML file.</p>"},{"location":"nf4_science/rnaseq/03_multi-sample/#3-enable-processing-paired-end-rnaseq-data","title":"3. Enable processing paired-end RNAseq data","text":"<p>Right now our workflow can only handle single-end RNAseq data. It's increasingly common to see paired-end RNAseq data, so we want to be able to handle that.</p> <p>Making the workflow completely agnostic of the data type would require using slightly more advanced Nextflow language features, so we're not going to do that here, but we can make a paired-end processing version to demonstrate what needs to be adapted.</p>"},{"location":"nf4_science/rnaseq/03_multi-sample/#31-make-a-copy-of-the-workflow-called-rnaseq_penf","title":"3.1. Make a copy of the workflow called <code>rnaseq_pe.nf</code>","text":"<pre><code>cp rnaseq.nf rnaseq_pe.nf\n</code></pre>"},{"location":"nf4_science/rnaseq/03_multi-sample/#32-modify-the-default-input_csv-to-point-to-the-paired-end-data","title":"3.2. Modify the default <code>input_csv</code> to point to the paired-end data","text":"<p>We provide a second CSV file containing sample IDs and paired FASTQ file paths in the <code>data/</code> directory</p> data/paired-end.csv<pre><code>sample_id,fastq_1,fastq_2\nENCSR000COQ1,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000COQ1_1.fastq.gz,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000COQ1_2.fastq.gz\nENCSR000COQ2,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000COQ2_1.fastq.gz,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000COQ2_2.fastq.gz\nENCSR000COR1,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000COR1_1.fastq.gz,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000COR1_2.fastq.gz\nENCSR000COR2,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000COR2_1.fastq.gz,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000COR2_2.fastq.gz\nENCSR000CPO1,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000CPO1_1.fastq.gz,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000CPO1_2.fastq.gz\nENCSR000CPO2,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000CPO2_1.fastq.gz,/workspaces/training/nf4-science/rnaseq/data/reads/ENCSR000CPO2_2.fastq.gz\n</code></pre> <p>Let's change the <code>input_csv</code> default to be the path to the <code>paired-end.csv</code> file.</p> rnaseq_pe.nf<pre><code>// Primary input\nparams.input_csv = \"data/paired-end.csv\"\n</code></pre>"},{"location":"nf4_science/rnaseq/03_multi-sample/#33-update-the-channel-factory","title":"3.3. Update the channel factory","text":"<p>We need to tell the <code>.map()</code> operator to grab both FASTQ file paths now.</p> <p>So <code>row -&gt; file(row.fastq_path)</code> becomes <code>row -&gt; [file(row.fastq_1), file(row.fastq_2)]</code></p> rnaseq_pe.nf<pre><code>    // Create input channel from the contents of a CSV file\n    read_ch = Channel.fromPath(params.input_csv)\n        .splitCsv(header:true)\n        .map { row -&gt; [file(row.fastq_1), file(row.fastq_2)] }\n</code></pre>"},{"location":"nf4_science/rnaseq/03_multi-sample/#34-make-a-paired-end-version-of-the-fastqc-process","title":"3.4. Make a paired-end version of the FASTQC process","text":"<p>Let's make a copy of the module so we can have both version on hand.</p> <pre><code>cp modules/fastqc.nf modules/fastqc_pe.nf\n</code></pre> <p>Open up the new <code>fastqc_pe.nf</code> module file in the code editor and make the following code changes:</p> <ul> <li>Change <code>fastqc $reads</code> to <code>fastqc ${reads}</code> in the <code>script</code> block (line 17) so that the <code>reads</code> input will be unpacked, since it's now a tuple of two paths instead of a single path.</li> <li>Replace <code>${reads.simpleName}</code> with a wildcard (<code>*</code>) to avoid having to handle the output files individually.</li> </ul> modules/fastqc_pe.nf<pre><code>    input:\n    path reads\n\n    output:\n    path \"*_fastqc.zip\", emit: zip\n    path \"*_fastqc.html\", emit: html\n\n    script:\n    \"\"\"\n    fastqc ${reads}\n    \"\"\"\n</code></pre> <p>Technically this generalizes the <code>FASTQC</code> process in a way that makes it able to handle either single-end or paired-end RNAseq data.</p> <p>Finally, update the module import statement to use the paired-end version of the module.</p> rnaseq_pe.nf<pre><code>include { FASTQC } from './modules/fastqc_pe.nf'\n</code></pre>"},{"location":"nf4_science/rnaseq/03_multi-sample/#35-make-a-paired-end-version-of-the-trim_galore-process","title":"3.5. Make a paired-end version of the TRIM_GALORE process","text":"<p>Make a copy of the module so we can have both version on hand.</p> <pre><code>cp modules/trim_galore.nf modules/trim_galore_pe.nf\n</code></pre> <p>Open up the new <code>trim_galore_pe.nf</code> module file in the code editor and make the following code changes:</p> <ul> <li>Change the input declaration from <code>path reads</code> to <code>tuple path(read1), path(read2)</code></li> <li>Update the command in the <code>script</code> block, replacing <code>$reads</code> with <code>--paired ${read1} ${read2}</code></li> <li>Update the output declarations to reflect the added files and different naming conventions, using wildcards to avoid having to list everything.</li> </ul> modules/trim_galore_pe.nf<pre><code>    input:\n    tuple path(read1), path(read2)\n\n    output:\n    tuple path(\"*_val_1.fq.gz\"), path(\"*_val_2.fq.gz\"), emit: trimmed_reads\n    path \"*_trimming_report.txt\", emit: trimming_reports\n    path \"*_val_1_fastqc.{zip,html}\", emit: fastqc_reports_1\n    path \"*_val_2_fastqc.{zip,html}\", emit: fastqc_reports_2\n\n    script:\n    \"\"\"\n    trim_galore --fastqc --paired ${read1} ${read2}\n    \"\"\"\n</code></pre> <p>Finally, update the module import statement to use the paired-end version of the module.</p> rnaseq_pe.nf<pre><code>include { TRIM_GALORE } from './modules/trim_galore_pe.nf'\n</code></pre>"},{"location":"nf4_science/rnaseq/03_multi-sample/#36-update-the-call-to-the-multiqc-process-to-expect-two-reports-from-trim_galore","title":"3.6. Update the call to the MULTIQC process to expect two reports from TRIM_GALORE","text":"<p>The <code>TRIM_GALORE</code> process now produces an additional output channel, so we need to feed that to MultiQC.</p> <p>Replace <code>TRIM_GALORE.out.fastqc_reports,</code> with <code>TRIM_GALORE.out.fastqc_reports_1,</code> plus <code>TRIM_GALORE.out.fastqc_reports_2,</code>:</p> rnaseq_pe.nf<pre><code>    // Comprehensive QC report generation\n    MULTIQC(\n        FASTQC.out.zip.mix(\n        FASTQC.out.html,\n        TRIM_GALORE.out.trimming_reports,\n        TRIM_GALORE.out.fastqc_reports_1,\n        TRIM_GALORE.out.fastqc_reports_2,\n        HISAT2_ALIGN.out.log\n        ).collect(),\n        params.report_id\n    )\n</code></pre> <p>While we're on MultiQC, let's also update the <code>report_id</code> parameter default from <code>\"all_single-end\"</code> to <code>\"all_paired-end\"</code>.</p> rnaseq_pe.nf<pre><code>/*\n * Pipeline parameters\n */\nparams.hisat2_index_zip = \"data/genome_index.tar.gz\"\nparams.report_id = \"all_paired-end\"\n</code></pre>"},{"location":"nf4_science/rnaseq/03_multi-sample/#37-make-a-paired-end-version-of-the-hisat2_align-process","title":"3.7. Make a paired-end version of the HISAT2_ALIGN process","text":"<p>Make a copy of the module so we can have both version on hand.</p> <pre><code>cp modules/hisat2_align.nf modules/hisat2_align_pe.nf\n</code></pre> <p>Open up the new <code>hisat2_align_pe.nf</code> module file in the code editor and make the following code changes:</p> <ul> <li>Change the input declaration from <code>path reads</code> to <code>tuple path(read1), path(read2)</code></li> <li>Update the command in the <code>script</code> block, replacing <code>-U $reads</code> with <code>-1 ${read1} -2 ${read2}</code></li> <li>Replace all instances of <code>${reads.simpleName}</code> with <code>${read1.simpleName}</code> in the command in the <code>script</code> block as well as in the output declarations.</li> </ul> modules/hisat2_align_pe.nf<pre><code>    input:\n    tuple path(read1), path(read2)\n    path index_zip\n\n    output:\n    path \"${read1.simpleName}.bam\", emit: bam\n    path \"${read1.simpleName}.hisat2.log\", emit: log\n\n    script:\n    \"\"\"\n    tar -xzvf $index_zip\n    hisat2 -x ${index_zip.simpleName} -1 ${read1} -2 ${read2} \\\n        --new-summary --summary-file ${read1.simpleName}.hisat2.log | \\\n        samtools view -bS -o ${read1.simpleName}.bam\n    \"\"\"\n</code></pre> <p>Finally, update the module import statement to use the paired-end version of the module.</p> rnaseq_pe.nf<pre><code>include { HISAT2_ALIGN } from './modules/hisat2_align_pe.nf'\n</code></pre>"},{"location":"nf4_science/rnaseq/03_multi-sample/#38-run-the-workflow-to-test-that-it-works","title":"3.8. Run the workflow to test that it works","text":"<pre><code>nextflow run rnaseq_pe.nf\n</code></pre> <p>We don't use <code>-resume</code> since this wouldn't cache, and there's twice as much data to process than before, but it should still complete in under a minute.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `rnaseq_pe.nf` [reverent_kare] DSL2 - revision: 9c376cc219\n\nexecutor &gt;  local (19)\n[c5/cbde15] FASTQC (5)       [100%] 6 of 6 \u2714\n[e4/fa2784] TRIM_GALORE (5)  [100%] 6 of 6 \u2714\n[3a/e23049] HISAT2_ALIGN (5) [100%] 6 of 6 \u2714\n[e6/a3ccd9] MULTIQC          [100%] 1 of 1 \u2714\n</code></pre> <p>And that's it! Now we have two slightly divergent versions of our workflow, one for single-end read data and one for paired-end data. The next logical step would be to make the workflow accept either data type on the fly, which is out of scope for this course, but we may tackle that in a follow-up.</p>"},{"location":"nf4_science/rnaseq/03_multi-sample/#takeaway","title":"Takeaway","text":"<p>You know how to adapt a single-sample workflow to parallelize processing of multiple samples, generate a comprehensive QC report and adapt the workflow to use paired-end read data if needed.</p>"},{"location":"nf4_science/rnaseq/03_multi-sample/#whats-next","title":"What's next?","text":"<p>Congratulations, you've completed the Nextflow For RNAseq mini-course! Celebrate your success and take a well deserved break!</p> <p>Next, we ask you to complete a very short survey about your experience with this training course, then we'll take you to a page with links to further training resources and helpful links.</p>"},{"location":"nf4_science/rnaseq/next_steps/","title":"Next Steps","text":"<p>Congrats again on completing the Nextflow For RNAseq training course and thank you for completing our survey!</p>"},{"location":"nf4_science/rnaseq/next_steps/#1-top-3-ways-to-level-up-your-nextflow-skills","title":"1. Top 3 ways to level up your Nextflow skills","text":"<p>Here are our top three recommendations for what to do next based on the course you just completed.</p>"},{"location":"nf4_science/rnaseq/next_steps/#11-apply-nextflow-to-other-scientific-analysis-use-cases","title":"1.1. Apply Nextflow to other scientific analysis use cases","text":"<p>Check out the Nextflow for Science page for a list of other short standalone courses that demonstrate how to apply the basic concepts and mechanisms presented in Hello Nextflow to common scientific analysis use cases.</p> <p>If you don't see your domain represented by a relatable use case, let us know in the Community forum so we can add it to our development list.</p>"},{"location":"nf4_science/rnaseq/next_steps/#12-get-started-with-nf-core","title":"1.2. Get started with nf-core","text":"<p>nf-core is a worldwide collaborative effort to develop standardized open-source pipelines for a wide range of scientific research applications.** The project includes over 100 pipelines that are available for use out of the box and well over 1400 process modules that can be integrated into your own projects, as well as a rich set of developer tools.</p> <p>The Hello nf-core training course will introduce you to the nf-core community-curated pipelines and development framework, designed to help you write reproducible, scalable, and standardized workflows. You\u2019ll learn how to use existing nf-core pipelines, contribute to their development, and even start building your own, supported by best practices and a vibrant community. If you\u2019re ready to apply your Nextflow skills in real-world projects, this is the perfect next step.</p>"},{"location":"nf4_science/rnaseq/next_steps/#13-master-more-advanced-nextflow-features","title":"1.3. Master more advanced Nextflow features","text":"<p>In the Hello courses, we keep the level of technical complexity low on purpose to avoid overloading you with information you don't need in order to get started with Nextflow. As you move forward with your work, you're going to want to learn how to use the full feature set and power of Nextflow.</p> <p>To that end, we are currently working on a collection of Side Quests, which are meant to be short standalone courses that go deep into specific topics like testing, metadata handling, using conditional statements and the differences between working on HPC vs. cloud.</p> <p>For any topics that's not covered there yet, browse the Fundamentals Training and Advanced Training to find training materials about the topics that interest you.</p>"},{"location":"nf4_science/rnaseq/next_steps/#2-check-out-seqera-platform","title":"2. Check out Seqera Platform","text":"<p>Seqera Platform is the best way to run Nextflow in practice.</p> <p>It is a cloud-based platform developed by the creators of Nextflow that you can connect to your own compute infrastructure (whether local, HPC or cloud) to make it much easier to launch and manage your workflows, as well as manage your data and run analyses interactively in a cloud environment.</p> <p>The Free Tier is available for free use by everyone (with usage quotas). Qualifying academics can get free Pro-level access (no usage limitations) through the Academic Program.</p> <p>Have a look at the Seqera Platform tutorials to see if this might be useful to you.</p>"},{"location":"nf4_science/rnaseq/next_steps/#thats-it-for-now","title":"That's it for now!","text":"<p>Good luck in your Nextflow journey and don't hesitate to let us know in the Community forum what else we could do to help.</p>"},{"location":"nf4_science/rnaseq/survey/","title":"Feedback survey","text":"<p>Before you move on, please complete this short 4-question survey to rate the training, share any feedback you may have about your experience, and let us know what else we could do to help you in your Nextflow journey.</p> <p>This should take you less than a minute to complete. Thank you for helping us improve our training materials for everyone!</p>"},{"location":"other/","title":"Index","text":"<p>This directory contains older training courses that are not actively maintained and that we may repurpose elsewhere or delete in the near future. The corresponding materials are not available within the training environment. You can still find the materials in the GitHub repository and download them for local use.</p>"},{"location":"other/hands_on/","title":"Nextflow course - Hands-on","text":"<p>This hands-on session shows how to implement a Nextflow Variant Calling analysis pipeline for RNA-seq data that is based on GATK best practices.</p> <p>Warning</p> <p>The content and pipeline in this course is aimed at teaching Nextflow, not bioinformatics. Software versions are outdated, among other things, so if you want a variant calling Nextflow pipeline for production you should use nf-core/sarek, nf-core/viralrecon or nf-core/rnavar instead.</p>"},{"location":"other/hands_on/#follow-the-training-video","title":"Follow the training video","text":"<p>We run a free online training event for this course approximately every six months. Videos are streamed to YouTube and questions are handled in the nf-core Slack community. You can watch the recording of the most recent training (September, 2023) below:</p>"},{"location":"other/hands_on/01_datasets/","title":"Data Description","text":"<p>The input data used to test the pipeline implementation is described below. For the purpose of this project, only a subset of the original data is used for most of the data types.</p>"},{"location":"other/hands_on/01_datasets/#genome-assembly","title":"Genome assembly","text":"<p><code>genome.fa</code></p> <p>The human genome assembly hg19 (GRCh37) from GenBank, chromosome 22 only.</p>"},{"location":"other/hands_on/01_datasets/#rna-seq-reads","title":"RNA-seq reads","text":"<p><code>ENCSR000COQ[12]_[12].fastq.gz</code></p> <p>The RNA-seq data comes from the human GM12878 cell line from whole cell, cytosol and nucleus extraction (see table below).</p> <p>The libraries are stranded PE76 Illumina GAIIx RNA-Seq from rRNA-depleted Poly-A+ long RNA (<code>&gt; 200</code> nucleotides in size).</p> <p>Only reads mapped to the 22q11^ locus of the human genome (<code>chr22:16000000-18000000</code>) are used.</p> ENCODE ID Cellular fraction Replicate ID File names ENCSR000COQ Whole Cell 12 <code>ENCSR000COQ1_1.fastq.gz</code><code>ENCSR000COQ2_1.fastq.gz</code> <code>ENCSR000COQ1_2.fastq.gz</code><code>ENCSR000COQ2_2.fastq.gz</code> ENCSR000CPO Nuclear 12 <code>ENCSR000CPO1_1.fastq.gz</code><code>ENCSR000CPO2_1.fastq.gz</code> <code>ENCSR000CPO1_2.fastq.gz</code><code>ENCSR000CPO2_2.fastq.gz</code> ENCSR000COR Cytosolic 12 <code>ENCSR000COR1_1.fastq.gz</code><code>ENCSR000COR2_1.fastq.gz</code> <code>ENCSR000COR1_2.fastq.gz</code><code>ENCSR000COR2_2.fastq.gz</code>"},{"location":"other/hands_on/01_datasets/#known-variants","title":"\"Known\" variants","text":"<p><code>known_variants.vcf.gz</code></p> <p>Known variants come from high confident variant calls for GM12878 from the Illumina Platinum Genomes project. These variant calls were obtained by taking into account pedigree information and the concordance of calls across different methods.</p> <p>We\u2019re using the subset from chromosome 22 only.</p>"},{"location":"other/hands_on/01_datasets/#blacklisted-regions","title":"Blacklisted regions","text":"<p><code>blacklist.bed</code></p> <p>Blacklisted regions are regions of the genomes with anomalous coverage. We use regions for the hg19 assembly, taken from the ENCODE project portal. These regions were identified with DNAse and ChiP-seq samples over ~60 human tissues/cell types, and had a very high ratio of multi-mapping to unique-mapping reads and high variance in mappability.</p>"},{"location":"other/hands_on/02_workflow/","title":"Workflow Description","text":"<p>The aim of the pipeline is to process raw RNA-seq data (in FASTQ format) and obtain the list of small variants, SNVs (SNPs and INDELs) for the downstream analysis. The pipeline is based on the GATK best practices for variant calling with RNAseq data and includes all major steps. In addition the pipeline includes SNVs postprocessing and quantification for allele specific expression.</p> <p>Samples processing is done independently for each replicate. This includes mapping of the reads, splitting at the CIGAR, reassigning mapping qualities and recalibrating base qualities.</p> <p>Variant calling is done simultaneously on bam files from all replicates. This allows to improve coverage of genomic regions and obtain more reliable results.</p>"},{"location":"other/hands_on/02_workflow/#software-manuals","title":"Software manuals","text":"<p>Documentation for all software used in the workflow can be found at the following links:</p> <ul> <li>samtools</li> <li>picard <code>CreateSequenceDictionary</code></li> <li>STAR</li> <li>vcftools</li> <li>GATK tools</li> <li><code>SplitNCigarReads</code></li> <li><code>BaseRecalibrator</code></li> <li><code>PrintReads</code></li> <li><code>HaplotypeCaller</code></li> <li><code>VariantFiltration</code></li> <li><code>ASEReadCounter</code></li> </ul>"},{"location":"other/hands_on/02_workflow/#pipeline-steps","title":"Pipeline steps","text":"<p>In order to get a general idea of the workflow, all the composing steps, together with the corresponding commands, are explained in the next sections.</p>"},{"location":"other/hands_on/02_workflow/#preparing-data","title":"Preparing data","text":"<p>This step prepares input files for the analysis. Genome indexes are created and variants overlapping blacklisted regions are filtered out.</p> <p>Genome indices with <code>samtools</code> and <code>picard</code> are produced first. They will be needed for GATK commands such as <code>Split'N'Trim</code>:</p> <pre><code>samtools faidx genome.fa\npicard CreateSequenceDictionary R= genome.fa O= genome.dict\n</code></pre> <p>Genome index for <code>STAR</code>, needed for RNA-seq reads mappings, is created next. Index files are written to the folder <code>genome_dir</code> :</p> <pre><code>STAR --runMode genomeGenerate \\\n     --genomeDir genome_dir \\\n     --genomeFastaFiles genome.fa \\\n     --runThreadN 4\n</code></pre> <p>Variants overlapping blacklisted regions are then filtered in order to reduce false positive calls [optional]:</p> <pre><code>vcftools --gzvcf known_variants.vcf.gz -c \\\n         --exclude-bed blacklist.bed \\\n         --recode | bgzip -c \\\n         &gt; known_variants.filtered.recode.vcf.gz\n</code></pre>"},{"location":"other/hands_on/02_workflow/#mapping-rna-seq-reads-to-the-reference","title":"Mapping RNA-seq reads to the reference","text":"<p>To align RNA-seq reads to the genome we\u2019re using STAR 2-pass approach. The first alignment creates a table with splice-junctions that is used to guide final alignments. The alignments at both steps are done with default parameters.</p> <p>Additional fields with the read groups, libraries and sample information are added into the final bam file at the second mapping step. As a result we do not need to run Picard processing step from GATK best practices.</p> <p>STAR 1-pass:</p> <pre><code>STAR --genomeDir genome_dir \\\n     --readFilesIn ENCSR000COQ1_1.fastq.gz ENCSR000COQ1_2.fastq.gz \\\n     --runThreadN 4 \\\n     --readFilesCommand zcat \\\n     --outFilterType BySJout \\\n     --alignSJoverhangMin 8 \\\n     --alignSJDBoverhangMin 1 \\\n     --outFilterMismatchNmax 999\n</code></pre> <p>Create new genome index using splice-junction table:</p> <pre><code>STAR --runMode genomeGenerate \\\n     --genomeDir genome_dir \\\n     --genomeFastaFiles genome.fa \\\n     --sjdbFileChrStartEnd SJ.out.tab \\\n     --sjdbOverhang 75 \\\n     --runThreadN 4\n</code></pre> <p>STAR 2-pass, final alignments:</p> <pre><code>STAR --genomeDir genome_dir \\\n     --readFilesIn ENCSR000COQ1_1.fastq.gz ENCSR000COQ1_2.fastq.gz \\\n     --runThreadN 4 \\\n     --readFilesCommand zcat \\\n     --outFilterType BySJout \\\n     --alignSJoverhangMin 8 \\\n     --alignSJDBoverhangMin 1 \\\n     --outFilterMismatchNmax 999 \\\n     --outSAMtype BAM SortedByCoordinate \\\n     --outSAMattrRGline ID:ENCSR000COQ1 LB:library PL:illumina PU:machine SM:GM12878\n</code></pre> <p>Index the resulting bam file:</p> <pre><code>samtools index final_alignments.bam\n</code></pre>"},{"location":"other/hands_on/02_workflow/#splitntrim-and-reassign-mapping-qualities","title":"Split\u2019N'Trim and reassign mapping qualities","text":"<p>The RNA-seq reads overlapping exon-intron junctions can produce false positive variants due to inaccurate splicing. To solve this problem the GATK team recommend to hard-clip any sequence that overlap intronic regions and developed a special tool for this purpose: <code>SplitNCigarReads</code>. The tool identifies Ns in the CIGAR string of the alignment and split reads at this position so that few new reads are created.</p> <p>At this step we also reassign mapping qualities to the alignments. This is important because STAR assign the value <code>255</code> (high quality) to \u201cunknown\u201d mappings that are meaningless to GATK and to variant calling in general.</p> <p>This step is done with recommended parameters from the GATK best practices.</p> <pre><code>java -jar /usr/gitc/GATK35.jar -T SplitNCigarReads \\\n                               -R genome.fa -I final_alignments.bam \\\n                               -o split.bam \\\n                               -rf ReassignOneMappingQuality \\\n                               -RMQF 255 -RMQT 60 \\\n                               -U ALLOW_N_CIGAR_READS \\\n                               --fix_misencoded_quality_scores\n</code></pre>"},{"location":"other/hands_on/02_workflow/#base-recalibration","title":"Base Recalibration","text":"<p>The proposed workflow does not include an indel re-alignment step, which is an optional step in the GATK best practices. We excluded that since it is quite time-intensive and does not really improve variant calling.</p> <p>We instead include a base re-calibration step. This step allows to remove possible systematic errors introduced by the sequencing machine during the assignment of read qualities. To do this, the list of known variants is used as a training set to the machine learning algorithm that models possible errors. Base quality scores are then adjusted based on the obtained results.</p> <pre><code>gatk3 -T BaseRecalibrator \\\n      --default_platform illumina \\\n      -cov ReadGroupCovariate \\\n      -cov QualityScoreCovariate \\\n      -cov CycleCovariate \\\n      -knownSites known_variants.filtered.recode.vcf.gz\\\n      -cov ContextCovariate \\\n      -R genome.fa -I split.bam \\\n      --downsampling_type NONE \\\n      -nct 4 \\\n      -o final.rnaseq.grp\n</code></pre> <pre><code>gatk3 -T PrintReads \\\n      -R genome.fa -I split.bam \\\n      -BQSR final.rnaseq.grp \\\n      -nct 4 \\\n      -o final.bam\n</code></pre>"},{"location":"other/hands_on/02_workflow/#variant-calling-and-variant-filtering","title":"Variant Calling and Variant filtering","text":"<p>The variant calling is done on the uniquely aligned reads only in order to reduce the number of false positive variants called:</p> <pre><code>(samtools view -H final.bam; samtools view final.bam| grep -w 'NH:i:1') \\\n    | samtools view -Sb -  &gt; final.uniq.bam\n</code></pre> <pre><code>samtools index final.uniq.bam\n</code></pre> <p>For variant calling we\u2019re using the GATK tool <code>HaplotypeCaller</code> with default parameters:</p> <pre><code>ls final.uniq.bam  &gt; bam.list\njava -jar /usr/gitc/GATK35.jar -T HaplotypeCaller \\\n                               -R genome.fa -I bam.list \\\n                               -dontUseSoftClippedBases \\\n                               -stand_call_conf 20.0 \\\n                               -o output.gatk.vcf.gz\n</code></pre> <p>Variant filtering is done as recommended in the GATK best practices:</p> <ul> <li>keep clusters of at least 3 SNPs that are within a window of 35 bases between them</li> <li>estimate strand bias using Fisher\u2019s Exact Test with values &gt; 30.0 (Phred-scaled p-value)</li> <li>use variant call confidence score <code>QualByDepth</code> (QD) with values &lt; 2.0. The QD is the QUAL score normalized by allele depth (AD) for a variant.</li> </ul> <pre><code>java -jar /usr/gitc/GATK35.jar -T VariantFiltration \\\n                               -R genome.fa -V output.gatk.vcf.gz \\\n                               -window 35 -cluster 3 \\\n                               -filterName FS -filter \"FS &gt; 30.0\" \\\n                               -filterName QD -filter \"QD &lt; 2.0\" \\\n                               -o final.vcf\n</code></pre>"},{"location":"other/hands_on/02_workflow/#variant-post-processing","title":"Variant Post-processing","text":"<p>For downstream analysis we\u2019re considering only sites that pass all filters and are covered with at least 8 reads:</p> <pre><code>grep -v '#' final.vcf \\\n    | awk '$7~/PASS/' \\\n    | perl -ne 'chomp($_); ($dp)=$_=~/DP\\\\=(\\\\d+)\\\\;/; if($dp&gt;=8){print $_.\"\\\\n\"};' \\\n    &gt; result.DP8.vcf\n</code></pre> <p>Filtered RNA-seq variants are compared with those obtained from DNA sequencing (from Illumina platinum genome project). Variants that are common to these two datasets are \"known\" SNVs. The ones present only in the RNA-seq cohort only are \"novel\".</p> <p>Note</p> <p>Known SNVs will be used for allele specific expression analysis.</p> <p>Novel variants will be used to detect RNA-editing events.</p> <p>We compare two variants files to detect common and different sites:</p> <pre><code>vcftools --vcf result.DP8.vcf --gzdiff known_SNVs.filtered.recode.vcf.gz --diff-site --out commonSNPs\n</code></pre> <p>Here we select sites present in both files (\"known\" SNVs only):</p> <pre><code>awk 'BEGIN{OFS=\"\\t\"} $4~/B/{print $1,$2,$3}' commonSNPs.diff.sites_in_files  &gt; test.bed\n</code></pre> <pre><code>vcftools --vcf final.vcf --bed test.bed --recode --keep-INFO-all --stdout &gt; known_snps.vcf\n</code></pre> <p>Plot a histogram with allele frequency distribution for \"known\" SNVs:</p> <pre><code>grep -v '#'  known_snps.vcf \\\n    | awk -F '\\\\t' '{print $10}' \\\n    | awk -F ':' '{print $2}'\\\n    | perl -ne 'chomp($_); \\\n    @v=split(/\\\\,/,$_); if($v[0]!=0 ||$v[1] !=0) \\\n    {print  $v[1]/($v[1]+$v[0]).\"\\\\n\"; }' \\\n    | awk '$1!=1' \\\n    &gt; AF.4R\n\ngghist.R -i AF.4R -o AF.histogram.pdf\n</code></pre> <p>Calculate read counts for each \"known\" SNVs per allele for allele specific expression analysis:</p> <pre><code>java -jar /usr/gitc/GATK35.jar \\\n     -R genome.fa \\\n     -T ASEReadCounter \\\n     -o ASE.tsv \\\n     -I bam.list \\\n     -sites known_snps.vcf\n</code></pre>"},{"location":"other/hands_on/03_setup/","title":"Environment Setup","text":""},{"location":"other/hands_on/03_setup/#github-codespaces","title":"GitHub Codespaces","text":"<p>This material intends to be a quick hands-on tutorial on Nextflow, so we prepared a GitHub Codespaces environment with everything you need to follow it. GitHub Codespaces offers a virtual machine with everything already set up for you, accessible from your web browser or built into your code editor (eg. VSCode). To start, click on the button below.</p> <p></p> <p>In the GitHub Codespaces window, you'll see a terminal. Type the following command to switch to the folder of this training material:</p> <pre><code>cd /workspaces/training/hands-on\n</code></pre>"},{"location":"other/hands_on/03_setup/#pipeline-data-and-scripts","title":"Pipeline data and scripts","text":"<p>All the files needed for the hands-on activity are stored in the directory shown below. This includes all input data (in the <code>data</code> directory) and the scripts required by the workflow (in the <code>bin</code> directory; see documentation).</p> <pre><code>tree /workspaces/training/hands-on\n</code></pre> <pre><code>hands-on\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 bin\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 gghist.R\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 blacklist.bed\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 genome.fa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 known_variants.vcf.gz\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 reads\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000COQ1_1.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000COQ1_2.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000COQ2_1.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000COQ2_2.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000COR1_1.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000COR1_2.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000COR2_1.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000COR2_2.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000CPO1_1.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000CPO1_2.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000CPO2_1.fastq.gz\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ENCSR000CPO2_2.fastq.gz\n\u251c\u2500\u2500 final_main.nf\n\u2514\u2500\u2500 nextflow.config\n\n4 directories, 19 files\n</code></pre>"},{"location":"other/hands_on/03_setup/#script-permission","title":"Script permission","text":"<p>Make sure the following R script has execute permissions:</p> <pre><code>chmod +x /workspaces/training/hands-on/bin/gghist.R\n</code></pre>"},{"location":"other/hands_on/03_setup/#pulling-the-docker-image","title":"Pulling the Docker image","text":"<p>Nextflow can pull Docker images at runtime, which is very useful as we usually work with multiple container images. The best practice when it comes to containers and Nextflow is to have a light container image for each process. This makes pulling/running/stopping containers faster and it's easier to debug, when compared to a bulky container image. Nextflow will make sure these container images are pulled, when not found locally, ran as containers, with volumes mounted plus many other things that you don't have to worry.</p> <p>Even though Nextflow takes care of that for you, let\u2019s just download manually one of the container images that we will use to see how Docker works:</p> <pre><code>docker pull quay.io/biocontainers/samtools:1.3.1--h0cf4675_11\n</code></pre> <p>It's a very lightweight container image, so the download of the layers should be fast and you will see the output below:</p> <pre><code>aca8d56a3290: Download complete\n445527a00ea8: Download complete\naa5436c16a6a: Download complete\nquay.io/biocontainers/samtools:1.3.1--h0cf4675_11\n</code></pre> <p>You can run this container and launch bash to interact with it by typing the following command:</p> <pre><code>docker run -ti --rm --entrypoint bash quay.io/biocontainers/samtools:1.3.1--h0cf4675_11\n</code></pre> <p>Once inside, you can check the version of samtools, for example. You should see something like:</p> <pre><code>root@0c3770b24a24:/# samtools --version\nsamtools 1.3.1\nUsing htslib 1.3.1\nCopyright (C) 2016 Genome Research Ltd.\nroot@0c3770b24a24:/#\n</code></pre> <p>Type <code>exit</code> to exit the container and come back to your shell.</p>"},{"location":"other/hands_on/04_implementation/","title":"Pipeline Implementation","text":""},{"location":"other/hands_on/04_implementation/#data-preparation","title":"Data preparation","text":"<p>A first step in any pipeline is to prepare the input data. You will find all the data required to run the pipeline in the folder <code>data</code> within the <code>/workspaces/training/hands-on</code> repository directory.</p> <p>There are four data inputs that we will use in this tutorial:</p> <ol> <li>Genome File (<code>data/genome.fa</code>)</li> <li>Human chromosome 22 in FASTA file format</li> <li>Read Files (<code>data/reads/</code>)</li> <li>Sample ENCSR000COQ1: 76bp paired-end reads (<code>ENCSR000COQ1_1.fq.gz</code> and <code>ENCSR000COQ1_2.fq.gz</code>).</li> <li>Variants File (<code>data/known_variants.vcf.gz</code>)</li> <li>Known variants, gzipped as a Variant Calling File (VCF) format.</li> <li>Blacklist File (<code>data/blacklist.bed</code>)</li> <li>Genomic locations which are known to produce artifacts and spurious variants in Browser Extensible Data (BED) format.</li> </ol>"},{"location":"other/hands_on/04_implementation/#input-parameters","title":"Input parameters","text":"<p>We can begin writing the pipeline by creating and editing a text file called <code>main.nf</code> from the <code>/workspaces/training/hands-on</code> repository directory with your favourite text editor. In this example we are using <code>code</code>:</p> <pre><code>cd /workspaces/training/hands-on\ncode main.nf\n</code></pre> <p>Edit this file to specify the input files as script parameters. Using this notation allows you to override them by specifying different values when launching the pipeline execution.</p> <p>Info</p> <p>Click the  icons in the code for explanations.</p> <pre><code>/*\n * Define the default parameters (1)\n */\n\nparams.genome     = \"${projectDir}/data/genome.fa\" // (2)!\nparams.variants   = \"${projectDir}/data/known_variants.vcf.gz\"\nparams.blacklist  = \"${projectDir}/data/blacklist.bed\"\nparams.reads      = \"${projectDir}/data/reads/ENCSR000COQ1_{1,2}.fastq.gz\" // (3)!\nparams.results    = \"results\" // (4)!\n</code></pre> <ol> <li>The <code>/*</code>, <code>*</code> and <code>*/</code> specify comment lines which are ignored by Nextflow.</li> <li>The <code>projectDir</code> variable represents the main script path location.</li> <li>The <code>reads</code> parameter uses a glob pattern to specify the forward (<code>ENCSR000COQ1_1.fq.gz</code>) and reverse (<code>ENCSR000COQ1_2.fq.gz</code>) reads (paired-end) of a sample.</li> <li>The <code>results</code> parameter is used to specify a directory called <code>results</code>.</li> </ol> <p>Tip</p> <p>You can copy the above text ( top right or Cmd+C), then move in the terminal window, open <code>code</code> and paste using the keyboard Cmd+V shortcut.</p> <p>Once you have the default parameters in the <code>main.nf</code> file, you can save and run the main script for the first time.</p> <p>Tip</p> <p>With <code>code</code> you can save and close the file with Ctrl+O, then Enter, followed by Ctrl+X.</p> <p>To run the main script use the following command:</p> <pre><code>nextflow run main.nf\n</code></pre> <p>You should see the script execute, print Nextflow version and pipeline revision and then exit.</p> <pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `main.nf` [elated_davinci] DSL2 - revision: 5187dd3166\n</code></pre> <p>Problem #1</p> <p>Great, now we need to define a channel variable to handle the read-pair files. To do that open the <code>main.nf</code> file and copy the lines below at the end of the file.</p> <p>Tip</p> <p>In <code>code</code> you can move to the end of the file using Ctrl+W and then Ctrl+V.</p> <p>This time you must fill the <code>BLANK</code> space with a channel factory that will create a channel out of the <code>params.reads</code> information.</p> <pre><code>workflow {\n    reads_ch = BLANK\n}\n</code></pre> <p>Tip</p> <p>Use the fromFilePairs channel factory.</p> <p>Once you think you have data organised, you can again run the pipeline. However this time, we can use the the <code>-resume</code> flag.</p> <pre><code>nextflow run main.nf -resume\n</code></pre> <p>Tip</p> <p>See here for more details about using the <code>resume</code> option.</p> Solution <pre><code>workflow {\n    reads_ch = Channel.fromFilePairs(params.reads) // (1)!\n}\n</code></pre> <ol> <li>Creates a channel using the fromFilePairs() channel factory.</li> </ol>"},{"location":"other/hands_on/04_implementation/#process-1a-create-a-fasta-genome-index","title":"Process 1A: Create a FASTA genome index","text":"<p>Now we have our inputs set up we can move onto the processes. In our first process we will create a genome index using samtools.</p> <p>The first process has the following structure:</p> <ul> <li>Name: <code>prepare_genome_samtools</code></li> <li>Command: create a genome index for the genome fasta with samtools</li> <li>Input: the genome fasta file</li> <li>Output: the samtools genome index file</li> </ul> <p>Problem #2</p> <p>Copy the code below and paste it at the end of <code>main.nf</code>, removing the previous workflow block. Be careful not to accidentally have multiple workflow blocks.</p> <p>Your aim is to replace the <code>BLANK</code> placeholder with the the correct process call.</p> <pre><code>/*\n * Process 1A: Create a FASTA genome index with samtools\n */\n\nprocess prepare_genome_samtools {\n    container 'quay.io/biocontainers/samtools:1.3.1--h0cf4675_11'\n\n    input:\n    path genome // (1)!\n\n    output:\n    path \"${genome}.fai\" // (2)!\n\n    script:\n    \"\"\"\n    samtools faidx ${genome}\n    \"\"\"\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    BLANK\n}\n</code></pre> <ol> <li>Take as input the genome file from the <code>params.genome</code> parameter and refer to it within the process as <code>genome</code></li> <li>The output is a path named just like the input file but adding <code>.fai</code> to the end, e.g. <code>ref_genome.fa.fai</code></li> </ol> <p>In plain english, the process could be written as:</p> <ul> <li>A process called <code>prepare_genome_samtools</code></li> <li>takes as input the genome file</li> <li>and creates as output a genome index file</li> <li>script: using samtools create the genome index from the genome file</li> </ul> Solution <pre><code>/*\n * Process 1A: Create a FASTA genome index with samtools\n */\n\nprocess prepare_genome_samtools {\n    container 'quay.io/biocontainers/samtools:1.3.1--h0cf4675_11'\n\n    input:\n    path genome\n\n    output:\n    path \"${genome}.fai\"\n\n    script:\n    \"\"\"\n    samtools faidx ${genome}\n    \"\"\"\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome) // (1)!\n}\n</code></pre> <ol> <li>The solution is to provide <code>params.genome</code> as input to the <code>prepare_genome_samtools</code> process.</li> </ol> <p>Warning</p> <p><code>params.genome</code> is just a regular variable, not a channel, but when passed as input to a process, it's automatically converted into a value channel.</p> <p>Now when we run the pipeline, we see that the process <code>prepare_genome_samtools</code> is submitted:</p> <p><pre><code>nextflow run main.nf -resume\n</code></pre> <pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `main.nf` [cranky_bose] - revision: d1df5b7267\nexecutor &gt;  local (1)\n[cd/47f882] process &gt; prepare_genome_samtools [100%] 1 of 1 \u2714\n</code></pre></p>"},{"location":"other/hands_on/04_implementation/#process-1b-create-a-fasta-genome-sequence-dictionary-with-picard-for-gatk","title":"Process 1B: Create a FASTA genome sequence dictionary with Picard for GATK","text":"<p>Our first process created the genome index for GATK using samtools. For the next process we must do something very similar, this time creating a genome sequence dictionary using Picard.</p> <p>The next process should have the following structure:</p> <ul> <li>Name: <code>prepare_genome_picard</code></li> <li>Command: create a genome dictionary for the genome fasta with Picard tools</li> <li>Input: the genome fasta file</li> <li>Output: the genome dictionary file</li> </ul> <p>Problem #3</p> <p>Your aim is to replace the <code>BLANK</code> placeholder with the the correct process call.</p> <p>Copy the code below and paste it at the end of <code>main.nf</code>, removing the previous workflow block.</p> <p>Info</p> <p>You can choose any channel output name that makes sense to you.</p> <pre><code>/*\n * Process 1B: Create a FASTA genome sequence dictionary with Picard for GATK\n */\n\nprocess prepare_genome_picard {\n    container 'quay.io/biocontainers/picard:1.141--hdfd78af_6'\n\n    input:\n    path genome\n\n    output:\n    path \"${genome.baseName}.dict\"\n\n    script:\n    \"\"\"\n    picard CreateSequenceDictionary R= ${genome} O= ${genome.baseName}.dict\n    \"\"\"\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    BLANK\n}\n</code></pre> <p>Note</p> <p><code>.baseName</code> returns the filename without the file suffix. If <code>\"${genome}\"</code> is <code>human.fa</code>, then <code>\"${genome.baseName}.dict\"</code> would be <code>human.dict</code>.</p> Solution <pre><code>/*\n * Process 1B: Create a FASTA genome sequence dictionary with Picard for GATK\n */\n\nprocess prepare_genome_picard {\n    container 'quay.io/biocontainers/picard:1.141--hdfd78af_6'\n\n    input:\n    path genome\n\n    output:\n    path \"${genome.baseName}.dict\"\n\n    script:\n    \"\"\"\n    picard CreateSequenceDictionary R= ${genome} O= ${genome.baseName}.dict\n    \"\"\"\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    prepare_genome_picard(params.genome) // (1)!\n}\n</code></pre> <ol> <li>The solution is to provide <code>params.genome</code> as input to the <code>prepare_genome_picard</code> process.</li> </ol>"},{"location":"other/hands_on/04_implementation/#process-1c-create-star-genome-index-file","title":"Process 1C: Create STAR genome index file","text":"<p>Next we must create a genome index for the STAR mapping software.</p> <p>The next process has the following structure:</p> <ul> <li>Name: <code>prepare_star_genome_index</code></li> <li>Command: create a STAR genome index for the genome fasta</li> <li>Input: the genome fasta file</li> <li>Output: a directory containing the STAR genome index</li> </ul> <p>Problem #4</p> <p>This is a similar exercise as problem 3.</p> <pre><code>/*\n * Process 1C: Create the genome index file for STAR\n */\n\nprocess prepare_star_genome_index {\n    container 'quay.io/biocontainers/star:2.7.10b--h6b7c446_1'\n\n    input:\n    path genome\n\n    output:\n    path 'genome_dir' // (1)!\n\n    script: // (2)!\n    \"\"\"\n    mkdir -p genome_dir\n\n    STAR --runMode genomeGenerate \\\n         --genomeDir genome_dir \\\n         --genomeFastaFiles ${genome} \\\n         --runThreadN ${task.cpus}\n    \"\"\"\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    prepare_genome_picard(params.genome)\n    BLANK\n}\n</code></pre> <ol> <li>The <code>output</code> is a <code>path</code> called <code>genome_dir</code></li> <li>Before running <code>STAR</code>, it creates the output directory that will contain the resulting STAR genome index.</li> </ol> <p>Info</p> <p>The output of the STAR genomeGenerate command is specified here as <code>genome_dir</code>.</p> Solution <pre><code>/*\n* Process 1C: Create the genome index file for STAR\n*/\n\nprocess prepare_star_genome_index {\n    container 'quay.io/biocontainers/star:2.7.10b--h6b7c446_1'\n\n    input:\n    path genome\n\n    output:\n    path 'genome_dir'\n\n    script:\n    \"\"\"\n    mkdir -p genome_dir\n\n    STAR --runMode genomeGenerate \\\n         --genomeDir genome_dir \\\n         --genomeFastaFiles ${genome} \\\n         --runThreadN ${task.cpus}\n    \"\"\"\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    prepare_genome_picard(params.genome)\n    prepare_star_genome_index(params.genome)\n}\n</code></pre> <p>Note</p> <p>The path in this case is a directory however it makes no difference, it could be a text file, for example.</p>"},{"location":"other/hands_on/04_implementation/#process-1d-filtered-and-recoded-set-of-variants","title":"Process 1D: Filtered and recoded set of variants","text":"<p>Next on to something a little more tricky. The next process takes two inputs: the variants file and the blacklist file.</p> <p>Info</p> <p>In Nextflow, tuples can be defined in the input or output using the <code>tuple</code> qualifier.</p> <p>The next process has the following structure:</p> <ul> <li>Name: <code>prepare_vcf_file</code></li> <li>Command: create a filtered and recoded set of variants</li> <li>Input:</li> <li>the variants file</li> <li>the blacklisted regions file</li> <li>Output: a tuple containing the filtered/recoded VCF file and the tab index (TBI) file.</li> </ul> <p>Problem #5</p> <p>You must fill in the <code>BLANK</code>.</p> <pre><code>/*\n * Process 1D: Create a file containing the filtered and recoded set of variants\n */\n\nprocess prepare_vcf_file {\n    container 'quay.io/biocontainers/mulled-v2-b9358559e3ae3b9d7d8dbf1f401ae1fcaf757de3:ac05763cf181a5070c2fdb9bb5461f8d08f7b93b-0'\n\n    input:\n    path variantsFile\n    path blacklisted\n\n    output: // (1)!\n    tuple path(\"${variantsFile.baseName}.filtered.recode.vcf.gz\"),\n          path(\"${variantsFile.baseName}.filtered.recode.vcf.gz.tbi\")\n\n    script:\n    \"\"\"\n    vcftools --gzvcf ${variantsFile} -c \\\n             --exclude-bed ${blacklisted} \\\n             --recode | bgzip -c \\\n             &gt; ${variantsFile.baseName}.filtered.recode.vcf.gz\n\n    tabix ${variantsFile.baseName}.filtered.recode.vcf.gz\n    \"\"\"\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    prepare_genome_picard(params.genome)\n    prepare_star_genome_index(params.genome)\n    BLANK\n}\n</code></pre> <ol> <li>The output is a tuple with two paths that here are files</li> </ol> <p>Broken down, here is what the script is doing:</p> <pre><code>vcftools --gzvcf ${variantsFile} -c \\ # (1)!\n         --exclude-bed ${blacklisted} \\ # (2)!\n         --recode | bgzip -c \\\n         &gt; ${variantsFile.baseName}.filtered.recode.vcf.gz # (3)!\n\ntabix ${variantsFile.baseName}.filtered.recode.vcf.gz # (4)!\n</code></pre> <ol> <li>The <code>variantsFile</code> variable contains the path to the file with the known variants</li> <li>The <code>blacklisted</code> variable contains the path to the file with the genomic locations which are known to produce artifacts and spurious variants</li> <li>The <code>&gt;</code> symbol is used to redirect the output to the file specified after it</li> <li><code>tabix</code> is used here to create the second output that we want to consider from this process</li> </ol> Solution <pre><code>/*\n * Process 1D: Create a file containing the filtered and recoded set of variants\n */\n\nprocess prepare_vcf_file {\n    container 'quay.io/biocontainers/mulled-v2-b9358559e3ae3b9d7d8dbf1f401ae1fcaf757de3:ac05763cf181a5070c2fdb9bb5461f8d08f7b93b-0'\n\n    input:\n    path variantsFile\n    path blacklisted\n\n    output:\n    tuple path(\"${variantsFile.baseName}.filtered.recode.vcf.gz\"),\n          path(\"${variantsFile.baseName}.filtered.recode.vcf.gz.tbi\")\n\n    script:\n    \"\"\"\n    vcftools --gzvcf ${variantsFile} -c \\\n             --exclude-bed ${blacklisted} \\\n             --recode | bgzip -c \\\n             &gt; ${variantsFile.baseName}.filtered.recode.vcf.gz\n\n    tabix ${variantsFile.baseName}.filtered.recode.vcf.gz\n    \"\"\"\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    prepare_genome_picard(params.genome)\n    prepare_star_genome_index(params.genome)\n    prepare_vcf_file(params.variants, params.blacklist)\n}\n</code></pre> <p>Try running the pipeline from the project directory with:</p> <pre><code>nextflow run main.nf -resume\n</code></pre> <p>Congratulations! Part 1 is now complete.</p> <p>We have all the data prepared and into channels ready for the more serious steps.</p>"},{"location":"other/hands_on/04_implementation/#process-2-star-mapping","title":"Process 2: STAR Mapping","text":"<p>In this process, for each sample, we align the reads to our genome using the STAR index we created previously.</p> <p>The process has the following structure:</p> <ul> <li>Name: <code>rnaseq_mapping_star</code></li> <li>Command: mapping of the RNA-Seq reads using STAR</li> <li>Input:</li> <li>the genome fasta file</li> <li>the STAR genome index</li> <li>a tuple containing the replicate id and paired read files</li> <li>Output: a tuple containing replicate id, aligned bam file &amp; aligned bam file index</li> </ul> <p>Problem #6</p> <p>Copy the code below and paste it at the end of <code>main.nf</code>, removing the previous workflow block.</p> <p>You must fill the <code>BLANK</code> space with the correct process call and inputs.</p> <pre><code>/*\n * Process 2: Align RNA-Seq reads to the genome with STAR\n */\n\nprocess rnaseq_mapping_star {\n    container 'quay.io/biocontainers/mulled-v2-52f8f283e3c401243cee4ee45f80122fbf6df3bb:e3bc54570927dc255f0e580cba1789b64690d611-0'\n\n    input:\n    path genome\n    path genomeDir\n    tuple val(replicateId), path(reads)\n\n    output:\n    tuple val(replicateId),\n          path('Aligned.sortedByCoord.out.bam'),\n          path('Aligned.sortedByCoord.out.bam.bai')\n\n    script:\n    \"\"\"\n    STAR --genomeDir ${genomeDir} \\\n         --readFilesIn ${reads} \\\n         --runThreadN ${task.cpus} \\\n         --readFilesCommand zcat \\\n         --outFilterType BySJout \\\n         --alignSJoverhangMin 8 \\\n         --alignSJDBoverhangMin 1 \\\n         --outFilterMismatchNmax 999\n\n    mkdir -p genomeDir\n    STAR --runMode genomeGenerate \\\n         --genomeDir genomeDir \\\n         --genomeFastaFiles ${genome} \\\n         --sjdbFileChrStartEnd SJ.out.tab \\\n         --sjdbOverhang 75 \\\n         --runThreadN ${task.cpus}\n\n    STAR --genomeDir genomeDir \\\n         --readFilesIn ${reads} \\\n         --runThreadN ${task.cpus} \\\n         --readFilesCommand zcat \\\n         --outFilterType BySJout \\\n         --alignSJoverhangMin 8 \\\n         --alignSJDBoverhangMin 1 \\\n         --outFilterMismatchNmax 999 \\\n         --outSAMtype BAM SortedByCoordinate \\\n         --outSAMattrRGline ID:${replicateId} LB:library PL:illumina \\\n                            PU:machine SM:GM12878\n\n    samtools index Aligned.sortedByCoord.out.bam\n    \"\"\"\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    prepare_genome_picard(params.genome)\n    prepare_star_genome_index(params.genome)\n    prepare_vcf_file(params.variants, params.blacklist)\n\n    BLANK\n}\n</code></pre> <p>Info</p> <p>The final command produces a bam index which is the full filename with an additional <code>.bai</code> suffix.</p> <p>Broken down, here is what the script is doing:</p> <pre><code>STAR --genomeDir ${genomeDir} \\ # (1)!\n     --readFilesIn ${reads} \\\n     --runThreadN ${task.cpus} \\\n     --readFilesCommand zcat \\\n     --outFilterType BySJout \\\n     --alignSJoverhangMin 8 \\\n     --alignSJDBoverhangMin 1 \\\n     --outFilterMismatchNmax 999\n\nmkdir -p genomeDir # (2)!\nSTAR --runMode genomeGenerate \\ # (3)!\n     --genomeDir genomeDir \\\n     --genomeFastaFiles ${genome} \\\n     --sjdbFileChrStartEnd SJ.out.tab \\\n     --sjdbOverhang 75 \\\n     --runThreadN ${task.cpus}\n\nSTAR --genomeDir genomeDir \\ # (4)!\n     --readFilesIn ${reads} \\\n     --runThreadN ${task.cpus} \\\n     --readFilesCommand zcat \\\n     --outFilterType BySJout \\\n     --alignSJoverhangMin 8 \\\n     --alignSJDBoverhangMin 1 \\\n     --outFilterMismatchNmax 999 \\\n     --outSAMtype BAM SortedByCoordinate \\\n     --outSAMattrRGline ID:${replicateId} LB:library PL:illumina \\\n                        PU:machine SM:GM12878\n\nsamtools index Aligned.sortedByCoord.out.bam # (5)!\n</code></pre> <ol> <li>Align reads to the reference genome</li> <li>Create output directory <code>genomeDir</code> for next STAR calls within the same task</li> <li>2nd pass (improve alignments using table of splice junctions and create a new index)</li> <li>Final read alignments</li> <li>Index the BAM file</li> </ol> Solution <pre><code>/*\n * Process 2: Align RNA-Seq reads to the genome with STAR\n */\n\nprocess rnaseq_mapping_star {\n    container 'quay.io/biocontainers/mulled-v2-52f8f283e3c401243cee4ee45f80122fbf6df3bb:e3bc54570927dc255f0e580cba1789b64690d611-0'\n\n    input:\n    path genome\n    path genomeDir\n    tuple val(replicateId), path(reads)\n\n    output:\n    tuple val(replicateId),\n          path('Aligned.sortedByCoord.out.bam'),\n          path('Aligned.sortedByCoord.out.bam.bai')\n\n    script:\n    \"\"\"\n    STAR --genomeDir ${genomeDir} \\\n         --readFilesIn ${reads} \\\n         --runThreadN ${task.cpus} \\\n         --readFilesCommand zcat \\\n         --outFilterType BySJout \\\n         --alignSJoverhangMin 8 \\\n         --alignSJDBoverhangMin 1 \\\n         --outFilterMismatchNmax 999\n\n    mkdir -p genomeDir\n    STAR --runMode genomeGenerate \\\n         --genomeDir genomeDir \\\n         --genomeFastaFiles ${genome} \\\n         --sjdbFileChrStartEnd SJ.out.tab \\\n         --sjdbOverhang 75 \\\n         --runThreadN ${task.cpus}\n\n    STAR --genomeDir genomeDir \\\n         --readFilesIn ${reads} \\\n         --runThreadN ${task.cpus} \\\n         --readFilesCommand zcat \\\n         --outFilterType BySJout \\\n         --alignSJoverhangMin 8 \\\n         --alignSJDBoverhangMin 1 \\\n         --outFilterMismatchNmax 999 \\\n         --outSAMtype BAM SortedByCoordinate \\\n         --outSAMattrRGline ID:${replicateId} LB:library PL:illumina \\\n                            PU:machine SM:GM12878\n\n    samtools index Aligned.sortedByCoord.out.bam\n    \"\"\"\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    prepare_genome_picard(params.genome)\n    prepare_star_genome_index(params.genome)\n    prepare_vcf_file(params.variants, params.blacklist)\n\n    rnaseq_mapping_star(params.genome,\n                        prepare_star_genome_index.out,\n                        reads_ch)\n}\n</code></pre> <p>The next step is a filtering step using GATK. For each sample, we split all the reads that contain N characters in their CIGAR string.</p>"},{"location":"other/hands_on/04_implementation/#process-3-gatk-split-on-n","title":"Process 3: GATK Split on N","text":"<p>The process creates <code>k+1</code> new reads (where <code>k</code> is the number of <code>N</code> cigar elements) that correspond to the segments of the original read beside/between the splicing events represented by the <code>N</code>s in the original CIGAR.</p> <p>The next process has the following structure:</p> <ul> <li>Name: <code>rnaseq_gatk_splitNcigar</code></li> <li>Command: split reads on Ns in CIGAR string using GATK</li> <li>Input:</li> <li>the genome fasta file</li> <li>the genome index made with samtools</li> <li>the genome dictionary made with picard</li> <li>a tuple containing replicate id, aligned bam file and aligned bam file index from the STAR mapping</li> <li>Output: a tuple containing the replicate id, the split bam file and the split bam index file</li> </ul> <p>Problem #7</p> <p>Copy the code below and paste it at the end of <code>main.nf</code>, removing the previous workflow block.</p> <p>You must fill the <code>BLANK</code> space with the correct process call and inputs.</p> <p>Warning</p> <p>There is an optional <code>tag</code> line added to the start of this process. The <code>tag</code> line allows you to assign a name to a specific task (single instance of a process). This is particularly useful when there are many samples/replicates which pass through the same process.</p> <pre><code>/*\n * Process 3: GATK Split on N\n */\n\nprocess rnaseq_gatk_splitNcigar {\n    container 'quay.io/broadinstitute/gotc-prod-gatk:1.0.0-4.1.8.0-1626439571'\n    tag \"${replicateId}\" // (1)!\n\n    input:\n    path genome // (2)!\n    path index // (3)!\n    path genome_dict // (4)!\n    tuple val(replicateId), path(bam), path(bai) // (5)!\n\n    output:\n    tuple val(replicateId), path('split.bam'), path('split.bai') // (6)!\n\n    script:\n    \"\"\"\n    java -jar /usr/gitc/GATK35.jar -T SplitNCigarReads \\\n                                   -R ${genome} -I ${bam} \\\n                                   -o split.bam \\\n                                   -rf ReassignOneMappingQuality \\\n                                   -RMQF 255 -RMQT 60 \\\n                                   -U ALLOW_N_CIGAR_READS \\\n                                   --fix_misencoded_quality_scores\n    \"\"\"\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    prepare_genome_picard(params.genome)\n    prepare_star_genome_index(params.genome)\n    prepare_vcf_file(params.variants, params.blacklist)\n\n    rnaseq_mapping_star(params.genome,\n                        prepare_star_genome_index.out,\n                        reads_ch)\n\n    BLANK\n}\n</code></pre> <ol> <li><code>tag</code> line using the replicate id as the tag.</li> <li>The genome fasta file</li> <li>The genome index in the output channel from the <code>prepare_genome_samtools</code> process</li> <li>The genome dictionary in the output channel from the <code>prepare_genome_picard</code> process</li> <li>The tuple containing the aligned reads in the output channel from the <code>rnaseq_mapping_star</code> process</li> <li>A tuple containing the replicate id, the split bam file and the split bam index</li> </ol> <p>Info</p> <p>The GATK command above automatically creates a bam index (<code>.bai</code>) of the <code>split.bam</code> output file</p> <p>Tip</p> <p>A <code>tag</code> line would also be useful in Process 2</p> <p>Broken down, here is what the script is doing:</p> <pre><code>java -jar /usr/gitc/GATK35.jar -T SplitNCigarReads \\ # (1)!\n                               -R ${genome} -I ${bam} \\ # (2)!\n                               -o split.bam \\ # (3)!\n                               -rf ReassignOneMappingQuality \\ # (4)!\n                               -RMQF 255 -RMQT 60 \\\n                               -U ALLOW_N_CIGAR_READS \\\n                               --fix_misencoded_quality_scores\n</code></pre> <ol> <li>Use the <code>SplitNCigarReads</code> tool from GATK</li> <li>Set the reference sequence file (<code>-R</code>) and the input files containing reads (<code>-I</code>)</li> <li>Write the output BAM file to <code>split.bam</code> (<code>-o</code>)</li> <li>Reassign mapping qualities too</li> </ol> Solution <pre><code>/*\n * Process 3: GATK Split on N\n */\n\nprocess rnaseq_gatk_splitNcigar {\n    container 'quay.io/broadinstitute/gotc-prod-gatk:1.0.0-4.1.8.0-1626439571'\n    tag \"${replicateId}\"\n\n    input:\n    path genome\n    path index\n    path genome_dict\n    tuple val(replicateId), path(bam), path(bai)\n\n    output:\n    tuple val(replicateId), path('split.bam'), path('split.bai')\n\n    script:\n    \"\"\"\n    java -jar /usr/gitc/GATK35.jar -T SplitNCigarReads \\\n                                   -R ${genome} -I ${bam} \\\n                                   -o split.bam \\\n                                   -rf ReassignOneMappingQuality \\\n                                   -RMQF 255 -RMQT 60 \\\n                                   -U ALLOW_N_CIGAR_READS \\\n                                   --fix_misencoded_quality_scores\n\n    \"\"\"\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    prepare_genome_picard(params.genome)\n    prepare_star_genome_index(params.genome)\n    prepare_vcf_file(params.variants, params.blacklist)\n\n    rnaseq_mapping_star(params.genome,\n                        prepare_star_genome_index.out,\n                        reads_ch)\n\n    rnaseq_gatk_splitNcigar(params.genome,\n                            prepare_genome_samtools.out,\n                            prepare_genome_picard.out,\n                            rnaseq_mapping_star.out)\n}\n</code></pre> <p>Next we perform a Base Quality Score Recalibration step using GATK.</p>"},{"location":"other/hands_on/04_implementation/#process-4-gatk-recalibrate","title":"Process 4: GATK Recalibrate","text":"<p>This step uses GATK to detect systematic errors in the base quality scores, select unique alignments and then index the resulting bam file with samtools. You can find details of the specific GATK BaseRecalibrator parameters here.</p> <p>The next process has the following structure:</p> <ul> <li>Name: <code>rnaseq_gatk_recalibrate</code></li> <li>Command: recalibrate reads from each replicate using GATK</li> <li>Input</li> <li>the genome fasta file</li> <li>the genome index made with samtools</li> <li>the genome dictionary made with picard</li> <li>a tuple containing replicate id, aligned bam file and aligned bam file index from process 3</li> <li>a tuple containing the filtered/recoded VCF file and the tab index (TBI) file from process 1D</li> <li>Output: a tuple containing the sample id, the unique bam file and the unique bam index file</li> </ul> <p>Problem #8</p> <p>Copy the code below and paste it at the end of <code>main.nf</code>, removing the previous workflow block.</p> <p>Your aim is to replace the <code>BLANK</code> placeholder with the the correct process call.</p> <pre><code>/*\n * Process 4: GATK Recalibrate\n */\n\nprocess rnaseq_gatk_recalibrate {\n    container 'quay.io/biocontainers/mulled-v2-aa1d7bddaee5eb6c4cbab18f8a072e3ea7ec3969:f963c36fd770e89d267eeaa27cad95c1c3dbe660-0'\n    tag \"${replicateId}\"\n\n    input:\n    path genome\n    path index\n    path dict\n    tuple val(replicateId), path(bam), path(bai) // (1)!\n    tuple path(prepared_variants_file),\n          path(prepared_variants_file_index) // (2)!\n\n    output: // (3)!\n    tuple val(sampleId),\n          path(\"${replicateId}.final.uniq.bam\"),\n          path(\"${replicateId}.final.uniq.bam.bai\")\n\n    script:\n    sampleId = replicateId.replaceAll(/[12]$/,'') // (4)!\n    \"\"\"\n    gatk3 -T BaseRecalibrator \\\n          --default_platform illumina \\\n          -cov ReadGroupCovariate \\\n          -cov QualityScoreCovariate \\\n          -cov CycleCovariate \\\n          -knownSites ${prepared_variants_file} \\\n          -cov ContextCovariate \\\n          -R ${genome} -I ${bam} \\\n          --downsampling_type NONE \\\n          -nct ${task.cpus} \\\n          -o final.rnaseq.grp\n\n    gatk3 -T PrintReads \\\n          -R ${genome} -I ${bam} \\\n          -BQSR final.rnaseq.grp \\\n          -nct ${task.cpus} \\\n          -o final.bam\n\n    (samtools view -H final.bam; samtools view final.bam | \\\n    grep -w 'NH:i:1') | samtools view -Sb -  &gt; ${replicateId}.final.uniq.bam\n\n    samtools index ${replicateId}.final.uniq.bam\n    \"\"\"\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    prepare_genome_picard(params.genome)\n    prepare_star_genome_index(params.genome)\n    prepare_vcf_file(params.variants, params.blacklist)\n\n    rnaseq_mapping_star(params.genome,\n                        prepare_star_genome_index.out,\n                        reads_ch)\n\n    rnaseq_gatk_splitNcigar(params.genome,\n                            prepare_genome_samtools.out,\n                            prepare_genome_picard.out,\n                            rnaseq_mapping_star.out)\n\n    BLANK\n}\n</code></pre> <ol> <li>The tuple containing the split reads in the output channel from the <code>rnaseq_gatk_splitNcigar</code> process.</li> <li>The tuple containing the filtered/recoded VCF file and the tab index (TBI) file in the output channel from the <code>prepare_vcf_file</code> process.</li> <li>The tuple containing the replicate id, the unique bam file and the unique bam index file which goes into two channels.</li> <li>Line specifying the filename of the output bam file</li> </ol> <p>Broken down, here is what the script is doing:</p> <pre><code>gatk3 -T BaseRecalibrator \\ # (1)!\n      --default_platform illumina \\\n      -cov ReadGroupCovariate \\\n      -cov QualityScoreCovariate \\\n      -cov CycleCovariate \\\n      -knownSites ${prepared_variants_file} \\\n      -cov ContextCovariate \\\n      -R ${genome} -I ${bam} \\\n      --downsampling_type NONE \\\n      -nct ${task.cpus} \\\n      -o final.rnaseq.grp\n\ngatk3 -T PrintReads \\\n      -R ${genome} -I ${bam} \\\n      -BQSR final.rnaseq.grp \\\n      -nct ${task.cpus} \\\n      -o final.bam\n\n(samtools view -H final.bam; samtools view final.bam | \\\ngrep -w 'NH:i:1') | samtools view -Sb -  &gt; ${replicateId}.final.uniq.bam # (2)!\n\nsamtools index ${replicateId}.final.uniq.bam # (3)!\n</code></pre> <ol> <li>Generates recalibration table for Base Quality Score Recalibration (BQSR)</li> <li>Select only unique alignments, no multimaps</li> <li>Index BAM file</li> </ol> Solution <pre><code>/*\n * Process 4: GATK Recalibrate\n */\n\nprocess rnaseq_gatk_recalibrate {\n    container 'quay.io/biocontainers/mulled-v2-aa1d7bddaee5eb6c4cbab18f8a072e3ea7ec3969:f963c36fd770e89d267eeaa27cad95c1c3dbe660-0'\n    tag \"${replicateId}\"\n\n    input:\n    path genome\n    path index\n    path dict\n    tuple val(replicateId), path(bam), path(bai)\n    tuple path(prepared_variants_file),\n          path(prepared_variants_file_index)\n\n    output:\n    tuple val(sampleId),\n          path(\"${replicateId}.final.uniq.bam\"),\n          path(\"${replicateId}.final.uniq.bam.bai\")\n\n    script:\n    sampleId = replicateId.replaceAll(/[12]$/,'')\n    \"\"\"\n    gatk3 -T BaseRecalibrator \\\n          --default_platform illumina \\\n          -cov ReadGroupCovariate \\\n          -cov QualityScoreCovariate \\\n          -cov CycleCovariate \\\n          -knownSites ${prepared_variants_file} \\\n          -cov ContextCovariate \\\n          -R ${genome} -I ${bam} \\\n          --downsampling_type NONE \\\n          -nct ${task.cpus} \\\n          -o final.rnaseq.grp\n\n    gatk3 -T PrintReads \\\n          -R ${genome} -I ${bam} \\\n          -BQSR final.rnaseq.grp \\\n          -nct ${task.cpus} \\\n          -o final.bam\n\n    (samtools view -H final.bam; samtools view final.bam | \\\n    grep -w 'NH:i:1') | samtools view -Sb -  &gt; ${replicateId}.final.uniq.bam\n\n    samtools index ${replicateId}.final.uniq.bam\n    \"\"\"\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    prepare_genome_picard(params.genome)\n    prepare_star_genome_index(params.genome)\n    prepare_vcf_file(params.variants, params.blacklist)\n\n    rnaseq_mapping_star(params.genome,\n                        prepare_star_genome_index.out,\n                        reads_ch)\n\n    rnaseq_gatk_splitNcigar(params.genome,\n                            prepare_genome_samtools.out,\n                            prepare_genome_picard.out,\n                            rnaseq_mapping_star.out)\n\n    rnaseq_gatk_recalibrate(params.genome,\n                            prepare_genome_samtools.out,\n                            prepare_genome_picard.out,\n                            rnaseq_gatk_splitNcigar.out,\n                            prepare_vcf_file.out)\n}\n</code></pre> <p>Now we are ready to perform the variant calling with GATK.</p>"},{"location":"other/hands_on/04_implementation/#process-5-gatk-variant-calling","title":"Process 5: GATK Variant Calling","text":"<p>This steps call variants with GATK HaplotypeCaller. You can find details of the specific GATK HaplotypeCaller parameters here.</p> <p>The next process has the following structure:</p> <ul> <li>Name: <code>rnaseq_call_variants</code></li> <li>Command: variant calling of each sample using GATK</li> <li>Input:</li> <li>the genome fasta file</li> <li>the genome index made with samtools</li> <li>the genome dictionary made with picard</li> <li>a tuple containing replicate id, aligned bam file and aligned bam file index from process 4</li> <li>Output: a tuple containing the sample id the resulting variant calling file (vcf)</li> </ul> <p>Problem #9</p> <p>Copy the code below and paste it at the end of <code>main.nf</code>, removing the previous workflow block. Be careful not to accidentally have multiple workflow blocks.</p> <p>Warning</p> <p>Note that in process 4, we used the sampleID (not replicateID) as the first element of the tuple in the output. Now we combine the replicates by grouping them on the sample ID. It follows from this that process 4 is run one time per replicate and process 5 is run one time per sample.</p> <p>Your aim is to replace the <code>BLANK</code> placeholder with the the correct process call.</p> <pre><code>/*\n * Process 5: GATK Variant Calling\n */\n\nprocess rnaseq_call_variants {\n    container 'quay.io/broadinstitute/gotc-prod-gatk:1.0.0-4.1.8.0-1626439571'\n    tag \"${sampleId}\" // (1)!\n\n    input:\n    path genome // (2)!\n    path index // (3)!\n    path dict  // (4)!\n    tuple val(sampleId), path(bam), path(bai) // (5)!\n\n    output:\n    tuple val(sampleId), path('final.vcf') // (6)!\n\n    script:\n    \"\"\"\n    echo \"${bam.join('\\n')}\" &gt; bam.list\n\n    java -jar /usr/gitc/GATK35.jar -T HaplotypeCaller \\\n                                   -R ${genome} -I bam.list \\\n                                   -dontUseSoftClippedBases \\\n                                   -stand_call_conf 20.0 \\\n                                   -o output.gatk.vcf.gz\n\n    java -jar /usr/gitc/GATK35.jar -T VariantFiltration \\\n                                   -R ${genome} -V output.gatk.vcf.gz \\\n                                   -window 35 -cluster 3 \\\n                                   -filterName FS -filter \"FS &gt; 30.0\" \\\n                                   -filterName QD -filter \"QD &lt; 2.0\" \\\n                                   -o final.vcf\n    \"\"\"\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    prepare_genome_picard(params.genome)\n    prepare_star_genome_index(params.genome)\n    prepare_vcf_file(params.variants, params.blacklist)\n\n    rnaseq_mapping_star(params.genome,\n                        prepare_star_genome_index.out,\n                        reads_ch)\n\n    rnaseq_gatk_splitNcigar(params.genome,\n                            prepare_genome_samtools.out,\n                            prepare_genome_picard.out,\n                            rnaseq_mapping_star.out)\n\n    rnaseq_gatk_recalibrate(params.genome,\n                            prepare_genome_samtools.out,\n                            prepare_genome_picard.out,\n                            rnaseq_gatk_splitNcigar.out,\n                            prepare_vcf_file.out)\n\n    BLANK\n}\n</code></pre> <ol> <li><code>tag</code> line with the using the sample id as the tag.</li> <li>The genome fasta file.</li> <li>The genome index in the output channel from the <code>prepare_genome_samtools</code> process.</li> <li>The genome dictionary in the output channel from the <code>prepare_genome_picard</code> process.</li> <li>The tuples grouped by sampleID in the output channel from the <code>rnaseq_gatk_recalibrate</code> process.</li> <li>The tuple containing the sample ID and final VCF file.</li> </ol> <p>Broken down, here is what the script is doing:</p> <pre><code>echo \"${bam.join('\\n')}\" &gt; bam.list # (1)!\n\njava -jar /usr/gitc/GATK35.jar -T HaplotypeCaller \\ # (2)!\n                               -R ${genome} -I bam.list \\\n                               -dontUseSoftClippedBases \\\n                               -stand_call_conf 20.0 \\\n                               -o output.gatk.vcf.gz\n\njava -jar /usr/gitc/GATK35.jar -T VariantFiltration \\ # (3)!\n                               -R ${genome} -V output.gatk.vcf.gz \\\n                               -window 35 -cluster 3 \\\n                               -filterName FS -filter \"FS &gt; 30.0\" \\\n                               -filterName QD -filter \"QD &lt; 2.0\" \\\n                               -o final.vcf\n</code></pre> <ol> <li>Create a file containing a list of BAM files</li> <li>Variant calling step</li> <li>Variant filtering step</li> </ol> Solution <pre><code>/*\n * Process 5: GATK Variant Calling\n */\n\nprocess rnaseq_call_variants {\n    container 'quay.io/broadinstitute/gotc-prod-gatk:1.0.0-4.1.8.0-1626439571'\n    tag \"${sampleId}\"\n\n    input:\n    path genome\n    path index\n    path dict\n    tuple val(sampleId), path(bam), path(bai)\n\n    output:\n    tuple val(sampleId), path('final.vcf')\n\n    script:\n    \"\"\"\n    echo \"${bam.join('\\n')}\" &gt; bam.list\n\n    java -jar /usr/gitc/GATK35.jar -T HaplotypeCaller \\\n                                   -R ${genome} -I bam.list \\\n                                   -dontUseSoftClippedBases \\\n                                   -stand_call_conf 20.0 \\\n                                   -o output.gatk.vcf.gz\n\n    java -jar /usr/gitc/GATK35.jar -T VariantFiltration \\\n                                   -R ${genome} -V output.gatk.vcf.gz \\\n                                   -window 35 -cluster 3 \\\n                                   -filterName FS -filter \"FS &gt; 30.0\" \\\n                                   -filterName QD -filter \"QD &lt; 2.0\" \\\n                                   -o final.vcf\n    \"\"\"\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    prepare_genome_picard(params.genome)\n    prepare_star_genome_index(params.genome)\n    prepare_vcf_file(params.variants, params.blacklist)\n\n    rnaseq_mapping_star(params.genome,\n                        prepare_star_genome_index.out,\n                        reads_ch)\n\n    rnaseq_gatk_splitNcigar(params.genome,\n                            prepare_genome_samtools.out,\n                            prepare_genome_picard.out,\n                            rnaseq_mapping_star.out)\n\n    rnaseq_gatk_recalibrate(params.genome,\n                            prepare_genome_samtools.out,\n                            prepare_genome_picard.out,\n                            rnaseq_gatk_splitNcigar.out,\n                            prepare_vcf_file.out)\n\n    rnaseq_gatk_recalibrate\n        .out\n        | groupTuple\n        | set { recalibrated_samples } // (1)!\n\n    rnaseq_call_variants(params.genome,\n                         prepare_genome_samtools.out,\n                         prepare_genome_picard.out,\n                         recalibrated_samples)\n}\n</code></pre> <ol> <li>New channel to aggregate the <code>bam</code> files from different replicates into sample level.</li> </ol>"},{"location":"other/hands_on/04_implementation/#processes-6a-and-6b-ase-rna-editing","title":"Processes 6A and 6B: ASE &amp; RNA Editing","text":"<p>In the final steps we will create processes for Allele-Specific Expression and RNA Editing Analysis.</p> <p>We must process the VCF result to prepare variants file for allele specific expression (ASE) analysis. We will implement both processes together.</p> <p>You should implement two processes having the following structure:</p> <ul> <li>1st process</li> <li>Name: <code>post_process_vcf</code></li> <li>Command: post-process the variant calling file (vcf) of each sample</li> <li>Input:<ul> <li>tuple containing the sample ID and vcf file</li> <li>a tuple containing the filtered/recoded VCF file and the tab index (TBI) file from process 1D</li> </ul> </li> <li>Output: a tuple containing the sample id, the variant calling file (vcf) and a file containing common SNPs</li> <li>2nd process</li> <li>Name: <code>prepare_vcf_for_ase</code></li> <li>Command: prepare the VCF for allele specific expression (ASE) and generate a figure in R.</li> <li>Input: a tuple containing the sample id, the variant calling file (vcf) and a file containing common SNPs</li> <li>Output:<ul> <li>a tuple containing the sample ID and known SNPs in the sample for ASE</li> <li>a figure of the SNPs generated in R as a PDF file</li> </ul> </li> </ul> <p>Problem #10</p> <p>Here we introduce the <code>publishDir</code> directive. This allows us to specify a location for the outputs of the process. See here for more details.</p> <p>You must have the output of process 6A become the input of process 6B.</p> <pre><code>/*\n * Processes 6: ASE &amp; RNA Editing\n */\n\nprocess post_process_vcf {\n    container 'quay.io/biocontainers/mulled-v2-b9358559e3ae3b9d7d8dbf1f401ae1fcaf757de3:ac05763cf181a5070c2fdb9bb5461f8d08f7b93b-0'\n    tag \"${sampleId}\"\n    publishDir \"${params.results}/${sampleId}\" // (1)!\n\n    input:\n    tuple val(sampleId), path('final.vcf')\n    tuple path('filtered.recode.vcf.gz'),\n          path('filtered.recode.vcf.gz.tbi')\n\n    output:\n    tuple val(sampleId),\n          path('final.vcf'),\n          path('commonSNPs.diff.sites_in_files')\n\n    script:\n    '''\n    grep -v '#' final.vcf | awk '$7~/PASS/' | perl -ne 'chomp($_); \\\n         ($dp)=$_=~/DP\\\\=(\\\\d+)\\\\;/; if($dp&gt;=8){print $_.\"\\\\n\"};' \\\n         &gt; result.DP8.vcf\n\n    vcftools --vcf result.DP8.vcf --gzdiff filtered.recode.vcf.gz \\\n             --diff-site --out commonSNPs\n    '''\n}\n\nprocess prepare_vcf_for_ase {\n    container 'cbcrg/callings-with-gatk:latest'\n    tag \"${sampleId}\"\n    publishDir \"${params.results}/${sampleId}\"\n\n    input:\n    tuple val(sampleId),\n          path('final.vcf'),\n          path('commonSNPs.diff.sites_in_files')\n\n    output:\n    tuple val(sampleId), path('known_snps.vcf'), emit: vcf_for_ASE\n    path 'AF.histogram.pdf'                    , emit: gghist_pdfs\n\n    script:\n    '''\n    awk 'BEGIN{OFS=\"\\t\"} $4~/B/{print $1,$2,$3}' \\\n        commonSNPs.diff.sites_in_files  &gt; test.bed\n\n    vcftools --vcf final.vcf --bed test.bed --recode --keep-INFO-all \\\n             --stdout &gt; known_snps.vcf\n\n    grep -v '#'  known_snps.vcf | awk -F '\\\\t' '{print $10}' \\\n                | awk -F ':' '{print $2}' | perl -ne 'chomp($_); \\\n                @v=split(/\\\\,/,$_); if($v[0]!=0 ||$v[1] !=0)\\\n                {print  $v[1]/($v[1]+$v[0]).\"\\\\n\"; }' | awk '$1!=1' \\\n                &gt;AF.4R\n\n    gghist.R -i AF.4R -o AF.histogram.pdf\n    '''\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    prepare_genome_picard(params.genome)\n    prepare_star_genome_index(params.genome)\n    prepare_vcf_file(params.variants, params.blacklist)\n\n    rnaseq_mapping_star(params.genome,\n                        prepare_star_genome_index.out,\n                        reads_ch)\n\n    rnaseq_gatk_splitNcigar(params.genome,\n                            prepare_genome_samtools.out,\n                            prepare_genome_picard.out,\n                            rnaseq_mapping_star.out)\n\n    rnaseq_gatk_recalibrate(params.genome,\n                            prepare_genome_samtools.out,\n                            prepare_genome_picard.out,\n                            rnaseq_gatk_splitNcigar.out,\n                            prepare_vcf_file.out)\n\n    rnaseq_gatk_recalibrate\n        .out\n        | groupTuple\n        | set { recalibrated_samples }\n\n    rnaseq_call_variants(params.genome,\n                         prepare_genome_samtools.out,\n                         prepare_genome_picard.out,\n                         recalibrated_samples)\n\n    BLANK\n}\n</code></pre> <ol> <li>The path to the <code>publishDir</code> process directive consists of variables that will be evaluated before saving the files over there</li> </ol> Solution <pre><code>/*\n * Processes 6: ASE &amp; RNA Editing\n */\n\nprocess post_process_vcf {\n    container 'quay.io/biocontainers/mulled-v2-b9358559e3ae3b9d7d8dbf1f401ae1fcaf757de3:ac05763cf181a5070c2fdb9bb5461f8d08f7b93b-0'\n    tag \"${sampleId}\"\n    publishDir \"${params.results}/${sampleId}\"\n\n    input:\n    tuple val(sampleId), path('final.vcf')\n    tuple path('filtered.recode.vcf.gz'),\n          path('filtered.recode.vcf.gz.tbi')\n\n    output:\n    tuple val(sampleId),\n          path('final.vcf'),\n          path('commonSNPs.diff.sites_in_files')\n\n    script:\n    '''\n    grep -v '#' final.vcf | awk '$7~/PASS/' | perl -ne 'chomp($_); \\\n            ($dp)=$_=~/DP\\\\=(\\\\d+)\\\\;/; if($dp&gt;=8){print $_.\"\\\\n\"};' \\\n            &gt; result.DP8.vcf\n\n    vcftools --vcf result.DP8.vcf --gzdiff filtered.recode.vcf.gz \\\n             --diff-site --out commonSNPs\n    '''\n}\n\nprocess prepare_vcf_for_ase {\n    container 'cbcrg/callings-with-gatk:latest'\n    tag \"${sampleId}\"\n    publishDir \"${params.results}/${sampleId}\"\n\n    input:\n    tuple val(sampleId),\n          path('final.vcf'),\n          path('commonSNPs.diff.sites_in_files')\n\n    output:\n    tuple val(sampleId), path('known_snps.vcf'), emit: vcf_for_ASE\n    path 'AF.histogram.pdf'                    , emit: gghist_pdfs\n\n    script:\n    '''\n    awk 'BEGIN{OFS=\"\\t\"} $4~/B/{print $1,$2,$3}' \\\n        commonSNPs.diff.sites_in_files  &gt; test.bed\n\n    vcftools --vcf final.vcf --bed test.bed --recode --keep-INFO-all \\\n             --stdout &gt; known_snps.vcf\n\n    grep -v '#'  known_snps.vcf | awk -F '\\\\t' '{print $10}' \\\n                | awk -F ':' '{print $2}' | perl -ne 'chomp($_); \\\n                @v=split(/\\\\,/,$_); if($v[0]!=0 ||$v[1] !=0)\\\n                {print  $v[1]/($v[1]+$v[0]).\"\\\\n\"; }' | awk '$1!=1' \\\n                &gt;AF.4R\n\n    gghist.R -i AF.4R -o AF.histogram.pdf\n    '''\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    prepare_genome_picard(params.genome)\n    prepare_star_genome_index(params.genome)\n    prepare_vcf_file(params.variants, params.blacklist)\n\n    rnaseq_mapping_star(params.genome,\n                        prepare_star_genome_index.out,\n                        reads_ch)\n\n    rnaseq_gatk_splitNcigar(params.genome,\n                            prepare_genome_samtools.out,\n                            prepare_genome_picard.out,\n                            rnaseq_mapping_star.out)\n\n    rnaseq_gatk_recalibrate(params.genome,\n                            prepare_genome_samtools.out,\n                            prepare_genome_picard.out,\n                            rnaseq_gatk_splitNcigar.out,\n                            prepare_vcf_file.out)\n\n    rnaseq_gatk_recalibrate\n        .out\n        | groupTuple\n        | set { recalibrated_samples }\n\n    rnaseq_call_variants(params.genome,\n                         prepare_genome_samtools.out,\n                         prepare_genome_picard.out,\n                         recalibrated_samples)\n\n    post_process_vcf(rnaseq_call_variants.out,\n                     prepare_vcf_file.out)\n    prepare_vcf_for_ase(post_process_vcf.out)\n}\n</code></pre> <p>The final step is the GATK ASEReadCounter.</p> <p>Problem #11</p> <p>We have seen the basics of using processes in Nextflow. Yet one of the features of Nextflow is the operations that can be performed on channels outside of processes. See here for details on the specific operators.</p> <p>Before we perform the GATK ASEReadCounter process, we must group the data for allele-specific expression. To do this we must combine channels.</p> <p>The output channel of the <code>rnaseq_gatk_recalibrate</code> process (<code>rnaseq_gatk_recalibrate.out</code>) emites tuples having the following structure, holding the final BAM/BAI files:</p> <pre><code>&lt; sample_id, file_bam, file_bai &gt;\n</code></pre> <p>The <code>vcf_for_ASE</code> channel emits tuples having the following structure:</p> <pre><code>&lt; sample_id, output.vcf &gt;\n</code></pre> <p>In the first operation, the BAM files are grouped together by sample id.</p> <p>Next, this resulting channel is merged with the VCFs having the same sample id.</p> <p>We must take the merged channel and creates a channel named <code>grouped_vcf_bam_bai_ch</code> emitting the following tuples:</p> <pre><code>&lt; sample_id, file_vcf, List[file_bam], List[file_bai] &gt;\n</code></pre> <p>Your aim is to fill in the <code>BLANKS</code> below.</p> <pre><code>recalibrated_samples\n    .BLANK // (1)!\n    .map { BLANK } // (2)!\n    .set { BLANK } // (3)!\n</code></pre> <ol> <li>An operator that joins two channels taking a key into consideration. See here for more details</li> <li>The map operator can apply any function to every item on a channel. In this case we take our tuple from the previous step, define the separate elements and create a new tuple.</li> <li>Rename the resulting as <code>grouped_vcf_bam_bai_ch</code></li> </ol> Solution <pre><code>workflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    prepare_genome_picard(params.genome)\n    prepare_star_genome_index(params.genome)\n    prepare_vcf_file(params.variants, params.blacklist)\n\n    rnaseq_mapping_star(params.genome,\n                        prepare_star_genome_index.out,\n                        reads_ch)\n\n    rnaseq_gatk_splitNcigar(params.genome,\n                            prepare_genome_samtools.out,\n                            prepare_genome_picard.out,\n                            rnaseq_mapping_star.out)\n\n    rnaseq_gatk_recalibrate(params.genome,\n                            prepare_genome_samtools.out,\n                            prepare_genome_picard.out,\n                            rnaseq_gatk_splitNcigar.out,\n                            prepare_vcf_file.out)\n\n    rnaseq_gatk_recalibrate\n        .out\n        | groupTuple\n        | set { recalibrated_samples }\n\n    rnaseq_call_variants(params.genome,\n                         prepare_genome_samtools.out,\n                         prepare_genome_picard.out,\n                         rnaseq_gatk_recalibrate.out)\n\n    post_process_vcf(rnaseq_call_variants.out,\n                     prepare_vcf_file.out)\n    prepare_vcf_for_ase(post_process_vcf.out)\n\n    recalibrated_samples\n        .join(prepare_vcf_for_ase.out.vcf_for_ASE)\n        .map { meta, bams, bais, vcf -&gt; [meta, vcf, bams, bais] }\n        .set { grouped_vcf_bam_bai_ch }\n}\n</code></pre>"},{"location":"other/hands_on/04_implementation/#process-7-allele-specific-expression-analysis-with-gatk-asereadcounter","title":"Process 7: Allele-Specific Expression analysis with GATK ASEReadCounter","text":"<p>Now we are ready for the final process.</p> <p>The next process has the following structure:</p> <ul> <li>Name: <code>ASE_knownSNPs</code></li> <li>Command: calculate allele counts at a set of positions with GATK tools</li> <li>Input:</li> <li>genome fasta file</li> <li>genome index file from samtools</li> <li>genome dictionary file</li> <li>the <code>grouped_vcf_bam_bai_ch</code> channel</li> <li>Output: the allele specific expression file (<code>ASE.tsv</code>)</li> </ul> <p>Problem #12</p> <p>You should construct the process from scratch, add the process call and inputs to the workflow block and run the pipeline in its entirety.</p> <pre><code>echo \"${bam.join('\\n')}\" &gt; bam.list\n\njava -jar /usr/gitc/GATK35.jar -R ${genome} \\\n                               -T ASEReadCounter \\\n                               -o ASE.tsv \\\n                               -I bam.list \\\n                               -sites ${vcf}\n</code></pre> Solution <pre><code>/*\n * Processes 7: Allele-Specific Expression analysis with GATK ASEReadCounter\n */\n\nprocess ASE_knownSNPs {\n    container 'quay.io/broadinstitute/gotc-prod-gatk:1.0.0-4.1.8.0-1626439571'\n    tag \"${sampleId}\"\n    publishDir \"${params.results}/${sampleId}\"\n\n    input:\n    path genome\n    path index\n    path dict\n    tuple val(sampleId), path(vcf), path(bam), path(bai)\n\n    output:\n    path \"ASE.tsv\"\n\n    script:\n    \"\"\"\n    echo \"${bam.join('\\n')}\" &gt; bam.list\n\n    java -jar /usr/gitc/GATK35.jar -R ${genome} \\\n                                   -T ASEReadCounter \\\n                                   -o ASE.tsv \\\n                                   -I bam.list \\\n                                   -sites ${vcf}\n    \"\"\"\n}\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    prepare_genome_samtools(params.genome)\n    prepare_genome_picard(params.genome)\n    prepare_star_genome_index(params.genome)\n    prepare_vcf_file(params.variants, params.blacklist)\n\n    rnaseq_mapping_star(params.genome,\n                        prepare_star_genome_index.out,\n                        reads_ch)\n\n    rnaseq_gatk_splitNcigar(params.genome,\n                        prepare_genome_samtools.out,\n                        prepare_genome_picard.out,\n                        rnaseq_mapping_star.out)\n\n    rnaseq_gatk_recalibrate(params.genome,\n                            prepare_genome_samtools.out,\n                            prepare_genome_picard.out,\n                            rnaseq_gatk_splitNcigar.out,\n                            prepare_vcf_file.out)\n\n    rnaseq_gatk_recalibrate\n        .out\n        | groupTuple\n        | set { recalibrated_samples }\n\n    rnaseq_call_variants(params.genome,\n                         prepare_genome_samtools.out,\n                         prepare_genome_picard.out,\n                         recalibrated_samples)\n\n    post_process_vcf(rnaseq_call_variants.out,\n                     prepare_vcf_file.out)\n    prepare_vcf_for_ase(post_process_vcf.out)\n\n    recalibrated_samples\n        .join(prepare_vcf_for_ase.out.vcf_for_ASE)\n        .map { meta, bams, bais, vcf -&gt; [meta, vcf, bams, bais] }\n        .set { grouped_vcf_bam_bai_ch }\n\n    ASE_knownSNPs(params.genome,\n                  prepare_genome_samtools.out,\n                  prepare_genome_picard.out,\n                  grouped_vcf_bam_bai_ch)\n}\n</code></pre> <p>Congratulations! If you made it this far you now have all the basics to create your own Nextflow workflows.</p>"},{"location":"other/hands_on/04_implementation/#results-overview","title":"Results overview","text":"<p>For each processed sample the pipeline stores results into a folder named after the sample identifier. These folders are created in the directory specified as a parameter in <code>params.results</code>.</p> <p>Result files for this workshop can be found in the folder <code>results</code> within the current folder. There you should see a directory called <code>ENCSR000COQ/</code> containing the following files:</p>"},{"location":"other/hands_on/04_implementation/#variant-calls","title":"Variant calls","text":"<p><code>final.vcf</code></p> <p>This file contains all somatic variants (SNVs) called from RNAseq data. You will see variants that pass all filters, with the <code>PASS</code> keyword in the 7th field of the vcf file (<code>filter status</code>), and also those that did not pass one or more filters.</p> <p><code>commonSNPs.diff.sites_in_files</code></p> <p>Tab-separated file with comparison between variants obtained from RNAseq and \"known\" variants from DNA.</p> <p>The file is sorted by genomic position and contains 8 fields:</p> 1 <code>CHROM</code> chromosome name; 2 <code>POS1</code> position of the SNV in file #1 (RNAseq data); 3 <code>POS2</code> position of SNV in file #2 (DNA \"known\" variants); 4 <code>IN_FILE</code> flag whether SNV is present in the file #1 1, in the file #2 2, or in both files B; 5 <code>REF1</code> reference sequence in the file 1; 6 <code>REF2</code> reference sequence in the file 2; 7 <code>ALT1</code> alternative sequence in the file 1; 8 <code>ALT2</code> alternative sequence in the file 2 <p><code>known_snps.vcf</code></p> <p>Variants that are common to RNAseq and \"known\" variants from DNA.</p>"},{"location":"other/hands_on/04_implementation/#allele-specific-expression-quantification","title":"Allele specific expression quantification","text":"<p><code>ASE.tsv</code></p> <p>Tab-separated file with allele counts at common SNVs positions (only SNVs from the file <code>known_snps.vcf</code>)</p> <p>The file is sorted by coordinates and contains 13 fields:</p> 1 <code>contig</code> contig, scaffold or chromosome name of the variant 2 <code>position</code> position of the variant 3 <code>variant ID</code> variant ID in the dbSNP 4 <code>refAllele</code> reference allele sequence 5 <code>altAllele</code> alternate allele sequence 6 <code>refCount</code> number of reads that support the reference allele 7 <code>altCount</code> number of reads that support the alternate allele 8 <code>totalCount</code> total number of reads at the site that support both reference and alternate allele and any other alleles present at the site 9 <code>lowMAPQDepth</code> number of reads that have low mapping quality 10 <code>lowBaseQDepth</code> number of reads that have low base quality 11 <code>rawDepth</code> total number of reads at the site that support both reference and alternate allele and any other alleles present at the site 12 <code>otherBases</code> number of reads that support bases other than reference and alternate bases 13 <code>improperPairs</code> number of reads that have malformed pairs"},{"location":"other/hands_on/04_implementation/#allele-frequency-histogram","title":"Allele frequency histogram","text":"<p><code>AF.histogram.pdf</code></p> <p>This file contains a histogram plot of allele frequency for SNVs common to RNA-seq and \"known\" variants from DNA.</p>"},{"location":"other/hands_on/04_implementation/#bonus-step","title":"Bonus step","text":"<p>Until now the pipeline has been executed using just a single sample (<code>ENCSR000COQ1</code>).</p> <p>Now we can re-execute the pipeline specifying a large set of samples by using the command shown below:</p> <pre><code>nextflow run main.nf -resume --reads 'data/reads/ENCSR000C*_{1,2}.fastq.gz'\n</code></pre> <p>Or run the final version of the Nextflow pipeline that is already prepared for you:</p> <pre><code>nextflow run final_main.nf -resume\n</code></pre> <p>It will immediately print an output similar to the one below:</p> <pre><code>Launching `final_main.nf` [evil_almeida] DSL2 - revision: 09802c02ae\nexecutor &gt;  local (5)\n[6d/fafc39] process &gt; prepare_genome_samtools                [100%] 1 of 1, cached: 1 \u2714\n[38/5e729e] process &gt; prepare_genome_picard                  [100%] 1 of 1, cached: 1 \u2714\n[cb/7d9286] process &gt; prepare_star_genome_index              [100%] 1 of 1, cached: 1 \u2714\n[f7/3b5953] process &gt; prepare_vcf_file                       [100%] 1 of 1, cached: 1 \u2714\n[29/83ed80] process &gt; rnaseq_mapping_star (3)                [ 16%] 1 of 6, cached: 1\n[e6/332dcc] process &gt; rnaseq_gatk_splitNcigar (ENCSR000COQ1) [100%] 1 of 1, cached: 1\n[9d/359eac] process &gt; rnaseq_gatk_recalibrate (ENCSR000COQ1) [100%] 1 of 1, cached: 1\n[-        ] process &gt; rnaseq_call_variants                   -\n[-        ] process &gt; post_process_vcf                       -\n[-        ] process &gt; prepare_vcf_for_ase                    -\n[-        ] process &gt; ASE_knownSNPs                          -\n</code></pre> <p>As you can see, it won't process everything from scratch. Even though we're running a different Nextflwo script, cache from previous runs will be taken into consideration.</p> <p>At the end, you should see something like the output below:</p> <pre><code>N E X T F L O W  ~  version 23.04.3\nLaunching `final_main.nf` [cranky_rosalind] DSL2 - revision: 09802c02ae\nexecutor &gt;  local (34)\n[0a/e7bf16] process &gt; prepare_genome_samtools                [100%] 1 of 1 \u2714\n[87/233b95] process &gt; prepare_genome_picard                  [100%] 1 of 1 \u2714\n[25/228128] process &gt; prepare_star_genome_index              [100%] 1 of 1 \u2714\n[e3/ef7c40] process &gt; prepare_vcf_file                       [100%] 1 of 1 \u2714\n[14/3eccab] process &gt; rnaseq_mapping_star (6)                [100%] 6 of 6 \u2714\n[71/ac2c3e] process &gt; rnaseq_gatk_splitNcigar (ENCSR000CPO2) [100%] 6 of 6 \u2714\n[79/47a057] process &gt; rnaseq_gatk_recalibrate (ENCSR000CPO2) [100%] 6 of 6 \u2714\n[36/75c49b] process &gt; rnaseq_call_variants (ENCSR000CPO)     [100%] 3 of 3 \u2714\n[22/c86980] process &gt; post_process_vcf (ENCSR000CPO)         [100%] 3 of 3 \u2714\n[11/e66735] process &gt; prepare_vcf_for_ase (ENCSR000CPO)      [100%] 3 of 3 \u2714\n[7d/b7376e] process &gt; ASE_knownSNPs (ENCSR000CPO)            [100%] 3 of 3 \u2714\n</code></pre> <p>You can notice that this time the pipeline spawns the execution of more tasks because three samples have been provided instead of one.</p> <p>This shows the ability of Nextflow to implicitly handle multiple parallel task executions depending on the specified pipeline input dataset.</p> <p>A fully functional version of this pipeline is available at the following GitHub repository: CalliNGS-NF.</p>"},{"location":"other/nf_customize/","title":"Configure nf-core","text":"<p>nf-core is a community effort to collaborate on a curated set of analysis pipelines built using Nextflow. It provides a standardized set of best practices, guidelines, and templates for building and sharing bioinformatics pipelines.</p> <p>nf-core pipelines are designed to be modular, scalable, and portable, allowing researchers to easily adapt and execute them using their data and compute resources. They are designed to work out of the box. However, many researchers will want to customize their execution to deploy them to different infrastructures, activate or deactivate pipeline features, and selectively customize tool parameters. For this, researchers need to understand how these pipelines fit together and how to apply configuration options.</p> <p>In this workshop, using the nf-core/demo pipeline as an example, you will learn how customize parameters and configuration files.</p> <p>Let's get started!</p> <p></p>"},{"location":"other/nf_customize/#learning-objectives","title":"Learning objectives","text":"<p>By the end of this workshop you will be able to:</p> <ul> <li>Discuss the ways an nf-core pipeline can be customized</li> <li>Identify the files and code contributing to the default pipeline configuration</li> <li>Utilize nf-core tooling to list, launch, and download pipelines</li> </ul>"},{"location":"other/nf_customize/#audience-prerequisites","title":"Audience &amp; prerequisites","text":"<p>Please note that this is not a beginner's workshop and familiarity with Nextflow, the command line, and common file formats is assumed.</p> <p>Prerequisites</p> <ul> <li>A GitHub account</li> <li>Experience with command line</li> <li>Experience writing pipelines with Nextflow</li> </ul>"},{"location":"other/nf_customize/01_orientation/","title":"Orientation","text":"<p>The GitHub Codespaces environment contains some test data that will be used in this workshop.</p> <p>Note</p> <p>Follow this link if you have not yet setup your GitHub Codespaces environment.</p>"},{"location":"other/nf_customize/01_orientation/#getting-started","title":"Getting started","text":"<p>You will complete this module in the <code>nf-customize/</code> folder.</p> <p>In this folder you will find three pairs of zipped fastq files (<code>*.fastq.gz</code>) in a <code>data/</code> folder and an example samplesheet (<code>samplesheet.csv</code>) in a <code>scripts/</code> folder.</p> <pre><code>.\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 gut_1.fastq.gz\n\u2502   \u251c\u2500\u2500 gut_2.fastq.gz\n\u2502   \u251c\u2500\u2500 liver_1.fastq.gz\n\u2502   \u251c\u2500\u2500 liver_2.fastq.gz\n\u2502   \u251c\u2500\u2500 lung_1.fastq.gz\n\u2502   \u2514\u2500\u2500 lung_2.fastq.gz\n\u2514\u2500\u2500 scripts\n    \u2514\u2500\u2500 samplesheet.csv\n</code></pre> <p>These files will be used in this training module.</p> <p>Exercise</p> <p>Open the GitHub Codespaces environment switch to the <code>nf-customize</code> folder. View the files in this folder using the <code>tree</code> command:</p> <pre><code>cd /workspaces/training/nf-customize\ntree .\n</code></pre> <p>Congratulations! You are now ready to start the workshop!</p>"},{"location":"other/nf_customize/02_nf-core/","title":"What is nf-core?","text":"<p>nf-core is a community effort to collect a curated set of analysis pipelines built using Nextflow.</p> <p>nf-core provides a standardised set of best practices, guidelines, and templates for building and sharing bioinformatics pipelines. These pipelines are designed to be modular, scalable, and portable, allowing researchers to easily adapt and execute them using their own data and compute resources.</p> <p>One of the key benefits of nf-core is that it promotes open development, testing, and peer review, ensuring that the pipelines are robust, well-documented, and validated against real-world datasets. This helps to increase the reliability and reproducibility of bioinformatics analyses and ultimately enables researchers to accelerate their scientific discoveries.</p> <p>Key Features of nf-core pipelines</p> <ul> <li>Documentation</li> <li>nf-core pipelines have extensive documentation covering installation, usage, and description of output files to ensure that you won't be left in the dark.</li> <li>CI Testing</li> <li>Every time a change is made to the pipeline code, nf-core pipelines use continuous-integration testing to ensure that nothing has broken.</li> <li>Stable Releases</li> <li>nf-core pipelines use GitHub releases to tag stable versions of the code and software, making pipeline runs totally reproducible.</li> <li>Packaged software</li> <li>Pipeline dependencies are automatically downloaded and handled using Docker, Singularity, Conda, or other software management tools. There is no need for any software installations.</li> <li>Portable and reproducible</li> <li>nf-core pipelines follow best practices to ensure maximum portability and reproducibility. The large community makes the pipelines exceptionally well-tested and easy to execute.</li> <li>Cloud-ready</li> <li>nf-core pipelines are tested on AWS after every major release. You can even browse results live on the website and use outputs for your own benchmarking.</li> </ul> <p>nf-core is published in Nature Biotechnology: Nat Biotechnol 38, 276\u2013278 (2020). Nature Biotechnology. An updated preprint is available at bioRxiv.</p>"},{"location":"other/nf_customize/02_nf-core/#nf-core-pipelines","title":"nf-core pipelines","text":"<p>There are currently 113 nf-core pipelines. These pipelines are at various stages of development, with 68 released, 32 under development, and 13 archived (October 2024).</p> <p>The nf-core website hosts a full list of pipelines, as well as their documentation, which can be explored.</p> <p></p> <p>Each released pipeline has a dedicated page that includes 6 documentation sections:</p> <ul> <li>Introduction: An introduction and overview of the pipeline</li> <li>Usage: Descriptions of how to execute the pipeline</li> <li>Parameters: Grouped pipeline parameters with descriptions</li> <li>Output: Descriptions and examples of the expected output files</li> <li>Results: Example output files generated from the full test dataset</li> <li>Releases &amp; Statistics: Pipeline version history and statistics</li> </ul> <p>Each section should be explored by a user to understand what the pipeline does and how it can be configured.</p> <p>Exercise</p> <p>Explore the nf-core website to see the range of resources available.</p>"},{"location":"other/nf_customize/02_nf-core/#pulling-an-nf-core-pipeline","title":"Pulling an nf-core pipeline","text":"<p>Unless you intend to develop an nf-core pipeline independently, you do not need to clone a copy of a pipeline.</p> <p>Instead, use Nextflow\u2019s <code>pull</code> command:</p> <pre><code>nextflow pull nf-core/demo\n</code></pre> <p>The <code>nextflow run</code> command</p> <p>The <code>nextflow run</code> command will also automatically <code>pull</code> the pipeline.</p> <p>Nextflow will <code>pull</code> the pipelines default GitHub branch if a pipeline version is not specified. The master branch is the default branch for nf-core pipelines with a stable release and the dev branch for pipelines that are still being developed.</p> <p>Pipelines pulled from GitHub using Nextflow are automatically stored in a Nextflow assets folder (default: <code>$HOME/.nextflow/assets/</code>).</p> <p>nf-core pipelines use GitHub releases to tag stable versions of the code and software. You can execute different versions of a pipeline using the <code>-revision</code> or <code>-r</code> option.</p> <p>Similarly, you can use the <code>-r</code> option to specify a specific GitHub branch. For example, the <code>dev</code> branch of the <code>nf-core/demo</code> pipeline could be pulled with the command:</p> <pre><code>nextflow pull nf-core/demo -r dev\n</code></pre> <p>If updates to a remote pipeline have been made, run the pull command to update or revert your local copy.</p> <p>Exercise</p> <p>Pull the <code>nf-core/demo</code> pipeline:</p> <pre><code>nextflow pull nf-core/demo\n</code></pre> <p>Use the <code>list</code> command to view your cached pipelines:</p> <pre><code>nextflow list\n</code></pre> <p>View your pulled pipelines in the nextflow assets folder:</p> <pre><code>ls $HOME/.nextflow/assets/\n</code></pre> <p>Congratulations! You have now ready to start running the <code>nf-core/demo</code> pipeline!</p>"},{"location":"other/nf_customize/03_execution/","title":"<code>nf-core/demo</code>","text":"<p><code>nf-core/demo</code> is a simple nf-core style pipeline for workshops and demonstrations.</p> <p>It was created using the full nf-core template and is designed to run and configure quickly.</p> <p>The <code>nf-core/demo</code> pipeline consists of three processes:</p> <ul> <li>(<code>FastQC</code>): Read quality control</li> <li>(<code>SEQTK_TRIM</code>): Trim low quality bases from FASTQ files</li> <li>(<code>MULTIQC</code>): Present quality control reports for raw reads</li> </ul> <p><code>nf-core/demo</code> takes a samplesheet that contains paths to FASTQ files as an input and will produce four output folders with logs and reports:</p> <ul> <li><code>fastqc/</code></li> <li><code>*_fastqc.html</code>: FastQC report containing quality metrics.</li> <li><code>*_fastqc.zip</code>: Zip archive containing the FastQC report, tab-delimited data file and plot images.</li> <li><code>fq/</code></li> <li><code>*.fastp.html</code>: Trimmed fq files.</li> <li><code>multiqc/</code></li> <li><code>multiqc_report.html</code>: a standalone HTML file that can be viewed in your web browser.</li> <li><code>multiqc_data/</code>: directory containing parsed statistics from the different tools used in the pipeline.</li> <li><code>multiqc_plots/</code>: directory containing static images from the report in various formats.</li> <li><code>pipeline_info/</code></li> <li>Reports generated by Nextflow</li> <li>Reports generated by nf-core</li> <li>Parameters file</li> </ul> <p>You can view the code for this pipeline on the <code>nf-core/demo</code> GitHub repository.</p> <p>To help you understand the expectations for running an nf-core pipeline, they come with extensive documentation about its parameters, usage, and outputs.</p> <p>The documentation for the <code>nf-core/demo</code> pipeline can be found on the nf-core/demo pipelines page.</p>"},{"location":"other/nf_customize/03_execution/#required-inputs","title":"Required inputs","text":"<p>Before running any nf-core pipeline you will need to check if any parameters are required.</p> <p>You can find required parameters on the pipelines parameters page.</p> <p>The parameters page of the <code>nf-core/demo</code> pipeline shows that this pipeline requires two parameters (<code>--input</code> and <code>--outdir</code>) to run.</p> <p></p> <p>Without these, the pipeline will not launch and will throw an error.</p>"},{"location":"other/nf_customize/03_execution/#-input","title":"<code>--input</code>","text":"<p>The <code>--input</code> parameter requires a path to comma-separated file (CSV) containing information about the samples in the experiment:</p> <pre><code>--input 'path/to/samplesheet.csv'\n</code></pre> <p>The nf-core/demo usage documentation describes the required <code>--input</code> as a comma-separated file (<code>.csv</code>). The <code>.csv</code> file must contain 3 columns with the headers <code>sample</code>, <code>fastq_1</code>, and <code>fastq_2</code>.</p> <p>The samplesheet file may consist of both single- and paired-end data and may look something like the one below:</p> samplesheet.csv<pre><code>sample,fastq_1,fastq_2\nSAMPLE1_PE,path/to/sample1_R1.fastq.gz,path/to/sample1_R2.fastq.gz\nSAMPLE2_PE,path/to/sample2_R1.fastq.gz,path/to/sample2_R2.fastq.gz\nSAMPLE3_SE,path/to/sample3_R1.fastq.gz,\n</code></pre>"},{"location":"other/nf_customize/03_execution/#-outdir","title":"<code>--outdir</code>","text":"<p>The <code>--output</code> parameter is used to name the output directory where the results will be saved. It takes a string as its input:</p> <pre><code>--output results\n</code></pre> <p>Note</p> <p>You do not need to create the output folder before you run the pipeline.</p>"},{"location":"other/nf_customize/03_execution/#testing-nf-coredemo-with-profiles","title":"Testing <code>nf-core/demo</code> with profiles","text":"<p>A profile is a set of configuration attributes that can be added to your execution command by using the <code>-profile</code> option:</p> <pre><code>-profile &lt;profile name&gt;\n</code></pre> <p>Configuration profiles are defined using the special scope <code>profile</code> within configuration files. Profiles group the attributes that belong to the same profile using a common prefix:</p> example.config<pre><code>profiles {\n  foo {\n    process.memory = '2 GB'\n  }\n\n  bar {\n    process.memory = '4 GB'\n  }\n}\n</code></pre>"},{"location":"other/nf_customize/03_execution/#-profile-test","title":"<code>-profile test</code>","text":"<p>Every nf-core pipeline comes with a <code>test</code> profile. This is a minimal set of configuration settings for the pipeline to run using a small test dataset that is hosted on the nf-core/test-datasets repository.</p> <p>The <code>test</code> profile is expected to run and can be used to help diagnose local issues before you scale up your analysis.</p> <p>The <code>test</code> profile for <code>nf-core/demo</code> is shown below:</p> conf/test.config<pre><code>/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    Nextflow config file for running minimal tests\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    Defines input files and everything required to run a fast and simple pipeline test.\n\n    Use as follows:\n        nextflow run nf-core/demo -profile test,&lt;docker/singularity&gt; --outdir &lt;OUTDIR&gt;\n\n----------------------------------------------------------------------------------------\n*/\n\nprocess {\n    resourceLimits = [\n        cpus: 4,\n        memory: '15.GB',\n        time: '1.h'\n    ]\n}\n\nparams {\n    config_profile_name        = 'Test profile'\n    config_profile_description = 'Minimal test dataset to check pipeline function'\n\n    // Input data\n    input  = 'https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/samplesheet/samplesheet_test_illumina_amplicon.csv'\n}\n</code></pre> <p>The <code>nf-core/demo</code> <code>test</code> profile already contains the input parameter (explained in more detail below). This means that the <code>--input</code> parameter does not need to be added to the execution command. However, the <code>outdir</code> parameter is not included in the <code>test</code> profile and must be added to the execution command using the <code>--outdir</code> flag.</p> <pre><code>nextflow run nf-core/demo -profile test --outdir results\n</code></pre> <p>Exercise</p> <p>Execute the <code>nf-core/demo</code> pipeline with the <code>test</code> profile and an output directory named <code>results</code>:</p> <pre><code>nextflow run nf-core/demo -profile test --outdir results\n</code></pre> <p>This execution is expected to fail!</p> <p>As the software required to run each process (e.g., seqtk) is not available in the GitHub Codespaces environment the exercise above is expected to fail:</p> <pre><code>Caused by:\n  Process `NFCORE_DEMO:DEMO:SEQTK_TRIM (SAMPLE2_PE)` terminated with an error exit status (127)\n&lt;truncated&gt;\n</code></pre> <p>Fortunately, nf-core pipelines come packed with directives for containers and environments that can be flexibly enabled using profiles for different software (e.g., <code>docker</code>, <code>singularity</code>, and <code>conda</code>):</p> <pre><code>-profile singularity\n</code></pre> <p>In GitHub Codespaces, you can add the <code>singularity</code> profile to your execution command and Nextflow will download and enable Singularity software images to run each process.</p> <p>The singularity profile is defined in the nextflow.config file in the main pipeline repository.</p> nextflow.config<pre><code>singularity {\n    singularity.enabled     = true\n    singularity.autoMounts  = true\n    conda.enabled           = false\n    docker.enabled          = false\n    podman.enabled          = false\n    shifter.enabled         = false\n    charliecloud.enabled    = false\n    apptainer.enabled       = false\n}\n</code></pre> <p>Multiple config files</p> <p>Multiple profiles can be included by separating them with a comma (e.g., <code>-profile test,singularity</code>).</p> <p>Exercise</p> <p>Amend your run command by adding the singularity profile:</p> <pre><code>nextflow run nf-core/demo -profile test,singularity --outdir results\n</code></pre> <p>The <code>nf-core/demo</code> pipeline should now run successfully!</p> <p>Note</p> <p>Singularity must be installed for this command to run.</p>"},{"location":"other/nf_customize/03_execution/#using-your-own-data","title":"Using your own data","text":"<p>Instead of using the <code>test</code> profile you can use the <code>--input</code> parameter to choose your own sample sheet as an input.</p> <p>As described above, the input is a CSV file with 3 columns and the headers <code>sample</code>, <code>fastq_1</code>, and <code>fastq_2</code>.</p> <p>The nf-core/demo pipeline will auto-detect whether a sample is single- or paired-end and if a sample has been sequenced more than once using the information provided in the sample sheet by default.</p> <p>Exercise</p> <p>Create a sample sheet for the paired-end reads for gut, liver, and lung samples in the data folder:</p> <ol> <li>Create a CSV file named <code>samplesheet.csv</code>:</li> </ol> <pre><code>code samplesheet.csv\n</code></pre> <ol> <li>Add the header line, and, for each sample, an id and the complete paths to the paired-end reads:</li> </ol> samplesheet.csv<pre><code>sample,fastq_1,fastq_2\ngut,/workspaces/training/nf-customize/data/gut_1.fastq.gz,/workspaces/training/nf-customize/data/gut_2.fastq.gz\nliver,/workspaces/training/nf-customize/data/liver_1.fastq.gz,/workspaces/training/nf-customize/data/liver_2.fastq.gz\nlung,/workspaces/training/nf-customize/data/lung_1.fastq.gz,/workspaces/training/nf-customize/data/lung_2.fastq.gz\n</code></pre> <p>Make sure you save this file in your working directory (<code>/workspaces/training/nf-customize/</code>)</p> <p>You can use you new samplesheet with the <code>--input</code> parameter in your execution command.</p> <p>In this case, the other parameters in the test profile (e.g., <code>config_profile_name</code>) can be ignored as they are not explicitly required by the pipeline or in this GitHub Codespaces environment.</p> <p>Exercise</p> <p>Run the <code>nf-core/demo</code> pipeline with the <code>singularity</code> profile and your newly created samplesheet as your input.</p> <pre><code>nextflow run nf-core/demo -profile singularity --input samplesheet.csv --outdir results\n</code></pre> <p>The pipeline should run successfully!</p> <p>Congratulations! You have now run the <code>nf-core/demo</code> pipeline using you own dataset!</p>"},{"location":"other/nf_customize/04_config/","title":"Configuration options","text":"<p>Each nf-core pipeline comes with a set of \u201csensible defaults\u201d. While the defaults are a great place to start, you will certainly want to modify these to fit your own purposes and system requirements.</p> <p>You do not need to edit the pipeline code to configure nf-core pipelines.</p> <p>Nextflow will look for configuration files in several locations when it is launched. As each source can contain conflicting settings, the sources are ranked to decide which settings to apply.</p> <p>Configuration sources are reported below and listed in order of priority:</p> <ol> <li>Parameters specified on the command line (<code>--parameter</code>)</li> <li>Parameters that are provided using the <code>-params-file</code> option</li> <li>Config file that are provided using the <code>-c</code> option</li> <li>The config file named <code>nextflow.config</code> in the current directory</li> <li>The config file named <code>nextflow.config</code> in the pipeline project directory</li> <li>The config file <code>$HOME/.nextflow/config</code></li> <li>Values defined within the pipeline script itself (e.g., <code>main.nf</code>)</li> </ol> <p>While some of these files are already included in the nf-core pipeline repository (e.g., the <code>nextflow.config</code> file in the nf-core pipeline repository), some are automatically identified on your local system (e.g., the <code>nextflow.config</code> in the launch directory), and others are only included if they are specified using <code>run</code> options (e.g., <code>-params-file</code>, and <code>-c</code>).</p> <p>Understanding how and when these files are interpreted by Nextflow is critical for the accurate configuration of a pipeline execution.</p>"},{"location":"other/nf_customize/04_config/#parameters","title":"Parameters","text":"<p>Parameters are pipeline specific settings that can be used to customize the execution of a pipeline.</p> <p>At the highest level, parameters can be customized using the command line. Any parameter can be configured on the command line by prefixing the parameter name with a double dash (<code>--</code>):</p> <pre><code>--&lt;parameter&gt;\n</code></pre> <p>Depending on the parameter type, you may be required to add additional information after your parameter flag. For example, for a string parameter, you would add the string after the parameter flag. For example, the <code>outdir</code> parameter required by the <code>nf-core/demo</code> pipeline:</p> <pre><code>nextflow nf-core/demo --outdir results\n</code></pre> <p>Every nf-core pipeline has a full list of parameters on the nf-core website. You will also be shown a description and the type of the parameter when viewing these parameters. Some parameters will also have additional text to help you understand how a parameter should be used.</p> <p>Parameters and their descriptions can also be viewed in the command line using the <code>run</code> command with the <code>--help</code> parameter.</p> <p>Exercise</p> <p>View the parameters for the <code>nf-core/demo</code> pipeline using the command line:</p> <pre><code>nextflow run nf-core/demo --help\n</code></pre> <p>You can also view these on the nf-core/demo parameters page.</p>"},{"location":"other/nf_customize/04_config/#default-configuration-files","title":"Default configuration files","text":"<p>All parameters have a default configuration that is defined using the <code>nextflow.config</code> file in the pipeline project directory. Most parameters are set to <code>null</code> or <code>false</code> by default.</p> <p>There are also several <code>includeConfig</code> statements in the <code>nextflow.config</code> file that are used to include additional <code>.config</code> files from the <code>conf/</code> folder. Each additional <code>.config</code> file contains categorized configuration information for your pipeline execution, some of which can be optionally included:</p> <ul> <li><code>base.config</code></li> <li>Included by the pipeline by default.</li> <li>Generous resource allocations using labels.</li> <li>Does not specify any method for software management and expects software to be available (or specified elsewhere).</li> <li><code>igenomes.config</code></li> <li>Included by the pipeline by default.</li> <li>Default configuration to access reference files stored on AWS iGenomes.</li> <li><code>igenomes_ignored.config</code></li> <li>Empty genomes dictionary to use when igenomes is ignored.</li> <li><code>modules.config</code></li> <li>Included by the pipeline by default.</li> <li>Module-specific configuration options (both mandatory and optional).</li> <li><code>test.config</code></li> <li>Only included if specified as a profile.</li> <li>A configuration profile to test the pipeline with a small test dataset.</li> <li><code>test_full.config</code></li> <li>Only included if specified as a profile.</li> <li>A configuration profile to test the pipeline with a full-size test dataset.</li> </ul> <p>Note</p> <p>Some configuration files contain the definition of profiles. For example, the <code>docker</code>, <code>singularity</code>, and <code>conda</code> profiles are defined in the <code>nextflow.config</code> file in the pipeline project directory.</p> <p>Profiles used by nf-core pipelines can be broadly categorized into two groups:</p> <ul> <li>Software management profiles</li> <li>Profiles for the management of software using software management tools, for example, <code>docker</code>, <code>singularity</code>, and <code>conda</code>.</li> <li>Test profiles</li> <li>Profiles to execute the pipeline with a standardized set of test data and parameters, for example, <code>test</code> and <code>test_full</code>.</li> </ul> <p>nf-core pipelines are required to define software containers and environments that can be activated using profiles. Although it is possible to run the pipelines with software installed by other methods (e.g., environment modules or manual installation), using Docker or Singularity is more sharable, convenient, and reproducible.</p>"},{"location":"other/nf_customize/04_config/#shared-configuration-files","title":"Shared configuration files","text":"<p>An <code>includeConfig</code> statement in the <code>nextflow.config</code> file is also used to include custom institutional profiles that have been submitted to the nf-core config repository. At run time, nf-core pipelines will fetch these configuration profiles from the nf-core config repository and make them available.</p> <p>For shared resources such as an HPC cluster, you may consider developing a shared institutional profile.</p> <p>This tutorial can be used to help setting up an institutional profile.</p>"},{"location":"other/nf_customize/04_config/#custom-parameter-and-configuration-files","title":"Custom parameter and configuration files","text":"<p>Nextflow will also look for files that are external to the pipeline project directory. These files include:</p> <ul> <li>The config file <code>$HOME/.nextflow/config</code></li> <li>A config file named <code>nextflow.config</code> in your current directory</li> <li>Custom files specified using the command line</li> <li>A parameter file that is provided using the <code>-params-file</code> option</li> <li>A config file that are provided using the <code>-c</code> option</li> </ul> <p>You do not need to use all of these files to run your pipeline.</p> <p>Parameter files</p> <p>Parameter files are <code>.json</code> files that can contain an unlimited number of parameters:</p> my-params.json<pre><code>{\n  \"&lt;parameter1_name&gt;\": 1,\n  \"&lt;parameter2_name&gt;\": \"&lt;string&gt;\",\n  \"&lt;parameter3_name&gt;\": true\n}\n</code></pre> <p>You can override default parameters by creating a <code>.json</code> file and passing it as a command-line argument using the <code>-param-file</code> option:</p> <pre><code>nextflow run nf-core/demo -profile singularity -param-file &lt;path/to/params.json&gt;\n</code></pre> <p>Exercise</p> <p>Add the <code>input</code> and <code>outdir</code> parameters to a params file. Give <code>input</code> the complete path to your sample sheet and give <code>outdir</code> the name <code>results_mycustomparams</code>:</p> <ol> <li>Create <code>mycustomparams.json</code>:</li> </ol> <pre><code>code mycustomparams.json\n</code></pre> <ol> <li>Add your <code>input</code> and <code>output</code> parameters:</li> </ol> mycustomparams.json<pre><code>{\n\"input\": \"/workspaces/training/nf-customize/samplesheet.csv\",\n\"outdir\": \"results_mycustomparams\"\n}\n</code></pre> <ol> <li>Run  <code>nf-core/demo</code> with your custom <code>mycustomparams.json</code> file:</li> </ol> <pre><code>nextflow run nf-core/demo -profile singularity -params-file mycustomparams.json\n</code></pre> <p>The pipeline should run successfully. You should be able to see a new results folder <code>results_mycustomparams</code> in your current directory.</p> <p>Configuration files</p> <p>Configuration files are <code>.config</code> files that can contain various pipeline properties and can be passed to Nextflow using the <code>-c</code> option in your execution command:</p> <pre><code>nextflow run nf-core/demo -profile singularity -params-file mycustomparams.json -c &lt;path/to/custom.config&gt;\n</code></pre> <p>Custom configuration files are the same format as the configuration file included in the pipeline directory.</p> <p>Configuration properties are organized into scopes by dot prefixing the property names with a scope identifier or grouping the properties in the same scope using the curly brackets notation:</p> custom.config<pre><code>alpha.x  = 1\nalpha.y  = 'string value'\n</code></pre> <p>Is equivalent to:</p> custom.config<pre><code>alpha {\n    x = 1\n    y = 'string value'\n}\n</code></pre> <p>Scopes allow you to quickly configure settings required to deploy a pipeline on different infrastructure using different software management.</p> <p>For example, the <code>executor</code> scope can be used to provide settings for the deployment of a pipeline on an HPC cluster. Similarly, the <code>singularity</code> scope controls how Singularity containers are executed by Nextflow.</p> <p>A common scenario is for users to write a custom configuration file specific to running a pipeline on their infrastructure.</p> <p>Warning</p> <p>Do not use <code>-c &lt;file&gt;</code> to specify parameters as this will result in errors. Custom config files specified with <code>-c</code> must only be used for tuning process resource specifications, other infrastructural tweaks (such as output directories), or module arguments (args).</p> <p>Multiple scopes can be included in the same <code>.config</code> file using a mix of dot prefixes and curly brackets:</p> example.config<pre><code>executor.name = \"sge\"\n\nsingularity {\n    enabled    = true\n    autoMounts = true\n}\n</code></pre> <p>See the Nextflow documentation for a full list of scopes.</p> <p>Exercise</p> <p>Instead of using the <code>singularity</code> profile a custom configuration file can be used to enable singularity. Create a custom configuration file and enable singularity and singularity auto mounts using the singularity scope.</p> <ol> <li>Create <code>mycustomconfig.config</code>:</li> </ol> <pre><code>code mycustomconfig.config\n</code></pre> <ol> <li>Add your configuration to the singularity scope:</li> </ol> mycustomconfig.config<pre><code>singularity {\n    enabled    = true\n    autoMounts = true\n}\n</code></pre> <ol> <li>Run <code>nf-core/demo</code> with your <code>mycustomconfig.config</code> in your execution command:</li> </ol> <pre><code>nextflow run nf-core/demo -profile test --outdir results_config -c mycustomconfig.config\n</code></pre> <p>The pipeline will run successfully.</p> <p>Multiple config files</p> <p>Multiple custom <code>.config</code> files can be included at execution by separating them with a comma (<code>,</code>).</p> <p>The <code>process</code> scope allows you to configure pipeline processes and is used extensively to define resources and additional arguments for modules.</p> <p>By default, process resources are allocated in the <code>conf/base.config</code> file using the <code>withLabel</code> selector:</p> conf/base.config<pre><code>process {\n    withLabel: BIG_JOB {\n        cpus = 16\n        memory = 64.GB\n    }\n}\n</code></pre> <p>Similarly, the <code>withName</code> selector enables the configuration of a process by name. By default, module parameters are defined in the <code>conf/modules.config</code> file:</p> conf/modules.config<pre><code>process {\n    withName: MYPROCESS {\n        cpus = 4\n        memory = 8.GB\n    }\n}\n</code></pre> <p>While some tool arguments are included as a part of a module. To make nf-core modules sharable across pipelines, most tool arguments are defined in the <code>conf/modules.conf</code> file in the pipeline code under the <code>ext.args</code> entry.</p> <p>Importantly, having these arguments outside the module also allows them to be customized at runtime.</p> <p>For example, if you wanted to add arguments to the <code>MULTIQC</code> process in the <code>nf-core/demo</code> pipeline, you could use the process scope and the <code>withName</code> selector:</p> example.config<pre><code>process {\n    withName : \"MULTIQC\" {\n        ext.args   = { \"&lt;your custom parameter&gt;\" }\n    }\n</code></pre> <p>An extended execution path of the module may be required to make it more specific if a process is used multiple times in a pipeline:</p> example.config<pre><code>process {\n    withName: \"NFCORE_DEMO:DEMO:MULTIQC\" {\n        ext.args = { \"&lt;your custom parameter&gt;\" }\n    }\n}\n</code></pre> <p>The extended execution path is built from the pipelines, subworkflows, and module used to execute the process.</p> <p>Exercise</p> <p>Modify your existing <code>mycustomconfig.config</code> by adding a process scope, with the <code>withName</code> selector, to add a custom title to your MultiQC report:</p> <ol> <li>Open <code>mycustomconfig.config</code>:</li> </ol> <pre><code>code mycustomconfig.config\n</code></pre> <ol> <li>Add a <code>process</code> scope and using the <code>withName</code> selector for <code>MULTIQC</code>, add <code>--title</code> flag with a custom report name.</li> </ol> mycustomconfig.config<pre><code>singularity {\n    enabled    = true\n    autoMounts = true\n}\n\nprocess {\n    withName: 'MULTIQC' {\n            ext.args   = { \"--title \\\"my_custom_title\\\"\" }\n        }\n}\n</code></pre> <ol> <li>Run <code>nf-core/demo</code> with your <code>mycustomconfig.config</code> in your execution command::</li> </ol> <pre><code>nextflow run nf-core/demo -profile test --outdir results_process -c mycustomconfig.config\n</code></pre> <p>View the <code>multiqc</code> folder inside your results directory:</p> <pre><code>ls results_process/multiqc/\n</code></pre>"},{"location":"other/nf_customize/04_config/#mixing-configuration-files","title":"Mixing configuration files","text":"<p>It is important to consider how the different configuration options interact during each execution and how you can apply these to minimize mistakes and extra configuration.</p> <p>Exercise</p> <p>Execute the <code>nf-core/demo</code> pipeline with the <code>singularity</code> profile, your <code>mycustomparams.json</code> file, your <code>mycustomconfig.config</code> file, and a command line flag <code>--outdir results_mixed</code>:</p> <pre><code>nextflow run nf-core/demo -profile singularity -params-file mycustomparams.json -c mycustomconfig.config --outdir results_mixed\n</code></pre> <p>You now have a new output directory named <code>results_mixed</code> despite the directory being named <code>results_customparams</code> in your custom parameters file.</p> <p>Exercise</p> <p>Consider how the different levels of configuration interacted. Mix and match configuration levels to rename your outputs.</p> <p>Congratulations! You have successfully customized the execution of the <code>nf-core/demo</code> pipeline using different methods!</p>"},{"location":"other/nf_customize/05_tools/","title":"Tools for users","text":"<p>nf-core tools has additional commands to help users execute pipelines. Although you do not need to use these commands to execute the nf-core pipelines they can greatly improve and simplify your experience.</p> <p>nf-core tools are for everyone and has commands to help both users and developers.</p> <p>For users, the tools make it easier to execute pipelines.</p> <p>For developers, the tools make it easier to develop pipelines using best practices. However, the developer commands will not be covered as a part of this workshop. If you are curious to learn more about nf-core developer tools you can visit the websites tools page and complete the develop a pipeline with the nf-core template training.</p> <p>Exercise</p> <p>View nf-core commands and options are available using the <code>nf-core --help</code> option.</p>"},{"location":"other/nf_customize/05_tools/#nf-core-pipelines-list","title":"<code>nf-core pipelines list</code>","text":"<p>The nf-core <code>list</code> command can be used to print a list of remote nf-core pipelines and local pipeline information.</p> <pre><code>nf-core pipelines list\n</code></pre> <p>The output shows the latest pipeline version number and when it was released. You will also be shown if and when a pipeline was pulled locally and whether you have the latest version.</p> <p>Keywords can also be supplied to help filter the pipelines based on matches in titles, descriptions, or topics:</p> <pre><code>nf-core pipelines list dna\n</code></pre> <p>Options can also be used to sort the pipelines by latest release (<code>-s release</code>, default), when you last pulled a pipeline locally (<code>-s pulled</code>), alphabetically (<code>-s name</code>), or number by the number of GitHub stars (<code>-s stars</code>).</p> <p>Exercise</p> <p>Filter the list of nf-core pipelines for those that are for <code>dna</code> and sort them by stars. Which <code>dna</code> pipeline has the most stars?</p> <pre><code>nf-core pipelines list dna -s stars\n</code></pre>"},{"location":"other/nf_customize/05_tools/#nf-core-pipelines-launch","title":"<code>nf-core pipelines launch</code>","text":"<p>A pipeline can have a large number of optional parameters. To help with this, the <code>nf-core pipelines launch</code> command is designed to help you write parameter files for when you launch your pipeline.</p> <p>The nf-core <code>launch</code> command takes one argument - either the name of an nf-core pipeline which will be pulled automatically or the path to a directory containing a Nextflow pipeline:</p> <pre><code>nf-core pipelines launch\n</code></pre> <p>You will first be asked about which version of the pipeline you would like to execute. Next, you will be given the choice between a web-based graphical interface or an interactive command-line wizard tool to enter the pipeline parameters. Both interfaces show documentation alongside each parameter, will generate a run ID, and will validate your inputs.</p> <p></p> <p>The nf-core <code>launch</code> tool uses the <code>nextflow_schema.json</code> file from a pipeline to give parameter descriptions, defaults, and grouping. If no file for the pipeline is found, one will be automatically generated at runtime.</p> <p>The <code>launch</code> tool will save your parameter variables as a <code>.json</code> file called <code>nf-params.json</code>. It will also suggest an execution command that includes the <code>-params-file</code> flag and your new <code>nf-params.json</code> file.</p> <p>The command line wizard will conclude by asking if you want to launch the pipeline. Any profiles or options that were set using the wizard will be included in your <code>run</code> command.</p> <p>Exercise</p> <p>Use <code>nf-core launch</code> to launch the the <code>nf-core/demo</code> pipeline.</p> <p>Make sure you add:</p> <ul> <li>The full path to your sample sheet (<code>input</code>)</li> <li>An output directory (<code>outdir</code>)</li> <li>The <code>singularity</code> profile</li> </ul> <p>Use the nf-core <code>launch</code> command for the <code>nf-core/demo</code> pipeline. Your <code>nf-params.json</code> file should look something like this:</p> nf-params.json<pre><code>{\n    \"input\": \"/workspaces/training/nf-customize/samplesheet.csv\",\n    \"outdir\": \"results\"\n}\n</code></pre> <p>Your final <code>run</code> command should look like this:</p> <pre><code>nextflow run nf-core/demo -profile singularity -params-file nf-params.json\n</code></pre> <p>You can also use the launch command directly from the nf-core launch website. In this case, you can configure your pipeline using the wizard and then copy the outputs to your terminal or use the run id generated by the wizard. You will need to be connected to the internet to use the run id.</p> <pre><code>nf-core pipelines launch --id &lt;run_id&gt;\n</code></pre>"},{"location":"other/nf_customize/05_tools/#nf-core-pipelines-download","title":"<code>nf-core pipelines download</code>","text":"<p>You may need to execute an nf-core pipeline on a server or HPC system that has no internet connection. In this case, you will need to fetch the pipeline files and manually transfer them to your offline system. The nf-core tooling has the <code>download</code> command to make this process easier and ensure accurate retrieval of correctly versioned code and software containers.</p> <p>The nf-core <code>download</code> command will download both the pipeline code and the institutional nf-core/configs files. It can also optionally download singularity image file.</p> <pre><code>nf-core pipelines download\n</code></pre> <p>The download tool will interactively prompt you for the required information is no arguments are supplied. Each prompt option has a flag and if all flags are supplied then it will run without a request for any additional user input:</p> <ul> <li>Pipeline name</li> <li>Name of pipeline you would like to download.</li> <li>Pipeline revision</li> <li>The revision you would like to download.</li> <li>Pull containers</li> <li>If you would like to download Singularity images.</li> <li>The path to a folder where you would like to store these images if you have not set your <code>NXF_SINGULARITY_CACHEDIR</code>.</li> <li>Choose compression type</li> <li>The compression type for Singularity images.</li> </ul> <p>Alternatively, you could build your own execution command with the command line options.</p> <p>Exercise</p> <p>Use the nf-core <code>download</code> command to download the <code>nf-core/demo</code> pipeline with it's uncompressed Singularity images.</p> <p>Use the nf-core <code>download</code> command for the <code>nf-core/demo</code> pipeline and follow the prompts.</p> <p>Your output should look like this:</p> <pre><code>INFO Saving 'nf-core/demo'\n  Pipeline revision: 'main'\n  Use containers: 'singularity'\n  Container library: 'quay.io'\n  Using $NXF_SINGULARITY_CACHEDIR': /workspaces/training/nf-customize/singularity'\n  Output directory: 'nf-core-demo_main'\n  Include default institutional configuration: 'False'\n</code></pre> <p>If the command has been executed successfully you will see outputs indicating the singularity images and pipeline are being downloaded.</p> <p>If you normally work on an offline system you would now move these files to your offline system and specify paths to these files using environmental variables, for example, the <code>NXF_SINGULARITY_CACHEDIR</code>.</p> <p>See Running offline to find out more about running a pipeline offline.</p> <p>Congratulations! You have now utilized nf-core tools for finding, launching and downloading pipeline!</p>"},{"location":"other/nf_develop/","title":"Develop nf-core","text":"<p>Nextflow provides a powerful way to develop pipelines. However, it does not provide standards for how pipelines should be developed. This gap has led to the establishment of pipeline registries, such as nf-core, with tools and implementation guidelines that provide support and standards for pipeline development.</p> <p>nf-core developed a base template to enable standardization and the adoption of good coding practices. The template establishes a pipeline file structure, with code, documentation, and continuous integration (CI) tests. The template plugs into the nf-core tooling, offering a full suite of commands that support pipeline usage and development.</p> <p>Leveraging the nf-core template enables accelerated maintenance and development of pipelines.</p> <p>Let's get started!</p> <p></p>"},{"location":"other/nf_develop/#learning-objectives","title":"Learning objectives","text":"<p>In this workshop, you will utilize the nf-core tooling to build a small pipeline using the nf-core template.</p> <p>By the end of this workshop you will be able to:</p> <ul> <li>Discuss the benefits of using the nf-core pipeline template</li> <li>Identify the features of nf-core pipeline template that enable best practices</li> <li>Add nf-core modules to a pipeline using the nf-core tooling</li> <li>Add parameters to a pipeline and update the schema using the nf-core tooling</li> </ul>"},{"location":"other/nf_develop/#audience-prerequisites","title":"Audience &amp; prerequisites","text":"<p>Please note that this is not a beginner's workshop and familiarity with Nextflow, the command line, and common file formats is assumed.</p> <p>Prerequisites</p> <ul> <li>A GitHub account</li> <li>Experience with command line</li> <li>Experience writing pipelines with Nextflow</li> </ul>"},{"location":"other/nf_develop/1_01_orientation/","title":"Orientation","text":"<p>The GitHub Codespaces environment contains some test data that will be used in this workshop.</p> <p>Note</p> <p>Follow this link if you have not yet setup your GitHub Codespaces environment.</p>"},{"location":"other/nf_develop/1_01_orientation/#getting-started","title":"Getting started","text":"<p>You will complete this module in the <code>nf-develop</code> folder.</p> <p>This is an empty directory where you will create an launch your pipeline.</p> <p>Exercise</p> <p>Open the GitHub Codespaces environment use the following command to switch to the empty <code>nf-develop</code> folder:</p> <pre><code>cd /workspaces/training/nf-develop\n</code></pre> <p>Congratulations! You are now ready to start the workshop!</p>"},{"location":"other/nf_develop/1_02_create/","title":"Getting started","text":"<p>The nf-core pipeline template is a standardized framework designed to streamline the development of Nextflow-based bioinformatics pipelines.</p> <p>Creating a pipeline using the nf-core template is greatly simplified by the nf-core tooling. It will help you create a pipeline using the set framework that can be modified to suit your own purposes.</p> <p>Here, you will use the nf-core template to kickstart your pipeline development using the latest version of Nextflow and the nf-core tooling.</p>"},{"location":"other/nf_develop/1_02_create/#creating-your-pipeline","title":"Creating your pipeline","text":"<p>nf-core tooling has commands for pipeline users and developers.</p> <p>View all of the tooling using the <code>nf-core --help</code> argument.</p> <pre><code>nf-core --help\n</code></pre> <p>Here we will focus on the tooling to assist pipeline developers, starting with the <code>nf-core pipelines create</code> command.</p>"},{"location":"other/nf_develop/1_02_create/#nf-core-pipelines-create","title":"<code>nf-core pipelines create</code>","text":"<p>The <code>nf-core pipelines create</code> command makes a new pipeline using the nf-core base template with a pipeline name, description, and author. It is the first and most important step for creating a pipeline that will integrate with the wider Nextflow ecosystem.</p> <pre><code>nf-core pipelines create\n</code></pre> <p>Running this command will open a Text User Interface (TUI) for pipeline creation.</p> <p>Template features can be flexibly included or excluded at the time of creation. You can still use the CLI by providing all values as parameters.</p> <p>Exercise</p> <p>Follow these steps create your first pipeline using the <code>nf-core pipelines create</code> TUI:</p> <ol> <li>Run the <code>nf-core pipelines create</code> command</li> <li>Select Let's go! on the welcome screen</li> <li>Select Custom on the Choose pipeline type screen</li> <li>Enter your pipeline details, replacing &lt; YOUR NAME &gt; with your own name, then select Next<ul> <li>GitHub organisation: myorg</li> <li>Workflow name: myfirstpipeline</li> <li>A short description of your pipeline: My first pipeline</li> <li>Name of the main author / authors: &lt; YOUR NAME &gt;</li> </ul> </li> <li>Select Continue on the Template features screen</li> <li>Select Finish on the Final details screen</li> <li>Wait for the pipeline to be created, then select Continue</li> <li>Select Finish without creating a repo on the Create GitHub repository screen</li> <li>Select Close on the HowTo create a GitHub repository page</li> </ol> <p>If run successfully, you will see a new folder in your current directory named <code>myorg-myfirstpipeline</code>.</p>"},{"location":"other/nf_develop/1_02_create/#template-tour","title":"Template tour","text":"<p>The nf-core pipeline template comes packed with a lot of files and folders.</p> <p>Here, almost everything was included in the template to create opportunities to explore these features.</p> <p>While the template may feel overwhelming, a complete understanding isn't required to start developing your pipeline.</p>"},{"location":"other/nf_develop/1_02_create/#workflows-subworkflows-and-modules","title":"Workflows, subworkflows, and modules","text":"<p>The nf-core pipeline template has a <code>main.nf</code> script that calls <code>myfirstpipeline.nf</code> from the <code>workflows</code> folder. The <code>myfirstpipeline.nf</code> file inside the workflows folder is the central pipeline file that is used to bring everything else together.</p> <p>Instead of having one large monolithic pipeline script, it's broken up into smaller script components, namely, modules and subworkflows:</p> <ul> <li>Modules: Wrappers around a single process</li> <li>Subworkflows: Two or more modules that are packaged together as a mini workflow</li> </ul> <p>Within your pipeline repository, <code>modules</code> and <code>subworkflows</code> are stored within <code>local</code> and <code>nf-core</code> folders. The <code>nf-core</code> folder is for components that have come from the nf-core GitHub repository while the <code>local</code> folder is for components that have been developed independently:</p> <pre><code>modules/\n\u251c\u2500\u2500 local\n\u2502   \u2514\u2500\u2500 &lt;toolname&gt;\n\u2502   \u2502   \u2514\u2500\u2500 main.nf\n\u2502   .\n\u2502\n\u2514\u2500\u2500 nf-core\n    \u251c\u2500\u2500 &lt;tool name&gt;\n    \u2502   \u251c\u2500\u2500 environment.yml\n    \u2502   \u251c\u2500\u2500 main.nf\n    \u2502   \u251c\u2500\u2500 meta.yml\n    \u2502   \u2514\u2500\u2500 tests\n    \u2502       \u251c\u2500\u2500 main.nf.test\n    \u2502       \u251c\u2500\u2500 main.nf.test.snap\n    \u2502       \u2514\u2500\u2500 tags.yml\n    .\n</code></pre> <p>Modules from nf-core follow a similar same structure and contain a small number of additional files that are used for testing using nf-test and documentation about the module.</p> <p>Note</p> <p>Some nf-core modules are also split into command specific directories:</p> <pre><code>\u2502\n\u2514\u2500\u2500 &lt;tool name&gt;\n    \u2514\u2500\u2500 &lt;command&gt;\n        \u251c\u2500\u2500 environment.yml\n        \u251c\u2500\u2500 main.nf\n        \u251c\u2500\u2500 meta.yml\n        \u2514\u2500\u2500 tests\n            \u251c\u2500\u2500 main.nf.test\n            \u251c\u2500\u2500 main.nf.test.snap\n            \u2514\u2500\u2500 tags.yml\n</code></pre> <p>Note</p> <p>The nf-core template does not come with a local modules folder by default.</p>"},{"location":"other/nf_develop/1_02_create/#configuration-files","title":"Configuration files","text":"<p>The nf-core pipeline template utilizes Nextflows flexible customization options and has a series of configuration files throughout the template.</p> <p>In the template, the <code>nextflow.config</code> file is a central configuration file and is used to set default values for parameters and other configuration options. The majority of these configuration options are applied by default while others (e.g., software dependency profiles) are included as optional profiles.</p> <p>There are several configuration files that are stored in the <code>conf</code> folder and are added to the configuration by default or optionally as profiles:</p> <ul> <li><code>base.config</code>: A 'blank slate' config file, appropriate for general use on most high performance compute environments.</li> <li><code>igenomes.config</code>: Defines reference genomes using iGenome paths.</li> <li><code>igenomes_ignored.config</code>: Empty genomes dictionary to use when igenomes is ignored</li> <li><code>modules.config</code>: Additional module directives and arguments.</li> <li><code>test.config</code>: A profile to run the pipeline with minimal test data.</li> <li><code>test_full.config</code>: A profile to run the pipeline with a full-sized test dataset.</li> </ul>"},{"location":"other/nf_develop/1_02_create/#nf-coreyml","title":"<code>.nf-core.yml</code>","text":"<p>The <code>.nf-core.yml</code> file is used to specify the repository type and manage linting tests.</p> .nf-core.yml<pre><code>bump_version: null\nlint:\n  files_exist:\n    - CODE_OF_CONDUCT.md\n    - assets/nf-core-myfirstpipeline_logo_light.png\n    - docs/images/nf-core-myfirstpipeline_logo_light.png\n    - docs/images/nf-core-myfirstpipeline_logo_dark.png\n    - .github/ISSUE_TEMPLATE/config.yml\n    - .github/workflows/awstest.yml\n    - .github/workflows/awsfulltest.yml\n  files_unchanged:\n    - CODE_OF_CONDUCT.md\n    - assets/nf-core-myfirstpipeline_logo_light.png\n    - docs/images/nf-core-myfirstpipeline_logo_light.png\n    - docs/images/nf-core-myfirstpipeline_logo_dark.png\n    - .github/ISSUE_TEMPLATE/bug_report.yml\n  multiqc_config:\n    - report_comment\n  nextflow_config:\n    - manifest.name\n    - manifest.homePage\n    - validation.help.beforeText\n    - validation.help.afterText\n    - validation.summary.beforeText\n    - validation.summary.afterText\nnf_core_version: 3.0.1\norg_path: null\nrepository_type: pipeline\ntemplate:\n  author: Chris\n  description: My first pipeline\n  force: true\n  is_nfcore: false\n  name: myfirstpipeline\n  org: myorg\n  outdir: .\n  skip_features: []\n  version: 1.0.0dev\nupdate: null\n</code></pre> <p>Note</p> <p>The <code>.nf-core.yml</code> file must match your template features for linting tests to pass.</p>"},{"location":"other/nf_develop/1_02_create/#nextflow_schemajson","title":"<code>nextflow_schema.json</code>","text":"<p>The <code>nextflow_schema.json</code> is a file used to store parameter related information including type, description and help text in a machine readable format. The schema is used for various purposes, including automated parameter validation, help text generation, and interactive parameter form rendering in UI interfaces.</p>"},{"location":"other/nf_develop/1_02_create/#github-actions-workflows","title":"GitHub actions workflows","text":"<p>Automated workflows are an important part of the nf-core pipeline template.</p> <p>By default, the template comes with several automated tests that utilize GitHub Actions, each of which are configured in the <code>.github/workflows</code> folder:</p> <ul> <li><code>branch.yml</code>: Sets the branch protection for the nf-core repository</li> <li><code>ci.yml</code>: Run small pipeline tests with the small test datasets</li> <li><code>clean-up.yml</code>: Automated testing for stale and closed GitHub issues and PRs in the nf-core repo</li> <li><code>download_pipeline.yml</code>: Test a pipeline download with <code>nf-core pipelines download</code>.</li> <li><code>fix-linting.yml</code>: Fix linting by adding a comment to a PR</li> <li><code>linting_comment.yml</code>: Triggered after the linting action and posts an automated comment to the PR, even if the PR is coming from a fork</li> <li><code>linting.yml</code>: Triggered on pushes and PRs to the repository and runs <code>nf-core pipelines lint</code> and markdown lint tests to ensure that the code meets the nf-core guidelines</li> <li><code>release-announcements.yml</code>: Automatic release toot and tweet announcements for nf-core pipeline releases</li> </ul> <p>Many of these tests are only configured for the nf-core repo. However, they can be modified for your repository or ignored if they are superfluous to your requirements.</p> <p>Read more about creating and modifying workflows on the GitHub Actions documentation webpage.</p> <p>Congratulations! You have now created a template pipeline, pushed it to GitHub and learned about important template files!</p>"},{"location":"other/nf_develop/1_03_pipeline/","title":"Adding modules","text":"<p>The nf-core pipeline template is a working pipeline and comes pre-configured with two modules:</p> <ul> <li>FastQC: A tool that performs quality control checks on raw sequence data coming from high throughput sequencing pipelines. It provides a modular set of analyses that can be used to give a quick impression of your data.</li> <li>MultiQC: A modular tool to aggregate results from bioinformatics analyses across many samples into a single report.</li> </ul> <p>Pre-configured modules are optional</p> <p>From nf-core tools 3.0 onwards pre-configured modules are optional and can be removed during template creation.</p>"},{"location":"other/nf_develop/1_03_pipeline/#testing-your-pipeline","title":"Testing your pipeline","text":"<p>You can use the <code>test</code> profile can be used to check if your pipeline is still working during your development cycle. You can also use it in GitHub Actions to test your pipeline during pull requests.</p> <p>The default template <code>test</code> profile leverages small test files that are stored in the nf-core test data GitHub repository as inputs for the pipeline.</p> <p>Additionally, the template comes with profiles for the management of software dependencies (e.g., <code>docker</code>, <code>singularity</code>, and <code>conda</code>). nf-core modules come with containers/images/recipes and profiles can be used to change the way dependencies are handled when you execute your pipeline.</p> <p>Warning</p> <p>If <code>-profile</code> for managing software dependencies is not specified, the pipeline will run locally and expect all software to be installed and available on <code>PATH</code>. This is not recommended.</p> <p>Additional test profiles can be created to test different parts of your pipeline and can also be added to GitHub actions.</p> <p>Exercise</p> <p>Run your pipeline with the <code>test</code> and <code>singularity</code> profile:</p> <pre><code>cd /workspaces/training/nf-develop\nnextflow run myorg-myfirstpipeline -profile test,singularity --outdir results\n</code></pre> <p>The pipeline should run successfully!</p>"},{"location":"other/nf_develop/1_03_pipeline/#adding-a-new-tool-to-your-pipeline","title":"Adding a new tool to your pipeline","text":"<p>Here, you will add another tool to your pipeline.</p> <p><code>Seqtk</code> is a fast and lightweight tool for processing sequences in the FASTA or FASTQ format. Here, you will use the <code>seqtk trim</code> command to trim FASTQ files.</p> <p>In your pipeline, you will add a new step that will take FASTQ files from the sample sheet as inputs and will produce trimmed fastq files that can be used as an input for other tools and version information about the seqtk tools to mix into the inputs for the MultiQC process.</p> <p>While you could develop a module for this tool independently, you can save a lot of time and effort by leveraging nf-core modules and subworkflows.</p> <p>nf-core modules and subworkflows are written and maintained by the nf-core community. They are designed to be flexible but may require additional configuration to suit different use cases. Currently, there are more than 1250 nf-core modules and 60 nf-core subworkflows (April 2024) available.</p> <p>Modules and subworkflows can be listed, installed, updated, removed, and patched using nf-core tooling.</p>"},{"location":"other/nf_develop/1_03_pipeline/#installing-the-seqtktrim-module","title":"Installing the <code>seqtk/trim</code> module","text":"<p>The <code>nf-core modules list</code> command can be used to show the modules in your local pipeline or the nf-core remote repository.</p> <pre><code>nf-core modules list remote\n</code></pre> <p>The <code>nf-core modules install</code> command can be used to install the <code>seqtk/trim</code> module directly from the nf-core repository:</p> <pre><code>nf-core modules install\n</code></pre> <p>Warning</p> <p>You need to be in the my-myfirstpipeline directory when executing <code>nf-core modules install</code></p> <p>You can follow the prompts to find and install the module you are interested in:</p> <pre><code>? Tool name: seqtk/trim\n</code></pre> <p>Once selected, the tooling will install the module in the <code>modules/nf-core/</code> folder and suggest code that you can add to your main workflow file (<code>workflows/mypipeline.nf</code>).</p> <pre><code>INFO     Installing 'seqtk/trim'\nINFO     Use the following statement to include this module:\n\ninclude { SEQTK_TRIM } from '../modules/nf-core/seqtk/trim/main'\n</code></pre> <p>Exercise</p> <p>Run the <code>nf-core modules install</code> command to add the <code>seqtk/trim</code> module to your pipeline.</p> <pre><code>cd my-myfirstpipeline\nnf-core modules install\n</code></pre> <p>To enable reporting and reproducibility, modules and subworkflows from the nf-core repository are tracked using hashes in the <code>modules.json</code> file. When modules are installed or removed using the nf-core tooling the <code>modules.json</code> file will be automatically updated.</p> <p>Exercise</p> <p>View your <code>modules.json</code> file and see if the <code>seqtk/trim</code> module is being tracked.</p>"},{"location":"other/nf_develop/1_03_pipeline/#adding-a-module-to-your-pipeline","title":"Adding a module to your pipeline","text":"<p>Although the module has been installed in your local pipeline repository, it is not yet added to your pipeline.</p> <p>The suggested <code>include</code> statement needs to be added to your <code>workflows/mypipeline.nf</code> file and the process call (with inputs) needs to be added to the workflow block.</p> workflows/mypipeline.nf<pre><code>include { FASTQC                 } from '../modules/nf-core/fastqc/main'\ninclude { SEQTK_TRIM             } from '../modules/nf-core/seqtk/trim/main'\ninclude { MULTIQC                } from '../modules/nf-core/multiqc/main'\n</code></pre> <p>Exercise</p> <p>Add the suggested <code>include</code> statement to your <code>mypipeline.nf</code> file.</p> workflows/mypipeline.nf<pre><code>include { SEQTK_TRIM             } from '../modules/nf-core/seqtk/trim/main'\n</code></pre> <p>To add the <code>SEQTK_TRIM</code> module to your workflow you will need to check what inputs are required.</p> <p>You can view the input channels for the module by opening the <code>./modules/nf-core/seqtk/trim/main.nf</code> file.</p> /modules/nf-core/seqtk/trim/main.nf<pre><code>input:\ntuple val(meta), path(reads)\n</code></pre> <p>Each nf-core module also has a <code>meta.yml</code> file which describes the inputs and outputs. This meta file is rendered on the nf-core website, or can be viewed using the <code>nf-core modules info</code> command.</p> <p>Exercise</p> <p>View information for the <code>seqtk/trim</code> module using the <code>nf-core modules info</code> command:</p> <pre><code>nf-core modules info seqtk/trim\n</code></pre> <p>Using this module information you can work out what inputs are required for the <code>SEQTK_TRIM</code> process:</p> <ol> <li> <p><code>tuple val(meta), path(reads)</code></p> <ul> <li>A tuple with a meta map and a list of FASTQ files</li> <li>The channel <code>ch_samplesheet</code> used by the <code>FASTQC</code> process can be used as the reads input.</li> </ul> </li> </ol> <p>As only one input channel required, and it already exists, it can be added to your <code>mypipeline.nf</code> file without any additional channel creation or modifications.</p> <p>Exercise</p> <p>Add the <code>SEQTK_TRIM</code> process to your <code>myfirstpipeline.nf</code> file.</p> workflows/myfirstpipeline.nf<pre><code>//\n// MODULE: Run SEQTK_TRIM\n//\nSEQTK_TRIM (\n    ch_samplesheet\n)\n</code></pre> <p>As with the inputs, you can view the outputs for the module by opening the <code>/modules/nf-core/seqtk/trim/main.nf</code> file and viewing the module metadata.</p> /modules/nf-core/seqtk/trim/main.nf<pre><code>output:\ntuple val(meta), path(\"*.fastq.gz\"), emit: reads\npath \"versions.yml\"                , emit: versions\n</code></pre> <p>To help with organization and readability it is beneficial to create named output channels.</p> <p>For <code>SEQTK_TRIM</code>, the <code>reads</code> output could be put into a channel named <code>ch_trimmed</code>.</p> workflows/mypipeline.nf<pre><code>ch_trimmed  = SEQTK_TRIM.out.reads\n</code></pre> <p>Similarly, it is beneficial immediately mix the versions of tools into the <code>ch_versions</code> channel so they can be used as an input for the <code>MULTIQC</code> process.</p> workflows/mypipeline.nf<pre><code>ch_versions = ch_versions.mix(SEQTK_TRIM.out.versions.first())\n</code></pre> <p>Exercise</p> <p>Create a channel named <code>ch_trimmed</code> from the <code>SEQTK_TRIM.out.reads</code> output mix the <code>SEQTK_TRIM.out.versions</code> output with the <code>ch_versions</code> channel.</p> workflows/mypipeline.nf<pre><code>ch_trimmed  = SEQTK_TRIM.out.reads\nch_versions = ch_versions.mix(SEQTK_TRIM.out.versions.first())\n</code></pre> <p>Note</p> <p>The <code>first</code> operator is used to emit the first item from <code>SEQTK_TRIM.out.versions</code> to avoid duplication.</p>"},{"location":"other/nf_develop/1_03_pipeline/#additional-configuration-options","title":"Additional configuration options","text":"<p>To prevent changing the nf-core modules, additional configuration options can be applied to a module using scopes within configuration files.</p> <p>The configuration of modules is commonly added to the <code>modules.conf</code> file in the <code>conf</code> folder. Process selectors (e.g., <code>withName</code>) are used to apply configuration to modules selectively. Process selectors must be used within the <code>process</code> scope.</p> <p>Extra configuration may also be applied as directives by using <code>args</code>. You can find many examples of how arguments are added to modules in nf-core pipelines, for example, the nf-core/rnaseq modules.config file.</p> <p>Exercise</p> <p>Add this snippet to your <code>conf/modules.config</code> file to save the trimmed FASTQ files reports in folders named using <code>meta.id</code>.</p> conf/modules.config<pre><code>withName: 'SEQTK_TRIM' {\n    publishDir = [\n        path: { \"${params.outdir}/fq/${meta.id}\" },\n        mode: params.publish_dir_mode,\n        pattern: \"*.{fastq.gz}\"\n    ]\n}\n</code></pre> <p>Closures</p> <p>Closures can be used in configuration files to inject code evaluated at runtime.</p>"},{"location":"other/nf_develop/1_03_pipeline/#checking-your-module-has-been-added","title":"Checking your module has been added","text":"<p>It is important to regularly check that you have not broken your pipeline during development. Testing often can help identify issues quicker as you have less files have been modified and mistakes will be easier to identify.</p> <p>The <code>test</code> profile is perfect for this use case.</p> <p>Exercise</p> <p>Test your profile to see if the <code>SEQTK_TRIM</code> process is working:</p> <pre><code>cd /workspaces/training/nf-develop\nnextflow run myorg-myfirstpipeline -profile test,singularity --outdir results\n</code></pre> <p>The pipeline should execute successfully with a new <code>SEQTK_TRIM</code> process shown in the terminal and result files.</p> <p>Congratulations! You have added your first nf-core module to the nf-core template!</p>"},{"location":"other/nf_develop/1_04_parameters/","title":"Adding parameters","text":"<p>Parameters that can be overridden, either using the command line or the Nextflow configuration file, and should be used for anything that a pipeline user may want to configure regularly.</p> <p>Here, as a simple example, you will add a new parameter to your pipeline that will skip the <code>SEQTK_TRIM</code> process.</p>"},{"location":"other/nf_develop/1_04_parameters/#default-values","title":"Default values","text":"<p>In the nf-core template the default values for parameters are set in the <code>nextflow.config</code> in the base repository.</p> <p>Any new parameters should be added to the <code>nextflow.config</code> with a default value within the <code>params</code> scope.</p> <p>Parameter names should be unique and easily identifiable.</p> <p>Exercise</p> <p>Add a new parameter <code>skip_trim</code> to your <code>nextflow.config</code> file and set it to <code>false</code>.</p> nextflow.config<pre><code>// Trimming\nskip_trim                   = false\n</code></pre>"},{"location":"other/nf_develop/1_04_parameters/#adding-parameters-to-your-pipeline","title":"Adding parameters to your pipeline","text":"<p>Parameters are accessible in the pipeline script.</p> <p>Here, an <code>if</code> statement that is depended on the <code>skip_trim</code> parameter can be used to control the execution of the <code>SEQTK_TRIM</code> process. An <code>!</code> can be used to imply the logical \"not\".</p> <p>Thus, if the <code>skip_trim</code> parameter is not <code>true</code>, the <code>SEQTK_TRIM</code> will be be executed.</p> <p>Exercise</p> <p>Add an <code>if</code> statement that is dependent on the <code>skip_trim</code> parameter to your pipeline.</p> workflows/mypipeline.nf<pre><code>//\n// MODULE: Run SEQTK_TRIM\n//\nif (!params.skip_trim) {\n    SEQTK_TRIM (\n        ch_samplesheet\n    )\n    ch_trimmed  = SEQTK_TRIM.out.reads\n    ch_versions = ch_versions.mix(SEQTK_TRIM.out.versions.first())\n}\n</code></pre> <p>Now your if statement has been added to your main workflow file and has a default setting in your <code>nextflow.config</code> file, you will be able to flexibly skip the new trimming step using the <code>skip_trim</code> parameter.</p> <p>Exercise</p> <p>Run your pipeline with your new <code>skip_trim</code> parameter to check it is working:</p> <pre><code>cd /workspaces/training/nf-develop/\nnextflow run myorg-myfirstpipeline -profile test,singularity --outdir results --skip_trim\n</code></pre> <p>You should see that the <code>SEQTK_TRIM</code> process has been skipped in your execution.</p>"},{"location":"other/nf_develop/1_04_parameters/#linting-your-changes","title":"Linting your changes","text":"<p>Linting is a static analysis process that helps ensure code quality by automatically identifying syntax errors, potential bugs, and adherence to coding standards. By enforcing consistency and best practices, linting enhances code readability, reduces errors, and streamlines the development workflow.</p> <p>As a part of nf-core tools, the <code>nf-core pipelines lint</code> command can be used to check for inconsistencies in your code, compare your code against source code, and compare your code against nf-core standards.</p> <p>Executing the <code>nf-core pipelines lint</code> command from within your pipeline repository will print a list of ignored tests, warnings, failed tests, and a summary.</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 LINT RESULTS SUMMARY  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [\u2714] 192 Tests Passed  \u2502\n\u2502 [?]   0 Tests Ignored \u2502\n\u2502 [!]  29 Test Warnings \u2502\n\u2502 [\u2717]   1 Test Failed   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Exercise</p> <p>Lint your pipeline:</p> <pre><code>cd /workspaces/training/nf-develop/myorg-mypipeline\nnf-core pipelines lint\n</code></pre> <p>It is expected some tests will fail</p>"},{"location":"other/nf_develop/1_04_parameters/#updating-nextflow_schemajson","title":"Updating <code>nextflow_schema.json</code>","text":"<p>If you have added parameters and they have not been documented in the <code>nextflow_schema.json</code> file then pipeline tests will fail during linting.</p> <pre><code>schema_params: Param skip_trim from nextflow config not found in nextflow_schema.json\n</code></pre> <p>For linting tests to pass the <code>nextflow_schema.json</code> file must be updated with the parameters that were added to your pipeline but have not been documented.</p> <p>The <code>nextflow_schema.json</code> file can get very big and very complicated very quickly.</p> <p>The <code>nf-core pipelines schema build</code> command is designed to support developers write, check, validate, and propose additions to your <code>nextflow_schema.json</code> file.</p> <pre><code>nf-core pipelines schema build\n</code></pre> <p>It will enable you to launch a web builder to edit this file in your web browser rather than trying to edit this file manually.</p> <pre><code>INFO     [\u2713] Default parameters match schema validation\nINFO     [\u2713] Pipeline schema looks valid (found 26 params)\n\u2728 Found 'params.skip_trim' in the pipeline config, but not in the schema. Add to pipeline schema? [y/n]: y\nINFO     Writing schema with 27 params: 'nextflow_schema.json'\n\ud83d\ude80  Launch web builder for customization and editing? [y/n]: y\n</code></pre> <p>Using the web builder you can add add details about your new parameters.</p> <p>The parameters that you have added to your pipeline will be added to the bottom of the <code>nf-core schema build</code> file. Some information about these parameters will be automatically filled based on the default value from your <code>nextflow.config</code>. You will be able to categorize your new parameters into a group, add icons, and add descriptions for each.</p> <p></p> <p>Note</p> <p>Ungrouped parameters in schema will cause a warning.</p> <p>Once you have made your edits you can click <code>Finished</code> and all changes will be automatically added to your <code>nextflow_schema.json</code> file.</p> <p>Exercise</p> <p>Execute the <code>nf-core pipelines schema build</code> command to update your schema.</p> <pre><code>cd /workspaces/training/nf-develop/myorg-mypipeline\nnf-core pipelines schema build\n</code></pre> <p>Make sure you add the <code>skip_trim</code> parameter to a new or existing group to avoid creating a new warning.</p> <p>Lint your pipeline again to see if the tests pass.</p> <pre><code>nf-core pipelines lint\n</code></pre> <p>All pipeline tests should pass.</p> <p>Congratulations! You have now added a new parameter to your pipeline and updated the schema for linting tests to pass!</p>"},{"location":"other/nf_develop/2_02_custom/","title":"Custom modules","text":"<p>nf-core offers a comprehensive set of modules that have been created and curated by the community. However, as a developer, you may be interested in bespoke pieces of software that are not apart of the nf-core repository or customizing a module that already exists.</p>"},{"location":"other/nf_develop/2_02_custom/#adding-a-local-module","title":"Adding a local module","text":"<p>Local modules</p>"},{"location":"other/nf_develop/2_02_custom/#patching-modules","title":"Patching modules","text":"<p>Although nf-core modules are written to be flexible you may want to modify them to better fit your purpose.</p> <p>The <code>nf-core lint</code> command will help manage nf-core components and test that they match the remote source they came from.</p> <p>For example, if you modify an nf-core module, it will no longer match the remote and a linting test of this module will fail.</p> <p>Exercise</p> <p>Edit the <code>SEQTK_TRIM</code> module by adding an <code>ed</code> to the end of <code>reads_fail</code>. Check to see if your change has caused the linting test to fail.</p> modules/nf-core/fastp/main.nf<pre><code>tuple val(meta), path('*.fail.fastq.gz')  , optional:true, emit: reads_failed\n</code></pre>"},{"location":"other/nf_develop/extra/","title":"Extra","text":"<p>In Part 2 of this workshop, you will use the nf-core tooling to continue development of your pipeline and customize parts of the nf-core template.</p> <p>By the end of this workshop you will be able to:</p> <ul> <li>Add local modules</li> <li>Patch nf-core modules</li> <li>Customize linting tests</li> <li>Sync TEMPLATE</li> <li>Bump versions</li> </ul>"},{"location":"other/nf_develop/extra/#customizing-the-template","title":"Customizing the template","text":""},{"location":"other/nf_develop/extra/#adding-a-custom-modules","title":"Adding a custom modules","text":"<p>nf-core offers a comprehensive set of modules that have been created and curated by the community. However, as a developer, you may be interested in bespoke pieces of software that are not apart of the nf-core repository or customizing a module that already exists.</p> <p>Running <code>nf-core lint</code> after you have modified an nf-core module will cause it to throw an error.</p> <pre><code>\u256d\u2500 [\u2717] 1 Module Test Failed \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502              \u2577                               \u2577                                            \u2502\n\u2502 Module name  \u2502 File path                     \u2502 Test message                               \u2502\n\u2502\u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\n\u2502 fastp        \u2502 modules/nf-core/fastp/main.nf \u2502 Local copy of module does not match remote \u2502\n\u2502              \u2575                               \u2575                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Changing a module does not mean you can't continue to use that module.</p> <p>The <code>nf-core modules patch</code> command allows you keep using the nf-core component without needing to make it into a <code>local</code> module for tests to pass. Instead, <code>nf-core modules patch</code> command creates a <code>diff</code> file that will keep track of the changes you made. If you subsequently update the module using the nf-core tooling, the <code>diff</code> file will be retained. If any subsequent changes to the module conflict with your <code>diff</code> file, you will be prompted to resolve the conflicts.</p> <pre><code>nf-core modules patch\n</code></pre> <p>The prompt can be followed to patch the <code>fastp</code> module.</p> <pre><code>? Module name: fastp\n</code></pre> <p>A patch file is created in the fastp module directory</p> <pre><code>...\nINFO     'modules/nf-core/fastp/tests/main.nf.test.snap' is unchanged\nINFO     'modules/nf-core/fastp/tests/tags.yml' is unchanged\nINFO     'modules/nf-core/fastp/tests/nextflow.config' is unchanged\nINFO     'modules/nf-core/fastp/tests/main.nf.test' is unchanged\nINFO     Patch file of 'modules/nf-core/fastp' written to 'modules/nf-core/fastp/fastp.diff'\n</code></pre> <p>Exercise</p> <p>Patch the <code>fastp</code> module to fix the linting error.</p>"},{"location":"other/nf_develop/extra/#ignoring-linting-tests","title":"Ignoring linting tests","text":"<p>The linting tests in the nf-core template are primarily designed for pipelines that are shared as a part of the nf-core project and you may want to ignore certain linting tests if they are not required.</p> <p>The <code>.nf-core.yml</code> file can be modified to choose which linting tests you would like to skip.</p> <p>For example, by default, there is a linting test that checks if the <code>CODE_OF_CONDUCT.md</code> matches the template. If the <code>CODE_OF_CONDUCT.md</code> has been modified the linting test will fail.</p> <pre><code>\u256d\u2500 [\u2717] 1 Pipeline Test Failed \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                 \u2502\n\u2502 files_unchanged: CODE_OF_CONDUCT.md does not match the template \u2502\n\u2502                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>If you were to alter the <code>CODE_OF_CONDUCT.md</code> the <code>nf-core lint</code> command will suggest a command to fix this warning.</p> <pre><code>Tip: Some of these linting errors can automatically be resolved with the following command:\n\n    nf-core lint --fix files_unchanged\n</code></pre> <p>However, if you wish to remove or modify this file you would need to ignore this test. To do this, you would need to list the <code>CODE_OF_CONDUCT.md</code> as a <code>files_unchanged:</code> in the <code>.nf-core.yml</code> file.</p> .nf-core.yml<pre><code>repository_type: pipeline\nlint:\n  files_unchanged:\n    - CODE_OF_CONDUCT.md\n</code></pre> <p>If you run <code>nf-core lint</code> again, you would see that the test is now ignored and there are no more failed tests.</p> <pre><code>\u256d\u2500 [?] 1 Pipeline Test Ignored \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                      \u2502\n\u2502 files_unchanged: File ignored due to lint config: CODE_OF_CONDUCT.md \u2502\n\u2502                                                                      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Exercise</p> <p>Edit the <code>CODE_OF_CONDUCT.md</code> in your pipeline repository (e.g., add another bullet point). Use the <code>nf-core lint</code> command to see if it passes or fails. Add the <code>CODE_OF_CONDUCT.md</code> as a <code>files_unchanged:</code> in the <code>.nf-core.yml</code> file and lint your pipeline again to show that the test has been ignored.</p> <p>A full list of checks, descriptions of how they work, and how they can be customized can be found on the tools documentation website.</p> <p>Bonus Exercise</p> <p>Modify the <code>.nf-core.yml</code> file to prevent <code>pipeline_todos</code> from showing as warnings in your lint tests.</p> .nf-core.yml<pre><code>repository_type: pipeline\nlint:\n    pipeline_todos: false\n    files_unchanged:\n        - CODE_OF_CONDUCT.md\n</code></pre>"},{"location":"other/nf_develop/extra/#custom-remote-modules","title":"Custom remote modules","text":"<p>As an individual or group, you may want to keep your own library of modules and/or subworkflows.</p> <p>The nf-core modules command comes with two flags for specifying a custom remote:</p> <ul> <li><code>--git-remote &lt;git remote url&gt;</code>: Specifies the repository from which the modules should be fetched as a git URL.</li> <li><code>--branch &lt;branch name&gt;</code>: Specifies the branch from which the modules should be fetched.</li> </ul> <p>Note that a custom remote must follow a similar directory structure to that of <code>nf-core/modules</code> for the nf-core commands to work properly.</p> <p>The directory where modules are installed will be prompted or obtained from <code>org_path</code> in the <code>.nf-core.yml</code> file, if it is available. If a module was located at <code>modules/my-folder/TOOL/SUBTOOL</code> your <code>.nf-core.yml</code> should have:</p> .nf-core.yml<pre><code>org_path: my-folder\n</code></pre> <p>The modules commands will, during initialization, try to pull changes from the remote repositories. If you want to disable this, for example, due to performance reasons, you can use the flag <code>--no-pull</code>. Commands will still need to clone repositories that have previously not been used.</p> <p>Private modules repositories</p> <p>In order to browse private repositories you have to configure the GitHub CLI auth and provide your credentials with the command below.</p> <pre><code>gh auth login\n</code></pre>"},{"location":"other/nf_develop/extra/#template-syncs","title":"<code>TEMPLATE</code> syncs","text":"<p>The template evolves as the ecosystem evolves.</p> <p>To keep nf-core pipelines up to date with improvements in the main template, you can use a method of synchronization with the <code>TEMPLATE</code> branch.</p> <p>To sync the template, you first need to commit and push your changes to GitHub. The <code>nf-core sync</code> command can then be used to update the <code>TEMPLATE</code> branch with the latest version of the nf-core template, so that these updates can be synchronized with the pipeline. It is run automatically for all pipelines whenever a new release of nf-core/tools (and the included template) is made.</p> <pre><code>nf-core sync\n</code></pre> <p>The tooling merges updates suggesting a git command</p> <pre><code>cd /workspaces/training/nf-develop/nf-core-myfirstpipeline\ngit merge TEMPLATE\n</code></pre> <p>Note</p> <p>For a newly created pipeline the <code>TEMPLATE</code> branch doesn't need to be synced.</p>"},{"location":"other/nf_develop/extra/#changes-to-the-template-structure","title":"Changes to the template structure","text":"<p>Occasionally, the structure of the nf-core pipeline template is updated during a new release of the nf-core tooling. Most of the time these changes are minor. However, sometimes, larger structural changes are adopted to align with changes in the wider ecosystem.</p> <p>For example, as of nf-core tools 2.13, the groovy code that once lived in the <code>lib</code> folder has been moved to <code>subworkflows/</code>. Moving this code has made it easier to find, modify, and test the code. Importantly, it's modular nature is paving the way for a more flexible template in the future.</p> <p>Note</p> <p>The <code>TEMPLATE</code> branch is essential for adopting these changes.</p>"},{"location":"other/nf_develop/extra/#update-minimum-nextflow-version","title":"Update minimum Nextflow version","text":"<p>The template also includes a minimum Nextflow version that is required for the pipeline to run. You can also change the required version of Nextflow using the <code>nf-core bump-version</code> command. These versions can also be used during GitHub Actions to test your pipeline with upper and lower version limits.</p> <pre><code>nf-core bump-version 23.04.0 --nextflow\n</code></pre> <p>Exercise</p> <p>Update the minimum version of Nextflow required to run your pipeline to <code>23.04.0</code> and push the changes to GitHub.</p> <p>Tagged versions</p> <p>Creating a tagged version release allows you to execute specific versions of your pipeline using the revision flag.</p>"},{"location":"other/nf_develop/extra/#bump-your-pipeline-version","title":"Bump your pipeline version","text":"<p>Having a universal way of versioning the development projects is the best way to track what is going on with the software as new features are added. This problem can be solved by following semantic versioning rules: <code>[major].[minor].[patch]</code></p> <p>For example, starting with a release version <code>1.4.3</code>, bumping the version to:</p> <ul> <li><code>1.4.4</code> would be a patch release for minor things such as fixing bugs.</li> <li><code>1.5</code> would be a minor release, for example adding some new features.</li> <li><code>2.0</code> would correspond to the major release where results would no longer be backward compatible.</li> </ul> <p>The pipeline version number is mentioned in a lot of different places in nf-core pipelines. The <code>nf-core bump-version</code> command updates the version for you automatically, so that you don't accidentally miss any. It can be used for each pipeline release, and again for the next development version after release.</p> <pre><code>nf-core bump-version 1.0\n</code></pre> <p>After you have updated the version of your pipeline, your changes can be pushed to GitHub.</p> <p>Exercise</p> <p>Bump your pipeline version to <code>1.0</code> using the <code>nf-core bump-version</code> command.</p>"},{"location":"other/nf_develop/extra/#push-your-changes-to-github","title":"Push your changes to GitHub","text":"<p>When you are satisfied with your improvements you can <code>add</code>, <code>commit</code>, and <code>push</code> your changes to GitHub.</p> <p>You can check which branch you are on using the <code>git branch</code> command.</p> <p>As your current branch <code>myFeature</code> has no upstream branch you will need to set the remote as upstream the first time you push your changes.</p> <p>Exercise</p> <p>Push your changes to your GitHub repository.</p> <pre><code>git add .\ngit commit -m \"Added fastp to pipeline\"\ngit push --set-upstream origin myFeature\n</code></pre> <p>Branch origin</p> <p>To automatically add an origin for branches without a tracking upstream, see <code>push.autoSetupRemote</code> in <code>git help config</code>.</p>"},{"location":"other/troubleshoot/","title":"Troubleshooting","text":"<p>Effective troubleshooting strategies are essential for identifying and resolving issues efficiently, minimizing downtime, and ensuring smooth development cycles. Investing in troubleshooting strategies not only resolves immediate issues but also cultivates a culture of problem-solving and continuous improvement.</p> <p>This training will focus on examples of common mistakes that are introduced into pipelines during development.</p> <p>Let's get started!</p> <p></p>"},{"location":"other/troubleshoot/#learning-objectives","title":"Learning objectives","text":"<p>In this workshop, you will utilize the <code>hello-gatk.nf</code> script created for the Hello Nextflow workshop.</p> <p>By the end of this workshop you will be able to:</p> <ul> <li>Describe Nextflow error outputs</li> <li>List common causes of pipeline errors</li> <li>Troubleshoot basic pipeline errors</li> </ul>"},{"location":"other/troubleshoot/#audience-prerequisites","title":"Audience &amp; prerequisites","text":"<p>Please note that this is not a beginner's workshop and familiarity with Nextflow, the command line, and common file formats is assumed.</p> <p>Prerequisites</p> <ul> <li>A GitHub account</li> <li>Experience with command line</li> <li>Experience writing pipelines with Nextflow</li> </ul>"},{"location":"other/troubleshoot/01_exercise/","title":"Exercise 1","text":"<p>Move into the exercise 1 directory and execute the <code>hello-gatk.nf</code> script.</p> <pre><code>cd /workspaces/training/troubleshoot/exercise1\nnextflow run hello-gatk.nf\n</code></pre> <p>Error message</p> <p>Nextflow stops the workflow execution and reports the failing task when a process execution exits with a non-zero exit status:</p> <pre><code>ERROR ~ Error executing process &gt; 'SAMTOOLS_INDEX (2)'\n\nCaused by:\n  Process `SAMTOOLS_INDEX (2)` terminated with an error exit status (127)\n\nCommand executed:\n\n  samtools index 'reads_father.bam'\n\nCommand exit status:\n  127\n\nCommand output:\n  (empty)\n\nCommand error:\n  .command.sh: line 2: samtools: command not found\n\nWork dir:\n  /workspaces/training/troubleshoot/exercise1/work/46/e01f274e2ef1735164061d62c51169\n\nTip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line\n\n-- Check '.nextflow.log' file for details\n</code></pre> <p>Caused by: A description of the error cause</p> <p>Command executed: The command executed</p> <p>Command exit status: The command exit status</p> <p>Command output: The command standard output, when available</p> <p>Command error: The command standard error</p> <p>Work dir: The command work directory</p> Solution <p>The command error <code>.command.sh: line 2: samtools: command not found</code> suggests that <code>samtools</code> was not available.</p> <p>Every process in the <code>hello-gatk.nf</code> pipeline contains a container directive with a relevant container image. However, in this case, docker was not enabled and the <code>samtools</code> tooling was not available locally.</p> <p>Enabling docker in the <code>nextflow.config</code> file will activate the docker containers for each process and resolve this error.</p> nextflow.config<pre><code>docker.enabled = true\n</code></pre>"},{"location":"other/troubleshoot/01_orientation/","title":"Orientation","text":"<p>This workshop should be completed using the Nextflow Training GitHub Codespaces Environment:</p> <p></p>"},{"location":"other/troubleshoot/01_orientation/#getting-started","title":"Getting started","text":"<p>You will need to move into the <code>troubleshoot</code> folder.</p> <p>You will need to copy and unzip the data required for this training.</p> <p>Exercise</p> <p>Use the following command to switch to the empty <code>troubleshoot</code> folder:</p> <pre><code>cd /workspaces/training/troubleshoot\ncp -r /workspaces/training/hello-nextflow/data/ .\ntar -zxvf data/ref.tar.gz -C data/\n</code></pre> <p>To check everything is working as expected you can run the <code>hello-gatk.nf</code> script located in the <code>troubleshoot</code> folder.</p> <p>If all of the data has been copied and unzipped correctly you should see the pipeline execute three processes:</p> <ul> <li><code>SAMTOOLS_INDEX</code></li> <li><code>GATK_HAPLOTYPECALLER</code></li> <li><code>GATK_JOINTGENOTYPING</code></li> </ul>"},{"location":"other/troubleshoot/02_exercise/","title":"Exercise 2","text":"<p>Move into the exercise 2 directory and execute the <code>hello-gatk.nf</code> script.</p> <pre><code>cd /workspaces/training/troubleshoot/exercise2\nnextflow run hello-gatk.nf\n</code></pre> <p>Error message</p> <p>Although there is no error message, the <code>GATK_HAPLOTYPECALLER</code> process is only being run once not three times as expected.</p> <pre><code>executor &gt;  local (5)\n[d2/839095] process &gt; SAMTOOLS_INDEX (1)       [100%] 3 of 3 \u2714\n[1a/2dafee] process &gt; GATK_HAPLOTYPECALLER (1) [100%] 1 of 1 \u2714\n[59/278fc4] process &gt; GATK_JOINTGENOTYPING (1) [100%] 1 of 1 \u2714\n</code></pre> Solution <p>A common mistake by developers is having the wrong channel type.</p> <p>Nextflow distinguishes two different kinds of channels: queue channels and value channels.</p> <ul> <li>A queue channel is an asynchronous unidirectional FIFO queue that connects two processes or operators. Channel elements are consumed till the channel is empty.</li> <li>A value channel (a.k.a. a singleton channel) is bound to a single value and it can be read unlimited times without consuming its contents.</li> </ul> <p>It is likely that one or more channel type has been converted to a queue channel and is only being read once.</p> <p>In this example, the <code>GATK_HAPLOTYPECALLER</code> process is only executing one task.</p> <p>You can see that the channel factory <code>Channel.of</code> has been used.</p> <p>While parameters are treated as value channels, the <code>Channel.of</code> channel factory has converted these inputs to <code>queue</code> channels and are only being read once.</p> <p>By removing these the inputs will be read multiple times.</p> <p>Currently: hello-gatk.nf<pre><code>GATK_HAPLOTYPECALLER(\n    SAMTOOLS_INDEX.out,\n    Channel.of(params.genome_reference),\n    Channel.of(params.genome_reference_index),\n    Channel.of(params.genome_reference_dict),\n    Channel.of(params.calling_intervals)\n)\n</code></pre></p> <p>After: <pre><code>GATK_HAPLOTYPECALLER(\n    SAMTOOLS_INDEX.out,\n    params.genome_reference,\n    params.genome_reference_index,\n    params.genome_reference_dict,\n    params.calling_intervals\n)\n</code></pre></p>"},{"location":"other/troubleshoot/03_exercise/","title":"Exercise 3","text":"<p>Move into the exercise 3 directory and execute the <code>hello-gatk.nf</code> script.</p> <pre><code>cd /workspaces/training/troubleshoot/exercise3\nnextflow run hello-gatk.nf\n</code></pre> <p>Error message</p> <pre><code>WARN: Input tuple does not match tuple declaration in process `GATK_HAPLOTYPECALLER` -- offending value: [NA12878, /workspaces/training/troubleshoot/exercise3/work/aa/e0741cf287ce3a0c23f4dc12604b8c/reads_mother.bam, /workspaces/training/troubleshoot/exercise3/work/aa/e0741cf287ce3a0c23f4dc12604b8c/reads_mother.bam.bai]\nERROR ~ Error executing process &gt; 'GATK_HAPLOTYPECALLER (1)'\n\nCaused by:\nNot a valid path value: 'NA12878'\n\n\nTip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`\n\n-- Check '.nextflow.log' file for details\n</code></pre> Solution <p>This warning can be broken down to help identify the cause:</p> <p>You can immediately see that the <code>GATK_HAPLOTYPECALLER</code> has an offending value <code>[NA12878, /workspaces/training/troubleshoot/exercise3/work/aa/e0741cf287ce3a0c23f4dc12604b8c/reads_mother.bam, /workspaces/training/troubleshoot/exercise3/work/aa/e0741cf287ce3a0c23f4dc12604b8c/reads_mother.bam.bai]</code>.</p> <p>Your offending value may be NA12878, NA12877, or NA12882</p> <p>In this example, the value 'NA12878' is not a valid path.</p> <p>There is only one tuple for the <code>GATK_HAPLOTYPECALLER</code> process:</p> hello.gatk.nf<pre><code>tuple path(input_bam), path(input_bam_index)\n</code></pre> <p>It can be seen that this tuple is made up of two paths and offending value has one value and two paths.</p> <p>It can be concluded that the tuple has a missing value in the first position.</p> <p>To resolve this error a <code>val</code> must be added to this input.</p> <p>The name of this <code>val</code> might be used in the script block and must be considered.</p> <p>While all of the variables in the script block are accounted for, the <code>id</code> value in the output has not been specified.</p> <p>It can be concluded that the input val should be <code>val(id)</code> to match the output.</p> hello-gatk.nf<pre><code>/*\n* Call variants with GATK HapolotypeCaller in GVCF mode\n*/\nprocess GATK_HAPLOTYPECALLER {\n\n    container \"broadinstitute/gatk:4.5.0.0\"\n\n    input:\n        tuple val(id), path(input_bam), path(input_bam_index)\n        path ref_fasta\n        path ref_index\n        path ref_dict\n        path interval_list\n\n    output:\n        tuple val(id), path(\"${input_bam}.g.vcf\"), path(\"${input_bam}.g.vcf.idx\")\n\n    \"\"\"\n    gatk HaplotypeCaller \\\n        -R ${ref_fasta} \\\n        -I ${input_bam} \\\n        -O ${input_bam}.g.vcf \\\n        -L ${interval_list} \\\n        -ERC GVCF\n    \"\"\"\n}\n</code></pre>"},{"location":"other/troubleshoot/04_exercise/","title":"Exercise 4","text":"<p>Move into the exercise 4 directory and execute the <code>hello-gatk.nf</code> script.</p> <pre><code>cd /workspaces/training/troubleshoot/exercise4\nnextflow run hello-gatk.nf\n</code></pre> <p>Error message</p> <pre><code>ERROR ~ Script compilation error\n- file : /workspaces/training/troubleshoot/exercise4/hello-gatk.nf\n- cause: Unexpected input: '{' @ line 100, column 10.\nworkflow {\n            ^\n\n1 error\n\nNOTE: If this is the beginning of a process or workflow, there may be a syntax error in the body, such as a missing or extra comma, for which a more specific error message could not be produced.\n\n-- Check '.nextflow.log' file for details\n</code></pre> Solution <p>There is no easy way to identify a syntax error in the body.</p> <p>As the error message suggests, this could be as simple as a missing comma.</p> <p>While there is no quick way to find the syntax error it is advisable to test regularly when you are developing your pipeline to find errors when you only have smaller sections of changes to check.</p> <p>After reviewing the workflow block, you will see that a comma is missing from the end of line 115.</p> hello-gatk.nf<pre><code>GATK_HAPLOTYPECALLER(\n    SAMTOOLS_INDEX.out,\n    params.genome_reference,\n    params.genome_reference_index,\n    params.genome_reference_dict\n    params.calling_intervals\n)\n</code></pre>"},{"location":"other/troubleshoot/05_exercise/","title":"Exercise 5","text":"<p>Move into the exercise 5 directory and execute the <code>hello-gatk.nf</code> script.</p> <pre><code>cd /workspaces/training/troubleshoot/exercise5\nnextflow run hello-gatk.nf\n</code></pre> <p>Error message</p> <pre><code>ERROR ~ Error executing process &gt; 'GATK_JOINTGENOTYPING (1)'\n\nCaused by:\nProcess `GATK_JOINTGENOTYPING (1)` terminated with an error exit status (2)\n\nCommand executed:\n\ngatk GenomicsDBImport         --sample-name-map family_trio_map.tsv         --genomicsdb-workspace-path family_trio_gdb         -L intervals.list\n\ngatk GenotypeGVCFs         -R ref.fasta         -V gendb://family_trio_gdb         -O family_trio.joint.vcf         -L intervals.list\n\nCommand exit status:\n2\n\nCommand output:\n(empty)\n\nCommand error:\n16:33:09.066 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so\n16:33:09.374 INFO  GenomicsDBImport - ------------------------------------------------------------\n16:33:09.380 INFO  GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.5.0.0\n16:33:09.380 INFO  GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/\n16:33:09.381 INFO  GenomicsDBImport - Executing as root@5f94bf7c2fe0 on Linux v6.1.75-060175-generic amd64\n16:33:09.381 INFO  GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.9+9-Ubuntu-122.04\n16:33:09.382 INFO  GenomicsDBImport - Start Date/Time: April 24, 2024 at 4:33:08 PM GMT\n16:33:09.383 INFO  GenomicsDBImport - ------------------------------------------------------------\n16:33:09.383 INFO  GenomicsDBImport - ------------------------------------------------------------\n16:33:09.385 INFO  GenomicsDBImport - HTSJDK Version: 4.1.0\n16:33:09.386 INFO  GenomicsDBImport - Picard Version: 3.1.1\n16:33:09.387 INFO  GenomicsDBImport - Built for Spark Version: 3.5.0\n16:33:09.388 INFO  GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2\n16:33:09.388 INFO  GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false\n16:33:09.388 INFO  GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true\n16:33:09.389 INFO  GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false\n16:33:09.389 INFO  GenomicsDBImport - Deflater: IntelDeflater\n16:33:09.389 INFO  GenomicsDBImport - Inflater: IntelInflater\n16:33:09.389 INFO  GenomicsDBImport - GCS max retries/reopens: 20\n16:33:09.389 INFO  GenomicsDBImport - Requester pays: disabled\n16:33:09.391 INFO  GenomicsDBImport - Initializing engine\n16:33:09.397 INFO  GenomicsDBImport - Shutting down engine\n[April 24, 2024 at 4:33:09 PM GMT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.\nRuntime.totalMemory()=201326592\n***********************************************************************\n\nA USER ERROR has occurred: Bad input: Sample name map file must have 2 or 3 fields per line in the format:\nSample        File\nor:\nSample        File    Index\nbut found line: \"NA12877/t/workspaces/training/troubleshoot/exercise5/work/db/0d4c6d0c8bad080cec4a1e09217159/reads_father.bam.g.vcf/t/workspaces/training/troubleshoot/exercise5/work/db/0d4c6d0c8bad080cec4a1e09217159/reads_father.bam.g.vcf.idx/nNA12882/t/workspaces/training/troubleshoot/exercise5/work/a7/84fe9e9038bed36bdbf620847b47c2/reads_son.bam.g.vcf/t/workspaces/training/troubleshoot/exercise5/work/a7/84fe9e9038bed36bdbf620847b47c2/reads_son.bam.g.vcf.idx/nNA12878/t/workspaces/training/troubleshoot/exercise5/work/59/a653a3855cf6f6c70bc7b42aff0a0d/reads_mother.bam.g.vcf/t/workspaces/training/troubleshoot/exercise5/work/59/a653a3855cf6f6c70bc7b42aff0a0d/reads_mother.bam.g.vcf.idx/n\" with 1 fields\n\n***********************************************************************\nSet the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.\nUsing GATK jar /gatk/gatk-package-4.5.0.0-local.jar\nRunning:\n    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.5.0.0-local.jar GenomicsDBImport --sample-name-map family_trio_map.tsv --genomicsdb-workspace-path family_trio_gdb -L intervals.list\n\nWork dir:\n/workspaces/training/troubleshoot/exercise5/work/a3/fa7fab1e9d1a1e0ec2d65e87b9cebf\n\nTip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`\n\n-- Check '.nextflow.log' file for details\n</code></pre> Solution <p>The error message from this execution can be broken down to give clues.</p> <pre><code>Command exit status:\n2\n</code></pre> <p>Exit code <code>2</code> suggests invalid usage of a shell built-in command. Examples of built-in commands include alias, echo, and printf. Alternatively, it could mean that you are trying to access a file or directory that doesn't exist or requires permissions.</p> <pre><code>A USER ERROR has occurred: Bad input: Sample name map file must have 2 or 3 fields per line in the format\n</code></pre> <p>The user error <code>Bad input: Sample name map</code> suggests that the sample name map is the problematic.</p> hello-gatk.nf<pre><code>gatk GenomicsDBImport \\\n    --sample-name-map ${sample_map} \\\n    --genomicsdb-workspace-path ${cohort_name}_gdb \\\n    -L ${interval_list}\n\ngatk GenotypeGVCFs \\\n    -R ${ref_fasta} \\\n    -V gendb://${cohort_name}_gdb \\\n    -O ${cohort_name}.joint.vcf \\\n    -L ${interval_list}\n</code></pre> <p>The GATK GenomicsDBImport documentation can be used to help us interstand the GenomicsDBImport specific error.</p> Argument name Default value Summary --sample-name-map null Path to file containing a mapping of sample name to file uri in tab delimited format. If this is specified then the header from the first sample will be treated as the merged header rather than merging the headers, and the sample names will be taken from this file. This may be used to rename input samples. This is a performance optimization that relaxes the normal checks for consistent headers. Using vcfs with incompatible headers may result in silent data corruption. <p>Although the documentation does not explicitly describe the error, it highlights the importance of the structure of the map, something that is also suggested in the error message.</p> <p>By reviewing the code where the <code>sample_map</code> channel is created, it can be seen that the map has <code>/</code> rather than <code>\\</code> for the tab delimiter.</p> hello-gatk.nf<pre><code>// Create a sample map of the output GVCFs\nsample_map = GATK_HAPLOTYPECALLER.out.collectFile(){ id, gvcf, idx -&gt;\n        [\"${params.cohort_name}_map.tsv\", \"${id}/t${gvcf}/t${idx}/n\"]\n}\n</code></pre> <p>By correcting this the error will be resolved.</p> hello-gatk.nf<pre><code>// Create a sample map of the output GVCFs\nsample_map = GATK_HAPLOTYPECALLER.out.collectFile(){ id, gvcf, idx -&gt;\n        [\"${params.cohort_name}_map.tsv\", \"${id}\\t${gvcf}\\t${idx}\\n\"]\n}\n</code></pre>"},{"location":"other/troubleshoot/06_exercise/","title":"Exercise 6","text":"<p>Move into the exercise 6 directory and execute the <code>hello-gatk.nf</code> script.</p> <pre><code>cd /workspaces/training/troubleshoot/exercise6\nnextflow run hello-gatk.nf\n</code></pre> <p>Error message</p> <pre><code>ERROR ~ No such file or directory: /workspaces/training/troubleshoot/exercise6/data/samplesheet.csv\n\n-- Check '.nextflow.log' file for details\n</code></pre> Solution <p>The error message suggests that the path to the <code>samplesheet.csv</code> is incorrect.</p> <p>Searching the file directory, indeed, <code>/workspaces/training/troubleshoot/exercise6/data/samplesheet.csv</code> does not exist.</p> <p>The complete path should be <code>/workspaces/training/troubleshoot/data/samplesheet.csv</code>.</p> <p>By examining how the path to <code>samplesheet.csv</code> is derived it can be seen that there is no obvious path error.</p> hello-gatk.nf<pre><code>// Execution environment setup\nparams.baseDir = \"/workspaces/training/troubleshoot\"\n$baseDir = params.baseDir\n\n// Primary input\nparams.reads_bam = \"${baseDir}/data/samplesheet.csv\"\n</code></pre> <p>However, it can be noted that there is a <code>$</code> before <code>baseDir</code> on line 7 which would suggest it is a variable.</p> <p><code>$baseDir</code> was once an implicit variable, and its usage here is causing the wrong path.</p> <p>By removing the <code>$</code> the proper path should be resolved.</p> hello-gatk.nf<pre><code>baseDir = params.baseDir\n</code></pre>"},{"location":"side_quests/","title":"Side Quests","text":"<p>This is a collection of standalone training mini-courses that go deeper into specific topics. You can go through them in any order.</p> <p>Let's get started! Click on the \"Open in GitHub Codespaces\" button below to launch the training environment (preferably in a separate tab), then read on while it loads.</p> <p></p>"},{"location":"side_quests/#prerequisites","title":"Prerequisites","text":"<p>The specific prerequisites of each mini-courses vary and are documented in the corresponding pages. Nevertheless, they all assume some minimal familiarity with the following:</p> <ul> <li>Experience with the command line</li> <li>Foundational Nextflow concepts and tooling covered in the Hello Nextflow beginner training course.</li> </ul> <p>For technical requirements and environment setup, see the Environment Setup mini-course.</p> <p>If this is the first time you delve into the Side Quests, make sure to check out the Orientation page first!</p> <p>Otherwise, select a side quest from the table below.</p>"},{"location":"side_quests/#side-quests_1","title":"Side Quests","text":"Side Quest Time Estimate for Teaching Nextflow development environment walkthrough 45 mins Essential Nextflow Scripting Patterns 90 mins Introduction to nf-core - Metadata in workflows 45 mins Splitting and Grouping 45 mins Testing with nf-test 1 hour Workflows of workflows 30 mins Working with files 45 mins Debugging workflows 1 hour <p>Let us know what other domains and use cases you'd like to see covered here by posting in the Training section of the community forum.</p>"},{"location":"side_quests/containers/","title":"Part 1: More Containers","text":"<p>[TODO]</p>"},{"location":"side_quests/containers/#1-how-to-find-or-make-container-images","title":"1. How to find or make container images","text":"<p>Some software developers provide container images for their software that are available on container registries like Docker Hub, but many do not. In this optional section, we'll show you to two ways to get a container image for tools you want to use in your Nextflow pipelines: using Seqera Containers and building the container image yourself.</p> <p>You'll be getting/building a container image for the <code>quote</code> pip package, which will be used in the exercise at the end of this section.</p>"},{"location":"side_quests/containers/#11-get-a-container-image-from-seqera-containers","title":"1.1. Get a container image from Seqera Containers","text":"<p>Seqera Containers is a free service that builds container images for pip and conda (including bioconda) installable tools. Navigate to Seqera Containers and search for the <code>quote</code> pip package.</p> <p></p> <p>Click on \"+Add\" and then \"Get Container\" to request a container image for the <code>quote</code> pip package.</p> <p></p> <p>If this is the first time a community container has been built for this version of the package, it may take a few minutes to complete. Click to copy the URI (e.g. <code>community.wave.seqera.io/library/pip_quote:ae07804021465ee9</code>) of the container image that was created for you.</p> <p>You can now use the container image to run the <code>quote</code> command and get a random saying from Grace Hopper.</p> <pre><code>docker run --rm community.wave.seqera.io/library/pip_quote:ae07804021465ee9 quote \"Grace Hopper\"\n</code></pre> <p>Output:</p> Output<pre><code>Humans are allergic to change. They love to say, 'We've always done it\nthis way.' I try to fight that. That's why I have a clock on my wall\nthat runs counter-clockwise.\n</code></pre>"},{"location":"side_quests/containers/#12-build-the-container-image-yourself","title":"1.2. Build the container image yourself","text":"<p>Let's use some build details from the Seqera Containers website to build the container image for the <code>quote</code> pip package ourselves. Return to the Seqera Containers website and click on the \"Build Details\" button.</p> <p>The first item we'll look at is the <code>Dockerfile</code>, a type of script file that contains all the commands needed to build the container image. We've added some explanatory comments to the Dockerfile below to help you understand what each part does.</p> Dockerfile<pre><code># Start from the micromamba base docker image\nFROM mambaorg/micromamba:1.5.10-noble\n# Copy the conda.yml file into the container\nCOPY --chown=$MAMBA_USER:$MAMBA_USER conda.yml /tmp/conda.yml\n# Install various utilities for Nextflow to use and the packages in the conda.yml file\nRUN micromamba install -y -n base -f /tmp/conda.yml \\\n    &amp;&amp; micromamba install -y -n base conda-forge::procps-ng \\\n    &amp;&amp; micromamba env export --name base --explicit &gt; environment.lock \\\n    &amp;&amp; echo \"&gt;&gt; CONDA_LOCK_START\" \\\n    &amp;&amp; cat environment.lock \\\n    &amp;&amp; echo \"&lt;&lt; CONDA_LOCK_END\" \\\n    &amp;&amp; micromamba clean -a -y\n# Run the container as the root user\nUSER root\n# Set the PATH environment variable to include the micromamba installation directory\nENV PATH=\"$MAMBA_ROOT_PREFIX/bin:$PATH\"\n</code></pre> <p>The second item we'll look at is the <code>conda.yml</code> file, which contains the list of packages that need to be installed in the container image.</p> conda.yml<pre><code>channels:\n- conda-forge\n- bioconda\ndependencies:\n- pip\n- pip:\n  - quote==3.0.0 #\n</code></pre> <p>Copy the contents of these files into the stubs located in the <code>containers/build</code> directory, then run the following command to build the container image yourself.</p> <p>Note</p> <p>We use the <code>-t quote:latest</code> flag to tag the container image with the name <code>quote</code> and the tag <code>latest</code>. We will be able to use this tag to refer to the container image when running it on this system.</p> <pre><code>docker build -t quote:latest containers/build\n</code></pre> <p>After it has finished building, you can run the container image you just built.</p> <pre><code>docker run --rm quote:latest quote \"Margaret Oakley Dayhoff\"\n</code></pre>"},{"location":"side_quests/containers/#takeaway","title":"Takeaway","text":"<p>You've learned two different ways to get a container image for a tool you want to use in your Nextflow pipelines: using Seqera Containers and building the container image yourself.</p>"},{"location":"side_quests/containers/#whats-next","title":"What's next?","text":"<p>You have everything you need to continue to the next chapter of this training series. You can also continue on with an optional exercise to fetch quotes on computer/biology pioneers using the <code>quote</code> container and output them using the <code>cowsay</code> container.</p>"},{"location":"side_quests/containers/#2-make-the-cow-quote-famous-scientists","title":"2. Make the cow quote famous scientists","text":"<p>This section contains some stretch exercises, to practice what you've learned so far. Doing these exercises is not required to understand later parts of the training, but provide a fun way to reinforce your learnings by figuring out how to make the cow quote famous scientists.</p> cowsay-output-Grace-Hopper.txt<pre><code>  _________________________________________________\n /                                                 \\\n| Humans are allergic to change. They love to       |\n| say, 'We've always done it this way.' I try to fi |\n| ght that. That's why I have a clock on my wall th |\n| at runs counter-clockwise.                        |\n| -Grace Hopper                                     |\n \\                                                 /\n  =================================================\n                                                 \\\n                                                  \\\n                                                    ^__^\n                                                    (oo)\\_______\n                                                    (__)\\       )\\/\\\n                                                        ||----w |\n                                                        ||     ||\n</code></pre>"},{"location":"side_quests/containers/#21-modify-the-hello-containersnf-script-to-use-a-getquote-process","title":"2.1. Modify the <code>hello-containers.nf</code> script to use a getQuote process","text":"<p>We have a list of computer and biology pioneers in the <code>containers/data/pioneers.csv</code> file. At a high level, to complete this exercise you will need to:</p> <ul> <li>Modify the default <code>params.input_file</code> to point to the <code>pioneers.csv</code> file.</li> <li>Create a <code>getQuote</code> process that uses the <code>quote</code> container to fetch a quote for each input.</li> <li>Connect the output of the <code>getQuote</code> process to the <code>cowsay</code> process to display the quote.</li> </ul> <p>For the <code>quote</code> container image, you can either use the one you built yourself in the previous stretch exercise or use the one you got from Seqera Containers .</p> <p>Hint</p> <p>A good choice for the <code>script</code> block of your getQuote process might be:     <pre><code>script:\n    def safe_author = author.tokenize(' ').join('-')\n    \"\"\"\n    quote \"$author\" &gt; quote-${safe_author}.txt\n    echo \"-${author}\" &gt;&gt; quote-${safe_author}.txt\n    \"\"\"\n</code></pre></p> <p>You can find a solution to this exercise in <code>containers/solutions/hello-containers-4.1.nf</code>.</p>"},{"location":"side_quests/containers/#22-modify-your-nextflow-pipeline-to-allow-it-to-execute-in-quote-and-sayhello-modes","title":"2.2. Modify your Nextflow pipeline to allow it to execute in <code>quote</code> and <code>sayHello</code> modes.","text":"<p>Add some branching logic using to your pipeline to allow it to accept inputs intended for both <code>quote</code> and <code>sayHello</code>. Here's an example of how to use an <code>if</code> statement in a Nextflow workflow:</p> hello-containers.nf<pre><code>workflow {\n    if (params.quote) {\n        ...\n    }\n    else {\n        ...\n    }\n    cowSay(text_ch)\n}\n</code></pre> <p>Hint</p> <p>You can use <code>new_ch = processName.out</code> to assign a name to the output channel of a process.</p> <p>You can find a solution to this exercise in <code>containers/solutions/hello-containers-4.2.nf</code>.</p>"},{"location":"side_quests/containers/#takeaway_1","title":"Takeaway","text":"<p>You know how to use containers in Nextflow to run processes, and how to build some branching logic into your pipelines!</p>"},{"location":"side_quests/containers/#whats-next_1","title":"What's next?","text":"<p>Celebrate, take a stretch break and drink some water!</p> <p>When you are ready, move on to Part 3 of this training series to learn how to apply what you've learned so far to a more realistic data analysis use case.</p>"},{"location":"side_quests/debugging/","title":"Debugging Workflows","text":"<p>Debugging is a critical skill that can save you hours of frustration and help you become a more effective Nextflow developer. Throughout your career, especially when you're starting out, you'll encounter bugs while building and maintaining your workflows. Learning systematic debugging approaches will help you identify and resolve issues quickly.</p>"},{"location":"side_quests/debugging/#what-you-should-know-first","title":"What you should know first","text":"<p>This guide assumes you've completed the Hello Nextflow training course and are comfortable with foundational Nextflow concepts including basic workflow structure, processes, channels, and configuration.</p> <p>This guide focuses on debugging techniques and workflows. For comprehensive coverage of IDE features that support debugging (syntax highlighting, error detection, etc.), see the dedicated IDE Features for Nextflow Development side quest. We recommend completing the IDE training beforehand.</p>"},{"location":"side_quests/debugging/#what-youll-learn-here","title":"What you'll learn here","text":"<p>This guide focuses on systematic debugging techniques for Nextflow workflows:</p> <ul> <li>Syntax error debugging: Using IDE features and Nextflow error messages effectively</li> <li>Channel debugging: Diagnosing data flow issues and channel structure problems</li> <li>Process debugging: Investigating execution failures and resource issues</li> <li>Built-in debugging tools: Leveraging Nextflow's preview mode, stub running, and work directories</li> <li>Systematic approaches: A four-phase methodology for efficient debugging</li> </ul> <p>By the end, you'll have a robust debugging methodology that transforms frustrating error messages into clear roadmaps for solutions.</p>"},{"location":"side_quests/debugging/#0-warmup","title":"0. Warmup","text":"<p>Let's move into the debugging exercise directory:</p> <pre><code>cd side-quests/debugging\n</code></pre> <p>You can set VSCode to focus on this directory:</p> <pre><code>code .\n</code></pre> <p>The directory contains example workflows with various types of bugs that we'll use for practice:</p> <pre><code>tree .\n</code></pre> Project structure<pre><code>.\n\u251c\u2500\u2500 bad_bash_var.nf\n\u251c\u2500\u2500 bad_channel_shape.nf\n\u251c\u2500\u2500 bad_channel_shape_viewed_debug.nf\n\u251c\u2500\u2500 bad_channel_shape_viewed.nf\n\u251c\u2500\u2500 bad_number_inputs.nf\n\u251c\u2500\u2500 badpractice_syntax.nf\n\u251c\u2500\u2500 bad_resources.nf\n\u251c\u2500\u2500 bad_syntax.nf\n\u251c\u2500\u2500 buggy_workflow.nf\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 sample_001.fastq.gz\n\u2502   \u251c\u2500\u2500 sample_002.fastq.gz\n\u2502   \u251c\u2500\u2500 sample_003.fastq.gz\n\u2502   \u251c\u2500\u2500 sample_004.fastq.gz\n\u2502   \u251c\u2500\u2500 sample_005.fastq.gz\n\u2502   \u2514\u2500\u2500 sample_data.csv\n\u251c\u2500\u2500 exhausted.nf\n\u251c\u2500\u2500 invalid_process.nf\n\u251c\u2500\u2500 missing_output.nf\n\u251c\u2500\u2500 missing_software.nf\n\u251c\u2500\u2500 missing_software_with_stub.nf\n\u251c\u2500\u2500 nextflow.config\n\u2514\u2500\u2500 no_such_var.nf\n\n1 directory, 22 files\n</code></pre> <p>These files represent common debugging scenarios you'll encounter in real-world development.</p>"},{"location":"side_quests/debugging/#takeaway","title":"Takeaway","text":"<p>You're set up with example files containing various types of bugs that we'll debug systematically throughout this guide.</p>"},{"location":"side_quests/debugging/#whats-next","title":"What's next?","text":"<p>Learn to identify and fix the most common type of error: syntax errors.</p>"},{"location":"side_quests/debugging/#1-syntax-errors","title":"1. Syntax Errors","text":"<p>Syntax errors are the most common type of error you'll encounter when writing Nextflow code. They occur when the code does not conform to the expected syntax rules of the Nextflow DSL. These errors prevent your workflow from running at all, so it's important to learn how to identify and fix them quickly.</p>"},{"location":"side_quests/debugging/#11-missing-braces","title":"1.1. Missing braces","text":"<p>One of the most common syntax errors, and sometimes one of the more complex ones to debug is missing or mismatched brackets.</p> <p>Let's start with a practical example.</p>"},{"location":"side_quests/debugging/#run-the-pipeline","title":"Run the pipeline","text":"<pre><code>nextflow run bad_syntax.nf\n</code></pre> <p>You'll see an error message like this:</p> Syntax error output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `bad_syntax.nf` [stupefied_bhabha] DSL2 - revision: ca6327fad2\n\nERROR ~ Script compilation error\n- file : /workspaces/training/side-quests/debugging/bad_syntax.nf\n- cause: Unexpected input: '{' @ line 3, column 23.\n   process PROCESS_FILES {\n                         ^\n\n1 error\n\nNOTE: If this is the beginning of a process or workflow, there may be a syntax error in the body, such as a missing or extra comma, for which a more specific error message could not be produced.\n\n -- Check '.nextflow.log' file for details\n</code></pre> <p>Key elements of syntax error messages:</p> <ul> <li>File location: Shows exactly which file contains the error (<code>- file : /workspaces/training/side-quests/debugging/bad_syntax.nf</code>)</li> <li>Error description: Explains what the parser found that it didn't expect (<code>- cause: Unexpected input: '{'</code>)</li> <li>Line and column: Points to where the parser encountered the problem (<code>@ line 3, column 23.</code>)</li> <li>Context: Shows the problematic line with a caret (^) pointing to location of an unclosed brace (<code>process PROCESS_FILES {</code>)</li> <li>Additional notes: Provides hints about common causes</li> </ul>"},{"location":"side_quests/debugging/#check-the-code","title":"Check the code","text":"<p>Now, let's examine <code>bad_syntax.nf</code> to understand what's causing the error:</p> bad_syntax.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    echo \"Processing ${sample_name}\" &gt; ${sample_name}_output.txt\n    \"\"\"\n// Missing closing brace for the process\n\nworkflow {\n\n    // Create input channel\n    input_ch = Channel.of('sample1', 'sample2', 'sample3')\n\n    // Call the process with the input channel\n    PROCESS_FILES(input_ch)\n}\n</code></pre> <p>For the purpose of this example we've left a comment for you to show where the error is. The Nextflow VSCode extension should also be giving you some hints about what might be wrong, putting the mismatched brace in red and highlighting the premature end of the file:</p> <p></p> <p>Debugging strategy for bracket errors:</p> <ol> <li>Use VS Code's bracket matching (place cursor next to a bracket)</li> <li>Check the Problems panel for bracket-related messages</li> <li>Ensure each opening <code>{</code> has a corresponding closing <code>}</code></li> </ol>"},{"location":"side_quests/debugging/#fix-the-code","title":"Fix the code","text":"<p>Replace the comment with the missing closing brace:</p> AfterBefore bad_syntax.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    echo \"Processing ${sample_name}\" &gt; ${sample_name}_output.txt\n    \"\"\"\n}  // Add the missing closing brace\n\nworkflow {\n\n    // Create input channel\n    input_ch = Channel.of('sample1', 'sample2', 'sample3')\n\n    // Call the process with the input channel\n    PROCESS_FILES(input_ch)\n}\n</code></pre> bad_syntax.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    echo \"Processing ${sample_name}\" &gt; ${sample_name}_output.txt\n    \"\"\"\n// Missing closing brace for the process\n\nworkflow {\n\n    // Create input channel\n    input_ch = Channel.of('sample1', 'sample2', 'sample3')\n\n    // Call the process with the input channel\n    PROCESS_FILES(input_ch)\n}\n</code></pre>"},{"location":"side_quests/debugging/#run-the-pipeline_1","title":"Run the pipeline","text":"<p>Now run the workflow again to confirm it works:</p> <pre><code>nextflow run bad_syntax.nf\n</code></pre>"},{"location":"side_quests/debugging/#12-using-incorrect-process-keywords-or-directives","title":"1.2. Using incorrect process keywords or directives","text":"<p>Another common syntax error is an invalid process definition. This can happen if you forget to define required blocks or use incorrect directives in the process definition.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_2","title":"Run the pipeline","text":"<pre><code>nextflow run invalid_process.nf\n</code></pre> <p>You'll see an error like:</p> Invalid process keyword error<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `invalid_process.nf` [nasty_jepsen] DSL2 - revision: da9758d614\n\nERROR ~ Script compilation error\n- file : /workspaces/training/side-quests/debugging/invalid_process.nf\n- cause: Invalid process definition -- Unknown keyword `inputs` @ line 5, column 5.\n       val sample_name\n       ^\n\n1 error\n\n\n -- Check '.nextflow.log' file for details\n</code></pre>"},{"location":"side_quests/debugging/#check-the-code_1","title":"Check the code","text":"<p>Let's examine <code>invalid_process.nf</code> to see what's wrong:</p> invalid_process.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    inputs:  // ERROR: Should be 'input' not 'inputs'\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    echo \"Processing ${sample_name}\" &gt; ${sample_name}_output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Create input channel\n    input_ch = Channel.of('sample1', 'sample2', 'sample3')\n\n    // Call the process with the input channel\n    PROCESS_FILES(input_ch)\n}\n</code></pre> <p>The error message was quite straightforward - we're using <code>inputs</code> instead of the correct <code>input</code> directive. You'll also see that the Nextflow VSCode exension is unhappy:</p> <p></p>"},{"location":"side_quests/debugging/#fix-the-code_1","title":"Fix the code","text":"<p>Replace the incorrect keyword with the correct one by referencing the documentation:</p> AfterBefore invalid_process.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:  // Fixed: Changed 'inputs' to 'input'\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    echo \"Processing ${sample_name}\" &gt; ${sample_name}_output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Create input channel\n    input_ch = Channel.of('sample1', 'sample2', 'sample3')\n\n    // Call the process with the input channel\n    PROCESS_FILES(input_ch)\n}\n</code></pre> invalid_process.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    inputs:  // ERROR: Should be 'input' not 'inputs'\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    echo \"Processing ${sample_name}\" &gt; ${sample_name}_output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Create input channel\n    input_ch = Channel.of('sample1', 'sample2', 'sample3')\n\n    // Call the process with the input channel\n    PROCESS_FILES(input_ch)\n}\n</code></pre>"},{"location":"side_quests/debugging/#run-the-pipeline_3","title":"Run the pipeline","text":"<p>Now run the workflow again to confirm it works:</p> <pre><code>nextflow run invalid_process.nf\n</code></pre>"},{"location":"side_quests/debugging/#13-using-bad-variable-names","title":"1.3. Using bad variable names","text":"<p>The variable names you use in your script blocks must be valid, derived either from inputs or from groovy code inserted before the script. But when you're wrangling complexity at the start of pipeline development, it's easy to make mistakes in variable naming, and Nextflow will let you know quickly.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_4","title":"Run the pipeline","text":"<pre><code>nextflow run no_such_var.nf\n</code></pre> <p>You should get a failure that looks like this:</p> No such variable error<pre><code>ERROR ~ Error executing process &gt; 'PROCESS_FILES (3)'\n\nCaused by:\n  No such variable: undefined_var -- Check script 'no_such_var.nf' at line: 15\n\n\nSource block:\n  def output_prefix = \"${sample_name}_processed\"\n  def timestamp = new Date().format(\"yyyy-MM-dd\")\n  \"\"\"\n  echo \"Processing ${sample_name} on ${timestamp}\" &gt; ${output_prefix}.txt\n  echo \"Using undefined variable: ${undefined_var}\" &gt;&gt; ${output_prefix}.txt  // ERROR: undefined_var not defined\n  \"\"\"\n\nTip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line\n\n -- Check '.nextflow.log' file for details\n</code></pre>"},{"location":"side_quests/debugging/#check-the-code_2","title":"Check the code","text":"<p>Let's examine <code>no_such_var.nf</code>:</p> no_such_var.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    // Define variables in Groovy code before the script\n    def output_prefix = \"${sample_name}_processed\"\n    def timestamp = new Date().format(\"yyyy-MM-dd\")\n\n    \"\"\"\n    echo \"Processing ${sample_name} on ${timestamp}\" &gt; ${output_prefix}.txt\n    echo \"Using undefined variable: ${undefined_var}\" &gt;&gt; ${output_prefix}.txt  // ERROR: undefined_var not defined\n    \"\"\"\n}\n\nworkflow {\n    input_ch = Channel.of('sample1', 'sample2', 'sample3')\n    PROCESS_FILES(input_ch)\n}\n</code></pre> <p>The error message indicates that the variable is not recognized in the script template, and there you go- you should be able to see <code>${undefined_var}</code> used in the script block, but not defined elsewhere.</p>"},{"location":"side_quests/debugging/#fix-the-code_2","title":"Fix the code","text":"<p>If you get a 'No such variable' error, you can fix it by either defining the variable (by correcting input variable names or editing groovy code before the script), or by removing it from the script block if it's not needed:</p> AfterBefore no_such_var.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    // Define variables in Groovy code before the script\n    def output_prefix = \"${sample_name}_processed\"\n    def timestamp = new Date().format(\"yyyy-MM-dd\")\n\n    \"\"\"\n    echo \"Processing ${sample_name} on ${timestamp}\" &gt; ${output_prefix}.txt\n    \"\"\"  // Removed the line with undefined_var\n}\n\nworkflow {\n    input_ch = Channel.of('sample1', 'sample2', 'sample3')\n    PROCESS_FILES(input_ch)\n}\n</code></pre> no_such_var.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    // Define variables in Groovy code before the script\n    def output_prefix = \"${sample_name}_processed\"\n    def timestamp = new Date().format(\"yyyy-MM-dd\")\n\n    \"\"\"\n    echo \"Processing ${sample_name} on ${timestamp}\" &gt; ${output_prefix}.txt\n    echo \"Using undefined variable: ${undefined_var}\" &gt;&gt; ${output_prefix}.txt  // ERROR: undefined_var not defined\n    \"\"\"\n}\n\nworkflow {\n    input_ch = Channel.of('sample1', 'sample2', 'sample3')\n    PROCESS_FILES(input_ch)\n}\n</code></pre>"},{"location":"side_quests/debugging/#run-the-pipeline_5","title":"Run the pipeline","text":"<p>Now run the workflow again to confirm it works:</p> <pre><code>nextflow run no_such_var.nf\n</code></pre>"},{"location":"side_quests/debugging/#14-bad-use-of-bash-variables","title":"1.4. Bad use of Bash variables","text":"<p>Starting out in Nextflow, it can be difficult to understand the difference between Nextflow (Groovy) and Bash variables. This can generate another form of the bad variable error that appears when trying to use variables in the Bash content of the script block.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_6","title":"Run the pipeline","text":"<pre><code>nextflow run bad_bash_var.nf\n</code></pre> <p>This throws the following error:</p> <pre><code>ERROR ~ Error executing process &gt; 'PROCESS_FILES (1)'\n\nCaused by:\n  No such variable: prefix -- Check script 'bad_bash_var.nf' at line: 11\n</code></pre>"},{"location":"side_quests/debugging/#check-the-code_3","title":"Check the code","text":"<p>Let's examine <code>bad_bash_var.nf</code> to see what's causing the issue:</p> bad_bash_var.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    prefix=\"${sample_name}_output\"\n    echo \"Processing ${sample_name}\" &gt; ${prefix}.txt  # ERROR: ${prefix} is Groovy syntax, not Bash\n    \"\"\"\n}\n</code></pre> <p>In this example, we're defining the <code>prefix</code> variable in Bash, but in a Nexflow process the <code>$</code> syntax we used to refer to it (<code>${prefix}</code>) is interpretes as a Groovy variable, not Bash. The variable doesn't exist in the Groovy context, so we get a 'no such variable' error.</p>"},{"location":"side_quests/debugging/#fix-the-code_3","title":"Fix the code","text":"<p>If you want to use a Bash variable, you must escape the dollar sign like this:</p> AfterBefore bad_bash_var.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    prefix=\"${sample_name}_output\"\n    echo \"Processing ${sample_name}\" &gt; \\${prefix}.txt  # Fixed: Escaped the dollar sign\n    \"\"\"\n}\n\nworkflow {\n    input_ch = Channel.of('sample1', 'sample2', 'sample3')\n    PROCESS_FILES(input_ch)\n}\n</code></pre> bad_bash_var.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    prefix=\"${sample_name}_output\"\n    echo \"Processing ${sample_name}\" &gt; ${prefix}.txt  # ERROR: ${prefix} is Groovy syntax, not Bash\n    \"\"\"\n}\n</code></pre> <p>This tells Nextflow to interpret this as a Bash variable.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_7","title":"Run the pipeline","text":"<p>Now run the workflow again to confirm it works:</p> <pre><code>nextflow run bad_bash_var.nf\n</code></pre> <p>Groovy vs Bash Variables</p> <p>For simple variable manipulations like string concatenation or prefix/suffix operations, it's usually more readable to use Groovy variables in the script section rather than Bash variables in the script block:</p> <pre><code>script:\ndef output_prefix = \"${sample_name}_processed\"\ndef output_file = \"${output_prefix}.txt\"\n\"\"\"\necho \"Processing ${sample_name}\" &gt; ${output_file}\n\"\"\"\n</code></pre> <p>This approach avoids the need to escape dollar signs and makes the code easier to read and maintain.</p>"},{"location":"side_quests/debugging/#15-non-fatal-syntax-warnings","title":"1.5. Non-Fatal Syntax Warnings","text":"<p>The Nextflow VSCode extension sometimes highlights issues that won't prevent execution but represent bad practices. For example, it's currently possible to define channels outside of the <code>workflow {}</code> block, but it's not good practice and the extension will highlight this as a potential issue. These warnings help you write better code even though they're not fatal errors.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_8","title":"Run the pipeline","text":"<pre><code>nextflow run badpractice_syntax.nf\n</code></pre> <p>When you run this workflow, it will execute successfully:</p> Successful execution despite bad practice<pre><code>N E X T F L O W   ~  version 25.04.3\n\nLaunching `badpractice_syntax.nf` [peaceful_euler] DSL2 - revision: 7b2c9a1d45\n\nexecutor &gt;  local (3)\n[a1/b2c3d4] process &gt; PROCESS_FILES (1) [100%] 3 of 3 \u2714\n</code></pre>"},{"location":"side_quests/debugging/#check-the-code_4","title":"Check the code","text":"<p>Let's examine <code>badpractice_syntax.nf</code> to see what the VSCode extension is warning about:</p> badpractice_syntax.nf<pre><code>#!/usr/bin/env nextflow\n\ninput_ch = Channel.of('sample1', 'sample2', 'sample3')  # WARNING: Channel defined outside workflow\n\nprocess PROCESS_FILES {\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_processed.txt\"\n\n    script:\n    // Define variables in Groovy code before the script\n    def output_prefix = \"${sample_name}_processed\"\n    def timestamp = new Date().format(\"yyyy-MM-dd\")\n\n    \"\"\"\n    echo \"Processing ${sample_name} on ${timestamp}\" &gt; ${output_prefix}.txt\n    \"\"\"\n}\n\nworkflow {\n    PROCESS_FILES(input_ch)\n}\n</code></pre> <p>The VSCode extension will highlight the <code>input_ch</code> variable as being defined outside the workflow block, which is not recommended:</p> <p></p> <p>This won't prevent execution but could lead to confusion or unexpected behavior in larger workflows.</p>"},{"location":"side_quests/debugging/#fix-the-code_4","title":"Fix the code","text":"<p>Follow the VSCode extension's recommendation by moving the channel definition inside the workflow block:</p> AfterBefore badpractice_syntax.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_processed.txt\"\n\n    script:\n    // Define variables in Groovy code before the script\n    def output_prefix = \"${sample_name}_processed\"\n    def timestamp = new Date().format(\"yyyy-MM-dd\")\n\n    \"\"\"\n    echo \"Processing ${sample_name} on ${timestamp}\" &gt; ${output_prefix}.txt\n    \"\"\"\n}\n\nworkflow {\n    input_ch = Channel.of('sample1', 'sample2', 'sample3')  # Moved inside workflow block\n    PROCESS_FILES(input_ch)\n}\n</code></pre> badpractice_syntax.nf<pre><code>#!/usr/bin/env nextflow\n\ninput_ch = Channel.of('sample1', 'sample2', 'sample3')  # WARNING: Channel defined outside workflow\n\nprocess PROCESS_FILES {\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_processed.txt\"\n\n    script:\n    // Define variables in Groovy code before the script\n    def output_prefix = \"${sample_name}_processed\"\n    def timestamp = new Date().format(\"yyyy-MM-dd\")\n\n    \"\"\"\n    echo \"Processing ${sample_name} on ${timestamp}\" &gt; ${output_prefix}.txt\n    \"\"\"\n}\n\nworkflow {\n    PROCESS_FILES(input_ch)\n}\n</code></pre>"},{"location":"side_quests/debugging/#run-the-pipeline_9","title":"Run the pipeline","text":"<p>Run the workflow again to confirm it still works and the VSCode warning is resolved:</p> <pre><code>nextflow run badpractice_syntax.nf\n</code></pre> <p>Tighter restrictions on such things will likely become enforced in future Nextflow versions, so it's good practice to keep your input channels defined within the workflow block, and in general to follow any other recommendations the extension makes.</p>"},{"location":"side_quests/debugging/#takeaway_1","title":"Takeaway","text":"<p>You can systematically identify and fix syntax errors using Nextflow error messages and IDE visual indicators. Common syntax errors include missing braces, incorrect process keywords, undefined variables, and improper use of Bash vs. Nextflow variables. The VSCode extension helps catch many of these before runtime. With these syntax debugging skills in your toolkit, you'll be able to quickly resolve the most common Nextflow syntax errors and move on to tackling more complex runtime issues.</p>"},{"location":"side_quests/debugging/#whats-next_1","title":"What's next?","text":"<p>Learn to debug more complex channel structure errors that occur even when syntax is correct.</p>"},{"location":"side_quests/debugging/#2-channel-structure-errors","title":"2. Channel Structure Errors","text":"<p>Channel structure errors are more subtle than syntax errors because the code is syntactically correct, but the data shapes don't match what processes expect. Nextflow will try to run the pipeline, but might find that the number of inputs doesn't match what it expects and fail. These errors typically only appear at runtime and require an understanding of the data flowing through your workflow.</p> <p>Debugging Channels with <code>.view()</code></p> <p>Throughout this section, remember that you can use the <code>.view()</code> operator to inspect channel content at any point in your workflow. This is one of the most powerful debugging tools for understanding channel structure issues. We'll explore this technique in detail in section 2.4, but feel free to use it as you work through the examples.</p> <pre><code>my_channel.view()  // Shows what's flowing through the channel\n</code></pre>"},{"location":"side_quests/debugging/#21-wrong-number-of-input-channels","title":"2.1. Wrong Number of Input Channels","text":"<p>This error occurs when you pass a different number of channels than a process expects.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_10","title":"Run the pipeline","text":"<pre><code>nextflow run bad_number_inputs.nf\n</code></pre> Wrong number of channels error<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `bad_number_inputs.nf` [high_mendel] DSL2 - revision: 955705c51b\n\nProcess `PROCESS_FILES` declares 1 input channel but 2 were specified\n\n -- Check script 'bad_number_inputs.nf' at line: 23 or see '.nextflow.log' file for more details\n</code></pre>"},{"location":"side_quests/debugging/#check-the-code_5","title":"Check the code","text":"<p>The error message clearly states that the process expects 1 input channel, but 2 were provided. Let's examine <code>bad_number_inputs.nf</code>:</p> bad_number_inputs.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:\n        val sample_name  // Process expects only 1 input\n\n    output:\n        path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    echo \"Processing ${sample_name}\" &gt; ${sample_name}_output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Create two separate channels\n    samples_ch = Channel.of('sample1', 'sample2', 'sample3')\n    files_ch = Channel.of('file1.txt', 'file2.txt', 'file3.txt')\n\n    // ERROR: Passing 2 channels but process expects only 1\n    PROCESS_FILES(samples_ch, files_ch)\n}\n</code></pre> <p>You should see the mismatched <code>PROCESS_FILES</code> call, supplying multiple input channels when the process only defines one. The VSCode extension will also under line process call in red, and supply a diagnostic message when you mouse over:</p> <p></p>"},{"location":"side_quests/debugging/#fix-the-code_5","title":"Fix the code","text":"<p>For this specific example, the process expects a single channel and doesn't require the second channel, so we can fix it by passing only the <code>samples_ch</code> channel:</p> AfterBefore bad_number_inputs.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:\n        val sample_name  // Process expects only 1 input\n\n    output:\n        path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    echo \"Processing ${sample_name}\" &gt; ${sample_name}_output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Create two separate channels\n    samples_ch = Channel.of('sample1', 'sample2', 'sample3')\n    files_ch = Channel.of('file1.txt', 'file2.txt', 'file3.txt')\n\n    // Fixed: Pass only the channel the process expects\n    PROCESS_FILES(samples_ch)\n}\n</code></pre> bad_number_inputs.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:\n        val sample_name  // Process expects only 1 input\n\n    output:\n        path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    echo \"Processing ${sample_name}\" &gt; ${sample_name}_output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Create two separate channels\n    samples_ch = Channel.of('sample1', 'sample2', 'sample3')\n    files_ch = Channel.of('file1.txt', 'file2.txt', 'file3.txt')\n\n    // ERROR: Passing 2 channels but process expects only 1\n    PROCESS_FILES(samples_ch, files_ch)\n}\n</code></pre>"},{"location":"side_quests/debugging/#run-the-pipeline_11","title":"Run the pipeline","text":"<pre><code>nextflow run bad_number_inputs.nf\n</code></pre> <p>More commonly than this example, you might add additional inputs to a process and forget to update the workflow call accordingly, which can lead to this type of error. Fortunately, this is one of the easier-to-understand and fix errors, as the error message is quite clear about the mismatch.</p>"},{"location":"side_quests/debugging/#22-channel-exhaustion-process-runs-fewer-times-than-expected","title":"2.2. Channel Exhaustion (Process Runs Fewer Times Than Expected)","text":"<p>Some channel structure errors are much more subtle and produce no errors at all. Probably the most common of these reflects a challenge that new Nextflow users face in understanding that queue channels can be exhausted and run out of items, meaning the workflow finishes prematurely.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_12","title":"Run the pipeline","text":"<pre><code>nextflow run exhausted.nf\n</code></pre> <p>When you run this workflow, it will execute without error, processing a single sample:</p> Exhausted channel output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `exhausted.nf` [extravagant_gauss] DSL2 - revision: 08cff7ba2a\n\nexecutor &gt;  local (1)\n[bd/f61fff] PROCESS_FILES (1) [100%] 1 of 1 \u2714\n</code></pre>"},{"location":"side_quests/debugging/#check-the-code_6","title":"Check the code","text":"<p>Let's examine <code>exhausted.nf</code> to see if that's right:</p> exhausted.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:\n    val reference\n    val sample_name\n\n    output:\n    path \"${output_prefix}.txt\"\n\n    script:\n    // Define variables in Groovy code before the script\n    output_prefix = \"${reference}_${sample_name}\"\n    def timestamp = new Date().format(\"yyyy-MM-dd\")\n\n    \"\"\"\n    echo \"Processing ${sample_name} on ${timestamp}\" &gt; ${output_prefix}.txt\n    \"\"\"\n}\n\nworkflow {\n\n    reference_ch = Channel.of('baseline_reference')\n    input_ch = Channel.of('sample1', 'sample2', 'sample3')\n\n    PROCESS_FILES(reference_ch, input_ch)\n}\n</code></pre> <p>The process only runs once instead of three times because the <code>reference_ch</code> channel is a queue channel that gets exhausted after the first process execution. When one channel is exhausted, the entire process stops, even if other channels still have items.</p> <p>This is a common pattern where you have a single reference file that needs to be reused across multiple samples. The solution is to convert the reference channel to a value channel that can be reused indefinitely.</p>"},{"location":"side_quests/debugging/#fix-the-code_6","title":"Fix the code","text":"<p>There are a couple of ways to address this depending on how many files are affected.</p> <p>Option 1: You have a single reference file that you are re-using a lot. You can simply create a value channel type, which can be used over and over again. There are three ways to do this:</p> <p>1. Use <code>Channel.value()</code>:</p> exhausted.nf (fixed - Option 1a)<pre><code>workflow {\n    reference_ch = Channel.value('baseline_reference')  // Value channel can be reused\n    input_ch = Channel.of('sample1', 'sample2', 'sample3')\n\n    PROCESS_FILES(reference_ch, input_ch)\n}\n</code></pre> <p>2. Use the <code>first()</code> operator:</p> exhausted.nf (fixed - Option 1b)<pre><code>workflow {\n    reference_ch = Channel.of('baseline_reference').first()  // Convert to value channel\n    input_ch = Channel.of('sample1', 'sample2', 'sample3')\n\n    PROCESS_FILES(reference_ch, input_ch)\n}\n</code></pre> <p>3. Use the <code>collect()</code> operator:</p> exhausted.nf (fixed - Option 1c)<pre><code>workflow {\n    reference_ch = Channel.of('baseline_reference').collect()  // Convert to value channel\n    input_ch = Channel.of('sample1', 'sample2', 'sample3')\n\n    PROCESS_FILES(reference_ch, input_ch)\n}\n</code></pre> <p>Option 2: In more complex scenarios, perhaps where you have multiple reference files for all samples in the sample channel, you can use the <code>combine</code> operator to create a new channel that combines the two channels into tuples:</p> exhausted.nf (fixed - Option 2)<pre><code>workflow {\n    reference_ch = Channel.of('baseline_reference')\n    input_ch = Channel.of('sample1', 'sample2', 'sample3')\n    combined_ch = reference_ch.combine(input_ch)  // Creates cartesian product\n\n    PROCESS_FILES(combined_ch)\n}\n</code></pre> <p><code>.combine()</code> generates a cartesian product of the two channels, so each item in <code>reference_ch</code> will be paired with each item in <code>input_ch</code>. This allows the process to run for each sample while still using the reference.</p> <p>Note: This requires the process input to be adjusted and therefore is not suitable in all situations.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_13","title":"Run the pipeline","text":"<p>Try one of the fixes above and run the workflow again:</p> <pre><code>nextflow run exhausted.nf\n</code></pre> <p>You should now see all three samples being processed instead of just one.</p>"},{"location":"side_quests/debugging/#23-wrong-channel-content-structure","title":"2.3. Wrong Channel Content Structure","text":"<p>When workflows reach a certain level of complexity, it can be a little difficult to keep track of the internal structures of each channel, and people commonly generate mismatches between what the process expects and what the channel actually contains. This is more subtle than the issue we discussed earlier, where the number of channels was incorrect. In this case, you can have the correct number of input channels, but the internal structure of one or more of those channels doesn't match what the process expects.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_14","title":"Run the pipeline","text":"<pre><code>nextflow run bad_channel_shape.nf\n</code></pre> <p>You will see an error like this:</p> Channel structure error<pre><code>Launching `bad_channel_shape.nf` [hopeful_pare] DSL2 - revision: ffd66071a1\n\nexecutor &gt;  local (3)\nexecutor &gt;  local (3)\n[3f/c2dcb3] PROCESS_FILES (3) [  0%] 0 of 3 \u2718\nERROR ~ Error executing process &gt; 'PROCESS_FILES (1)'\n\nCaused by:\n  Missing output file(s) `[sample1, file1.txt]_output.txt` expected by process `PROCESS_FILES (1)`\n\n\nCommand executed:\n\n  echo \"Processing [sample1, file1.txt]\" &gt; [sample1, file1.txt]_output.txt\n\nCommand exit status:\n  0\n\nCommand output:\n  (empty)\n\nWork dir:\n  /workspaces/training/side-quests/debugging/work/d6/1fb69d1d93300bbc9d42f1875b981e\n\nTip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line\n\n -- Check '.nextflow.log' file for details\n</code></pre>"},{"location":"side_quests/debugging/#check-the-code_7","title":"Check the code","text":"<p>The square brackets in the error message provide the clue here - the process is treating the tuple as a single value, which is not what we want. Let's examine <code>bad_channel_shape.nf</code>:</p> bad_channel_shape.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:\n        val sample_name  // Expects single value, gets tuple\n\n    output:\n        path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    echo \"Processing ${sample_name}\" &gt; ${sample_name}_output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Channel emits tuples, but process expects single values\n    input_ch = Channel.of(\n      ['sample1', 'file1.txt'],\n      ['sample2', 'file2.txt'],\n      ['sample3', 'file3.txt']\n    )\n    PROCESS_FILES(input_ch)\n}\n</code></pre> <p>You can see that we're generating a channel composed of tuples: <code>['sample1', 'file1.txt']</code>, but the process expects a single value, <code>val sample_name</code>. The command executed shows that the process is trying to create a file named <code>[sample3, file3.txt]_output.txt</code>, which is not the intended output.</p>"},{"location":"side_quests/debugging/#fix-the-code_7","title":"Fix the code","text":"<p>To fix this, if the process requires both inputs we could adjust the process to accept a tuple:</p> Option 1: Accept tuple in processOption 2: Extract first element AfterBefore bad_channel_shape.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:\n        tuple val(sample_name), path(file_name)  // Fixed: Accept tuple\n\n    output:\n        path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    echo \"Processing ${sample_name}\" &gt; ${sample_name}_output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Channel emits tuples, but process expects single values\n    input_ch = Channel.of(\n      ['sample1', 'file1.txt'],\n      ['sample2', 'file2.txt'],\n      ['sample3', 'file3.txt']\n    )\n    PROCESS_FILES(input_ch)\n}\n</code></pre> bad_channel_shape.nf<pre><code>#!/usr/bin/env nextflow\n\nprocess PROCESS_FILES {\n    input:\n        val sample_name  // Expects single value, gets tuple\n\n    output:\n        path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    echo \"Processing ${sample_name}\" &gt; ${sample_name}_output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Channel emits tuples, but process expects single values\n    input_ch = Channel.of(\n      ['sample1', 'file1.txt'],\n      ['sample2', 'file2.txt'],\n      ['sample3', 'file3.txt']\n    )\n    PROCESS_FILES(input_ch)\n}\n</code></pre> AfterBefore bad_channel_shape.nf<pre><code>workflow {\n\n    // Channel emits tuples, but process expects single values\n    input_ch = Channel.of(\n      ['sample1', 'file1.txt'],\n      ['sample2', 'file2.txt'],\n      ['sample3', 'file3.txt']\n    )\n    PROCESS_FILES(input_ch.map { it[0] })  // Fixed: Extract first element\n}\n</code></pre> bad_channel_shape.nf<pre><code>workflow {\n\n    // Channel emits tuples, but process expects single values\n    input_ch = Channel.of(\n      ['sample1', 'file1.txt'],\n      ['sample2', 'file2.txt'],\n      ['sample3', 'file3.txt']\n    )\n    PROCESS_FILES(input_ch)\n}\n</code></pre>"},{"location":"side_quests/debugging/#run-the-pipeline_15","title":"Run the pipeline","text":"<p>Pick one of the solutions and re-run the workflow:</p> <pre><code>nextflow run bad_channel_shape.nf\n</code></pre>"},{"location":"side_quests/debugging/#24-channel-debugging-techniques","title":"2.4. Channel Debugging Techniques","text":""},{"location":"side_quests/debugging/#using-view-for-channel-inspection","title":"Using <code>.view()</code> for Channel Inspection","text":"<p>The most powerful debugging tool for channels is the <code>.view()</code> operator. With <code>.view()</code>, you can understand the shape of your channels at all stages to help with debugging.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_16","title":"Run the pipeline","text":"<p>Run <code>bad_channel_shape_viewed.nf</code> to see this in action:</p> <pre><code>nextflow run bad_channel_shape_viewed.nf\n</code></pre> <p>You'll see output like this:</p> Channel debugging output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `bad_channel_shape_viewed.nf` [maniac_poisson] DSL2 - revision: b4f24dc9da\n\nexecutor &gt;  local (3)\n[c0/db76b3] PROCESS_FILES (3) [100%] 3 of 3 \u2714\nChannel content: [sample1, file1.txt]\nChannel content: [sample2, file2.txt]\nChannel content: [sample3, file3.txt]\nAfter mapping: sample1\nAfter mapping: sample2\nAfter mapping: sample3\n</code></pre>"},{"location":"side_quests/debugging/#check-the-code_8","title":"Check the code","text":"<p>Let's examine <code>bad_channel_shape_viewed.nf</code> to see how <code>.view()</code> is used:</p> bad_channel_shape_viewed.nf<pre><code>workflow {\n\n    // Channel emits tuples, but process expects single values\n    input_ch = Channel.of(\n      ['sample1', 'file1.txt'],\n      ['sample2', 'file2.txt'],\n      ['sample3', 'file3.txt']\n    )\n    .view { \"Channel content: $it\" }  // Debug: Show original channel content\n    .map { tuple -&gt; tuple[0] }        // Transform: Extract first element\n    .view { \"After mapping: $it\" }    // Debug: Show transformed channel content\n\n    PROCESS_FILES(input_ch)\n}\n</code></pre>"},{"location":"side_quests/debugging/#fix-the-code_8","title":"Fix the code","text":"<p>To save you from using <code>.view()</code> operations excessively in future to understand channel content, it's advisable to add some comments to help:</p> bad_channel_shape_viewed.nf (with comments)<pre><code>workflow {\n\n    // Channel emits tuples, but process expects single values\n    input_ch = Channel.of(\n            ['sample1', 'file1.txt'],\n            ['sample2', 'file2.txt'],\n            ['sample3', 'file3.txt'],\n        ) // [sample_name, file_name]\n        .map { tuple -&gt; tuple[0] } // sample_name\n\n    PROCESS_FILES(input_ch)\n}\n</code></pre> <p>This will become more important as your workflows grow in complexity and channel structure becomes more opaque.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_17","title":"Run the pipeline","text":"<pre><code>nextflow run bad_channel_shape_viewed.nf\n</code></pre>"},{"location":"side_quests/debugging/#takeaway_2","title":"Takeaway","text":"<p>Many channel structure errors can be created with valid Nextflow syntax. You can debug channel structure errors by understanding data flow, using <code>.view()</code> operators for inspection, and recognizing error message patterns like square brackets indicating unexpected tuple structures.</p>"},{"location":"side_quests/debugging/#whats-next_2","title":"What's next?","text":"<p>Learn about errors created by process definitions.</p>"},{"location":"side_quests/debugging/#3-process-structure-errors","title":"3. Process Structure Errors","text":"<p>Most of the errors you encounter related to processes will related to mistakes you have made in forming the command, or to issues related to the underlying software. That said, similarly to the channel issues above, you can make mistakes in the process definition that don't quality as syntax errors, but which will cause errors at run time.</p>"},{"location":"side_quests/debugging/#31-missing-output-files","title":"3.1. Missing Output Files","text":"<p>One common error when writing processes is to do something that generates a mismatch between what the process expects and what is generated.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_18","title":"Run the pipeline","text":"<pre><code>nextflow run missing_output.nf\n</code></pre> <p>You'll see an error like this:</p> Missing output files error<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `missing_output.nf` [zen_stone] DSL2 - revision: 37ff61f926\n\nexecutor &gt;  local (3)\nexecutor &gt;  local (3)\n[fd/2642e9] process &gt; PROCESS_FILES (2) [ 66%] 2 of 3, failed: 2\nERROR ~ Error executing process &gt; 'PROCESS_FILES (3)'\n\nCaused by:\n  Missing output file(s) `sample3.txt` expected by process `PROCESS_FILES (3)`\n\n\nCommand executed:\n\n  echo \"Processing sample3\" &gt; sample3_output.txt\n\nCommand exit status:\n  0\n\nCommand output:\n  (empty)\n\nWork dir:\n  /workspaces/training/side-quests/debugging/work/02/9604d49fb8200a74d737c72a6c98ed\n\nTip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line\n\n -- Check '.nextflow.log' file for details\n</code></pre>"},{"location":"side_quests/debugging/#check-the-code_9","title":"Check the code","text":"<p>The error message indicates that the process expected to produce an output file named <code>sample3.txt</code>, but the script actually creates <code>sample3_output.txt</code>. Let's examine the process definition in <code>missing_output.nf</code>:</p> missing_output.nf<pre><code>process PROCESS_FILES {\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}.txt\"  // Expects: sample3.txt\n\n    script:\n    \"\"\"\n    echo \"Processing ${sample_name}\" &gt; ${sample_name}_output.txt  // Creates: sample3_output.txt\n    \"\"\"\n}\n</code></pre> <p>You should see that there is a mismatch between the output file name in the <code>output:</code> block, and the one used in the script. This mismatch causes the process to fail. If you encounter this sort of error, go back and check that the outputs match between your process definition and your output block.</p> <p>If the problem still isn't clear, check the work directory itself to identify the actual output files created:</p> <pre><code>\u276f ls -h work/02/9604d49fb8200a74d737c72a6c98ed\nsample3_output.txt\n</code></pre> <p>For this example this would highlight to us that a <code>_output</code> suffix is being incorporated into the output file name, contrary to our <code>output:</code> definition.</p>"},{"location":"side_quests/debugging/#fix-the-code_9","title":"Fix the code","text":"<p>Fix the mismatch by making the output filename consistent:</p> AfterBefore missing_output.nf<pre><code>process PROCESS_FILES {\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"  // Fixed: Match the script output\n\n    script:\n    \"\"\"\n    echo \"Processing ${sample_name}\" &gt; ${sample_name}_output.txt\n    \"\"\"\n}\n</code></pre> missing_output.nf<pre><code>process PROCESS_FILES {\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}.txt\"  // Expects: sample3.txt\n\n    script:\n    \"\"\"\n    echo \"Processing ${sample_name}\" &gt; ${sample_name}_output.txt  // Creates: sample3_output.txt\n    \"\"\"\n}\n</code></pre>"},{"location":"side_quests/debugging/#run-the-pipeline_19","title":"Run the pipeline","text":"<pre><code>nextflow run missing_output.nf\n</code></pre>"},{"location":"side_quests/debugging/#32-missing-software","title":"3.2. Missing software","text":"<p>Another class of errors occurs due to mistakes in software provisioning. <code>missing_software.nf</code> is a syntactically valid workflow, but it depends on some external software to provide the <code>cowpy</code> command it uses.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_20","title":"Run the pipeline","text":"<pre><code>nextflow run missing_software.nf\n</code></pre> <p>You will see an error like this:</p> Missing software error<pre><code>ERROR ~ Error executing process &gt; 'PROCESS_FILES (3)'\n\nCaused by:\n  Process `PROCESS_FILES (3)` terminated with an error exit status (127)\n\n\nCommand executed:\n\n  cowpy sample3 &gt; sample3_output.txt\n\nCommand exit status:\n  127\n\nCommand output:\n  (empty)\n\nCommand error:\n  .command.sh: line 2: cowpy: command not found\n\nWork dir:\n  /workspaces/training/side-quests/debugging/work/82/42a5bfb60c9c6ee63ebdbc2d51aa6e\n\nTip: you can try to figure out what's wrong by changing to the process work directory and showing the script file named `.command.sh`\n\n -- Check '.nextflow.log' file for details\n</code></pre> <p>The process doesn't have access to the command we're specifying. Sometimes this is because a script is present in the workflow <code>bin</code> directory, but has not been made executable. Other times it is because the software is not installed in the container or environment where the workflow is running.</p>"},{"location":"side_quests/debugging/#check-the-code_10","title":"Check the code","text":"<p>Look out for that <code>127</code> exit code - it tells you exactly the problem. Let's examine <code>missing_software.nf</code>:</p> missing_software.nf<pre><code>process PROCESS_FILES {\n\n    container 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    cowpy ${sample_name} &gt; ${sample_name}_output.txt\n    \"\"\"\n}\n</code></pre>"},{"location":"side_quests/debugging/#fix-the-code_10","title":"Fix the code","text":"<p>We've been a little disingenuous here, and there's actually nothing wrong with the code. We just need to specify the necessary configuration to run the process in such a way that it has access to the command in question. In this case the process has a container definition, so all we need to do is run the workflow with Docker enabled.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_21","title":"Run the pipeline","text":"<p>We've set up a Docker profile for you in <code>nextflow.config</code>, so you can run the workflow with:</p> <pre><code>nextflow run missing_software.nf -profile docker\n</code></pre> <p>This should run successfully now.</p> <p>Note</p> <p>To learn more about how nextflow uses containers, go back to Hello Nextflow</p>"},{"location":"side_quests/debugging/#33-bad-resource-configuration","title":"3.3. Bad resource configuration","text":"<p>In production usage, you'll be configuring resources on your processes. For example <code>memory</code> defines the maximum amount of memory available to your process, and if the process exceeds that, your scheduler will typically kill the process and return an exit code of <code>137</code>. We can't demonstrate that here because we're using the <code>local</code> executor, but we can show something similar with <code>time</code>.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_22","title":"Run the pipeline","text":"<p><code>bad_resources.nf</code> has process configuration with an unrealistic bound on time of 1 millisecond:</p> <pre><code>nextflow run bad_resources.nf -profile docker\n</code></pre> <p>This gives us an error:</p> Resource time limit error<pre><code>ERROR ~ Error executing process &gt; 'PROCESS_FILES (1)'\n\nCaused by:\nProcess exceeded running time limit (1ms)\n</code></pre>"},{"location":"side_quests/debugging/#check-the-code_11","title":"Check the code","text":"<p>Let's examine <code>bad_resources.nf</code>:</p> bad_resources.nf<pre><code>process PROCESS_FILES {\n\n    time '1 ms'  // ERROR: Unrealistic time limit\n\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    sleep 1  // Takes 1 second, but time limit is 1ms\n    cowpy ${sample_name} &gt; ${sample_name}_output.txt\n    \"\"\"\n}\n</code></pre> <p>We know the process will take longer than a second (we've added a sleep in there to make sure), but the process is set to time out after 1 millisecond. Someone has been a little unrealistic with their configuration!</p>"},{"location":"side_quests/debugging/#fix-the-code_11","title":"Fix the code","text":"<p>Increase the time limit to a realistic value:</p> AfterBefore bad_resources.nf<pre><code>process PROCESS_FILES {\n\n    time '100 s'  // Fixed: Realistic time limit\n\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    sleep 1\n    cowpy ${sample_name} &gt; ${sample_name}_output.txt\n    \"\"\"\n}\n</code></pre> bad_resources.nf<pre><code>process PROCESS_FILES {\n\n    time '1 ms'  // ERROR: Unrealistic time limit\n\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    sleep 1  // Takes 1 second, but time limit is 1ms\n    cowpy ${sample_name} &gt; ${sample_name}_output.txt\n    \"\"\"\n}\n</code></pre>"},{"location":"side_quests/debugging/#run-the-pipeline_23","title":"Run the pipeline","text":"<pre><code>nextflow run bad_resources.nf -profile docker\n</code></pre> <p>If you make sure to read your error messages failures like this should not puzzle you for too long. But make sure you understand the resource requirements of the commands you are running so that you can configure your resource directives appropriately.</p>"},{"location":"side_quests/debugging/#34-process-debugging-techniques","title":"3.4. Process Debugging Techniques","text":"<p>When processes fail or behave unexpectedly, you need systematic techniques to investigate what went wrong. The work directory contains all the information you need to debug process execution.</p>"},{"location":"side_quests/debugging/#using-work-directory-inspection","title":"Using Work Directory Inspection","text":"<p>The most powerful debugging tool for processes is examining the work directory. When a process fails, Nextflow creates a work directory for that specific process execution containing all the files needed to understand what happened.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_24","title":"Run the pipeline","text":"<p>Let's use the <code>missing_output.nf</code> example from earlier to demonstrate work directory inspection (re-generate an output naming mismatch if you need to):</p> <pre><code>nextflow run missing_output.nf\n</code></pre> <p>You'll see an error like this:</p> Missing output error<pre><code>  Missing output file(s) `sample2.txt` expected by process `PROCESS_FILES (2)`\n</code></pre>"},{"location":"side_quests/debugging/#check-the-work-directory","title":"Check the work directory","text":"<p>When you get this error, the work directory contains all the debugging information. Find the work directory path from the error message and examine its contents:</p> <pre><code># Find the work directory from the error message\nls work/02/9604d49fb8200a74d737c72a6c98ed/\n</code></pre> <p>You can then examine the key files:</p>"},{"location":"side_quests/debugging/#check-the-command-script","title":"Check the Command Script","text":"<p>The <code>.command.sh</code> file shows exactly what command was executed:</p> <pre><code># View the executed command\ncat work/02/9604d49fb8200a74d737c72a6c98ed/.command.sh\n</code></pre> <p>This reveals:</p> <ul> <li>Variable substitution: Whether Nextflow variables were properly expanded</li> <li>File paths: Whether input files were correctly located</li> <li>Command structure: Whether the script syntax is correct</li> </ul> <p>Common issues to look for:</p> <ul> <li>Missing quotes: Variables containing spaces need proper quoting</li> <li>Wrong file paths: Input files that don't exist or are in wrong locations</li> <li>Incorrect variable names: Typos in variable references</li> <li>Missing environment setup: Commands that depend on specific environments</li> </ul>"},{"location":"side_quests/debugging/#check-error-output","title":"Check Error Output","text":"<p>The <code>.command.err</code> file contains the actual error messages:</p> <pre><code># View error output\ncat work/02/9604d49fb8200a74d737c72a6c98ed/.command.err\n</code></pre> <p>This file will show:</p> <ul> <li>Exit codes: 127 (command not found), 137 (killed), etc.</li> <li>Permission errors: File access issues</li> <li>Software errors: Application-specific error messages</li> <li>Resource errors: Memory/time limit exceeded</li> </ul>"},{"location":"side_quests/debugging/#check-standard-output","title":"Check Standard Output","text":"<p>The <code>.command.out</code> file shows what your command produced:</p> <pre><code># View standard output\ncat work/02/9604d49fb8200a74d737c72a6c98ed/.command.out\n</code></pre> <p>This helps verify:</p> <ul> <li>Expected output: Whether the command produced the right results</li> <li>Partial execution: Whether the command started but failed partway through</li> <li>Debug information: Any diagnostic output from your script</li> </ul>"},{"location":"side_quests/debugging/#check-the-exit-code","title":"Check the Exit Code","text":"<p>The <code>.exitcode</code> file contains the exit code for the process:</p> <pre><code># View exit code\ncat work/*/*/.exitcode\n</code></pre> <p>Common exit codes and their meanings:</p> <ul> <li>Exit code 127: Command not found - check software installation</li> <li>Exit code 137: Process killed - check memory/time limits</li> </ul>"},{"location":"side_quests/debugging/#check-file-existence","title":"Check File Existence","text":"<p>When processes fail due to missing output files, check what files were actually created:</p> <pre><code># List all files in the work directory\nls -la work/02/9604d49fb8200a74d737c72a6c98ed/\n</code></pre> <p>This helps identify:</p> <ul> <li>File naming mismatches: Output files with different names than expected</li> <li>Permission issues: Files that couldn't be created</li> <li>Path problems: Files created in wrong directories</li> </ul> <p>In our example earlier, this confirmed to us that while our expected <code>sample3.txt</code> wasn't present, <code>sample3_output.txt</code> was:</p> <pre><code>\u276f ls -h work/02/9604d49fb8200a74d737c72a6c98ed\nsample3_output.txt\n</code></pre>"},{"location":"side_quests/debugging/#takeaway_3","title":"Takeaway","text":"<p>Process debugging requires examining work directories to understand what went wrong. Key files include <code>.command.sh</code> (the executed script), <code>.command.err</code> (error messages), and <code>.command.out</code> (standard output). Exit codes like 127 (command not found) and 137 (process killed) provide immediate diagnostic clues about the type of failure.</p>"},{"location":"side_quests/debugging/#whats-next_3","title":"What's next?","text":"<p>Learn about Nextflow's built-in debugging tools and systematic approaches to troubleshooting.</p>"},{"location":"side_quests/debugging/#4-built-in-debugging-tools-and-advanced-techniques","title":"4. Built-in Debugging Tools and Advanced Techniques","text":"<p>Nextflow provides several powerful built-in tools for debugging and analyzing workflow execution. These tools help you understand what went wrong, where it went wrong, and how to fix it efficiently.</p>"},{"location":"side_quests/debugging/#41-real-time-process-output","title":"4.1. Real-time Process Output","text":"<p>Sometimes you need to see what's happening inside running processes. You can enable real-time process output, which shows you exactly what each task is doing as it executes.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_25","title":"Run the pipeline","text":"<p><code>bad_channel_shape_viewed.nf</code> from our earlier examples printed channel content using <code>.view()</code>, but we can also use the <code>debug</code> directive to echo variables from within the process itself, which we demonstrate in <code>bad_channel_shape_viewed_debug.nf</code>. Run the workflow:</p> <pre><code>nextflow run bad_channel_shape_viewed_debug.nf\n</code></pre> <p>You will see output like this:</p> Real-time process output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `bad_channel_shape_viewed_debug.nf` [agitated_crick] DSL2 - revision: ea3676d9ec\n\nexecutor &gt;  local (3)\n[c6/2dac51] process &gt; PROCESS_FILES (3) [100%] 3 of 3 \u2714\nChannel content: [sample1, file1.txt]\nChannel content: [sample2, file2.txt]\nChannel content: [sample3, file3.txt]\nAfter mapping: sample1\nAfter mapping: sample2\nAfter mapping: sample3\nSample name inside process is sample2\n\nSample name inside process is sample1\n\nSample name inside process is sample3\n</code></pre>"},{"location":"side_quests/debugging/#check-the-code_12","title":"Check the code","text":"<p>Let's examine <code>bad_channel_shape_viewed_debug.nf</code> to see how the <code>debug</code> directive works:</p> bad_channel_shape_viewed_debug.nf<pre><code>process PROCESS_FILES {\n    debug true  // Enable real-time output\n\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    echo \"Sample name inside process is ${sample_name}\"\n    echo \"Processing ${sample_name}\" &gt; ${sample_name}_output.txt\n    \"\"\"\n}\n</code></pre> <p>The <code>debug</code> directive can be a quick and convenient way to understand the environment of a process.</p>"},{"location":"side_quests/debugging/#42-preview-mode","title":"4.2. Preview Mode","text":"<p>Sometimes you want to catch problems before any processes run. Nextflow provides a flag for this kind of proactive debugging: <code>-preview</code>.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_26","title":"Run the pipeline","text":"<p>The preview mode lets you test workflow logic without executing commands. This can be quite useful for quickly checking the structure of your workflow and ensuring that processes are connected correctly without running any actual commands.</p> <p>For example, for our first syntax error from earlier:</p> <pre><code>nextflow run bad_syntax.nf -preview\n</code></pre> <p>!!! note:</p> <pre><code>If you fixed the file, reintroduce the syntax error by changing `input` to `inputs` before you run the command\n</code></pre> <p>You'll see output like this:</p> Preview mode output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `bad_syntax.nf` [sick_fermi] DSL2 - revision: ca6327fad2\n\nERROR ~ Script compilation error\n- file : /workspaces/training/side-quests/debugging/bad_syntax.nf\n- cause: Unexpected input: '{' @ line 3, column 23.\n   process PROCESS_FILES {\n                         ^\n\n1 error\n\nNOTE: If this is the beginning of a process or workflow, there may be a syntax error in the body, such as a missing or extra comma, for which a more specific error message could not be produced.\n\n -- Check '.nextflow.log' file for details\n</code></pre> <p>Preview mode is particularly useful for catching syntax errors early without running any processes. It validates the workflow structure and process connections before execution.</p>"},{"location":"side_quests/debugging/#43-stub-running-for-logic-testing","title":"4.3. Stub Running for Logic Testing","text":"<p>Sometimes errors are difficult to debug because commands take too long, require special software, or fail for complex reasons. Stub running lets you test workflow logic without executing the actual commands.</p>"},{"location":"side_quests/debugging/#run-the-pipeline_27","title":"Run the pipeline","text":"<p>When you're developing a Nextflow process, you can use the <code>stub</code> directive to define 'dummy' commands that generate outputs of the correct form without running the real command. This approach is particularly valuable when you want to verify that your workflow logic is correct before dealing with the complexities of the actual software.</p> <p>For example, remember our <code>missing_software.nf</code> from earlier? The one where we had missing software that prevented the workflow running until we added <code>-profile docker</code>? <code>missing_software_with_stub.nf</code> is a very similar workflow. If we run it in the same way, we will generate the same error:</p> <pre><code>nextflow run missing_software_with_stub.nf\n</code></pre> Missing software error with stub<pre><code>ERROR ~ Error executing process &gt; 'PROCESS_FILES (3)'\n\nCaused by:\n  Process `PROCESS_FILES (3)` terminated with an error exit status (127)\n\n\nCommand executed:\n\n  cowpy sample3 &gt; sample3_output.txt\n\nCommand exit status:\n  127\n\nCommand output:\n  (empty)\n\nCommand error:\n  .command.sh: line 2: cowpy: command not found\n\nWork dir:\n  /workspaces/training/side-quests/debugging/work/82/42a5bfb60c9c6ee63ebdbc2d51aa6e\n\nTip: you can try to figure out what's wrong by changing to the process work directory and showing the script file named `.command.sh`\n\n -- Check '.nextflow.log' file for details\n</code></pre> <p>However, this workflow will not produce errors if we run it with <code>-stub-run</code>, even without the <code>docker</code> profile:</p> <pre><code>nextflow run missing_software_with_stub.nf -stub-run\n</code></pre>"},{"location":"side_quests/debugging/#check-the-code_13","title":"Check the code","text":"<p>Let's examine <code>missing_software_with_stub.nf</code>:</p> missing_software.nf (with stub)<pre><code>process PROCESS_FILES {\n\n    container 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n\n    input:\n    val sample_name\n\n    output:\n    path \"${sample_name}_output.txt\"\n\n    script:\n    \"\"\"\n    cowpy ${sample_name} &gt; ${sample_name}_output.txt\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch ${sample_name}_output.txt\n    \"\"\"\n}\n</code></pre> <p>Relative to <code>missing_software.nf</code>, this process has a <code>stub:</code> directive specifying a command to be used instead of the one specified in <code>script:</code>, in the event that that Nextflow is run in stub mode.</p> <p>The <code>touch</code> command we're using here doesn't depend on any software or appropriate inputs, and will run in all situations, allowing us to debug workflow logic without worrying about the process internals.</p> <p>Stub running helps debug:</p> <ul> <li>Channel structure and data flow</li> <li>Process connections and dependencies</li> <li>Parameter propagation</li> <li>Workflow logic without software dependencies</li> </ul>"},{"location":"side_quests/debugging/#44-systematic-debugging-approach","title":"4.4. Systematic Debugging Approach","text":"<p>Now that you've learned individual debugging techniques - from trace files and work directories to preview mode, stub running, and resource monitoring - let's tie them together into a systematic methodology. Having a structured approach prevents you from getting overwhelmed by complex errors and ensures you don't miss important clues.</p> <p>This methodology combines all the tools we've covered into an efficient workflow:</p> <p>Four-Phase Debugging Method:</p> <p>Phase 1: Syntax Error Resolution (5 minutes)</p> <ol> <li>Check for red underlines in VSCode or your IDE</li> <li>Run <code>nextflow run workflow.nf -preview</code> to identify syntax issues</li> <li>Fix all syntax errors (missing braces, trailing commas, etc.)</li> <li>Ensure the workflow parses successfully before proceeding</li> </ol> <p>Phase 2: Quick Assessment (5 minutes)</p> <ol> <li>Read runtime error messages carefully</li> <li>Check if it's a runtime, logic, or resource error</li> <li>Use preview mode to test basic workflow logic</li> </ol> <p>Phase 3: Detailed Investigation (15-30 minutes)</p> <ol> <li>Find the work directory of the failed task</li> <li>Examine log files</li> <li>Add <code>.view()</code> operators to inspect channels</li> <li>Use <code>-stub-run</code> to test workflow logic without execution</li> </ol> <p>Phase 4: Fix and Validate (15 minutes)</p> <ol> <li>Make minimal targeted fixes</li> <li>Test with resume: <code>nextflow run workflow.nf -resume</code></li> <li>Verify complete workflow execution</li> </ol> <p>Using Resume for Efficient Debugging</p> <p>Once you've identified a problem, you need an efficient way to test your fixes without wasting time re-running successful parts of your workflow. Nextflow's <code>-resume</code> functionality is invaluable for debugging.</p> <p>You will have encountered <code>-resume</code> if you've worked through Hello Nextflow, and it's important that you make good use of it when debugging to save yourself waiting while the processes before your problem process run.</p> <p>Resume debugging strategy:</p> <ol> <li>Run workflow until failure</li> <li>Examine work directory for failed task</li> <li>Fix the specific issue</li> <li>Resume to test only the fix</li> <li>Repeat until workflow completes</li> </ol>"},{"location":"side_quests/debugging/#debugging-configuration-profile","title":"Debugging Configuration Profile","text":"<p>To make this systematic approach even more efficient, you can create a dedicated debugging configuration that automatically enables all the tools you need:</p> nextflow.config (debug profile)<pre><code>profiles {\n    debug {\n        process {\n            debug = true\n            cleanup = false\n\n            // Conservative resources for debugging\n            maxForks = 1\n            memory = '2.GB'\n            cpus = 1\n        }\n    }\n}\n</code></pre> <p>Then you can run the pipeline with this profile enabled:</p> <pre><code>nextflow run workflow.nf -profile debug\n</code></pre>"},{"location":"side_quests/debugging/#45-practical-debugging-exercise","title":"4.5. Practical Debugging Exercise","text":"<p>Now it's time to put the systematic debugging approach into practice. The workflow <code>buggy_workflow.nf</code> contains several common errors that represent the types of issues you'll encounter in real-world development.</p> <p>Exercise</p> <p>Use the systematic debugging approach to identify and fix all errors in <code>buggy_workflow.nf</code>. This workflow attempts to process sample data from a CSV file but contains multiple intentional bugs representing common debugging scenarios.</p> <p>Start by running the workflow to see the first error:</p> <pre><code>nextflow run buggy_workflow.nf\n</code></pre> <p>Apply the four-phase debugging method you've learned:</p> <p>Phase 1: Syntax Error Resolution - Check for red underlines in VSCode or your IDE - Run <code>nextflow run workflow.nf -preview</code> to identify syntax issues - Fix all syntax errors (missing braces, trailing commas, etc.) - Ensure the workflow parses successfully before proceeding</p> <p>Phase 2: Quick Assessment - Read runtime error messages carefully - Identify whether errors are runtime, logic, or resource-related - Use <code>-preview</code> mode to test basic workflow logic</p> <p>Phase 3: Detailed Investigation - Examine work directories for failed tasks - Add <code>.view()</code> operators to inspect channels - Check log files in work directories - Use <code>-stub-run</code> to test workflow logic without execution</p> <p>Phase 4: Fix and Validate - Make targeted fixes - Use <code>-resume</code> to test fixes efficiently - Verify complete workflow execution</p> <p>Debugging Tools at Your Disposal: <pre><code># Preview mode for syntax checking\nnextflow run buggy_workflow.nf -preview\n\n# Debug profile for detailed output\nnextflow run buggy_workflow.nf -profile debug\n\n# Stub running for logic testing\nnextflow run buggy_workflow.nf -stub-run\n\n# Resume after fixes\nnextflow run buggy_workflow.nf -resume\n</code></pre></p> Solution <p>The <code>buggy_workflow.nf</code> contains 9 or 10 distinct errors (depending how you count) covering all major debugging categories. Here's a systematic breakdown of each error and how to fix it</p> <p>Let's start with those syntax errors:</p> <p>Error 1: Syntax Error - Trailing Comma <pre><code>output:\n    path \"${sample_id}_result.txt\",  // ERROR: Trailing comma\n</code></pre> Fix: Remove the trailing comma <pre><code>output:\n    path \"${sample_id}_result.txt\"\n</code></pre></p> <p>Error 2: Syntax Error - Missing Closing Brace <pre><code>script:\n\"\"\"\necho \"Processing: ${sample}\"\ncat ${input_file} &gt; ${sample}_result.txt\n\"\"\"\n// ERROR: Missing closing brace for processFiles process\n</code></pre> Fix: Add the missing closing brace <pre><code>\"\"\"\necho \"Processing: ${sample_id}\"\ncat ${input_file} &gt; ${sample_id}_result.txt\n\"\"\"\n}  // Add missing closing brace\n</code></pre></p> <p>Error 3: Variable Name Error <pre><code>echo \"Processing: ${sample}\"     // ERROR: should be sample_id\ncat ${input_file} &gt; ${sample}_result.txt  // ERROR: should be sample_id\n</code></pre> Fix: Use the correct input variable name <pre><code>echo \"Processing: ${sample_id}\"\ncat ${input_file} &gt; ${sample_id}_result.txt\n</code></pre></p> <p>Error 4: Undefined Variable Error <pre><code>heavy_ch = heavyProcess(sample_ids)  // ERROR: sample_ids undefined\n</code></pre> Fix: Use the correct channel and extract sample IDs <pre><code>heavy_ch = heavyProcess(input_ch)\n</code></pre></p> <p>At this point the workflow will run, but we'll still be getting errors (e.g. <code>Path value cannot be null</code> in <code>processFiles</code>), caused by bad channel structure.</p> <p>Error 5: Channel Structure Error - Wrong Map Output <pre><code>.map { row -&gt; row.sample_id }  // ERROR: processFiles expects tuple\n</code></pre> Fix: Return the tuple structure that processFiles expects <pre><code>.map { row -&gt; [row.sample_id, file(row.fastq_path)] }\n</code></pre></p> <p>But this will break our for for running <code>heavyProcess()</code> above, so we'll need to use a map to pass just the sample IDs to that process:</p> <p>Error 6: Bad channel structure for heavyProcess <pre><code>heavy_ch = heavyProcess(input_ch)  // ERROR: input_ch now has 2 elements per emission- heavyProcess only needs 1 (the first)\n</code></pre> Fix: Use the correct channel and extract sample IDs <pre><code>heavy_ch = heavyProcess(input_ch.map{it[0]})\n</code></pre></p> <p>Now we get a but further but receive an error about <code>No such variable: i</code>, because we didn't escape a Bash variable.</p> <p>Error 7: Bash Variable Escaping Error <pre><code>echo \"Heavy computation $i for ${sample_id}\"  // ERROR: $i not escaped\n</code></pre> Fix: Escape the bash variable <pre><code>echo \"Heavy computation \\${i} for ${sample_id}\"\n</code></pre></p> <p>Now we get <code>Process exceeded running time limit (1ms)</code>, so we fix the run time limit for the relevant process:</p> <p>Error 8: Resource Configuration Error <pre><code>time '1 ms'  // ERROR: Unrealistic time limit\n</code></pre> Fix: Increase to a realistic time limit <pre><code>time '100 s'\n</code></pre></p> <p>Next we have a <code>Missing output file(s)</code> errror to resolve:</p> <p>Error 9: Output File Name Mismatch <pre><code>done &gt; ${sample_id}.txt  // ERROR: Wrong filename, should match output declaration\n</code></pre> Fix: Match the output declaration <pre><code>done &gt; ${sample_id}_heavy.txt\n</code></pre></p> <p>The first two processes ran, but not the third.</p> <p>Error 10: Output File Name Mismatch <pre><code>file_ch = Channel.fromPath(\"*.txt\") // Error: attempting to take input from the pwd rather than a process\nhandleFiles(file_ch)\n</code></pre> Fix: Take the output from the previous process <pre><code>handleFiles(heavyProcess.out)\n</code></pre></p> <p>With that, the whole workflow should run.</p> <p>Complete Corrected Workflow: <pre><code>#!/usr/bin/env nextflow\n\n/*\n* Buggy workflow for debugging exercises\n* This workflow contains several intentional bugs for learning purposes\n*/\n\n// Parameters with missing validation\nparams.input = 'data/sample_data.csv'\nparams.output = 'results'\n\n/*\n* Process with input/output mismatch\n*/\nprocess processFiles {\n    publishDir \"${params.output}/processed\", mode: 'copy'\n\n    input:\n        tuple val(sample_id), path(input_file)\n\n    output:\n        path \"${sample_id}_result.txt\"\n\n    script:\n    \"\"\"\n    echo \"Processing: ${sample_id}\"\n    cat ${input_file} &gt; ${sample_id}_result.txt\n    \"\"\"\n}\n\n/*\n* Process with resource issues\n*/\nprocess heavyProcess {\n    publishDir \"${params.output}/heavy\", mode: 'copy'\n\n    time '100 s'\n\n    input:\n        val sample_id\n\n    output:\n        path \"${sample_id}_heavy.txt\"\n\n    script:\n    \"\"\"\n    # Simulate heavy computation\n    for i in {1..1000000}; do\n        echo \"Heavy computation \\$i for ${sample_id}\"\n    done &gt; ${sample_id}_heavy.txt\n    \"\"\"\n}\n\n/*\n* Process with file handling issues\n*/\nprocess handleFiles {\n    publishDir \"${params.output}/files\", mode: 'copy'\n\n    input:\n        path input_file\n\n    output:\n        path \"processed_${input_file}\"\n\n    script:\n    \"\"\"\n    if [ -f \"${input_file}\" ]; then\n        cp ${input_file} processed_${input_file}\n    fi\n    \"\"\"\n}\n\n/*\n* Main workflow with channel issues\n*/\nworkflow {\n\n    // Channel with incorrect usage\n    input_ch = Channel\n        .fromPath(params.input)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample_id, file(row.fastq_path)] }\n\n    processed_ch = processFiles(input_ch)\n\n    heavy_ch = heavyProcess(input_ch.map{it[0]})\n\n    handleFiles(heavyProcess.out)\n}\n</code></pre></p> <p>Error Categories Covered:</p> <ul> <li>Syntax errors: Missing braces, trailing commas, undefined variables</li> <li>Channel structure errors: Wrong data shapes, undefined channels</li> <li>Process errors: Output file mismatches, variable escaping</li> <li>Resource errors: Unrealistic time limits</li> </ul> <p>Key Debugging Lessons:</p> <ol> <li>Read error messages carefully - they often point directly to the problem</li> <li>Use systematic approaches - fix one error at a time and test with <code>-resume</code></li> <li>Understand data flow - channel structure errors are often the most subtle</li> <li>Check work directories - when processes fail, the logs tell you exactly what went wrong</li> </ol>"},{"location":"side_quests/debugging/#summary","title":"Summary","text":"<p>In this side quest, we've learned:</p> <p>1. How to identify and fix syntax errors:</p> <ul> <li>Interpreting Nextflow error messages and locating problems</li> <li>Common syntax errors: missing braces, incorrect keywords, undefined variables</li> <li>Distinguishing between Nextflow (Groovy) and Bash variables</li> <li>Using VS Code extension features for early error detection</li> </ul> <p>2. How to debug channel structure issues:</p> <ul> <li>Understanding channel cardinality and exhaustion issues</li> <li>Debugging channel content structure mismatches</li> <li>Using <code>.view()</code> operators for channel inspection</li> <li>Recognizing error patterns like square brackets in output</li> </ul> <p>3. How to troubleshoot process execution problems:</p> <ul> <li>Diagnosing missing output file errors</li> <li>Understanding exit codes (127 for missing software, 137 for memory issues)</li> <li>Investigating work directories and command files</li> <li>Configuring resources appropriately</li> </ul> <p>4. How to use Nextflow's built-in debugging tools:</p> <ul> <li>Leveraging preview mode and real-time debugging</li> <li>Implementing stub running for logic testing</li> <li>Applying resume for efficient debugging cycles</li> <li>Following a four-phase systematic debugging methodology</li> </ul> <p>Quick Debugging Reference</p> <p>Syntax errors? \u2192 Check VSCode warnings, run <code>nextflow run workflow.nf -preview</code></p> <p>Channel issues? \u2192 Use <code>.view()</code> to inspect content: <code>my_channel.view()</code></p> <p>Process failures? \u2192 Check work directory files:</p> <ul> <li><code>.command.sh</code> - the executed script</li> <li><code>.command.err</code> - error messages</li> <li><code>.exitcode</code> - exit status (127 = command not found, 137 = killed)</li> </ul> <p>Mysterious behavior? \u2192 Run with <code>-stub-run</code> to test workflow logic</p> <p>Made fixes? \u2192 Use <code>-resume</code> to save time testing: <code>nextflow run workflow.nf -resume</code></p>"},{"location":"side_quests/debugging/#whats-next_4","title":"What's next?","text":"<p>Check out the Nextflow documentation for more advanced debugging features and best practices. You might want to:</p> <ul> <li>Add more comprehensive error handling to your workflows</li> <li>Write tests for edge cases and error conditions using nf-test</li> <li>Set up monitoring and logging for production workflows</li> <li>Learn about other debugging tools like profiling and performance analysis</li> <li>Explore more advanced debugging techniques for complex workflows</li> </ul> <p>Remember: Effective debugging is a skill that improves with practice. The systematic methodology and comprehensive toolkit you've developed here will serve you well throughout your Nextflow development journey.</p>"},{"location":"side_quests/dev_environment/","title":"Development Environment","text":"<p>Modern Integrated Development Environments (IDEs) can dramatically transform your Nextflow development experience. This side quest focuses specifically on leveraging VS Code and its Nextflow extension to write code faster, catch errors early, and navigate complex workflows efficiently.</p> <p>This isn't a traditional tutorial</p> <p>Unlike other training modules, this guide is organized as a collection of quick hints, tips, and practical examples rather than a step-by-step tutorial. Each section can be explored independently based on your interests and current development needs. Feel free to jump around and focus on the features that will be most immediately useful to your workflow development.</p>"},{"location":"side_quests/dev_environment/#what-you-should-know-first","title":"What you should know first","text":"<p>This guide assumes you've completed the Hello Nextflow training course and are comfortable with foundational Nextflow concepts including:</p> <ul> <li>Basic workflow structure: Understanding processes, workflows, and how they connect together</li> <li>Channel operations: Creating channels, passing data between processes, and using basic operators</li> <li>Modules and organization: Creating reusable modules and using include statements</li> <li>Configuration basics: Using <code>nextflow.config</code> for parameters, process directives, and profiles</li> </ul>"},{"location":"side_quests/dev_environment/#what-youll-learn-here","title":"What you'll learn here","text":"<p>This guide focuses on IDE productivity features that will make you a more efficient Nextflow developer:</p> <ul> <li>Advanced syntax highlighting: Understanding what VS Code is showing you about your code structure</li> <li>Intelligent auto-completion: Leveraging context-aware suggestions for faster code writing</li> <li>Error detection and diagnostics: Catching syntax errors before you run your workflow</li> <li>Code navigation: Quickly moving between processes, modules, and definitions</li> <li>Formatting and organization: Maintaining consistent, readable code style</li> <li>AI-assisted development (optional): Using modern AI tools integrated with your IDE</li> </ul> <p>Why IDE features now?</p> <p>You've likely already been using VS Code during the Hello Nextflow course, but we kept the focus on learning Nextflow fundamentals rather than IDE features. Now that you're comfortable with basic Nextflow concepts like processes, workflows, channels, and modules, you're ready to leverage the sophisticated IDE features that will make you a more efficient developer.</p> <p>Think of this as \"leveling up\" your development environment - the same editor you've been using has much more powerful capabilities that become truly valuable once you understand what they're helping you with.</p>"},{"location":"side_quests/dev_environment/#0-setup-and-warmup","title":"0. Setup and Warmup","text":"<p>Let's set up a workspace specifically for exploring IDE features:</p> Navigate to the IDE features directory<pre><code>cd side-quests/ide_features\n</code></pre> <p>Open this directory in VS Code:</p> Open VS Code in current directory<pre><code>code .\n</code></pre> <p>The <code>ide_features</code> directory contains example workflows that demonstrate various IDE features:</p> Show directory structure<pre><code>tree .\n</code></pre> Project structure<pre><code>tree .\n.\n\u251c\u2500\u2500 basic_workflow.nf\n\u251c\u2500\u2500 complex_workflow.nf\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 sample_001.fastq.gz\n\u2502   \u251c\u2500\u2500 sample_002.fastq.gz\n\u2502   \u251c\u2500\u2500 sample_003.fastq.gz\n\u2502   \u251c\u2500\u2500 sample_004.fastq.gz\n\u2502   \u251c\u2500\u2500 sample_005.fastq.gz\n\u2502   \u2514\u2500\u2500 sample_data.csv\n\u251c\u2500\u2500 modules\n\u2502   \u251c\u2500\u2500 fastqc.nf\n\u2502   \u251c\u2500\u2500 star.nf\n\u2502   \u2514\u2500\u2500 utils.nf\n\u2514\u2500\u2500 nextflow.config\n\n3 directories, 12 files\n</code></pre> <p>About the Example Files</p> <ul> <li><code>basic_workflow.nf</code> is a working basic workflow that you can run and modify</li> <li><code>complex_workflow.nf</code> is designed for illustration only to demonstrate navigation features - it may not run successfully but shows realistic multi-file workflow structure</li> </ul>"},{"location":"side_quests/dev_environment/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":"<p>Some of the features in this guide will use optional keyboard shortcuts. You may well be accessing this material via GitHub Codespaces in browser, and in this case sometimes the shortcuts will not work as expected because they're used for other things in your system.</p> <p>If you are running VS Code locally, as you probably will be when you're actually writing workflows, the shortcuts will work as described.</p> <p>If you are using a Mac, some (not all) keyboard shortcuts will use \"cmd\" instead of \"ctrl\", and we'll indicate this in the text like <code>Ctrl/Cmd</code>.</p>"},{"location":"side_quests/dev_environment/#01-installing-the-nextflow-extension","title":"0.1. Installing the Nextflow Extension","text":"<p>Already Using Devcontainers?</p> <p>If you're working in GitHub Codespaces or using a local devcontainer, the Nextflow extension is likely already installed and configured for you. You can skip the manual installation steps below and proceed directly to exploring the extension features.</p> <p>To install the extension manually:</p> <ol> <li>Open VS Code</li> <li>Go to the Extensions view by clicking the extensions icon to the left:  (shortcut <code>Ctrl/Cmd+Shift+X</code> if you're running VSCode locally)</li> <li>Search for \"Nextflow\"</li> <li>Install the official Nextflow extension</li> </ol> <p></p>"},{"location":"side_quests/dev_environment/#02-workspace-layout","title":"0.2. Workspace Layout","text":"<p>Since you've been using VS Code throughout Hello Nextflow, you're already familiar with the basics. Here's how to organize your workspace efficiently for this session:</p> <ul> <li>Editor Area: For viewing and editing files. You can split this into multiple panes to compare files side by side.</li> <li>File Explorer click () (<code>Ctrl/Cmd+Shift+E</code>): The local files and folders on your system. Keep this open on the left to navigate between files</li> <li>Integrated Terminal (<code>Ctrl+Shift+</code> backtick for both Windows and MacOS): A terminal for interacting with the computer at the bottom. Use this to run Nextflow or other commands.</li> <li>Problems Panel (<code>Ctrl+Shift+M</code>): VS Code will show any errors and problems it detects here. This is useful for highlighting issues at a glance.</li> </ul> <p>You can drag panels around or hide them (<code>Ctrl/Cmd+B</code> to toggle the sidebar) to customize your layout as we work through the examples.</p>"},{"location":"side_quests/dev_environment/#takeaway","title":"Takeaway","text":"<p>You have VS Code set up with the Nextflow extension and understand the workspace layout for efficient development.</p>"},{"location":"side_quests/dev_environment/#whats-next","title":"What's next?","text":"<p>Learn how syntax highlighting helps you understand Nextflow code structure at a glance.</p>"},{"location":"side_quests/dev_environment/#1-syntax-highlighting-and-code-structure","title":"1. Syntax Highlighting and Code Structure","text":"<p>Now that your workspace is set up, let's explore how VS Code's syntax highlighting helps you read and write Nextflow code more effectively.</p>"},{"location":"side_quests/dev_environment/#11-nextflow-syntax-elements","title":"1.1. Nextflow Syntax Elements","text":"<p>Open <code>basic_workflow.nf</code> to see syntax highlighting in action:</p> <p></p> <p>Notice how VS Code highlights:</p> <ul> <li>Keywords (<code>process</code>, <code>workflow</code>, <code>input</code>, <code>output</code>, <code>script</code>) in distinct colors</li> <li>String literals and parameters with different styling</li> <li>Comments in a muted color</li> <li>Variables and function calls with appropriate emphasis</li> <li>Code blocks with proper indentation guides</li> </ul> <p>Theme-Dependent Colors</p> <p>The specific colors you see will depend on your VS Code theme (dark/light mode), color settings, and any customizations you've made. The important thing is that different syntax elements are visually distinguished from each other, making code structure easier to understand regardless of your chosen color scheme.</p>"},{"location":"side_quests/dev_environment/#12-understanding-code-structure","title":"1.2. Understanding Code Structure","text":"<p>The syntax highlighting helps you quickly identify:</p> <ul> <li>Process boundaries: Clear distinction between different processes</li> <li>Input/output blocks: Easy to spot data flow definitions</li> <li>Script blocks: The actual commands being executed</li> <li>Channel operations: Data transformation steps</li> <li>Configuration directives: Process-specific settings</li> </ul> <p>This visual organization becomes invaluable when working with complex workflows containing multiple processes and intricate data flows.</p>"},{"location":"side_quests/dev_environment/#takeaway_1","title":"Takeaway","text":"<p>You understand how VS Code's syntax highlighting helps you read Nextflow code structure and identify different language elements for faster development.</p>"},{"location":"side_quests/dev_environment/#whats-next_1","title":"What's next?","text":"<p>Learn how intelligent auto-completion speeds up code writing with context-aware suggestions.</p>"},{"location":"side_quests/dev_environment/#2-intelligent-auto-completion","title":"2. Intelligent Auto-completion","text":"<p>VS Code's auto-completion features help you write code faster and with fewer errors by suggesting appropriate options based on context.</p>"},{"location":"side_quests/dev_environment/#21-context-aware-suggestions","title":"2.1. Context-Aware Suggestions","text":"<p>The auto-completion options vary depending on where you are in your code:</p>"},{"location":"side_quests/dev_environment/#channel-operations","title":"Channel Operations","text":"<p>Open <code>basic_workflow.nf</code> again and try typing <code>Channel.</code> in the workflow block:</p> <p></p> <p>You'll see suggestions for:</p> <ul> <li><code>fromPath()</code> - Create channel from file paths</li> <li><code>fromFilePairs()</code> - Create channel from paired files</li> <li><code>of()</code> - Create channel from values</li> <li><code>fromSRA()</code> - Create channel from SRA accessions</li> <li>And many more...</li> </ul> <p>This helps you quickly find the right channel factory to use without needing to remember exact method names.</p> <p>You can also discover the operators available to apply to channels. For example, type <code>FASTQC.out.html.</code> to see available operations:</p> <p></p>"},{"location":"side_quests/dev_environment/#process-directives","title":"Process Directives","text":"<p>Inside a process script block, type <code>task.</code> to see available runtime properties:</p> <p></p>"},{"location":"side_quests/dev_environment/#configuration","title":"Configuration","text":"<p>Open nextflow.config and type <code>process.</code> anywhere to see available process directives:</p> <p></p> <p>You'll see suggestions for:</p> <ul> <li><code>executor</code></li> <li><code>memory</code></li> <li><code>cpus</code></li> </ul> <p>This saves time when configuring processes and works across different configuration scopes. For example, try typing <code>docker.</code> to see Docker-specific configuration options.</p>"},{"location":"side_quests/dev_environment/#takeaway_2","title":"Takeaway","text":"<p>You can use VS Code's intelligent auto-completion to discover available channel operations, process directives, and configuration options without memorizing syntax.</p>"},{"location":"side_quests/dev_environment/#whats-next_2","title":"What's next?","text":"<p>Learn how real-time error detection helps you catch issues before running your workflow, simply by reading the code.</p>"},{"location":"side_quests/dev_environment/#3-error-detection-and-diagnostics","title":"3. Error Detection and Diagnostics","text":"<p>VS Code's real-time error detection helps you catch issues before running your workflow.</p>"},{"location":"side_quests/dev_environment/#31-syntax-error-detection","title":"3.1. Syntax Error Detection","text":"<p>Let's create a deliberate error to see the detection in action. Open <code>basic_workflow.nf</code> and change the process name from <code>FASTQC</code> to <code>FASTQ</code> (or any other invalid name). VS Code will immediately highlight the error in the workflow block with a red squiggly underline:</p> <p></p>"},{"location":"side_quests/dev_environment/#32-problems-panel","title":"3.2. Problems Panel","text":"<p>Beyond individual error highlighting, VS Code provides a centralized Problems panel that aggregates all errors, warnings, and info messages across your workspace. Open it with <code>Ctrl/Cmd+Shift+M</code> and use the filter icon to show only errors relevant to the current file:</p> <p></p> <p>Click on any issue to jump directly to the problematic line</p> <p></p> <p>Fix the error by changing the process name back to <code>FASTQC</code>.</p>"},{"location":"side_quests/dev_environment/#33-common-error-patterns","title":"3.3. Common Error Patterns","text":"<p>Common errors in Nextflow syntax include:</p> <ul> <li>Missing brackets: Unmatched <code>{</code> or <code>}</code></li> <li>Incomplete blocks: Missing required sections in processes</li> <li>Invalid syntax: Malformed Nextflow DSL</li> <li>Typos in keywords: Misspelled process directives</li> <li>Channel mismatches: Type incompatibilities</li> </ul> <p>The Nextflow language server highlights these issues in the Problems panel. You can check these out early to avoid syntax errors while running a pipeline.</p>"},{"location":"side_quests/dev_environment/#takeaway_3","title":"Takeaway","text":"<p>You can use VS Code's error detection and Problems panel to catch syntax errors and issues before running your workflow, saving time and preventing frustration.</p>"},{"location":"side_quests/dev_environment/#whats-next_3","title":"What's next?","text":"<p>Learn how to efficiently navigate between processes, modules, and definitions in complex workflows.</p>"},{"location":"side_quests/dev_environment/#4-code-navigation-and-symbol-management","title":"4. Code Navigation and Symbol Management","text":"<p>Efficient navigation is crucial when working with complex workflows spanning multiple files. To understand this, replace the process definition in <code>basic_workflow.nf</code> with an import for the module we've provided you:</p> AfterBefore basic_workflow.nf<pre><code>include { FASTQC } from './modules/fastqc.nf'\n</code></pre> basic_workflow.nf<pre><code>process FASTQC {\n    tag \"${sample_id}\"\n    publishDir \"${params.output_dir}/fastqc\", mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"*.html\"), emit: html\n    tuple val(sample_id), path(\"*.zip\"), emit: zip\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    fastqc \\\\\n        ${args} \\\\\n        --threads ${task.cpus} \\\\\n        ${reads}\n    \"\"\"\n}\n</code></pre>"},{"location":"side_quests/dev_environment/#41-go-to-definition","title":"4.1. Go to Definition","text":"<p>If you mouse over a process name like <code>FASTQC</code>, you'll see a popup with the module interface (inputs and outputs):</p> <p></p> <p>This feature is particularly valuable when authoring workflows, as it allows you to understand the module interface without opening the module file directly.</p> <p>You can quickly navigate to any process, module, or variable definition using Ctrl/Cmd-click . Mouse over the link to the module file at the top of the script, and follow the link as suggested:</p> <p></p> <p>The same thing works for process names. Go back to <code>basic_workflow.nf</code> and try this on the <code>FASTQC</code> process name in the workflow block. This links you directly to the process name (which is the same as the module file in this example, but could be part-way through a much larger file).</p> <p>To go back to where you were, use Alt+\u2190 (or Ctrl+- on Mac). This is a powerful way to explore code without losing your place.</p> <p>Now let's explore navigation in a more complex workflow using <code>complex_workflow.nf</code> (the illustration-only file mentioned earlier). This workflow contains multiple processes defined in separate module files, as well as some inline ones. While complex multi-file structures can be challenging to navigate manually, the ability to jump to definitions makes exploration much more manageable.</p> <ol> <li>Open <code>complex_workflow.nf</code></li> <li>Navigate to module definitions</li> <li>Use Alt+\u2190 (or Ctrl+-) to navigate back</li> <li>Navigate to the <code>FASTQC</code> process name in the workflow block. This links you directly to the process name (which is the same as the module file in this example, but could be part-way through a much larger file).</li> <li>Navigate back again</li> <li>Navigate to the <code>TRIM_GALORE</code> process in the workflow block. This is defined inline, so it won't take you to a separate file, but it will still show you the process definition, and you can still navigate back to where you were.</li> </ol>"},{"location":"side_quests/dev_environment/#42-symbol-navigation","title":"4.2. Symbol Navigation","text":"<p>With <code>complex_workflow.nf</code> still open, you can get an overview of all symbols in the file by typing <code>@</code> into the search bar at the top of VSCode (the keyboard shortcut is <code>Ctrl/Cmd+Shift+O</code>, but may not work in Codespaces). This opens the symbol navigation panel, which lists all symbols in the current file:</p> <p></p> <p>This shows:</p> <ul> <li>All process definitions</li> <li>Workflow definitions (there are two workflows defined in this file)</li> <li>Function definitions</li> </ul> <p>Start typing to filter results.</p>"},{"location":"side_quests/dev_environment/#43-find-all-references","title":"4.3. Find All References","text":"<p>Understanding where a process or variable is used throughout your codebase can be very helpful. For instance, if you want to find all references to the <code>FASTQC</code> process, start by navigating to its definition. You can do this by opening <code>modules/fastqc.nf</code> directly, or by using VS Code's quick navigation feature with <code>Ctrl/Cmd-click</code> as we did above. Once at the process definition, right-click on the <code>FASTQC</code> process name and select \"Find All References\" from the context menu to see all instances where it is used.</p> <p></p> <p>This feature displays all instances where <code>FASTQC</code> is referenced within your workspace, including its usage in the two distinct workflows. This insight is crucial for assessing the potential impact of modifications to the <code>FASTQC</code> process.</p>"},{"location":"side_quests/dev_environment/#44-outline-panel","title":"4.4. Outline Panel","text":"<p>The Outline panel, located in the Explorer sidebar (click ), provides a convenient overview of all symbols in your current file. This feature allows you to quickly navigate and manage the structure of your code by displaying functions, variables, and other key elements in a hierarchical view.</p> <p></p> <p>Use the Outline panel to navigate quickly to different parts of your code without using the file browser.</p>"},{"location":"side_quests/dev_environment/#45-dag-visualization","title":"4.5. DAG visualization","text":"<p>VS Code's Nextflow extension can visualize your workflow as a Directed Acyclic Graph (DAG). This helps you understand the data flow and dependencies between processes. Open <code>complex_workflow.nf</code> and click the \"Preview DAG\" button above <code>workflow {</code> (the second <code>workflow</code> block in this file):</p> <p></p> <p>This is just the 'entry' workflow, but you can also preview the DAG for the inner workflows by clicking the \"Preview DAG\" button above the workflow <code>RNASEQ_PIPELINE {</code> further up:</p> <p></p> <p>For this workflow, you can use the nodes in the DAG to navigate to the corresponding process definitions in the code. Click on a node, and it will take you to the relevant process definition in the editor. Particularly when a workflow grows to a large size, this can really help you to navigate around the code and understand how the processes are connected.</p>"},{"location":"side_quests/dev_environment/#takeaway_4","title":"Takeaway","text":"<p>You can navigate complex workflows efficiently using go-to-definition, symbol search, find references, and DAG visualization to understand code structure and dependencies.</p>"},{"location":"side_quests/dev_environment/#whats-next_4","title":"What's next?","text":"<p>Learn how to work effectively across multiple interconnected files in larger Nextflow projects.</p>"},{"location":"side_quests/dev_environment/#5-working-across-multiple-files","title":"5. Working Across Multiple Files","text":"<p>Real Nextflow development involves working with multiple interconnected files. Let's explore how VS Code helps you manage complex projects efficiently.</p>"},{"location":"side_quests/dev_environment/#51-quick-file-navigation","title":"5.1. Quick File Navigation","text":"<p>With <code>complex_workflow.nf</code> open, you'll notice it imports several modules. Let's practice quick navigation between them.</p> <p>Press Ctrl+P (or Cmd+P) and start typing \"fast\":</p> <p>VS Code will show you matching files. Select <code>modules/fastqc.nf</code> to jump there instantly. This is much faster than clicking through the file explorer when you know roughly what file you're looking for.</p> <p>Try this with other patterns:</p> <ul> <li>Type \"star\" to find the STAR alignment module file (<code>star.nf</code>)</li> <li>Type \"utils\" to find utility functions file (<code>utils.nf</code>)</li> <li>Type \"config\" to jump to configuration files (<code>nextflow.config</code>)</li> </ul>"},{"location":"side_quests/dev_environment/#52-split-editor-for-multi-file-development","title":"5.2. Split Editor for Multi-file Development","text":"<p>When working with modules, you often need to see both the main workflow and module definitions simultaneously. Let's set this up:</p> <ol> <li>Open <code>complex_workflow.nf</code></li> <li>Open <code>modules/fastqc.nf</code> in a new tab</li> <li>Right-click on the <code>modules/fastqc.nf</code> tab and select \"Split Right\"</li> <li>Now you can see both files side by side</li> </ol> <p></p> <p>This is invaluable when:</p> <ul> <li>Checking module interfaces while writing workflow calls, and the preview is not enough</li> <li>Comparing similar processes across different modules</li> <li>Debugging data flow between workflow and modules</li> </ul>"},{"location":"side_quests/dev_environment/#53-project-wide-search","title":"5.3. Project-wide Search","text":"<p>Sometimes you need to find where specific patterns are used across your entire project. Press <code>Ctrl/Cmd+Shift+F</code> to open the search panel.</p> <p>Try searching for <code>publishDir</code> across the workspace:</p> <p></p> <p>This shows you every file that uses publish directories, helping you:</p> <ul> <li>Understand output organization patterns</li> <li>Find examples of specific directives</li> <li>Ensure consistency across modules</li> </ul>"},{"location":"side_quests/dev_environment/#takeaway_5","title":"Takeaway","text":"<p>You can manage complex multi-file projects using quick file navigation, split editors, and project-wide search to work efficiently across workflows and modules.</p>"},{"location":"side_quests/dev_environment/#whats-next_5","title":"What's next?","text":"<p>Learn how code formatting and maintenance features keep your workflows organized and readable.</p>"},{"location":"side_quests/dev_environment/#6-code-formatting-and-maintenance","title":"6. Code Formatting and Maintenance","text":"<p>Proper code formatting is essential not only for aesthetics but also for enhancing readability, comprehension, and the ease of updating complex workflows.</p>"},{"location":"side_quests/dev_environment/#61-automatic-formatting-in-action","title":"6.1. Automatic Formatting in Action","text":"<p>Open <code>basic_workflow.nf</code> and deliberately mess up the formatting:</p> <ul> <li>Remove some indentation: Highlight the entire document and press <code>shift+tab</code> lots of times to remove as many indentations as possible.</li> <li>Add extra spaces in random places: the <code>Channel.fromPath</code> statement, add 30 spaces after the <code>(</code>.</li> <li>Break some lines awkwardly: Add a new line between the the <code>.view {</code> operator and the <code>Processing sample:</code> string but do not add a corresponding newline before the closing parenthesis <code>}</code>.</li> </ul> <p>Now press <code>Shift+Alt+F</code> (or <code>Shift+Option+F</code> on MacOS) to auto-format:</p> <p>VS Code immediately:</p> <ul> <li>Fixes indentation to show process structure clearly</li> <li>Aligns similar elements consistently</li> <li>Removes unnecessary whitespace</li> <li>Maintains readable line breaks</li> </ul> <p>Note that automatic formatting may not resolve every code style issue. The Nextflow language server aims to keep your code tidy, but it also respects your personal preferences in certain areas. For example, if you remove indentation inside the <code>script</code> block of a process, the formatter will leave it as-is, since you might intentionally prefer that style.</p> <p>Currently, there is no strict style enforcement for Nextflow, so the language server offers some flexibility. However, it will consistently apply formatting rules around method and function definitions to maintain clarity.</p>"},{"location":"side_quests/dev_environment/#62-code-organization-features","title":"6.2. Code Organization Features","text":""},{"location":"side_quests/dev_environment/#quick-commenting","title":"Quick Commenting","text":"<p>Select a block of code in your workflow and press Ctrl+/ (or Cmd+/) to comment it out:</p> <pre><code>// workflow {\n//     ch_input = Channel.fromPath(params.input)\n//         .splitCsv(header: true)\n//         .map { row -&gt; [row.sample_id, file(row.fastq_path)] }\n//\n//     FASTQC(ch_input)\n// }\n</code></pre> <p>This is perfect for:</p> <ul> <li>Temporarily disabling parts of workflows during development</li> <li>Adding explanatory comments to complex channel operations</li> <li>Documenting workflow sections</li> </ul> <p>Use Ctrl+/ (or Cmd+/) again to uncomment the code.</p>"},{"location":"side_quests/dev_environment/#code-folding-for-overview","title":"Code Folding for Overview","text":"<p>In <code>complex_workflow.nf</code>, notice the small arrows next to process definitions. Click them to fold (collapse) processes:</p> <p></p> <p>This gives you a high-level overview of your workflow structure without getting lost in implementation details.</p>"},{"location":"side_quests/dev_environment/#bracket-matching","title":"Bracket Matching","text":"<p>Place your cursor next to any <code>{</code> or <code>}</code> bracket and VS Code highlights the matching bracket. Use Ctrl+Shift+\\ (or Cmd+Shift+\\) to jump between matching brackets.</p> <p>This is crucial for:</p> <ul> <li>Understanding process boundaries</li> <li>Finding missing or extra brackets</li> <li>Navigating nested workflow structures</li> </ul>"},{"location":"side_quests/dev_environment/#multi-line-selection-and-editing","title":"Multi-line Selection and Editing","text":"<p>For editing multiple lines simultaneously, VS Code offers powerful multi-cursor capabilities:</p> <ul> <li>Multi-line selection: Hold Ctrl+Alt (or Cmd+Option for MacOS) and use arrow keys to select multiple lines</li> <li>Multi-line indenting: Select multiple lines and use Tab to indent or Shift+Tab to outdent entire blocks</li> </ul> <p>This is particularly useful for:</p> <ul> <li>Indenting entire process blocks consistently</li> <li>Adding comments to multiple lines at once</li> <li>Editing similar parameter definitions across multiple processes</li> </ul>"},{"location":"side_quests/dev_environment/#takeaway_6","title":"Takeaway","text":"<p>You can maintain clean, readable code using automatic formatting, commenting features, code folding, bracket matching, and multi-line editing to organize complex workflows efficiently.</p>"},{"location":"side_quests/dev_environment/#whats-next_6","title":"What's next?","text":"<p>Learn how VS Code integrates with your broader development workflow beyond just editing code.</p>"},{"location":"side_quests/dev_environment/#7-development-workflow-integration","title":"7. Development Workflow Integration","text":"<p>VS Code integrates well with your development workflow beyond just editing code.</p>"},{"location":"side_quests/dev_environment/#71-version-control-integration","title":"7.1. Version Control Integration","text":"<p>Codespaces and Git Integration</p> <p>If you're working in GitHub Codespaces, some Git integration features may not work as expected, particularly keyboard shortcuts for Source Control. You may have also declined to open the directory as a Git repository during initial setup, which is fine for training purposes.</p> <p>If your project is a git repository (as this is), VS Code shows:</p> <ul> <li>Modified files with colored indicators</li> <li>Git status in the status bar</li> <li>Inline diff views</li> <li>Commit and push capabilities</li> </ul> <p>Open the Source Control panel using the source control button () (<code>Ctrl+Shift+G</code> or <code>Cmd+Shift+G</code> if you're working with VSCode locally) to see git changes and stage commits directly in the editor.</p> <p></p>"},{"location":"side_quests/dev_environment/#72-running-and-inspecting-workflows","title":"7.2. Running and Inspecting Workflows","text":"<p>Let's run a workflow and then inspect the results. In the integrated terminal (<code>Ctrl+Shift+</code> backtick in both Windows and MacOS), run the basic workflow:</p> Run the basic workflow<pre><code>nextflow run basic_workflow.nf --input data/sample_data.csv --output_dir results\n</code></pre> <p>While the workflow runs, you'll see real-time output in the terminal. After completion, you can use VS Code to inspect results without leaving your editor:</p> <ol> <li>Navigate to work directories: Use the file explorer or terminal to browse <code>.nextflow/work</code></li> <li>Open log files: Click on log file paths in terminal output to open them directly in VS Code</li> <li>Inspect outputs: Browse published results directories in the file explorer</li> <li>View execution reports: Open HTML reports directly in VS Code or your browser</li> </ol> <p>This keeps everything in one place rather than switching between multiple applications.</p>"},{"location":"side_quests/dev_environment/#takeaway_7","title":"Takeaway","text":"<p>You can integrate VS Code with version control and workflow execution to manage your entire development process from a single interface.</p>"},{"location":"side_quests/dev_environment/#whats-next_7","title":"What's next?","text":"<p>See how all these IDE features work together in your daily development workflow.</p>"},{"location":"side_quests/dev_environment/#8-recap-and-quick-notes","title":"8. Recap and quick notes","text":"<p>Here are some quick notes on each of the IDE features discussed above:</p>"},{"location":"side_quests/dev_environment/#81-starting-a-new-feature","title":"8.1. Starting a New Feature","text":"<ol> <li>Quick file open (<code>Ctrl+P</code> or <code>Cmd+P</code>) to find relevant existing modules</li> <li>Split editor to view similar processes side by side</li> <li>Symbol navigation (<code>Ctrl+Shift+O</code> or <code>Cmd+Shift+O</code>) to understand file structure</li> <li>Auto-completion to write new code quickly</li> </ol>"},{"location":"side_quests/dev_environment/#82-debugging-issues","title":"8.2. Debugging Issues","text":"<ol> <li>Problems panel (<code>Ctrl+Shift+M</code> or <code>Cmd+Shift+M</code>) to see all errors at once</li> <li>Go to definition (<code>Ctrl-click</code> or <code>Cmd-click</code>) to understand process interfaces</li> <li>Find all references to see how processes are used</li> <li>Project-wide search to find similar patterns or issues</li> </ol>"},{"location":"side_quests/dev_environment/#83-refactoring-and-improvement","title":"8.3. Refactoring and Improvement","text":"<ol> <li>Project-wide search (<code>Ctrl+Shift+F</code> or <code>Cmd+Shift+F</code>) to find patterns</li> <li>Auto-formatting (<code>Shift+Alt+F</code> or <code>Shift+Option+F</code>) to maintain consistency</li> <li>Code folding to focus on structure</li> <li>Git integration to track changes</li> </ol>"},{"location":"side_quests/dev_environment/#summary","title":"Summary","text":"<p>You have now had a whistle-stop tour of VS Code's IDE features for Nextflow development. These tools will make you significantly more productive by:</p> <ul> <li>Reducing errors through real-time syntax checking</li> <li>Speeding up development with intelligent auto-completion</li> <li>Improving navigation in complex multi-file workflows</li> <li>Maintaining quality through consistent formatting</li> <li>Enhancing understanding through advanced highlighting and structure visualization</li> </ul> <p>We don't expect you to remember everything, but now you know that these features exist you will be able to find them when you need them. As you continue developing Nextflow workflows, these IDE features will become second nature, allowing you to focus on writing high-quality code rather than wrestling with syntax and structure.</p>"},{"location":"side_quests/dev_environment/#whats-next_8","title":"What's next?","text":"<p>Apply these IDE skills while working through other training modules, for example:</p> <ul> <li>nf-test: Create comprehensive test suites for your workflows</li> <li>Hello nf-core: Build production-quality pipelines with community standards</li> </ul> <p>The true power of these IDE features emerges as you work on larger, more complex projects. Start incorporating them into your workflow gradually\u2014within a few sessions, they'll become second nature and transform how you approach Nextflow development.</p> <p>From catching errors before they slow you down to navigating complex codebases with ease, these tools will make you a more confident and efficient developer.</p> <p>Happy coding!</p>"},{"location":"side_quests/essential_scripting_patterns/","title":"Essential Nextflow Scripting Patterns","text":"<p>Nextflow is a programming language that runs on the Java Virtual Machine. While Nextflow is built on Groovy and shares much of its syntax, Nextflow is more than just \"Groovy with extensions\" -- it is a standalone language with a fully-specified syntax and standard library.</p> <p>You can write a lot of Nextflow without venturing beyond basic syntax for variables, maps, and lists. Most Nextflow tutorials focus on workflow orchestration (channels, processes, and data flow), and you can go surprisingly far with just that.</p> <p>However, when you need to manipulate data, parse complex filenames, implement conditional logic, or build robust production workflows, it helps to think about two distinct aspects of your code: dataflow (channels, operators, processes, and workflows) and scripting (the code inside closures, functions, and process scripts). While this distinction is somewhat arbitrary\u2014it's all Nextflow code\u2014it provides a useful mental model for understanding when you're orchestrating your pipeline versus when you're manipulating data. Mastering both dramatically improves your ability to write clear, maintainable workflows.</p> <p>This side quest takes you on a hands-on journey from basic concepts to production-ready patterns. We'll transform a simple CSV-reading workflow into a sophisticated bioinformatics pipeline, evolving it step-by-step through realistic challenges:</p> <ul> <li>Understanding boundaries: Distinguish between dataflow operations and scripting, and understand how they work together</li> <li>Data manipulation: Extract, transform, and subset maps and collections using powerful operators</li> <li>String processing: Parse complex file naming schemes with regex patterns and master variable interpolation</li> <li>Reusable functions: Extract complex logic into named functions for cleaner, more maintainable workflows</li> <li>Dynamic logic: Build processes that adapt to different input types and use closures for dynamic resource allocation</li> <li>Conditional routing: Intelligently route samples through different processes based on their metadata characteristics</li> <li>Safe operations: Handle missing data gracefully with null-safe operators and validate inputs with clear error messages</li> <li>Configuration-based handlers: Use workflow event handlers for logging, notifications, and lifecycle management</li> </ul>"},{"location":"side_quests/essential_scripting_patterns/#0-warmup","title":"0. Warmup","text":""},{"location":"side_quests/essential_scripting_patterns/#01-prerequisites","title":"0.1. Prerequisites","text":"<p>Before taking on this side quest you should:</p> <ul> <li>Complete the Hello Nextflow tutorial or have equivalent experience</li> <li>Understand basic Nextflow concepts (processes, channels, workflows)</li> <li>Have basic familiarity with common programming constructs (variables, maps, lists)</li> </ul> <p>This tutorial will explain programming concepts as we encounter them, so you don't need extensive programming experience. We'll start with fundamental concepts and build up to advanced patterns.</p>"},{"location":"side_quests/essential_scripting_patterns/#02-starting-point","title":"0.2. Starting Point","text":"<p>Navigate to the project directory:</p> Navigate to project directory<pre><code>cd side-quests/essential_scripting_patterns\n</code></pre> <p>The <code>data</code> directory contains sample files and a main workflow file we'll evolve throughout.</p> Directory contents<pre><code>&gt; tree\n.\n\u251c\u2500\u2500 collect.nf\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 samples.csv\n\u2502   \u2514\u2500\u2500 sequences\n\u2502       \u251c\u2500\u2500 SAMPLE_001_S1_L001_R1_001.fastq\n\u2502       \u251c\u2500\u2500 SAMPLE_002_S2_L001_R1_001.fastq\n\u2502       \u2514\u2500\u2500 SAMPLE_003_S3_L001_R1_001.fastq\n\u251c\u2500\u2500 main.nf\n\u251c\u2500\u2500 modules\n\u2502   \u251c\u2500\u2500 fastp.nf\n\u2502   \u251c\u2500\u2500 generate_report.nf\n\u2502   \u2514\u2500\u2500 trimgalore.nf\n\u2514\u2500\u2500 nextflow.config\n\n3 directories, 10 files\n</code></pre> <p>Our sample CSV contains information about biological samples that need different processing based on their characteristics:</p> samples.csv<pre><code>sample_id,organism,tissue_type,sequencing_depth,file_path,quality_score\nSAMPLE_001,human,liver,30000000,data/sequences/SAMPLE_001_S1_L001_R1_001.fastq,38.5\nSAMPLE_002,mouse,brain,25000000,data/sequences/SAMPLE_002_S2_L001_R1_001.fastq,35.2\nSAMPLE_003,human,kidney,45000000,data/sequences/SAMPLE_003_S3_L001_R1_001.fastq,42.1\n</code></pre> <p>We'll use this realistic dataset to explore practical programming techniques that you'll encounter in real bioinformatics workflows.</p>"},{"location":"side_quests/essential_scripting_patterns/#1-dataflow-vs-scripting-understanding-the-boundaries","title":"1. Dataflow vs Scripting: Understanding the Boundaries","text":""},{"location":"side_quests/essential_scripting_patterns/#11-identifying-whats-what","title":"1.1. Identifying What's What","text":"<p>When writing Nextflow workflows, it's important to distinguish between dataflow (how data moves through channels and processes) and scripting (the code that manipulates data and makes decisions). Let's build a workflow demonstrating how they work together.</p>"},{"location":"side_quests/essential_scripting_patterns/#step-1-basic-nextflow-workflow","title":"Step 1: Basic Nextflow Workflow","text":"<p>Start with a simple workflow that just reads the CSV file (we've already done this for you in <code>main.nf</code>):</p> main.nf<pre><code>workflow {\n    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .view()\n}\n</code></pre> <p>The <code>workflow</code> block defines our pipeline structure, while <code>channel.fromPath()</code> creates a channel from a file path. The <code>.splitCsv()</code> operator processes the CSV file and converts each row into a map data structure.</p> <p>Run this workflow to see the raw CSV data:</p> Test basic workflow<pre><code>nextflow run main.nf\n</code></pre> <p>You should see output like:</p> Raw CSV data<pre><code>Launching `main.nf` [marvelous_tuckerman] DSL2 - revision: 6113e05c17\n\n[sample_id:SAMPLE_001, organism:human, tissue_type:liver, sequencing_depth:30000000, file_path:data/sequences/SAMPLE_001_S1_L001_R1_001.fastq, quality_score:38.5]\n[sample_id:SAMPLE_002, organism:mouse, tissue_type:brain, sequencing_depth:25000000, file_path:data/sequences/SAMPLE_002_S2_L001_R1_001.fastq, quality_score:35.2]\n[sample_id:SAMPLE_003, organism:human, tissue_type:kidney, sequencing_depth:45000000, file_path:data/sequences/SAMPLE_003_S3_L001_R1_001.fastq, quality_score:42.1]\n</code></pre>"},{"location":"side_quests/essential_scripting_patterns/#step-2-adding-the-map-operator","title":"Step 2: Adding the Map Operator","text":"<p>Now we're going to add scripting to transform the data, using the <code>.map()</code> operator you will probably already be familiar with. This operator takes a 'closure' where we can write code to transform each item.</p> <p>Note</p> <p>A closure is a block of code that can be passed around and executed later. Think of it as a function that you define inline. Closures are written with curly braces <code>{ }</code> and can take parameters. They're fundamental to how Nextflow operators work and if you've been writing Nextflow for a while, you may already have been using them without realizing it!</p> <p>Here's what that map operation looks like:</p> AfterBefore main.nf<pre><code>    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            return row\n        }\n        .view()\n</code></pre> main.nf<pre><code>    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .view()\n</code></pre> <p>This is our first closure - an anonymous function you can pass as an argument (similar to lambdas in Python or arrow functions in JavaScript). Closures are essential for working with Nextflow operators.</p> <p>The closure <code>{ row -&gt; return row }</code> takes a parameter <code>row</code> (could be any name: <code>item</code>, <code>sample</code>, etc.).</p> <p>When the <code>.map()</code> operator processes each channel item, it passes that item to your closure. Here, <code>row</code> holds one CSV row at a time.</p> <p>Apply this change and run the workflow:</p> Test map operator<pre><code>nextflow run main.nf\n</code></pre> <p>You'll see the same output as before, because we're simply returning the input unchanged. This confirms that the map operator is working correctly. Now let's start transforming the data.</p>"},{"location":"side_quests/essential_scripting_patterns/#step-3-creating-a-map-data-structure","title":"Step 3: Creating a Map Data Structure","text":"<p>Now we're going to write scripting logic inside our closure to transform each row of data. This is where we process individual data items rather than orchestrating data flow.</p> AfterBefore main.nf<pre><code>    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            // Scripting for data transformation\n            def sample_meta = [\n                id: row.sample_id.toLowerCase(),\n                organism: row.organism,\n                tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n                depth: row.sequencing_depth.toInteger(),\n                quality: row.quality_score.toDouble()\n            ]\n            return sample_meta\n        }\n        .view()\n</code></pre> main.nf<pre><code>    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            return row\n        }\n        .view()\n</code></pre> <p>The <code>sample_meta</code> map is a key-value data structure (like dictionaries in Python, objects in JavaScript, or hashes in Ruby) storing related information: sample ID, organism, tissue type, sequencing depth, and quality score.</p> <p>We use string manipulation methods like <code>.toLowerCase()</code> and <code>.replaceAll()</code> to clean up our data, and type conversion methods like <code>.toInteger()</code> and <code>.toDouble()</code> to convert string data from the CSV into the appropriate numeric types.</p> <p>Apply this change and run the workflow:</p> Test map data structure<pre><code>nextflow run main.nf\n</code></pre> <p>You should see the refined map output like:</p> Transformed metadata<pre><code>[id:sample_001, organism:human, tissue:liver, depth:30000000, quality:38.5]\n[id:sample_002, organism:mouse, tissue:brain, depth:25000000, quality:35.2]\n[id:sample_003, organism:human, tissue:kidney, depth:45000000, quality:42.1]\n</code></pre>"},{"location":"side_quests/essential_scripting_patterns/#step-4-adding-conditional-logic","title":"Step 4: Adding Conditional Logic","text":"<p>Now let's add more scripting - this time using a ternary operator to make decisions based on data values.</p> <p>Make the following change:</p> AfterBefore main.nf<pre><code>    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def sample_meta = [\n                id: row.sample_id.toLowerCase(),\n                organism: row.organism,\n                tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n                depth: row.sequencing_depth.toInteger(),\n                quality: row.quality_score.toDouble()\n            ]\n            def priority = sample_meta.quality &gt; 40 ? 'high' : 'normal'\n            return sample_meta + [priority: priority]\n        }\n        .view()\n</code></pre> main.nf<pre><code>    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def sample_meta = [\n                id: row.sample_id.toLowerCase(),\n                organism: row.organism,\n                tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n                depth: row.sequencing_depth.toInteger(),\n                quality: row.quality_score.toDouble()\n            ]\n            return sample_meta\n        }\n        .view()\n</code></pre> <p>The ternary operator is a shorthand for an if/else statement that follows the pattern <code>condition ? value_if_true : value_if_false</code>. This line means: \"If the quality is greater than 40, use 'high', otherwise use 'normal'\". Its cousin, the Elvis operator (<code>?:</code>), provides default values when something is null or empty - we'll explore that pattern later in this tutorial.</p> <p>The map addition operator <code>+</code> creates a new map rather than modifying the existing one. This line creates a new map that contains all the key-value pairs from <code>sample_meta</code> plus the new <code>priority</code> key.</p> <p>Note</p> <p>Never modify maps passed into closures - always create new ones using <code>+</code> (for example). In Nextflow, the same data often flows through multiple operations simultaneously. Modifying a map in-place can cause unpredictable side effects when other operations reference that same object. Creating new maps ensures each operation has its own clean copy.</p> <p>Run the modified workflow:</p> Test conditional logic<pre><code>nextflow run main.nf\n</code></pre> <p>You should see output like:</p> Metadata with priority<pre><code>[id:sample_001, organism:human, tissue:liver, depth:30000000, quality:38.5, priority:normal]\n[id:sample_002, organism:mouse, tissue:brain, depth:25000000, quality:35.2, priority:normal]\n[id:sample_003, organism:human, tissue:kidney, depth:45000000, quality:42.1, priority:high]\n</code></pre> <p>We've successfully added conditional logic to enrich our metadata with a priority level based on quality scores.</p>"},{"location":"side_quests/essential_scripting_patterns/#step-45-subsetting-maps-with-submap","title":"Step 4.5: Subsetting Maps with <code>.subMap()</code>","text":"<p>While the <code>+</code> operator adds keys to a map, sometimes you need to do the opposite - extract only specific keys. The <code>.subMap()</code> method is perfect for this.</p> <p>Let's add a line to create a simplified version of our metadata that only contains identification fields:</p> AfterBefore main.nf<pre><code>    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            // Scripting for data transformation\n            def sample_meta = [\n                id: row.sample_id.toLowerCase(),\n                organism: row.organism,\n                tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n                depth: row.sequencing_depth.toInteger(),\n                quality: row.quality_score.toDouble()\n            ]\n            def id_only = sample_meta.subMap(['id', 'organism', 'tissue'])\n            println \"ID fields only: ${id_only}\"\n\n            def priority = sample_meta.quality &gt; 40 ? 'high' : 'normal'\n            return sample_meta + [priority: priority]\n        }\n        .view()\n</code></pre> main.nf<pre><code>    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            // Scripting for data transformation\n            def sample_meta = [\n                id: row.sample_id.toLowerCase(),\n                organism: row.organism,\n                tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n                depth: row.sequencing_depth.toInteger(),\n                quality: row.quality_score.toDouble()\n            ]\n            def priority = sample_meta.quality &gt; 40 ? 'high' : 'normal'\n            return sample_meta + [priority: priority]\n        }\n        .view()\n</code></pre> <p>Run the modified workflow:</p> Test subMap<pre><code>nextflow run main.nf\n</code></pre> <p>You should see output showing both the full metadata displayed by the <code>view()</code> operation and the extracted subset we printed with <code>println</code>:</p> SubMap results<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [peaceful_cori] DSL2 - revision: 4cc4a8340f\n\nID fields only: [id:sample_001, organism:human, tissue:liver]\nID fields only: [id:sample_002, organism:mouse, tissue:brain]\nID fields only: [id:sample_003, organism:human, tissue:kidney]\n[id:sample_001, organism:human, tissue:liver, depth:30000000, quality:38.5, priority:normal]\n[id:sample_002, organism:mouse, tissue:brain, depth:25000000, quality:35.2, priority:normal]\n[id:sample_003, organism:human, tissue:kidney, depth:45000000, quality:42.1, priority:high]\n</code></pre> <p>The <code>.subMap()</code> method takes a list of keys and returns a new map containing only those keys. If a key doesn't exist in the original map, it's simply not included in the result.</p> <p>This is particularly useful when you need to create different metadata versions for different processes - some might need full metadata while others need only minimal identification fields.</p> <p>Now remove those println statements to restore your workflow to its previous state, as we don't need them going forward.</p> <p>Map Operations Summary</p> <ul> <li>Add keys: <code>map1 + [new_key: value]</code> - Creates new map with additional keys</li> <li>Extract keys: <code>map1.subMap(['key1', 'key2'])</code> - Creates new map with only specified keys</li> <li>Both operations create new maps - Original maps remain unchanged</li> </ul>"},{"location":"side_quests/essential_scripting_patterns/#step-5-combining-maps-and-returning-results","title":"Step 5: Combining Maps and Returning Results","text":"<p>So far, we've only been returning what the Nextflow community calls the 'meta map', and we've been ignoring the files those metadata relate to. But if you're writing Nextflow workflows, you probably want to do something with those files.</p> <p>Let's output a channel structure comprising a tuple of 2 elements: the enriched metadata map and the corresponding file path. This is a common pattern in Nextflow for passing data to processes.</p> AfterBefore main.nf<pre><code>    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def sample_meta = [\n                id: row.sample_id.toLowerCase(),\n                organism: row.organism,\n                tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n                depth: row.sequencing_depth.toInteger(),\n                quality: row.quality_score.toDouble()\n            ]\n            def priority = sample_meta.quality &gt; 40 ? 'high' : 'normal'\n            return tuple( sample_meta + [priority: priority], file(row.file_path) )\n        }\n        .view()\n</code></pre> main.nf<pre><code>    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def sample_meta = [\n                id: row.sample_id.toLowerCase(),\n                organism: row.organism,\n                tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n                depth: row.sequencing_depth.toInteger(),\n                quality: row.quality_score.toDouble()\n            ]\n            def priority = sample_meta.quality &gt; 40 ? 'high' : 'normal'\n            return sample_meta + [priority: priority]\n        }\n        .view()\n</code></pre> <p>Apply this change and run the workflow:</p> Test complete workflow<pre><code>nextflow run main.nf\n</code></pre> <p>You should see output like:</p> Complete workflow output<pre><code>[[id:sample_001, organism:human, tissue:liver, depth:30000000, quality:38.5, priority:normal], /workspaces/training/side-quests/essential_scripting_patterns/data/sequences/SAMPLE_001_S1_L001_R1_001.fastq]\n[[id:sample_002, organism:mouse, tissue:brain, depth:25000000, quality:35.2, priority:normal], /workspaces/training/side-quests/essential_scripting_patterns/data/sequences/SAMPLE_002_S2_L001_R1_001.fastq]\n[[id:sample_003, organism:human, tissue:kidney, depth:45000000, quality:42.1, priority:high], /workspaces/training/side-quests/essential_scripting_patterns/data/sequences/SAMPLE_003_S3_L001_R1_001.fastq]\n</code></pre> <p>This <code>[meta, file]</code> tuple structure is a common pattern in Nextflow for passing both metadata and associated files to processes.</p> <p>Note</p> <p>Maps and Metadata: Maps are fundamental to working with metadata in Nextflow. For a more detailed explanation of working with metadata maps, see the Working with metadata side quest.</p> <p>Our workflow demonstrates the core pattern: dataflow operations (<code>workflow</code>, <code>channel.fromPath()</code>, <code>.splitCsv()</code>, <code>.map()</code>, <code>.view()</code>) orchestrate how data moves through the pipeline, while scripting (maps <code>[key: value]</code>, string methods, type conversions, ternary operators) inside the <code>.map()</code> closure handles the transformation of individual data items.</p>"},{"location":"side_quests/essential_scripting_patterns/#12-understanding-different-types-channel-vs-list","title":"1.2. Understanding Different Types: Channel vs List","text":"<p>So far, so good, we can distinguish between dataflow operations and scripting. But what about when the same method name exists in both contexts?</p> <p>A perfect example is the <code>collect</code> method, which exists for both Channel types and List types in the Nextflow standard library. The <code>collect()</code> method on a List transforms each element, while the <code>collect()</code> operator on a Channel gathers all channel emissions into a single-item channel.</p> <p>Let's demonstrate this with some sample data, starting by refreshing ourselves on what the Channel <code>collect()</code> operator does. Check out <code>collect.nf</code>:</p> collect.nf<pre><code>def sample_ids = ['sample_001', 'sample_002', 'sample_003']\n\n// channel.collect() - groups multiple channel emissions into one\nch_input = channel.fromList(sample_ids)\nch_input.view { sample -&gt; \"Individual channel item: ${sample}\" }\nch_collected = ch_input.collect()\nch_collected.view { list -&gt; \"channel.collect() result: ${list} (${list.size()} items grouped into 1)\" }\n</code></pre> <p>Steps:</p> <ul> <li>Define a List of sample IDs</li> <li>Create a channel with <code>fromList()</code> that emits each sample ID separately</li> <li>Print each item with <code>view()</code> as it flows through</li> <li>Gather all items into a single list with the Channel's <code>collect()</code> operator</li> <li>Print the collected result (single item containing all sample IDs) with a second <code>view()</code></li> </ul> <p>We've changed the structure of the channel, but we haven't changed the data itself.</p> <p>Run the workflow to confirm this:</p> Test collect operations<pre><code>nextflow run collect.nf\n</code></pre> Different collect behaviors<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `collect.nf` [loving_mendel] DSL2 - revision: e8d054a46e\n\nIndividual channel item: sample_001\nIndividual channel item: sample_002\nIndividual channel item: sample_003\nchannel.collect() result: [sample_001, sample_002, sample_003] (3 items grouped into 1)\n</code></pre> <p><code>view()</code> returns an output for every channel emission, so we know that this single output contains all 3 original items grouped into one list.</p> <p>Now let's see the <code>collect</code> method on a List in action. Modify <code>collect.nf</code> to apply the List's <code>collect</code> method to the original list of sample IDs:</p> AfterBefore main.nf<pre><code>def sample_ids = ['sample_001', 'sample_002', 'sample_003']\n\n// channel.collect() - groups multiple channel emissions into one\nch_input = channel.fromList(sample_ids)\nch_input.view { sample -&gt; \"Individual channel item: ${sample}\" }\nch_collected = ch_input.collect()\nch_collected.view { list -&gt; \"channel.collect() result: ${list} (${list.size()} items grouped into 1)\" }\n\n// List.collect() - transforms each element, preserves structure\ndef formatted_ids = sample_ids.collect { id -&gt;\n    id.toUpperCase().replace('SAMPLE_', 'SPECIMEN_')\n}\nprintln \"List.collect() result: ${formatted_ids} (${sample_ids.size()} items transformed into ${formatted_ids.size()})\"\n</code></pre> main.nf<pre><code>def sample_ids = ['sample_001', 'sample_002', 'sample_003']\n\n// channel.collect() - groups multiple channel emissions into one\nch_input = channel.fromList(sample_ids)\nch_input.view { sample -&gt; \"Individual channel item: ${sample}\" }\nch_collected = ch_input.collect()\nch_collected.view { list -&gt; \"channel.collect() result: ${list} (${list.size()} items grouped into 1)\" }\n</code></pre> <p>In this new snippet we:</p> <ul> <li>Define a new variable <code>formatted_ids</code> that uses the List's <code>collect</code> method to transform each sample ID in the original list</li> <li>Print the result using <code>println</code></li> </ul> <p>Run the modified workflow:</p> Test List collect<pre><code>nextflow run collect.nf\n</code></pre> List collect results<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `collect.nf` [cheeky_stonebraker] DSL2 - revision: 2d5039fb47\n\nList.collect() result: [SPECIMEN_001, SPECIMEN_002, SPECIMEN_003] (3 items transformed into 3)\nIndividual channel item: sample_001\nIndividual channel item: sample_002\nIndividual channel item: sample_003\nchannel.collect() result: [sample_001, sample_002, sample_003] (3 items grouped into 1)\n</code></pre> <p>This time, we have NOT changed the structure of the data, we still have 3 items in the list, but we HAVE transformed each item using the List's <code>collect</code> method to produce a new list with modified values. This is similar to using the <code>map</code> operator on a Channel, but it's operating on a List data structure rather than a channel.</p> <p><code>collect</code> is an extreme case we're using here to make a point. The key lesson is that when you're writing workflows, always distinguish between data structures (Lists, Maps, etc.) and channels (dataflow constructs). Operations can share names but behave completely differently depending on the type they're called on.</p>"},{"location":"side_quests/essential_scripting_patterns/#13-the-spread-operator-shorthand-for-property-extraction","title":"1.3. The Spread Operator (<code>*.</code>) - Shorthand for Property Extraction","text":"<p>Related to the List's <code>collect</code> method is the spread operator (<code>*.</code>), which provides a concise way to extract properties from collections. It's essentially syntactic sugar for a common <code>collect</code> pattern.</p> <p>Let's add a demonstration to our <code>collect.nf</code> file:</p> AfterBefore collect.nf<pre><code>def sample_ids = ['sample_001', 'sample_002', 'sample_003']\n\n// channel.collect() - groups multiple channel emissions into one\nch_input = channel.fromList(sample_ids)\nch_input.view { sample -&gt; \"Individual channel item: ${sample}\" }\nch_collected = ch_input.collect()\nch_collected.view { list -&gt; \"channel.collect() result: ${list} (${list.size()} items grouped into 1)\" }\n\n// List.collect() - transforms each element, preserves structure\ndef formatted_ids = sample_ids.collect { id -&gt;\n    id.toUpperCase().replace('SAMPLE_', 'SPECIMEN_')\n}\nprintln \"List.collect() result: ${formatted_ids} (${sample_ids.size()} items transformed into ${formatted_ids.size()})\"\n\n// Spread operator - concise property access\ndef sample_data = [[id: 's1', quality: 38.5], [id: 's2', quality: 42.1], [id: 's3', quality: 35.2]]\ndef all_ids = sample_data*.id\nprintln \"Spread operator result: ${all_ids}\"\n</code></pre> collect.nf<pre><code>def sample_ids = ['sample_001', 'sample_002', 'sample_003']\n\n// channel.collect() - groups multiple channel emissions into one\nch_input = channel.fromList(sample_ids)\nch_input.view { sample -&gt; \"Individual channel item: ${sample}\" }\nch_collected = ch_input.collect()\nch_collected.view { list -&gt; \"channel.collect() result: ${list} (${list.size()} items grouped into 1)\" }\n\n// List.collect() - transforms each element, preserves structure\ndef formatted_ids = sample_ids.collect { id -&gt;\n    id.toUpperCase().replace('SAMPLE_', 'SPECIMEN_')\n}\nprintln \"List.collect() result: ${formatted_ids} (${sample_ids.size()} items transformed into ${formatted_ids.size()})\"\n</code></pre> <p>Run the updated workflow:</p> Test spread operator<pre><code>nextflow run collect.nf\n</code></pre> <p>You should see output like:</p> Spread operator output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `collect.nf` [cranky_galileo] DSL2 - revision: 5f3c8b2a91\n\nList.collect() result: [SPECIMEN_001, SPECIMEN_002, SPECIMEN_003] (3 items transformed into 3)\nSpread operator result: [s1, s2, s3]\nIndividual channel item: sample_001\nIndividual channel item: sample_002\nIndividual channel item: sample_003\nchannel.collect() result: [sample_001, sample_002, sample_003] (3 items grouped into 1)\n</code></pre> <p>The spread operator <code>*.</code> is a shorthand for a common collect pattern:</p> <pre><code>// These are equivalent:\ndef ids = samples*.id\ndef ids = samples.collect { it.id }\n\n// Also works with method calls:\ndef names = files*.getName()\ndef names = files.collect { it.getName() }\n</code></pre> <p>The spread operator is particularly useful when you need to extract a single property from a list of objects - it's more readable than writing out the full <code>collect</code> closure.</p> <p>When to Use Spread vs Collect</p> <ul> <li>Use spread (<code>*.</code>) for simple property access: <code>samples*.id</code>, <code>files*.name</code></li> <li>Use collect for transformations or complex logic: <code>samples.collect { it.id.toUpperCase() }</code>, <code>samples.collect { [it.id, it.quality &gt; 40] }</code></li> </ul>"},{"location":"side_quests/essential_scripting_patterns/#takeaway","title":"Takeaway","text":"<p>In this section, you've learned:</p> <ul> <li>Dataflow vs scripting: Channel operators orchestrate how data flows through your pipeline, while scripting transforms individual data items</li> <li>Understanding types: The same method name (like <code>collect</code>) can behave differently depending on the type it's called on (Channel vs List)</li> <li>Context matters: Always be aware of whether you're working with channels (dataflow) or data structures (scripting)</li> </ul> <p>Understanding these boundaries is essential for debugging, documentation, and writing maintainable workflows.</p> <p>Next we'll dive deeper into string processing capabilities, which are essential for handling real-world data.</p>"},{"location":"side_quests/essential_scripting_patterns/#2-string-processing-and-dynamic-script-generation","title":"2. String Processing and Dynamic Script Generation","text":"<p>Mastering string processing separates brittle workflows from robust pipelines. This section covers parsing complex file names, dynamic script generation, and variable interpolation.</p>"},{"location":"side_quests/essential_scripting_patterns/#21-pattern-matching-and-regular-expressions","title":"2.1. Pattern Matching and Regular Expressions","text":"<p>Bioinformatics files often have complex naming conventions encoding metadata. Let's extract this automatically using pattern matching with regular expressions.</p> <p>We're going to return to our <code>main.nf</code> workflow and add some pattern matching logic to extract additional sample information from file names. The FASTQ files in our dataset follow Illumina-style naming conventions with names like <code>SAMPLE_001_S1_L001_R1_001.fastq.gz</code>. These might look cryptic, but they actually encode useful metadata like sample ID, lane number, and read direction. We're going to use regex capabilities to parse these names.</p> <p>Make the following change to your existing <code>main.nf</code> workflow:</p> AfterBefore main.nf<pre><code>        .map { row -&gt;\n            // Scripting for data transformation\n            def sample_meta = [\n                id: row.sample_id.toLowerCase(),\n                organism: row.organism,\n                tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n                depth: row.sequencing_depth.toInteger(),\n                quality: row.quality_score.toDouble()\n            ]\n            def fastq_path = file(row.file_path)\n\n            def m = (fastq_path.name =~ /^(.+)_S(\\d+)_L(\\d{3})_(R[12])_(\\d{3})\\.fastq(?:\\.gz)?$/)\n            def file_meta = m ? [\n                sample_num: m[0][2].toInteger(),\n                lane: m[0][3],\n                read: m[0][4],\n                chunk: m[0][5]\n            ] : [:]\n\n            def priority = sample_meta.quality &gt; 40 ? 'high' : 'normal'\n            return tuple(sample_meta + file_meta + [priority: priority], fastq_path)\n        }\n</code></pre> main.nf<pre><code>        .map { row -&gt;\n            // Scripting for data transformation\n            def sample_meta = [\n                id: row.sample_id.toLowerCase(),\n                organism: row.organism,\n                tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n                depth: row.sequencing_depth.toInteger(),\n                quality: row.quality_score.toDouble()\n            ]\n            def priority = sample_meta.quality &gt; 40 ? 'high' : 'normal'\n            return tuple(sample_meta + [priority: priority], file(row.file_path))\n        }\n</code></pre> <p>This demonstrates key string processing concepts:</p> <ol> <li>Regular expression literals using <code>~/pattern/</code> syntax - this creates a regex pattern without needing to escape backslashes</li> <li>Pattern matching with the <code>=~</code> operator - this attempts to match a string against a regex pattern</li> <li>Matcher objects that capture groups with <code>[0][1]</code>, <code>[0][2]</code>, etc. - <code>[0]</code> refers to the entire match, <code>[1]</code>, <code>[2]</code>, etc. refer to captured groups in parentheses</li> </ol> <p>Let's break down the regex pattern <code>^(.+)_S(\\d+)_L(\\d{3})_(R[12])_(\\d{3})\\.fastq(?:\\.gz)?$</code>:</p> Pattern Matches Captures <code>^(.+)</code> Sample name from start Group 1: sample name <code>_S(\\d+)</code> Sample number <code>_S1</code>, <code>_S2</code>, etc. Group 2: sample number <code>_L(\\d{3})</code> Lane number <code>_L001</code> Group 3: lane (3 digits) <code>_(R[12])</code> Read direction <code>_R1</code> or <code>_R2</code> Group 4: read direction <code>_(\\d{3})</code> Chunk number <code>_001</code> Group 5: chunk (3 digits) <code>\\.fastq(?:\\.gz)?$</code> File extension <code>.fastq</code> or <code>.fastq.gz</code> Not captured (?: is non-capturing) <p>This parses Illumina-style naming conventions to extract metadata automatically.</p> <p>Run the modified workflow:</p> Test pattern matching<pre><code>nextflow run main.nf\n</code></pre> <p>You should see output with metadata enriched from the file names, like</p> Metadata with file parsing<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [clever_pauling] DSL2 - revision: 605d2058b4\n\n[[id:sample_001, organism:human, tissue:liver, depth:30000000, quality:38.5, sample_num:1, lane:001, read:R1, chunk:001, priority:normal], /workspaces/training/side-quests/essential_scripting_patterns/data/sequences/SAMPLE_001_S1_L001_R1_001.fastq]\n[[id:sample_002, organism:mouse, tissue:brain, depth:25000000, quality:35.2, sample_num:2, lane:001, read:R1, chunk:001, priority:normal], /workspaces/training/side-quests/essential_scripting_patterns/data/sequences/SAMPLE_002_S2_L001_R1_001.fastq]\n[[id:sample_003, organism:human, tissue:kidney, depth:45000000, quality:42.1, sample_num:3, lane:001, read:R1, chunk:001, priority:high], /workspaces/training/side-quests/essential_scripting_patterns/data/sequences/SAMPLE_003_S3_L001_R1_001.fastq]\n</code></pre>"},{"location":"side_quests/essential_scripting_patterns/#22-dynamic-script-generation-in-processes","title":"2.2. Dynamic Script Generation in Processes","text":"<p>Process script blocks are essentially multi-line strings that get passed to the shell. You can use conditional logic (if/else, ternary operators) to dynamically generate different script strings based on input characteristics. This is essential for handling diverse input types\u2014like single-end vs paired-end sequencing reads\u2014without duplicating process definitions.</p> <p>Let's add a process to our workflow that demonstrates this pattern. Open <code>modules/fastp.nf</code> and take a look:</p> modules/fastp.nf<pre><code>process FASTP {\n    container 'community.wave.seqera.io/library/fastp:0.24.0--62c97b06e8447690'\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"*_trimmed*.fastq.gz\"), emit: reads\n\n    script:\n    \"\"\"\n    fastp \\\\\n        --in1 ${reads[0]} \\\\\n        --in2 ${reads[1]} \\\\\n        --out1 ${meta.id}_trimmed_R1.fastq.gz \\\\\n        --out2 ${meta.id}_trimmed_R2.fastq.gz \\\\\n        --json ${meta.id}.fastp.json \\\\\n        --html ${meta.id}.fastp.html \\\\\n        --thread $task.cpus\n    \"\"\"\n}\n</code></pre> <p>The process takes FASTQ files as input and runs the <code>fastp</code> tool to trim adapters and filter low-quality reads. Unfortunately, the person who wrote this process didn't allow for the single-end reads we have in our example dataset. Let's add it to our workflow and see what happens:</p> <p>First, include the module at the very first line of your <code>main.nf</code> workflow:</p> main.nf<pre><code>include { FASTP } from './modules/fastp.nf'\n</code></pre> <p>Then modify the <code>workflow</code> block to connect the <code>ch_samples</code> channel to the <code>FASTP</code> process:</p> AfterBefore main.nf<pre><code>workflow {\n\n    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def sample_meta = [\n                id: row.sample_id.toLowerCase(),\n                organism: row.organism,\n                tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n                depth: row.sequencing_depth.toInteger(),\n                quality: row.quality_score.toDouble()\n            ]\n            def fastq_path = file(row.file_path)\n\n            def m = (fastq_path.name =~ /^(.+)_S(\\d+)_L(\\d{3})_(R[12])_(\\d{3})\\.fastq(?:\\.gz)?$/)\n            def file_meta = m ? [\n                sample_num: m[0][2].toInteger(),\n                lane: m[0][3],\n                read: m[0][4],\n                chunk: m[0][5]\n            ] : [:]\n\n            def priority = sample_meta.quality &gt; 40 ? 'high' : 'normal'\n            return tuple(sample_meta + file_meta + [priority: priority], fastq_path)\n        }\n\n    ch_fastp = FASTP(ch_samples)\n}\n</code></pre> main.nf<pre><code>workflow {\n\n    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def sample_meta = [\n                id: row.sample_id.toLowerCase(),\n                organism: row.organism,\n                tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n                depth: row.sequencing_depth.toInteger(),\n                quality: row.quality_score.toDouble()\n            ]\n            def fastq_path = file(row.file_path)\n\n            def m = (fastq_path.name =~ /^(.+)_S(\\d+)_L(\\d{3})_(R[12])_(\\d{3})\\.fastq(?:\\.gz)?$/)\n            def file_meta = m ? [\n                sample_num: m[0][2].toInteger(),\n                lane: m[0][3],\n                read: m[0][4],\n                chunk: m[0][5]\n            ] : [:]\n\n            def priority = sample_meta.quality &gt; 40 ? 'high' : 'normal'\n            return [sample_meta + file_meta + [priority: priority], file(row.file_path)]\n        }\n        .view()\n}\n</code></pre> <p>Run this modified workflow:</p> Test fastp process<pre><code>nextflow run main.nf\n</code></pre> <p>You'll see a long error trace with some content like:</p> Process error<pre><code>ERROR ~ Error executing process &gt; 'FASTP (3)'\n\nCaused by:\n  Process `FASTP (3)` terminated with an error exit status (255)\n\n\nCommand executed:\n\n  fastp \\\n      --in1 SAMPLE_003_S3_L001_R1_001.fastq \\\n      --in2 null \\\n      --out1 sample_003_trimmed_R1.fastq.gz \\\n      --out2 sample_003_trimmed_R2.fastq.gz \\\n      --json sample_003.fastp.json \\\n      --html sample_003.fastp.html \\\n      --thread 2\n\nCommand exit status:\n  255\n\nCommand output:\n  (empty)\n</code></pre> <p>You can see that the process is trying to run <code>fastp</code> with a <code>null</code> value for the second input file, which is causing it to fail. This is because our dataset contains single-end reads, but the process is hardcoded to expect paired-end reads (two input files at a time).</p> <p>Fix this by adding conditional logic to the <code>FASTP</code> process <code>script:</code> block. An if/else statement checks read file count and adjusts the command accordingly.</p> AfterBefore main.nf<pre><code>    script:\n    // Simple single-end vs paired-end detection\n    def is_single = reads instanceof List ? reads.size() == 1 : true\n\n    if (is_single) {\n        def input_file = reads instanceof List ? reads[0] : reads\n        \"\"\"\n        fastp \\\\\n            --in1 ${input_file} \\\\\n            --out1 ${meta.id}_trimmed.fastq.gz \\\\\n            --json ${meta.id}.fastp.json \\\\\n            --html ${meta.id}.fastp.html \\\\\n            --thread $task.cpus\n        \"\"\"\n    } else {\n        \"\"\"\n        fastp \\\\\n            --in1 ${reads[0]} \\\\\n            --in2 ${reads[1]} \\\\\n            --out1 ${meta.id}_trimmed_R1.fastq.gz \\\\\n            --out2 ${meta.id}_trimmed_R2.fastq.gz \\\\\n            --json ${meta.id}.fastp.json \\\\\n            --html ${meta.id}.fastp.html \\\\\n            --thread $task.cpus\n        \"\"\"\n    }\n</code></pre> main.nf<pre><code>        script:\n        \"\"\"\n        fastp \\\\\n            --in1 ${reads[0]} \\\\\n            --in2 ${reads[1]} \\\\\n            --out1 ${meta.id}_trimmed_R1.fastq.gz \\\\\n            --out2 ${meta.id}_trimmed_R2.fastq.gz \\\\\n            --json ${meta.id}.fastp.json \\\\\n            --html ${meta.id}.fastp.html \\\\\n            --thread $task.cpus\n        \"\"\"\n    }\n</code></pre> <p>Now the workflow can handle both single-end and paired-end reads gracefully. The conditional logic checks the number of input files and constructs the appropriate command for <code>fastp</code>. Let's see if it works:</p> Test dynamic fastp<pre><code>nextflow run main.nf\n</code></pre> Successful run<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [adoring_rosalind] DSL2 - revision: 04b1cd93e9\n\nexecutor &gt;  local (3)\n[31/a8ad4d] process &gt; FASTP (3) [100%] 3 of 3 \u2714\n</code></pre> <p>Looks good! If we check the actual commands that were run (customise for your task hash):</p> Check commands executed<pre><code>cat work/31/a8ad4d95749e685a6d842d3007957f/.command.sh\n</code></pre> <p>We can see that Nextflow correctly picked the right command for single-end reads:</p> .command.sh<pre><code>#!/bin/bash -ue\nfastp \\\n    --in1 SAMPLE_003_S3_L001_R1_001.fastq \\\n    --out1 sample_003_trimmed.fastq.gz \\\n    --json sample_003.fastp.json \\\n    --html sample_003.fastp.html \\\n    --thread 2\n</code></pre> <p>Another common usage of dynamic script logic can be seen in the Nextflow for Science Genomics module. In that module, the GATK process being called can take multiple input files, but each must be prefixed with <code>-V</code> to form a correct command line. The process uses scripting to transform a collection of input files (<code>all_gvcfs</code>) into the correct command arguments:</p> command line manipulation for GATK<pre><code>    script:\n    def gvcfs_line = all_gvcfs.collect { gvcf -&gt; \"-V ${gvcf}\" }.join(' ')\n    \"\"\"\n    gatk GenomicsDBImport \\\n        ${gvcfs_line} \\\n        -L ${interval_list} \\\n        --genomicsdb-workspace-path ${cohort_name}_gdb\n    \"\"\"\n</code></pre> <p>These patterns of using scripting in process script blocks are extremely powerful and can be applied in many scenarios - from handling variable input types to building complex command-line arguments from file collections, making your processes truly adaptable to the diverse requirements of real-world data.</p>"},{"location":"side_quests/essential_scripting_patterns/#23-variable-interpolation-nextflow-and-shell-variables","title":"2.3. Variable Interpolation: Nextflow and Shell Variables","text":"<p>Process scripts mix Nextflow variables, shell variables, and command substitutions, each with different interpolation syntax. Using the wrong syntax causes errors. Let's explore these with a process that creates a processing report.</p> <p>Take a look a the module file <code>modules/generate_report.nf</code>:</p> modules/generate_report.nf<pre><code>process GENERATE_REPORT {\n\n    publishDir 'results/reports', mode: 'copy'\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    path \"${meta.id}_report.txt\"\n\n    script:\n    \"\"\"\n    echo \"Processing ${reads}\" &gt; ${meta.id}_report.txt\n    echo \"Sample: ${meta.id}\" &gt;&gt; ${meta.id}_report.txt\n    \"\"\"\n}\n</code></pre> <p>This process writes a simple report with the sample ID and filename. Now let's run it to see what happens when we need to mix different types of variables.</p> <p>Include the process in your <code>main.nf</code> and add it to the workflow:</p> AfterBefore main.nf<pre><code>include { FASTP } from './modules/fastp.nf'\ninclude { GENERATE_REPORT } from './modules/generate_report.nf'\n\nworkflow {\n    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def sample_meta = [\n                id: row.sample_id.toLowerCase(),\n                organism: row.organism,\n                tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n                depth: row.sequencing_depth.toInteger(),\n                quality: row.quality_score.toDouble()\n            ]\n            def fastq_path = file(row.file_path)\n\n            def m = (fastq_path.name =~ /^(.+)_S(\\d+)_L(\\d{3})_(R[12])_(\\d{3})\\.fastq(?:\\.gz)?$/)\n            def file_meta = m ? [\n                sample_num: m[0][2].toInteger(),\n                lane: m[0][3],\n                read: m[0][4],\n                chunk: m[0][5]\n            ] : [:]\n\n            def priority = sample_meta.quality &gt; 40 ? 'high' : 'normal'\n            return tuple(sample_meta + file_meta + [priority: priority], fastq_path)\n        }\n\n    ch_fastp = FASTP(ch_samples)\n    GENERATE_REPORT(ch_samples)\n}\n</code></pre> main.nf<pre><code>include { FASTP } from './modules/fastp.nf'\n\nworkflow {\n    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def sample_meta = [\n                id: row.sample_id.toLowerCase(),\n                organism: row.organism,\n                tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n                depth: row.sequencing_depth.toInteger(),\n                quality: row.quality_score.toDouble()\n            ]\n            def fastq_path = file(row.file_path)\n\n            def m = (fastq_path.name =~ /^(.+)_S(\\d+)_L(\\d{3})_(R[12])_(\\d{3})\\.fastq(?:\\.gz)?$/)\n            def file_meta = m ? [\n                sample_num: m[0][2].toInteger(),\n                lane: m[0][3],\n                read: m[0][4],\n                chunk: m[0][5]\n            ] : [:]\n\n            def priority = sample_meta.quality &gt; 40 ? 'high' : 'normal'\n            return tuple(sample_meta + file_meta + [priority: priority], fastq_path)\n        }\n\n    ch_fastp = FASTP(ch_samples)\n}\n</code></pre> <p>Now run the workflow and check the generated reports in <code>results/reports/</code>. They should contain basic information about each sample.</p> <p>But what if we want to add information about when and where the processing occurred? Let's modify the process to use shell variables and a bit of command substitution to include the current user, hostname, and date in the report:</p> AfterBefore modules/generate_report.nf<pre><code>    script:\n    \"\"\"\n    echo \"Processing ${reads}\" &gt; ${meta.id}_report.txt\n    echo \"Sample: ${meta.id}\" &gt;&gt; ${meta.id}_report.txt\n    echo \"Processed by: ${USER}\" &gt;&gt; ${meta.id}_report.txt\n    echo \"Hostname: $(hostname)\" &gt;&gt; ${meta.id}_report.txt\n    echo \"Date: $(date)\" &gt;&gt; ${meta.id}_report.txt\n    \"\"\"\n</code></pre> modules/generate_report.nf<pre><code>    script:\n    \"\"\"\n    echo \"Processing ${reads}\" &gt; ${meta.id}_report.txt\n    echo \"Sample: ${meta.id}\" &gt;&gt; ${meta.id}_report.txt\n    \"\"\"\n</code></pre> <p>If you run this, you'll notice an error or unexpected behavior - Nextflow tries to interpret <code>$(hostname)</code> as a Nextflow variable that doesn't exist:</p> Error with shell variables<pre><code>unknown recognition error type: groovyjarjarantlr4.v4.runtime.LexerNoViableAltException\nERROR ~ Module compilation error\n- file : /workspaces/training/side-quests/essential_scripting_patterns/modules/generate_report.nf\n- cause: token recognition error at: '(' @ line 16, column 22.\n       echo \"Hostname: $(hostname)\" &gt;&gt; ${meta.id}_report.txt\n                        ^\n\n1 error\n</code></pre> <p>We need to escape it so Bash can handle it instead.</p> <p>Fix this by escaping the shell variables and command substitutions with a backslash (<code>\\</code>):</p> AfterBefore modules/generate_report.nf<pre><code>    script:\n    \"\"\"\n    echo \"Processing ${reads}\" &gt; ${meta.id}_report.txt\n    echo \"Sample: ${meta.id}\" &gt;&gt; ${meta.id}_report.txt\n    echo \"Processed by: \\${USER}\" &gt;&gt; ${meta.id}_report.txt\n    echo \"Hostname: \\$(hostname)\" &gt;&gt; ${meta.id}_report.txt\n    echo \"Date: \\$(date)\" &gt;&gt; ${meta.id}_report.txt\n    \"\"\"\n</code></pre> modules/generate_report.nf<pre><code>    script:\n    \"\"\"\n    echo \"Processing ${reads}\" &gt; ${meta.id}_report.txt\n    echo \"Sample: ${meta.id}\" &gt;&gt; ${meta.id}_report.txt\n    echo \"Processed by: ${USER}\" &gt;&gt; ${meta.id}_report.txt\n    echo \"Hostname: $(hostname)\" &gt;&gt; ${meta.id}_report.txt\n    echo \"Date: $(date)\" &gt;&gt; ${meta.id}_report.txt\n    \"\"\"\n</code></pre> <p>Now it works! The backslash (<code>\\</code>) tells Nextflow \"don't interpret this, pass it through to Bash.\"</p>"},{"location":"side_quests/essential_scripting_patterns/#takeaway_1","title":"Takeaway","text":"<p>In this section, you've learned string processing techniques:</p> <ul> <li>Regular expressions for file parsing: Using the <code>=~</code> operator and regex patterns (<code>~/pattern/</code>) to extract metadata from complex file naming conventions</li> <li>Dynamic script generation: Using conditional logic (if/else, ternary operators) to generate different script strings based on input characteristics</li> <li>Variable interpolation: Understanding when Nextflow interprets strings vs when the shell does</li> <li><code>${var}</code> - Nextflow variables (interpolated by Nextflow at workflow compile time)</li> <li><code>\\${var}</code> - Shell environment variables (escaped, passed to bash at runtime)</li> <li><code>\\$(cmd)</code> - Shell command substitution (escaped, executed by bash at runtime)</li> </ul> <p>These string processing and generation patterns are essential for handling the diverse file formats and naming conventions you'll encounter in real-world bioinformatics workflows.</p>"},{"location":"side_quests/essential_scripting_patterns/#3-creating-reusable-functions","title":"3. Creating Reusable Functions","text":"<p>Complex workflow logic inline in channel operators or process definitions reduces readability and maintainability. Functions let you extract this logic into named, reusable components.</p> <p>Our map operation has grown long and complex. Let's extract it into a reusable function using the <code>def</code> keyword.</p> <p>To illustrate what that looks like with our existing workflow, make the modification below, using <code>def</code> to define a reusable function called <code>separateMetadata</code>:</p> AfterBefore main.nf<pre><code>include { FASTP } from './modules/fastp.nf'\ninclude { GENERATE_REPORT } from './modules/generate_report.nf'\n\ndef separateMetadata(row) {\n    def sample_meta = [\n        id: row.sample_id.toLowerCase(),\n        organism: row.organism,\n        tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n        depth: row.sequencing_depth.toInteger(),\n        quality: row.quality_score.toDouble()\n    ]\n    def fastq_path = file(row.file_path)\n\n    def m = (fastq_path.name =~ /^(.+)_S(\\d+)_L(\\d{3})_(R[12])_(\\d{3})\\.fastq(?:\\.gz)?$/)\n    def file_meta = m ? [\n        sample_num: m[0][2].toInteger(),\n        lane: m[0][3],\n        read: m[0][4],\n        chunk: m[0][5]\n    ] : [:]\n\n    def priority = sample_meta.quality &gt; 40 ? 'high' : 'normal'\n    return tuple(sample_meta + file_meta + [priority: priority], fastq_path)\n}\n\nworkflow {\n    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map{ row -&gt; separateMetadata(row) }\n\n    ch_fastp = FASTP(ch_samples)\n    GENERATE_REPORT(ch_samples)\n}\n</code></pre> main.nf<pre><code>include { FASTP } from './modules/fastp.nf'\ninclude { GENERATE_REPORT } from './modules/generate_report.nf'\n\nworkflow {\n    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def sample_meta = [\n                id: row.sample_id.toLowerCase(),\n                organism: row.organism,\n                tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n                depth: row.sequencing_depth.toInteger(),\n                quality: row.quality_score.toDouble()\n            ]\n            def fastq_path = file(row.file_path)\n\n            def m = (fastq_path.name =~ /^(.+)_S(\\d+)_L(\\d{3})_(R[12])_(\\d{3})\\.fastq(?:\\.gz)?$/)\n            def file_meta = m ? [\n                sample_num: m[0][2].toInteger(),\n                lane: m[0][3],\n                read: m[0][4],\n                chunk: m[0][5]\n            ] : [:]\n\n            def priority = sample_meta.quality &gt; 40 ? 'high' : 'normal'\n            return tuple(sample_meta + file_meta + [priority: priority], fastq_path)\n        }\n\n    ch_fastp = FASTP(ch_samples)\n    GENERATE_REPORT(ch_samples)\n}\n</code></pre> <p>By extracting this logic into a function, we've reduced the actual workflow logic down to something much cleaner:</p> minimal workflow<pre><code>    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map{ row -&gt; separateMetadata(row) }\n\n    ch_fastp = FASTP(ch_samples)\n    GENERATE_REPORT(ch_samples)\n</code></pre> <p>This makes the workflow logic much easier to read and understand at a glance. The function <code>separateMetadata</code> encapsulates all the complex logic for parsing and enriching metadata, making it reusable and testable.</p> <p>Run the workflow to make sure it still works:</p> Test reusable function<pre><code>nextflow run main.nf\n</code></pre> Function results<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [admiring_panini] DSL2 - revision: 8cc832e32f\n\nexecutor &gt;  local (6)\n[8c/2e3f91] process &gt; FASTP (3)           [100%] 3 of 3 \u2714\n[7a/1b4c92] process &gt; GENERATE_REPORT (3) [100%] 3 of 3 \u2714\n</code></pre> <p>The output should show both processes completing successfully. The workflow is now much cleaner and easier to maintain, with all the complex metadata processing logic encapsulated in the <code>separateMetadata</code> function.</p>"},{"location":"side_quests/essential_scripting_patterns/#takeaway_2","title":"Takeaway","text":"<p>In this section, you've learned function creation:</p> <ul> <li>Defining functions with <code>def</code>: The keyword for creating named functions (like <code>def</code> in Python or <code>function</code> in JavaScript)</li> <li>Function scope: Functions defined at the script level are accessible throughout your Nextflow workflow</li> <li>Return values: Functions automatically return the last expression, or use explicit <code>return</code></li> <li>Cleaner code: Extracting complex logic into functions is a fundamental software engineering practice in any language</li> </ul> <p>Next, we'll explore how to use closures in process directives for dynamic resource allocation.</p>"},{"location":"side_quests/essential_scripting_patterns/#4-dynamic-resource-directives-with-closures","title":"4. Dynamic Resource Directives with Closures","text":"<p>So far we've used scripting in the <code>script</code> block of processes. But closures (introduced in Section 1.1) are also incredibly useful in process directives, especially for dynamic resource allocation. Let's add resource directives to our FASTP process that adapt based on the sample characteristics.</p> <p>Currently, our FASTP process uses default resources. Let's make it smarter by allocating more CPUs for high-depth samples. Edit <code>modules/fastp.nf</code> to include a dynamic <code>cpus</code> directive and a static <code>memory</code> directive:</p> AfterBefore modules/fastp.nf<pre><code>process FASTP {\n    container 'community.wave.seqera.io/library/fastp:0.24.0--62c97b06e8447690'\n\n    cpus { meta.depth &gt; 40000000 ? 2 : 1 }\n    memory 2.GB\n\n    input:\n    tuple val(meta), path(reads)\n</code></pre> modules/fastp.nf<pre><code>process FASTP {\n    container 'community.wave.seqera.io/library/fastp:0.24.0--62c97b06e8447690'\n\n    input:\n    tuple val(meta), path(reads)\n</code></pre> <p>The closure <code>{ meta.depth &gt; 40000000 ? 2 : 1 }</code> uses the ternary operator (covered in Section 1.1) and is evaluated for each task, allowing per-sample resource allocation. High-depth samples (&gt;40M reads) get 2 CPUs, while others get 1 CPU.</p> <p>Accessing Input Variables in Directives</p> <p>The closure can access any input variables (like <code>meta</code> here) because Nextflow evaluates these closures in the context of each task execution.</p> <p>Run the workflow again:</p> Test resource allocation<pre><code>nextflow run main.nf -ansi-log false\n</code></pre> <p>We're using the <code>-ansi-log false</code> option to make it easier to see the task hashes.</p> Resource allocation output<pre><code>N E X T F L O W  ~  version 25.04.3\nLaunching `main.nf` [fervent_albattani] DSL2 - revision: fa8f249759\n[bd/ff3d41] Submitted process &gt; FASTP (2)\n[a4/a3aab2] Submitted process &gt; FASTP (1)\n[48/6db0c9] Submitted process &gt; FASTP (3)\n[ec/83439d] Submitted process &gt; GENERATE_REPORT (3)\n[bd/15d7cc] Submitted process &gt; GENERATE_REPORT (2)\n[42/699357] Submitted process &gt; GENERATE_REPORT (1)\n</code></pre> <p>You can check the exact <code>docker</code> command that was run to see the CPU allocation for any given task:</p> Check docker command<pre><code>cat work/48/6db0c9e9d8aa65e4bb4936cd3bd59e/.command.run | grep \"docker run\"\n</code></pre> <p>You should see something like:</p> docker command<pre><code>    docker run -i --cpu-shares 4096 --memory 2048m -e \"NXF_TASK_WORKDIR\" -v /workspaces/training/side-quests/essential_scripting_patterns:/workspaces/training/side-quests/essential_scripting_patterns -w \"$NXF_TASK_WORKDIR\" --name $NXF_BOXID community.wave.seqera.io/library/fastp:0.24.0--62c97b06e8447690 /bin/bash -ue /workspaces/training/side-quests/essential_scripting_patterns/work/48/6db0c9e9d8aa65e4bb4936cd3bd59e/.command.sh\n</code></pre> <p>In this example we've chosen an example that requested 2 CPUs (<code>--cpu-shares 2048</code>), because it was a high-depth sample, but you should see different CPU allocations depending on the sample depth. Try this for the other tasks as well.</p> <p>Another powerful pattern is using <code>task.attempt</code> for retry strategies. To show why this is useful, we're going to start by reducing the memory allocation to FASTP to less than it needs. Change the <code>memory</code> directive in <code>modules/fastp.nf</code> to <code>1.GB</code>:</p> AfterBefore modules/fastp.nf<pre><code>process FASTP {\n    container 'community.wave.seqera.io/library/fastp:0.24.0--62c97b06e8447690'\n\n    cpus { meta.depth &gt; 40000000 ? 4 : 2 }\n    memory 1.GB\n\n    input:\n    tuple val(meta), path(reads)\n</code></pre> modules/fastp.nf<pre><code>process FASTP {\n    container 'community.wave.seqera.io/library/fastp:0.24.0--62c97b06e8447690'\n\n    cpus { meta.depth &gt; 40000000 ? 4 : 2 }\n    memory 2.GB\n\n    input:\n    tuple val(meta), path(reads)\n</code></pre> <p>... and run the workflow again:</p> Test insufficient memory<pre><code>nextflow run main.nf\n</code></pre> <p>You'll see an error indicating that the process was killed for exceeding memory limits:</p> Memory error output<pre><code>Command exit status:\n  137\n\nCommand output:\n  (empty)\n\nCommand error:\n  Detecting adapter sequence for read1...\n  No adapter detected for read1\n\n  .command.sh: line 7:   101 Killed                  fastp --in1 SAMPLE_002_S2_L001_R1_001.fastq --out1 sample_002_trimmed.fastq.gz --json sample_002.fastp.json --html sample_002.fastp.html --thread 2\n</code></pre> <p>This is a very common scenario in real-world workflows - sometimes you just don't know how much memory a task will need until you run it. To make our workflow more robust, we can implement a retry strategy that increases memory allocation on each attempt, once again using a Groovy closure. Modify the <code>memory</code> directive to multiply the base memory by <code>task.attempt</code>, and add <code>errorStrategy 'retry'</code> and <code>maxRetries 2</code> directives:</p> AfterBefore modules/fastp.nf<pre><code>process FASTP {\n    container 'community.wave.seqera.io/library/fastp:0.24.0--62c97b06e8447690'\n\n    cpus { meta.depth &gt; 40000000 ? 4 : 2 }\n    memory { 1.GB * task.attempt }\n    errorStrategy 'retry'\n    maxRetries 2\n\n    input:\n    tuple val(meta), path(reads)\n</code></pre> modules/fastp.nf<pre><code>process FASTP {\n    container 'community.wave.seqera.io/library/fastp:0.24.0--62c97b06e8447690'\n\n    cpus { meta.depth &gt; 40000000 ? 4 : 2 }\n    memory 2.GB\n\n    input:\n    tuple val(meta), path(reads)\n</code></pre> <p>Now if the process fails due to insufficient memory, Nextflow will retry with more memory:</p> <ul> <li>First attempt: 1 GB (task.attempt = 1)</li> <li>Second attempt: 2.GB (task.attempt = 2)</li> </ul> <p>... and so on, up to the <code>maxRetries</code> limit.</p>"},{"location":"side_quests/essential_scripting_patterns/#takeaway_3","title":"Takeaway","text":"<p>Dynamic directives with closures let you:</p> <ul> <li>Allocate resources based on input characteristics</li> <li>Implement automatic retry strategies with increasing resources</li> <li>Combine multiple factors (metadata, attempt number, priorities)</li> <li>Use conditional logic for complex resource calculations</li> </ul> <p>This makes your workflows both more efficient (not over-allocating) and more robust (automatic retry with more resources).</p>"},{"location":"side_quests/essential_scripting_patterns/#5-conditional-logic-and-process-control","title":"5. Conditional Logic and Process Control","text":"<p>Previously, we used <code>.map()</code> with scripting to transform channel data. Now we'll use conditional logic to control which processes execute based on data\u2014essential for flexible workflows adapting to different sample types.</p> <p>Nextflow's dataflow operators take closures evaluated at runtime, enabling conditional logic to drive workflow decisions based on channel content.</p>"},{"location":"side_quests/essential_scripting_patterns/#51-routing-with-branch","title":"5.1. Routing with <code>.branch()</code>","text":"<p>For example, let's pretend that our sequencing samples need to be trimmed with FASTP only if they're human samples with a coverage above a certain threshold. Mouse samples or low-coverage samples should be run with Trimgalore instead (this is a contrived example, but it illustrates the point).</p> <p>We've provided a simple Trimgalore process in <code>modules/trimgalore.nf</code>, take a look if you like, but the details aren't important for this exercise. The key point is that we want to route samples based on their metadata.</p> <p>Include the new from in <code>modules/trimgalore.nf</code>:</p> AfterBefore main.nf<pre><code>include { FASTP } from './modules/fastp.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\n</code></pre> main.nf<pre><code>include { FASTP } from './modules/fastp.nf'\n</code></pre> <p>... and then modify your <code>main.nf</code> workflow to branch samples based on their metadata and route them through the appropriate trimming process, like this:</p> AfterBefore main.nf<pre><code>    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map(separateMetadata)\n\n    trim_branches = ch_samples\n        .branch { meta, reads -&gt;\n            fastp: meta.organism == 'human' &amp;&amp; meta.depth &gt;= 30000000\n            trimgalore: true\n        }\n\n    ch_fastp = FASTP(trim_branches.fastp)\n    ch_trimgalore = TRIMGALORE(trim_branches.trimgalore)\n    GENERATE_REPORT(ch_samples)\n</code></pre> main.nf<pre><code>    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map(separateMetadata)\n\n    ch_fastp = FASTP(ch_samples)\n    GENERATE_REPORT(ch_samples)\n</code></pre> <p>Run this modified workflow:</p> Test conditional trimming<pre><code>nextflow run main.nf\n</code></pre> Conditional trimming results<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [adoring_galileo] DSL2 - revision: c9e83aaef1\n\nexecutor &gt;  local (6)\n[1d/0747ac] process &gt; FASTP (2)           [100%] 2 of 2 \u2714\n[cc/c44caf] process &gt; TRIMGALORE (1)      [100%] 1 of 1 \u2714\n[34/bd5a9f] process &gt; GENERATE_REPORT (1) [100%] 3 of 3 \u2714\n</code></pre> <p>Here, we've used small but mighty conditional expressions inside the <code>.branch{}</code> operator to route samples based on their metadata. Human samples with high coverage go through <code>FASTP</code>, while all other samples go through <code>TRIMGALORE</code>.</p>"},{"location":"side_quests/essential_scripting_patterns/#52-using-filter-with-truthiness","title":"5.2. Using <code>.filter()</code> with Truthiness","text":"<p>Another powerful pattern for controlling workflow execution is the <code>.filter()</code> operator, which uses a closure to determine which items should continue down the pipeline. Inside the filter closure, you'll write boolean expressions that decide which items pass through.</p> <p>Nextflow (like many dynamic languages) has a concept of \"truthiness\" that determines what values evaluate to <code>true</code> or <code>false</code> in boolean contexts:</p> <ul> <li>Truthy: Non-null values, non-empty strings, non-zero numbers, non-empty collections</li> <li>Falsy: <code>null</code>, empty strings <code>\"\"</code>, zero <code>0</code>, empty collections <code>[]</code> or <code>[:]</code>, <code>false</code></li> </ul> <p>This means <code>meta.id</code> alone (without explicit <code>!= null</code>) checks if the ID exists and isn't empty. Let's use this to filter out samples that don't meet our quality requirements.</p> <p>Add the following before the branch operation:</p> AfterBefore main.nf<pre><code>    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map(separateMetadata)\n\n    // Filter out invalid or low-quality samples\n    ch_valid_samples = ch_samples\n        .filter { meta, reads -&gt;\n            meta.id &amp;&amp; meta.organism &amp;&amp; meta.depth &gt;= 25000000\n        }\n\n    trim_branches = ch_valid_samples\n        .branch { meta, reads -&gt;\n            fastp: meta.organism == 'human' &amp;&amp; meta.depth &gt;= 30000000\n            trimgalore: true\n        }\n</code></pre> main.nf<pre><code>    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map(separateMetadata)\n\n    trim_branches = ch_samples\n        .branch { meta, reads -&gt;\n            fastp: meta.organism == 'human' &amp;&amp; meta.depth &gt;= 30000000\n            trimgalore: true\n        }\n</code></pre> <p>Run the workflow again:</p> Test filtering samples<pre><code>nextflow run main.nf\n</code></pre> <p>Because we've chosen a filter that excludes some samples, you should see fewer tasks executed:</p> Filtered samples results<pre><code>N E X T F L O W  ~  version 25.04.3\nLaunching `main.nf` [lonely_williams] DSL2 - revision: d0b3f121ec\n[94/b48eac] Submitted process &gt; FASTP (2)\n[2c/d2b28f] Submitted process &gt; GENERATE_REPORT (2)\n[65/2e3be4] Submitted process &gt; GENERATE_REPORT (1)\n[94/b48eac] NOTE: Process `FASTP (2)` terminated with an error exit status (137) -- Execution is retried (1)\n[3e/0d8664] Submitted process &gt; TRIMGALORE (1)\n[6a/9137b0] Submitted process &gt; FASTP (1)\n[6a/9137b0] NOTE: Process `FASTP (1)` terminated with an error exit status (137) -- Execution is retried (1)\n[83/577ac0] Submitted process &gt; GENERATE_REPORT (3)\n[a2/5117de] Re-submitted process &gt; FASTP (1)\n[1f/a1a4ca] Re-submitted process &gt; FASTP (2)\n</code></pre> <p>The filter expression <code>meta.id &amp;&amp; meta.organism &amp;&amp; meta.depth &gt;= 25000000</code> combines truthiness with explicit comparisons:</p> <ul> <li><code>meta.id &amp;&amp; meta.organism</code> checks that both fields exist and are non-empty (using truthiness)</li> <li><code>meta.depth &gt;= 25000000</code> ensures sufficient sequencing depth with an explicit comparison</li> </ul> <p>Truthiness in Practice</p> <p>The expression <code>meta.id &amp;&amp; meta.organism</code> is more concise than writing: <pre><code>meta.id != null &amp;&amp; meta.id != '' &amp;&amp; meta.organism != null &amp;&amp; meta.organism != ''\n</code></pre></p> <p>This makes filtering logic much cleaner and easier to read.</p>"},{"location":"side_quests/essential_scripting_patterns/#takeaway_4","title":"Takeaway","text":"<p>In this section, you've learned to use conditional logic to control workflow execution using the closure interfaces of Nextflow operators like <code>.branch{}</code> and <code>.filter{}</code>, leveraging truthiness to write concise conditional expressions.</p> <p>Our pipeline now intelligently routes samples through appropriate processes, but production workflows need to handle invalid data gracefully. Let's make our workflow robust against missing or null values.</p>"},{"location":"side_quests/essential_scripting_patterns/#6-safe-navigation-and-elvis-operators","title":"6. Safe Navigation and Elvis Operators","text":"<p>Our <code>separateMetadata</code> function currently assumes all CSV fields are present and valid. But what happens with incomplete data? Let's find out.</p>"},{"location":"side_quests/essential_scripting_patterns/#61-the-problem-accessing-properties-that-dont-exist","title":"6.1. The Problem: Accessing Properties That Don't Exist","text":"<p>Let's say we want to add support for optional sequencing run information. In some labs, samples might have an additional field for the sequencing run ID or batch number, but our current CSV doesn't have this column. Let's try to access it anyway.</p> <p>Modify the <code>separateMetadata</code> function to include a run_id field:</p> AfterBefore main.nf<pre><code>def separateMetadata(row) {\n    def sample_meta = [\n        id: row.sample_id.toLowerCase(),\n        organism: row.organism,\n        tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n        depth: row.sequencing_depth.toInteger(),\n        quality: row.quality_score.toDouble()\n    ]\n    def run_id = row.run_id.toUpperCase()\n</code></pre> main.nf<pre><code>def separateMetadata(row) {\n    def sample_meta = [\n        id: row.sample_id.toLowerCase(),\n        organism: row.organism,\n        tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n        depth: row.sequencing_depth.toInteger(),\n        quality: row.quality_score.toDouble()\n    ]\n</code></pre> <p>Now run the workflow:</p> <pre><code>nextflow run main.nf\n</code></pre> <p>It crashes with a NullPointerException:</p> Null pointer error<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [trusting_torvalds] DSL2 - revision: b56fbfbce2\n\nERROR ~ Cannot invoke method toUpperCase() on null object\n\n -- Check script 'main.nf' at line: 13 or see '.nextflow.log' file for more details\n</code></pre> <p>The problem is that <code>row.run_id</code> returns <code>null</code> because the <code>run_id</code> column doesn't exist in our CSV. When we try to call <code>.toUpperCase()</code> on <code>null</code>, it crashes. This is where the safe navigation operator saves the day.</p>"},{"location":"side_quests/essential_scripting_patterns/#62-safe-navigation-operator","title":"6.2. Safe Navigation Operator (<code>?.</code>)","text":"<p>The safe navigation operator (<code>?.</code>) returns <code>null</code> instead of throwing an exception when called on a <code>null</code> value. If the object before <code>?.</code> is <code>null</code>, the entire expression evaluates to <code>null</code> without executing the method.</p> <p>Update the function to use safe navigation:</p> AfterBefore main.nf<pre><code>def separateMetadata(row) {\n    def sample_meta = [\n        id: row.sample_id.toLowerCase(),\n        organism: row.organism,\n        tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n        depth: row.sequencing_depth.toInteger(),\n        quality: row.quality_score.toDouble()\n    ]\n    def run_id = row.run_id?.toUpperCase()\n</code></pre> main.nf<pre><code>def separateMetadata(row) {\n    def sample_meta = [\n        id: row.sample_id.toLowerCase(),\n        organism: row.organism,\n        tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n        depth: row.sequencing_depth.toInteger(),\n        quality: row.quality_score.toDouble()\n    ]\n    def run_id = row.run_id.toUpperCase()\n</code></pre> <p>Run again:</p> <pre><code>nextflow run main.nf\n</code></pre> <p>No crash! The workflow now handles the missing field gracefully. When <code>row.run_id</code> is <code>null</code>, the <code>?.</code> operator prevents the <code>.toUpperCase()</code> call, and <code>run_id</code> becomes <code>null</code> instead of causing an exception.</p>"},{"location":"side_quests/essential_scripting_patterns/#63-elvis-operator-for-defaults","title":"6.3. Elvis Operator (<code>?:</code>) for Defaults","text":"<p>The Elvis operator (<code>?:</code>) provides default values when the left side is \"falsy\" (as explained previously). It's named after Elvis Presley because <code>?:</code> looks like his famous hair and eyes when viewed sideways!</p> <p>Now that we're using safe navigation, <code>run_id</code> will be <code>null</code> for samples without that field. Let's use the Elvis operator to provide a default value and add it to our <code>sample_meta</code> map:</p> AfterBefore main.nf<pre><code>def separateMetadata(row) {\n    def sample_meta = [\n        id: row.sample_id.toLowerCase(),\n        organism: row.organism,\n        tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n        depth: row.sequencing_depth.toInteger(),\n        quality: row.quality_score.toDouble()\n    ]\n    def run_id = row.run_id?.toUpperCase() ?: 'UNSPECIFIED'\n    sample_meta.run = run_id\n</code></pre> main.nf<pre><code>def separateMetadata(row) {\n    def sample_meta = [\n        id: row.sample_id.toLowerCase(),\n        organism: row.organism,\n        tissue: row.tissue_type.replaceAll('_', ' ').toLowerCase(),\n        depth: row.sequencing_depth.toInteger(),\n        quality: row.quality_score.toDouble()\n    ]\n    def run_id = row.run_id?.toUpperCase()\n</code></pre> <p>Also add a <code>view()</code> operator in the workflow to see the results:</p> AfterBefore main.nf<pre><code>    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map{ row -&gt; separateMetadata(row) }\n        .view()\n</code></pre> main.nf<pre><code>    ch_samples = channel.fromPath(\"./data/samples.csv\")\n        .splitCsv(header: true)\n        .map{ row -&gt; separateMetadata(row) }\n</code></pre> <p>and run the workflow:</p> <pre><code>nextflow run main.nf\n</code></pre> <p>You'll see output like this:</p> View output with run field<pre><code>[[id:sample_001, organism:human, tissue:liver, depth:30000000, quality:38.5, run:UNSPECIFIED, sample_num:1, lane:001, read:R1, chunk:001, priority:normal], /workspaces/training/side-quests/essential_scripting_patterns/data/sequences/SAMPLE_001_S1_L001_R1_001.fastq]\n[[id:sample_002, organism:mouse, tissue:brain, depth:25000000, quality:35.2, run:UNSPECIFIED, sample_num:2, lane:001, read:R1, chunk:001, priority:normal], /workspaces/training/side-quests/essential_scripting_patterns/data/sequences/SAMPLE_002_S2_L001_R1_001.fastq]\n[[id:sample_003, organism:human, tissue:kidney, depth:45000000, quality:42.1, run:UNSPECIFIED, sample_num:3, lane:001, read:R1, chunk:001, priority:high], /workspaces/training/side-quests/essential_scripting_patterns/data/sequences/SAMPLE_003_S3_L001_R1_001.fastq]\n</code></pre> <p>Perfect! Now all samples have a <code>run</code> field with either their actual run ID (in uppercase) or the default value 'UNSPECIFIED'. The combination of <code>?.</code> and <code>?:</code> provides both safety (no crashes) and sensible defaults.</p> <p>Take out the <code>.view()</code> operator now that we've confirmed it works.</p> <p>Combining Safe Navigation and Elvis</p> <p>The pattern <code>value?.method() ?: 'default'</code> is common in production workflows:</p> <ul> <li><code>value?.method()</code> - Safely calls method, returns <code>null</code> if <code>value</code> is <code>null</code></li> <li><code>?: 'default'</code> - Provides fallback if result is <code>null</code></li> </ul> <p>This pattern handles missing/incomplete data gracefully.</p> <p>Use these operators consistently in functions, operator closures (<code>.map{}</code>, <code>.filter{}</code>), process scripts, and config files. They prevent crashes when handling real-world data.</p>"},{"location":"side_quests/essential_scripting_patterns/#takeaway_5","title":"Takeaway","text":"<ul> <li>Safe navigation (<code>?.</code>): Prevents crashes on null values - returns null instead of throwing exception</li> <li>Elvis operator (<code>?:</code>): Provides defaults - <code>value ?: 'default'</code></li> <li>Combining: <code>value?.method() ?: 'default'</code> is the common pattern</li> </ul> <p>These operators make workflows resilient to incomplete data - essential for real-world work.</p>"},{"location":"side_quests/essential_scripting_patterns/#7-validation-with-error-and-logwarn","title":"7. Validation with <code>error()</code> and <code>log.warn</code>","text":"<p>Sometimes you need to stop the workflow immediately if input parameters are invalid. In Nextflow, you can use built-in functions like <code>error()</code> and <code>log.warn</code>, as well as standard programming constructs like <code>if</code> statements and boolean logic, to implement validation logic. Let's add validation to our workflow.</p> <p>Create a validation function before your workflow block, call it from the workflow, and change the channel creation to use a parameter for the CSV file path. If the parameter is missing or the file doesn't exist, call <code>error()</code> to stop execution with a clear message.</p> AfterBefore main.nf<pre><code>include { FASTP } from './modules/fastp.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\ninclude { GENERATE_REPORT } from './modules/generate_report.nf'\n\ndef validateInputs() {\n    // Check input parameter is provided\n    if (!params.input) {\n        error(\"Input CSV file path not provided. Please specify --input &lt;file.csv&gt;\")\n    }\n\n    // Check CSV file exists\n    if (!file(params.input).exists()) {\n        error(\"Input CSV file not found: ${params.input}\")\n    }\n}\n...\nworkflow {\n    validateInputs()\n    ch_samples = channel.fromPath(params.input)\n</code></pre> main.nf<pre><code>include { FASTP } from './modules/fastp.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\ninclude { GENERATE_REPORT } from './modules/generate_report.nf'\n\n...\nworkflow {\n    ch_samples = channel.fromPath(\"./data/samples.csv\")\n</code></pre> <p>Now try running without the CSV file:</p> <pre><code>nextflow run main.nf\n</code></pre> <p>The workflow stops immediately with a clear error message instead of failing mysteriously later!</p> Validation error output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [confident_coulomb] DSL2 - revision: 07059399ed\n\nWARN: Access to undefined parameter `input` -- Initialise it to a default value eg. `params.input = some_value`\nInput CSV file path not provided. Please specify --input &lt;file.csv&gt;\n</code></pre> <p>Now run it with a non-existent file:</p> <pre><code>nextflow run main.nf --input ./data/nonexistent.csv\n</code></pre> <p>Observe the error:</p> File not found error output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [cranky_gates] DSL2 - revision: 26839ae3eb\n\nInput CSV file not found: ./data/nonexistent.csv\n</code></pre> <p>Finally, run it with the correct file:</p> <pre><code>nextflow run main.nf --input ./data/samples.csv\n</code></pre> <p>This time it runs successfully.</p> <p>You can also add validation within the <code>separateMetadata</code> function. Let's use the non-fatal <code>log.warn</code> to issue warnings for samples with low sequencing depth, but still allow the workflow to continue:</p> AfterBefore main.nf<pre><code>    def priority = sample_meta.quality &gt; 40 ? 'high' : 'normal'\n\n    // Validate data makes sense\n    if (sample_meta.depth &lt; 30000000) {\n        log.warn \"Low sequencing depth for ${sample_meta.id}: ${sample_meta.depth}\"\n    }\n\n    return tuple(sample_meta + file_meta + [priority: priority], fastq_path)\n}\n</code></pre> main.nf<pre><code>    def priority = sample_meta.quality &gt; 40 ? 'high' : 'normal'\n\n    return tuple(sample_meta + file_meta + [priority: priority], fastq_path)\n}\n</code></pre> <p>Run the workflow again with the original CSV:</p> <pre><code>nextflow run main.nf --input ./data/samples.csv\n</code></pre> <p>... and you'll see a warning about low sequencing depth for one of the samples:</p> Warning output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [awesome_goldwasser] DSL2 - revision: a31662a7c1\n\nexecutor &gt;  local (5)\n[ce/df5eeb] process &gt; FASTP (2)           [100%] 2 of 2 \u2714\n[-        ] process &gt; TRIMGALORE          -\n[d1/7d2b4b] process &gt; GENERATE_REPORT (3) [100%] 3 of 3 \u2714\nWARN: Low sequencing depth for sample_002: 25000000\n</code></pre>"},{"location":"side_quests/essential_scripting_patterns/#takeaway_6","title":"Takeaway","text":"<ul> <li><code>error()</code>: Stops workflow immediately with clear message</li> <li><code>log.warn</code>: Issues warnings without stopping workflow</li> <li>Early validation: Check inputs before processing to fail fast with helpful errors</li> <li>Validation functions: Create reusable validation logic that can be called at workflow start</li> </ul> <p>Proper validation makes workflows more robust and user-friendly by catching problems early with clear error messages.</p>"},{"location":"side_quests/essential_scripting_patterns/#8-workflow-event-handlers","title":"8. Workflow Event Handlers","text":"<p>Up until now, we've been writing code in our workflow scripts and process definitions. But there's one more important feature you should know about: workflow event handlers.</p> <p>Event handlers are closures that run at specific points in your workflow's lifecycle. They're perfect for adding logging, notifications, or cleanup operations. These handlers should be defined in your workflow script alongside your workflow definition.</p>"},{"location":"side_quests/essential_scripting_patterns/#81-the-oncomplete-handler","title":"8.1. The <code>onComplete</code> Handler","text":"<p>The most commonly used event handler is <code>onComplete</code>, which runs when your workflow finishes (whether it succeeded or failed). Let's add one to summarize our pipeline results.</p> <p>Add the event handler to your <code>main.nf</code> file, inside your workflow definition:</p> AfterBefore main.nf<pre><code>    ch_fastp = FASTP(trim_branches.fastp)\n    ch_trimgalore = TRIMGALORE(trim_branches.trimgalore)\n    GENERATE_REPORT(ch_samples)\n\n    workflow.onComplete = {\n        println \"\"\n        println \"Pipeline execution summary:\"\n        println \"==========================\"\n        println \"Completed at: ${workflow.complete}\"\n        println \"Duration    : ${workflow.duration}\"\n        println \"Success     : ${workflow.success}\"\n        println \"workDir     : ${workflow.workDir}\"\n        println \"exit status : ${workflow.exitStatus}\"\n        println \"\"\n    }\n}\n</code></pre> main.nf<pre><code>    ch_fastp = FASTP(trim_branches.fastp)\n    ch_trimgalore = TRIMGALORE(trim_branches.trimgalore)\n    GENERATE_REPORT(ch_samples)\n}\n</code></pre> <p>This closure runs when the workflow completes. Inside, you have access to the <code>workflow</code> object which provides useful properties about the execution.</p> <p>Run your workflow and you'll see this summary appear at the end!</p> Run with onComplete handler<pre><code>nextflow run main.nf --input ./data/samples.csv -ansi-log false\n</code></pre> onComplete output<pre><code>N E X T F L O W  ~  version 25.04.3\nLaunching `main.nf` [marvelous_boltzmann] DSL2 - revision: a31662a7c1\nWARN: Low sequencing depth for sample_002: 25000000\n[9b/d48e40] Submitted process &gt; FASTP (2)\n[6a/73867a] Submitted process &gt; GENERATE_REPORT (2)\n[79/ad0ac5] Submitted process &gt; GENERATE_REPORT (1)\n[f3/bda6cb] Submitted process &gt; FASTP (1)\n[34/d5b52f] Submitted process &gt; GENERATE_REPORT (3)\n\nPipeline execution summary:\n==========================\nCompleted at: 2025-10-10T12:14:24.885384+01:00\nDuration    : 2.9s\nSuccess     : true\nworkDir     : /workspaces/training/side-quests/essential_scripting_patterns/work\nexit status : 0\n</code></pre> <p>Let's make it more useful by adding conditional logic:</p> AfterBefore main.nf<pre><code>    ch_fastp = FASTP(trim_branches.fastp)\n    ch_trimgalore = TRIMGALORE(trim_branches.trimgalore)\n    GENERATE_REPORT(ch_samples)\n\n    workflow.onComplete = {\n        println \"\"\n        println \"Pipeline execution summary:\"\n        println \"==========================\"\n        println \"Completed at: ${workflow.complete}\"\n        println \"Duration    : ${workflow.duration}\"\n        println \"Success     : ${workflow.success}\"\n        println \"workDir     : ${workflow.workDir}\"\n        println \"exit status : ${workflow.exitStatus}\"\n        println \"\"\n\n        if (workflow.success) {\n            println \"\u2705 Pipeline completed successfully!\"\n        } else {\n            println \"\u274c Pipeline failed!\"\n            println \"Error: ${workflow.errorMessage}\"\n        }\n    }\n}\n</code></pre> main.nf<pre><code>    ch_fastp = FASTP(trim_branches.fastp)\n    ch_trimgalore = TRIMGALORE(trim_branches.trimgalore)\n    GENERATE_REPORT(ch_samples)\n\n    workflow.onComplete = {\n        println \"\"\n        println \"Pipeline execution summary:\"\n        println \"==========================\"\n        println \"Completed at: ${workflow.complete}\"\n        println \"Duration    : ${workflow.duration}\"\n        println \"Success     : ${workflow.success}\"\n        println \"workDir     : ${workflow.workDir}\"\n        println \"exit status : ${workflow.exitStatus}\"\n        println \"\"\n    }\n}\n</code></pre> <p>Now we get an even more informative summary, including a success/failure message and the output directory if specified:</p> Enhanced onComplete output<pre><code>N E X T F L O W  ~  version 25.04.3\nLaunching `main.nf` [boring_linnaeus] DSL2 - revision: a31662a7c1\nWARN: Low sequencing depth for sample_002: 25000000\n[e5/242efc] Submitted process &gt; FASTP (2)\n[3b/74047c] Submitted process &gt; GENERATE_REPORT (3)\n[8a/7a57e6] Submitted process &gt; GENERATE_REPORT (1)\n[a8/b1a31f] Submitted process &gt; GENERATE_REPORT (2)\n[40/648429] Submitted process &gt; FASTP (1)\n\nPipeline execution summary:\n==========================\nCompleted at: 2025-10-10T12:16:00.522569+01:00\nDuration    : 3.6s\nSuccess     : true\nworkDir     : /workspaces/training/side-quests/essential_scripting_patterns/work\nexit status : 0\n\n\u2705 Pipeline completed successfully!\n</code></pre> <p>You can also write the summary to a file using file operations:</p> main.nf - Writing summary to file<pre><code>workflow {\n    // ... your workflow code ...\n\n    workflow.onComplete = {\n        def summary = \"\"\"\n        Pipeline Execution Summary\n        ===========================\n        Completed: ${workflow.complete}\n        Duration : ${workflow.duration}\n        Success  : ${workflow.success}\n        Command  : ${workflow.commandLine}\n        \"\"\"\n\n        println summary\n\n        // Write to a log file\n        def log_file = file(\"${workflow.launchDir}/pipeline_summary.txt\")\n        log_file.text = summary\n    }\n}\n</code></pre>"},{"location":"side_quests/essential_scripting_patterns/#82-the-onerror-handler","title":"8.2. The <code>onError</code> Handler","text":"<p>Besides <code>onComplete</code>, there is one other event handler you can use: <code>onError</code>, which runs only if the workflow fails:</p> main.nf - onError handler<pre><code>workflow {\n    // ... your workflow code ...\n\n    workflow.onError = {\n        println \"=\"* 50\n        println \"Pipeline execution failed!\"\n        println \"Error message: ${workflow.errorMessage}\"\n        println \"=\"* 50\n\n        // Write detailed error log\n        def error_file = file(\"${workflow.launchDir}/error.log\")\n        error_file.text = \"\"\"\n        Workflow Error Report\n        =====================\n        Time: ${new Date()}\n        Error: ${workflow.errorMessage}\n        Error report: ${workflow.errorReport ?: 'No detailed report available'}\n        \"\"\"\n\n        println \"Error details written to: ${error_file}\"\n    }\n}\n</code></pre> <p>You can use multiple handlers together in your workflow script:</p> main.nf - Combined handlers<pre><code>workflow {\n    // ... your workflow code ...\n\n    workflow.onError = {\n        println \"Workflow failed: ${workflow.errorMessage}\"\n    }\n\n    workflow.onComplete = {\n        def duration_mins = workflow.duration.toMinutes().round(2)\n        def status = workflow.success ? \"SUCCESS \u2705\" : \"FAILED \u274c\"\n\n        println \"\"\"\n        Pipeline finished: ${status}\n        Duration: ${duration_mins} minutes\n        \"\"\"\n    }\n}\n</code></pre>"},{"location":"side_quests/essential_scripting_patterns/#takeaway_7","title":"Takeaway","text":"<p>In this section, you've learned:</p> <ul> <li>Event handler closures: Closures in your workflow script that run at different lifecycle points</li> <li><code>onComplete</code> handler: For execution summaries and result reporting</li> <li><code>onError</code> handler: For error handling and logging failures</li> <li>Workflow object properties: Accessing <code>workflow.success</code>, <code>workflow.duration</code>, <code>workflow.errorMessage</code>, etc.</li> </ul> <p>Event handlers show how you can use the full power of the Nextflow language within your workflow scripts to add sophisticated logging and notification capabilities.</p>"},{"location":"side_quests/essential_scripting_patterns/#summary","title":"Summary","text":"<p>Throughout this side quest, you've built a comprehensive sample processing pipeline that evolved from basic metadata handling to a sophisticated, production-ready workflow. Each section built upon the previous, demonstrating how programming constructs transform simple workflows into powerful data processing systems.</p> <p>Here's how we progressively enhanced our pipeline:</p> <ol> <li> <p>Dataflow vs Scripting: You learned to distinguish between dataflow operations (channel orchestration) and scripting (code that manipulates data), including the crucial differences between operations on different types like <code>collect</code> on Channel vs List.</p> </li> <li> <p>Advanced String Processing: You mastered regular expressions for parsing file names, dynamic script generation in processes, and variable interpolation (Nextflow vs Bash vs Shell).</p> </li> <li> <p>Creating Reusable Functions: You learned to extract complex logic into named functions that can be called from channel operators, making workflows more readable and maintainable.</p> </li> <li> <p>Dynamic Resource Directives with Closures: You explored using closures in process directives for adaptive resource allocation based on input characteristics.</p> </li> <li> <p>Conditional Logic and Process Control: You added intelligent routing using <code>.branch()</code> and <code>.filter()</code> operators, leveraging truthiness for concise conditional expressions.</p> </li> <li> <p>Safe Navigation and Elvis Operators: You made the pipeline robust against missing data using <code>?.</code> for null-safe property access and <code>?:</code> for providing default values.</p> </li> <li> <p>Validation with error() and log.warn: You learned to validate inputs early and fail fast with clear error messages.</p> </li> <li> <p>Configuration Event Handlers: You learned to use workflow event handlers (<code>onComplete</code> and <code>onError</code>) for logging, notifications, and lifecycle management.</p> </li> </ol>"},{"location":"side_quests/essential_scripting_patterns/#key-benefits","title":"Key Benefits","text":"<ul> <li>Clearer code: Understanding dataflow vs scripting helps you write more organized workflows</li> <li>Robust handling: Safe navigation and Elvis operators make workflows resilient to missing data</li> <li>Flexible processing: Conditional logic lets your workflows process different sample types appropriately</li> <li>Adaptive resources: Dynamic directives optimize resource usage based on input characteristics</li> </ul>"},{"location":"side_quests/essential_scripting_patterns/#from-simple-to-sophisticated","title":"From Simple to Sophisticated","text":"<p>This pipeline evolved from basic data processing to production-ready workflows:</p> <ol> <li>Simple: CSV processing and metadata extraction (dataflow vs scripting boundaries)</li> <li>Intelligent: Regex parsing, variable interpolation, dynamic script generation</li> <li>Maintainable: Reusable functions for cleaner, testable code</li> <li>Efficient: Dynamic resource allocation and retry strategies</li> <li>Adaptive: Conditional routing based on sample characteristics</li> <li>Robust: Safe navigation, Elvis operators, early validation</li> <li>Observable: Event handlers for logging and lifecycle management</li> </ol> <p>This progression mirrors the real-world evolution of bioinformatics pipelines - from research prototypes handling a few samples to production systems processing thousands of samples across laboratories and institutions. Every challenge you solved and pattern you learned reflects actual problems developers face when scaling Nextflow workflows.</p>"},{"location":"side_quests/essential_scripting_patterns/#next-steps","title":"Next Steps","text":"<p>With these fundamentals mastered, you're ready to:</p> <ul> <li>Write cleaner workflows with proper separation between dataflow and scripting</li> <li>Master variable interpolation to avoid common pitfalls with Nextflow, Bash, and shell variables</li> <li>Use dynamic resource directives for efficient, adaptive workflows</li> <li>Transform file collections into properly formatted command-line arguments</li> <li>Handle different file naming conventions and input formats gracefully using regex and string processing</li> <li>Build reusable, maintainable code using advanced closure patterns and functional programming</li> <li>Process and organize complex datasets using collection operations</li> <li>Add validation, error handling, and logging to make your workflows production-ready</li> <li>Implement workflow lifecycle management with event handlers</li> </ul> <p>Continue practicing these patterns in your own workflows, and refer to the Nextflow language reference when you need to explore more advanced features.</p>"},{"location":"side_quests/essential_scripting_patterns/#key-concepts-reference","title":"Key Concepts Reference","text":"<ul> <li>Language Boundaries</li> </ul> Dataflow vs Scripting examples<pre><code>// Dataflow: channel orchestration\nchannel.fromPath('*.fastq').splitCsv(header: true)\n\n// Scripting: data processing on collections\nsample_data.collect { it.toUpperCase() }\n</code></pre> <ul> <li>String Processing</li> </ul> String processing examples<pre><code>// Pattern matching\nfilename =~ ~/^(\\w+)_(\\w+)_(\\d+)\\.fastq$/\n\n// Function with conditional return\ndef parseSample(filename) {\n    def matcher = filename =~ pattern\n    return matcher ? [valid: true, data: matcher[0]] : [valid: false]\n}\n\n// File collection to command arguments (in process script block)\nscript:\ndef file_args = input_files.collect { file -&gt; \"--input ${file}\" }.join(' ')\n\"\"\"\nanalysis_tool ${file_args} --output results.txt\n\"\"\"\n</code></pre> <ul> <li>Error Handling</li> </ul> Error handling patterns<pre><code>try {\n    def errors = validateSample(sample)\n    if (errors) throw new RuntimeException(\"Invalid: ${errors.join(', ')}\")\n} catch (Exception e) {\n    println \"Error: ${e.message}\"\n}\n</code></pre> <ul> <li>Essential Operators</li> </ul> Essential operators examples<pre><code>// Safe navigation and Elvis operators\ndef id = data?.sample?.id ?: 'unknown'\nif (sample.files) println \"Has files\"  // Groovy Truth\n\n// Slashy strings for regex\ndef pattern = /^\\w+_R[12]\\.fastq$/\ndef script = \"\"\"\necho \"Processing ${sample.id}\"\nanalysis --depth ${depth ?: 1_000_000}\n\"\"\"\n</code></pre> <ul> <li>Advanced Closures</li> </ul> Advanced closure patterns<pre><code>// Named closures and composition\ndef enrichData = normalizeId &gt;&gt; addQualityCategory &gt;&gt; addFlags\ndef processor = generalFunction.curry(fixedParam)\n\n// Closures with scope access\ndef collectStats = { data -&gt; stats.count++; return data }\n</code></pre> <ul> <li>Collection Operations Collection operations examples<pre><code>// Filter, group, and organize data\ndef high_quality = samples.findAll { it.quality &gt; 40 }\ndef by_organism = samples.groupBy { it.organism }\ndef file_names = files*.getName()  // Spread operator\ndef all_files = nested_lists.flatten()\n</code></pre></li> </ul>"},{"location":"side_quests/essential_scripting_patterns/#resources","title":"Resources","text":"<ul> <li>Nextflow Language Reference</li> <li>Nextflow Operators</li> <li>Nextflow Script Syntax</li> <li>Nextflow Standard Library</li> </ul>"},{"location":"side_quests/if_else/","title":"Part 2: If - Else","text":"<p>[TODO]</p>"},{"location":"side_quests/if_else/#1-make-the-cow-quote-famous-scientists","title":"1. Make the cow quote famous scientists","text":"<p>This section contains some stretch exercises, to practice what you've learned so far. Doing these exercises is not required to understand later parts of the training, but provide a fun way to reinforce your learnings by figuring out how to make the cow quote famous scientists.</p> cowsay-output-Grace-Hopper.txt<pre><code>  _________________________________________________\n /                                                 \\\n| Humans are allergic to change. They love to       |\n| say, 'We've always done it this way.' I try to fi |\n| ght that. That's why I have a clock on my wall th |\n| at runs counter-clockwise.                        |\n| -Grace Hopper                                     |\n \\                                                 /\n  =================================================\n                                                 \\\n                                                  \\\n                                                    ^__^\n                                                    (oo)\\_______\n                                                    (__)\\       )\\/\\\n                                                        ||----w |\n                                                        ||     ||\n</code></pre>"},{"location":"side_quests/if_else/#11-modify-the-hello-containersnf-script-to-use-a-getquote-process","title":"1.1. Modify the <code>hello-containers.nf</code> script to use a getQuote process","text":"<p>We have a list of computer and biology pioneers in the <code>containers/data/pioneers.csv</code> file. At a high level, to complete this exercise you will need to:</p> <ul> <li>Modify the default <code>params.input_file</code> to point to the <code>pioneers.csv</code> file.</li> <li>Create a <code>getQuote</code> process that uses the <code>quote</code> container to fetch a quote for each input.</li> <li>Connect the output of the <code>getQuote</code> process to the <code>cowsay</code> process to display the quote.</li> </ul> <p>For the <code>quote</code> container image, you can either use the one you built yourself in the previous stretch exercise or use the one you got from Seqera Containers .</p> <p>Hint</p> <p>A good choice for the <code>script</code> block of your getQuote process might be:     <pre><code>script:\n    def safe_author = author.tokenize(' ').join('-')\n    \"\"\"\n    quote \"$author\" &gt; quote-${safe_author}.txt\n    echo \"-${author}\" &gt;&gt; quote-${safe_author}.txt\n    \"\"\"\n</code></pre></p> <p>You can find a solution to this exercise in <code>containers/solutions/hello-containers-4.1.nf</code>.</p>"},{"location":"side_quests/if_else/#12-modify-your-nextflow-pipeline-to-allow-it-to-execute-in-quote-and-sayhello-modes","title":"1.2. Modify your Nextflow pipeline to allow it to execute in <code>quote</code> and <code>sayHello</code> modes.","text":"<p>Add some branching logic using to your pipeline to allow it to accept inputs intended for both <code>quote</code> and <code>sayHello</code>. Here's an example of how to use an <code>if</code> statement in a Nextflow workflow:</p> hello-containers.nf<pre><code>workflow {\n    if (params.quote) {\n        ...\n    }\n    else {\n        ...\n    }\n    cowSay(text_ch)\n}\n</code></pre> <p>Hint</p> <p>You can use <code>new_ch = processName.out</code> to assign a name to the output channel of a process.</p> <p>You can find a solution to this exercise in <code>containers/solutions/hello-containers-4.2.nf</code>.</p>"},{"location":"side_quests/if_else/#takeaway","title":"Takeaway","text":"<p>You know how to use containers in Nextflow to run processes, and how to build some branching logic into your pipelines!</p>"},{"location":"side_quests/if_else/#whats-next","title":"What's next?","text":"<p>Celebrate, take a stretch break and drink some water!</p> <p>When you are ready, move on to Part 3 of this training series to learn how to apply what you've learned so far to a more realistic data analysis use case.</p>"},{"location":"side_quests/metadata/","title":"Metadata in workflows","text":"<p>In any scientific analysis, we rarely work with just the raw data files. Each file comes with its own additional information: what it is, where it came from, and what makes it special. This extra information is what we call metadata.</p> <p>Metadata is data describing other data. Metadata tracks important details about files and experimental conditions, and helps tailor analyses to each dataset's unique characteristics.</p> <p>Think of it like a library catalog: while books contain the actual content (raw data), the catalog cards provide essential information about each book\u2014when it was published, who wrote it, where to find it (metadata). In Nextflow pipelines, metadata can be used to:</p> <ul> <li>Track file-specific information throughout the workflow</li> <li>Configure processes based on file characteristics</li> <li>Group related files for joint analysis</li> </ul> <p>We'll explore how to handle metadata in workflows. Starting with a simple datasheet containing basic file information, you'll learn how to:</p> <ul> <li>Read and parse file metadata from CSV files</li> <li>Create and manipulate metadata maps</li> <li>Add new metadata fields during workflow execution</li> <li>Use metadata to customize process behavior</li> </ul> <p>These skills will help you build more robust and flexible pipelines that can handle complex file relationships and processing requirements.</p> <p>In this tutorial, we'll tackle a common data processing scenario: handling multiple files where each file needs different processing based on its characteristics.</p> <p>We have greeting files in different languages (French, German, Spanish, Italian, English), but we don't know which language each file contains. Our challenge is to:</p> <ol> <li>Identify the language in each file automatically</li> <li>Group files by language family (Germanic vs Romance languages)</li> <li>Customize the processing for each file based on its language and metadata</li> <li>Organize outputs by language group</li> </ol> <p>This represents a typical workflow pattern where file-specific metadata drives processing decisions - exactly the kind of problem that metadata maps solve elegantly.</p> <p>Let's dive in!</p>"},{"location":"side_quests/metadata/#0-warmup","title":"0. Warmup","text":""},{"location":"side_quests/metadata/#01-prerequisites","title":"0.1. Prerequisites","text":"<p>Before taking on this side quest you should:</p> <ul> <li>Complete the Hello Nextflow tutorial</li> <li>Understand basic Nextflow concepts (processes, channels, operators)</li> </ul>"},{"location":"side_quests/metadata/#02-starting-point","title":"0.2. Starting Point","text":"<p>Let's move into the project directory.</p> Navigate to the project directory<pre><code>cd side-quests/metadata\n</code></pre> <p>You can set VSCode to focus on this directory:</p> Open VSCode in current directory<pre><code>code .\n</code></pre> <p>You'll find a <code>data</code> directory containing a samplesheet and a main workflow file.</p> Directory contents<pre><code>&gt; tree\n.\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 bonjour.txt\n\u2502   \u251c\u2500\u2500 ciao.txt\n\u2502   \u251c\u2500\u2500 guten_tag.txt\n\u2502   \u251c\u2500\u2500 hallo.txt\n\u2502   \u251c\u2500\u2500 hello.txt\n\u2502   \u251c\u2500\u2500 hola.txt\n\u2502   \u251c\u2500\u2500 salut.txt\n\u2502   \u2514\u2500\u2500 samplesheet.csv\n\u251c\u2500\u2500 main.nf\n\u2514\u2500\u2500 nextflow.config\n</code></pre> <p>The datasheet contains information about different files and some associated data that we will use in this exercise to tailor our analysis to each file. Each data file contains greetings in different languages, but we don't know what language they are in. The datasheet has 3 columns:</p> <ul> <li><code>id</code>: self-explanatory, an ID given to the file</li> <li><code>character</code>: a character name, that we will use later to draw different creatures</li> <li><code>data</code>: paths to <code>.txt</code> files that contain phrases in different languages</li> </ul> datasheet.csv<pre><code>id,character,recording\nsampleA,squirrel,/workspaces/training/side-quests/metadata/data/bonjour.txt\nsampleB,tux,/workspaces/training/side-quests/metadata/data/guten_tag.txt\nsampleC,sheep,/workspaces/training/side-quests/metadata/data/hallo.txt\nsampleD,turkey,/workspaces/training/side-quests/metadata/data/hello.txt\nsampleE,stegosaurus,/workspaces/training/side-quests/metadata/data/hola.txt\nsampleF,moose,/workspaces/training/side-quests/metadata/data/salut.txt\nsampleG,turtle,/workspaces/training/side-quests/metadata/data/ciao.txt\n</code></pre>"},{"location":"side_quests/metadata/#1-read-in-datasheet","title":"1. Read in datasheet","text":""},{"location":"side_quests/metadata/#11-read-in-datasheet-with-splitcsv","title":"1.1. Read in datasheet with splitCsv","text":"<p>Let's start by reading in the datasheet with <code>splitCsv</code>. In the main workflow file, you'll see that we've already started the workflow:</p> main.nf<pre><code>workflow  {\n\n    ch_samplesheet = Channel.fromPath(\"./data/samplesheet.csv\")\n\n}\n</code></pre> <p>Note</p> <p>Throughout this tutorial, we'll use the <code>ch_</code> prefix for all channel variables to clearly indicate they are Nextflow channels.</p> AfterBefore main.nf<pre><code>    ch_samplesheet = Channel.fromPath(\"./data/samplesheet.csv\")\n        .splitCsv(header: true)\n        .view()\n</code></pre> main.nf<pre><code>    ch_samplesheet = Channel.fromPath(\"./data/samplesheet.csv\")\n</code></pre> <p>We can use the <code>splitCsv</code> operator to split the datasheet into a channel of maps, where each map represents a row from the CSV file.</p> <p>A map is a key-value data structure similar to dictionaries in Python, objects in JavaScript, or hashes in Ruby. For example:</p> <pre><code>// Groovy map\ndef my_map = [id:'sampleA', character:'squirrel']\nprintln my_map.id  // Prints: sampleA\n</code></pre> <p>Try it yourself</p> <p>You can run this example to see how maps look like with:</p> Run map demo example<pre><code>nextflow run examples/map_demo.nf\n</code></pre> <p>The <code>header: true</code> option tells Nextflow to use the first row of the CSV file as the header row, which will be used as keys for the values. Let's see what Nextflow can see after reading with <code>splitCsv</code>. Run the pipeline with the <code>view()</code> operator we added above:</p> <p>Run the pipeline:</p> Read the datasheet<pre><code>nextflow run main.nf\n</code></pre> Read datasheet with splitCsv<pre><code> N E X T F L O W   ~  version 24.10.4\n\nLaunching `main.nf` [exotic_albattani] DSL2 - revision: c0d03cec83\n\n[id:sampleA, character:squirrel, recording:/workspaces/training/side-quests/metadata/data/bonjour.txt]\n[id:sampleB, character:tux, recording:/workspaces/training/side-quests/metadata/data/guten_tag.txt]\n[id:sampleC, character:sheep, recording:/workspaces/training/side-quests/metadata/data/hallo.txt]\n[id:sampleD, character:turkey, recording:/workspaces/training/side-quests/metadata/data/hello.txt]\n[id:sampleE, character:stegosaurus, recording:/workspaces/training/side-quests/metadata/data/hola.txt]\n[id:sampleF, character:moose, recording:/workspaces/training/side-quests/metadata/data/salut.txt]\n[id:sampleG, character:turtle, recording:/workspaces/training/side-quests/metadata/data/ciao.txt]\n</code></pre> <p>We can see that each row from the CSV file has been converted into a map with keys matching the header row. Each map entry corresponds to a column in our datasheet:</p> <ul> <li><code>id</code></li> <li><code>character</code></li> <li><code>recording</code></li> </ul> <p>This format makes it easy to access specific fields from each file. For example, we could access the file ID with <code>id</code> or the txt file path with <code>recording</code>. The output above shows each row from the CSV file converted into a map with keys matching the header row.</p> <p>Let's access a specific column, the <code>character</code> column, that we read in from the datasheet, and print it. We can use the Nextflow <code>map</code> operator to iterate over each item in our channel and return specific entry of our map object:</p> AfterBefore main.nf<pre><code>        .splitCsv(header: true)\n        .map{ row -&gt;\n            row.character\n        }\n        .view()\n</code></pre> main.nf<pre><code>        .splitCsv(header: true)\n        .view()\n</code></pre> <p>and rerun:</p> Print the creatures<pre><code>nextflow run main.nf\n</code></pre> Print the creatures<pre><code>squirrel\ntux\nsheep\nturkey\nstegosaurus\nmoose\nturtle\n</code></pre> <p>Success, we can use the map structures derived from our datasheet to access the values from individual columns for each row.</p> <p>Now that we've successfully read in the datasheet and have access to the data in each row, we can begin implementing our pipeline logic.</p>"},{"location":"side_quests/metadata/#12-separate-metadata-and-data","title":"1.2. Separate metadata and data","text":"<p>In the datasheet, we have both the input files and data about the input files (<code>id</code>, <code>character</code>), the metadata. As we progress through the workflow, we generate more metadata about each file. So far, every column from the datasheet has become a separate item in the channel items we derived using <code>splitCsv()</code>. Every process that consumes this channel would need to be configured with this structure in mind:</p> <pre><code>    input:\n    tuple val(id), val(character), file(recording)\n</code></pre> <p>This means that as soon as somebody added an extra column to the datasheet, the workflow would start producing errors, because the shape of the channel would no longer match what the process expected. It also makes the process hard to share with others who might have slightly different input data, and we end up hard-coding variables into the process that aren't needed by the script block.</p> <p>To avoid this, we need to find a way of keeping the channel structure consistent however many columns that datasheet contains, and we can do that by keeping all the metadata in a single part of the channel tuples we call simply the <code>meta map</code>.</p> <p>Let's use this and separate our metadata from the file path. We'll use the <code>map</code> operator to restructure our channel elements into a tuple consisting of the meta map and file:</p> AfterBefore main.nf<pre><code>    .map { row -&gt;\n            [ [id: row.id, character: row.character], row.recording ]\n    }\n</code></pre> main.nf<pre><code>    .map { row -&gt;\n            row.character\n    }\n</code></pre> <p>Let's run it:</p> View meta map<pre><code>nextflow run main.nf\n</code></pre> View meta map<pre><code> N E X T F L O W   ~  version 24.10.4\n\nLaunching `main.nf` [lethal_booth] DSL2 - revision: 0d8f844c07\n\n[[id:sampleA, character:squirrel], /workspaces/training/side-quests/metadata/data/bonjour.txt]\n[[id:sampleB, character:tux], /workspaces/training/side-quests/metadata/data/guten_tag.txt]\n[[id:sampleC, character:sheep], /workspaces/training/side-quests/metadata/data/hallo.txt]\n[[id:sampleD, character:turkey], /workspaces/training/side-quests/metadata/data/hello.txt]\n[[id:sampleE, character:stegosaurus], /workspaces/training/side-quests/metadata/data/hola.txt]\n[[id:sampleF, character:moose], /workspaces/training/side-quests/metadata/data/salut.txt]\n[[id:sampleG, character:turtle], /workspaces/training/side-quests/metadata/data/ciao.txt]\n</code></pre> <p>Now, each item in the channel has only two items, the metadata map (e.g. <code>[id:sampleA, character:squirrel]</code>) and the data file described by that metadata (e.g. <code>/workspaces/training/side-quests/metadata/data/bonjour.txt</code>).</p> <p>Now we can write processes to consume the channel without hard-coding the metadata items into the input specification:</p> <pre><code>    input:\n    tuple val(meta), file(recording)\n</code></pre> <p>Additional columns in the datasheet will make more metadata available in the <code>meta</code> map, but won't change the channel shape.</p>"},{"location":"side_quests/metadata/#takeaway","title":"Takeaway","text":"<p>In this section, you've learned:</p> <ul> <li>Why metadata is important: Keeping metadata with your data preserves important file information throughout the workflow.</li> <li>How to read in a datasheets: Using <code>splitCsv</code> to read CSV files with header information and transform rows into structured data</li> <li>How to create a meta map: Separating metadata from file data using tuple structure <code>[ [id:value, ...], file ]</code></li> </ul>"},{"location":"side_quests/metadata/#2-manipulating-metadata","title":"2. Manipulating metadata","text":""},{"location":"side_quests/metadata/#21-copying-input-metadata-to-output-channels","title":"2.1. Copying input metadata to output channels","text":"<p>Now we want to process our files with unidentified languages. Let's add a process definition before the <code>workflow</code> that can identify the language in each file:</p> AfterBefore main.nf<pre><code>/*\n * Use langid to predict the language of each input file\n */\nprocess IDENTIFY_LANGUAGE {\n\n    container 'community.wave.seqera.io/library/pip_langid:b2269f456a5629ff'\n\n    input:\n    tuple val(meta), path(file)\n\n    output:\n    tuple val(meta), path(file), stdout\n\n    script:\n    \"\"\"\n    langid &lt; ${file} -l en,de,fr,es,it | sed -E \"s/.*\\\\('([a-z]+)'.*/\\\\1/\" | tr -d '\\\\n'\n    \"\"\"\n}\n\nworkflow {\n</code></pre> main.nf<pre><code>workflow  {\n</code></pre> <p>The tool langid is used for language identification. It comes pre-trained on a set of languages. For a given phrase, it outputs a language prediction and a probability score for each guess to the console. In the <code>script</code> section, we use sed to remove the probability score, clean up the string by removing newline characters, and return only the language prediction. Since the output is printed directly to the console, we use Nextflow\u2019s <code>stdout</code> output qualifier to capture and pass the string as output.</p> <p>Let's include the process, then run, and view it:</p> AfterBefore main.nf<pre><code>            [ [id: row.id, character: row.character], row.recording ]\n        }\n\n    ch_prediction = IDENTIFY_LANGUAGE(ch_samplesheet)\n    ch_prediction.view()\n</code></pre> main.nf<pre><code>            [ [id:row.id, character:row.character], row.recording ]\n        }\n        .view()\n</code></pre> Identify languages<pre><code>nextflow run main.nf\n</code></pre> Identify languages<pre><code> N E X T F L O W   ~  version 24.10.4\n\nLaunching `main.nf` [voluminous_mcnulty] DSL2 - revision: f9bcfebabb\n\nexecutor &gt;  local (7)\n[4e/f722fe] IDENTIFY_LANGUAGE (7) [100%] 7 of 7 \u2714\n[[id:sampleA, character:squirrel], /workspaces/training/side-quests/metadata/work/eb/f7148ebdd898fbe1136bec6a714acb/bonjour.txt, fr]\n[[id:sampleB, character:tux], /workspaces/training/side-quests/metadata/work/16/71d72410952c22cd0086d9bca03680/guten_tag.txt, de]\n[[id:sampleD, character:turkey], /workspaces/training/side-quests/metadata/work/c4/b7562adddc1cc0b7d414ec45d436eb/hello.txt, en]\n[[id:sampleC, character:sheep], /workspaces/training/side-quests/metadata/work/ea/04f5d979429e4455e14b9242fb3b45/hallo.txt, de]\n[[id:sampleF, character:moose], /workspaces/training/side-quests/metadata/work/5a/6c2b84bf8fadb98e28e216426be079/salut.txt, fr]\n[[id:sampleE, character:stegosaurus], /workspaces/training/side-quests/metadata/work/af/ee7c69bcab891c40d0529305f6b9e7/hola.txt, es]\n[[id:sampleG, character:turtle], /workspaces/training/side-quests/metadata/work/4e/f722fe47271ba7ebcd69afa42964ca/ciao.txt, it]\n</code></pre> <p>Neat, for each of our files, we now have a language predicted.</p> <p>Note</p> <p>\"de\" stands for \"deutsch\", the German word for \"german\"</p> <p>You may have noticed something else: we kept the metadata of our files and associated it with our new piece of information. We achieved this by adding the <code>meta</code> map to the output tuple in the process:</p> main.nf<pre><code>output:\ntuple val(meta), path(file), stdout\n</code></pre> <p>This is a useful way to ensure the metadata stays connected with any new information that is generated.</p> <p>Note</p> <p>Another compelling reason to use meta maps in this way is that they make it easier to associate related results that share the same identifiers. As you learned in \"Hello Nextflow\", you can't rely on the order of items in channels to match results across them. Instead, you must use keys to associate data correctly - and meta maps provide an ideal structure for this purpose. We explore this use case in detail in Splitting &amp; Grouping.</p>"},{"location":"side_quests/metadata/#22-using-process-outputs-to-augment-metadata","title":"2.2. Using process outputs to augment metadata","text":"<p>Given that this is more data about the files, let's add it to our meta map. We can use the <code>map</code> operator again to create a new key <code>lang</code> and set the value to the predicted language:</p> AfterBefore main.nf<pre><code>    ch_prediction = IDENTIFY_LANGUAGE(ch_samplesheet)\n    ch_languages = ch_prediction\n        .map { meta, file, lang -&gt;\n            [meta + [lang: lang], file]\n        }\n        .view()\n}\n</code></pre> main.nf<pre><code>    ch_prediction = IDENTIFY_LANGUAGE(ch_samplesheet)\n    ch_prediction.view()\n</code></pre> <p>The <code>map</code> operator takes each channel element and processes it to create a modified version. Inside the closure <code>{ meta, file, lang -&gt; ... }</code>, we then take the existing <code>meta</code> map, create a new map <code>[lang:lang]</code>, and merge both together using <code>+</code>.</p> <p>The <code>+</code> operator in Groovy merges two maps together. So if our original <code>meta</code> was <code>[id:sampleA, character:squirrel]</code>, then <code>meta + [lang:'fr']</code> creates a new map: <code>[id:sampleA, character:squirrel, lang:fr]</code>.</p> <p>Note</p> <p>The <code>+</code> notation with maps creates an entirely new map object, which is what we want. If we'd done something like <code>meta.lang = lang</code> we'd have been modifying the original object, which can lead to unpredictable effects.</p> View new meta map key<pre><code>nextflow run main.nf -resume\n</code></pre> View new meta map key<pre><code> N E X T F L O W   ~  version 24.10.4\n\nLaunching `main.nf` [cheeky_fermat] DSL2 - revision: d096281ee4\n\n[4e/f722fe] IDENTIFY_LANGUAGE (7) [100%] 7 of 7, cached: 7 \u2714\n[[id:sampleA, character:squirrel, lang:fr], /workspaces/training/side-quests/metadata/work/eb/f7148ebdd898fbe1136bec6a714acb/bonjour.txt]\n[[id:sampleB, character:tux, lang:de], /workspaces/training/side-quests/metadata/work/16/71d72410952c22cd0086d9bca03680/guten_tag.txt]\n[[id:sampleC, character:sheep, lang:de], /workspaces/training/side-quests/metadata/work/ea/04f5d979429e4455e14b9242fb3b45/hallo.txt]\n[[id:sampleD, character:turkey, lang:en], /workspaces/training/side-quests/metadata/work/c4/b7562adddc1cc0b7d414ec45d436eb/hello.txt]\n[[id:sampleF, character:moose, lang:fr], /workspaces/training/side-quests/metadata/work/5a/6c2b84bf8fadb98e28e216426be079/salut.txt]\n[[id:sampleE, character:stegosaurus, lang:es], /workspaces/training/side-quests/metadata/work/af/ee7c69bcab891c40d0529305f6b9e7/hola.txt]\n[[id:sampleG, character:turtle, lang:it], /workspaces/training/side-quests/metadata/work/4e/f722fe47271ba7ebcd69afa42964ca/ciao.txt]\n</code></pre> <p>Nice, we expanded our meta map with new information we gathered in the pipeline. After running the language prediction, each element in the output channel looks like this:</p> <pre><code>[meta, file, lang]  // e.g. [[id:sampleA, character:squirrel], bonjour.txt, fr]\n</code></pre>"},{"location":"side_quests/metadata/#23-assign-a-language-group-using-conditionals","title":"2.3. Assign a language group using conditionals","text":"<p>Now that we have our language predictions, let's use the information to assign them into new groups. In our example data, we have provided data sets that belong either to <code>germanic</code> (either English or German) or <code>romance</code> (French, Spanish, Italian) languages.</p> <p>We can add another <code>map</code> operator to assign either group (add this below your last map operation).</p> AfterBefore main.nf<pre><code>        .map { meta, file, lang -&gt;\n            [meta + [lang: lang], file]\n        }\n        .map { meta, file -&gt;\n\n            def lang_group = \"unknown\"\n            if (meta.lang.equals(\"de\") || meta.lang.equals('en')) {\n                lang_group = \"germanic\"\n            }\n            else if (meta.lang in [\"fr\", \"es\", \"it\"]) {\n                lang_group = \"romance\"\n            }\n\n            [meta + [lang_group: lang_group], file]\n        }\n        .view()\n</code></pre> main.nf<pre><code>        .map { meta, file, lang -&gt;\n            [meta + [lang: lang], file]\n        }\n        .view()\n</code></pre> <p>Let's rerun it</p> View language groups<pre><code>nextflow run main.nf -resume\n</code></pre> View language groups<pre><code> N E X T F L O W   ~  version 24.10.4\n\nLaunching `main.nf` [wise_almeida] DSL2 - revision: 46778c3cd0\n\n[da/652cc6] IDENTIFY_LANGUAGE (7) [100%] 7 of 7, cached: 7 \u2714\n[[id:sampleA, character:squirrel, lang:fr, lang_group:romance], /workspaces/training/side-quests/metadata/data/bonjour.txt]\n[[id:sampleB, character:tux, lang:de, lang_group:germanic], /workspaces/training/side-quests/metadata/data/guten_tag.txt]\n[[id:sampleC, character:sheep, lang:de, lang_group:germanic], /workspaces/training/side-quests/metadata/data/hallo.txt]\n[[id:sampleD, character:turkey, lang:en, lang_group:germanic], /workspaces/training/side-quests/metadata/data/hello.txt]\n[[id:sampleE, character:stegosaurus, lang:es, lang_group:romance], /workspaces/training/side-quests/metadata/data/hola.txt]\n[[id:sampleF, character:moose, lang:fr, lang_group:romance], /workspaces/training/side-quests/metadata/data/salut.txt]\n[[id:sampleG, character:turtle, lang:it, lang_group:romance], /workspaces/training/side-quests/metadata/data/ciao.txt]\n</code></pre> <p>Let's understand how this transformation works. The <code>map</code> operator here again takes a closure that processes each element in the channel. Inside the closure, we're using an if-clause to create a new language group classification.</p> <p>Here's what's happening step by step:</p> <ul> <li>Create a new field <code>lang_group</code>: We set a default to <code>unknown</code></li> <li>Extract existing metadata: We access <code>meta.lang</code> (the language we predicted earlier) from the existing meta map</li> <li>Apply conditional logic: We use an if-clause to determine the language group based on the language: Is <code>meta.lang</code> either <code>de</code> or <code>en</code>, we re-assign <code>lang_group</code> to <code>germanic</code>, if <code>fr</code>, <code>es</code>, or <code>it</code>, then we re-assign to <code>romance</code></li> <li>Merge with existing metadata: We use <code>meta + [lang_group:lang_group]</code> in the same way as before to combine the existing meta map with our new field</li> </ul> <p>The resulting channel elements maintain their <code>[meta, file]</code> structure, but the meta map now includes this new classification.</p>"},{"location":"side_quests/metadata/#takeaway_1","title":"Takeaway","text":"<p>In this section, you've learned how to :</p> <ul> <li>Apply input metadata to output channels: Copying metadata in this way allows us to associate results later on based on metadata content.</li> <li>Create custom keys: You created two new keys in your meta map, merging them with <code>meta + [new_key:value]</code> into the existing meta map. One based on a computed value from a process, and one based on a condition you set in the <code>map</code> operator.</li> </ul> <p>These allow you to associate new and existing metadata with files as you progress through your pipeline.</p>"},{"location":"side_quests/metadata/#3-use-meta-map-information-in-a-process","title":"3. Use meta map information in a process","text":"<p>Let's make some fun characters say the phrases from the files our channel. In the hello-nextflow training, you already encountered the <code>cowpy</code> package, a python implementation of a tool called <code>cowsay</code> that generates ASCII art to display arbitrary text inputs in a fun way. We will re-use a process from there.</p> <p>Copy in the process before your workflow block:</p> AfterBefore main.nf<pre><code>/*\n * Generate ASCII art with cowpy\n*/\nprocess COWPY {\n\n    publishDir \"results/\", mode: 'copy'\n\n    container 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n\n    input:\n    tuple val(meta), path(input_file)\n\n    output:\n    tuple val(meta), path(\"cowpy-${input_file}\")\n\n    script:\n    \"\"\"\n    cat $input_file | cowpy -c \"stegosaurus\" &gt; cowpy-${input_file}\n    \"\"\"\n\n}\n\nworkflow{}\n</code></pre> main.nf<pre><code>workflow{\n</code></pre>"},{"location":"side_quests/metadata/#31-use-meta-map-information-in-the-process-definition","title":"3.1. Use meta map information in the process definition","text":"<p>Let's run our files through <code>COWPY</code> and remove our <code>view</code> statement:</p> AfterBefore main.nf<pre><code>            [ meta + [lang_group: lang_group], file ]\n        }\n    COWPY(ch_languages)\n</code></pre> main.nf<pre><code>            [meta + [lang_group: lang_group], file]\n        }\n        .view()\n</code></pre> <p>The process definition as provided would direct results to the <code>results</code> folder, but let's make a tweak to be a little smarter. Given we have been trying to figure out what languages our samples were in, let's group the samples by language in the output directory. Earlier, we added the predicted language to the <code>meta</code> map. We can access this <code>key</code> in the process and use it in the <code>publishDir</code> directive:</p> AfterBefore main.nf<pre><code>process COWPY {\n\n    publishDir \"results/${meta.lang_group}\", mode: 'copy'\n\n    container 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n</code></pre> main.nf<pre><code>process COWPY {\n\n    publishDir \"results/\", mode: 'copy'\n\n    container 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n</code></pre> <p>Let's run this:</p> Use cowpy<pre><code>nextflow run main.nf -resume\n</code></pre> <p>You should now see a new folder called <code>results</code>:</p> Results folder<pre><code>results/\n\u251c\u2500\u2500 germanic\n\u2502   \u251c\u2500\u2500 cowpy-guten_tag.txt\n\u2502   \u251c\u2500\u2500 cowpy-hallo.txt\n\u2502   \u2514\u2500\u2500 cowpy-hello.txt\n\u2514\u2500\u2500 romance\n    \u251c\u2500\u2500 cowpy-bonjour.txt\n    \u251c\u2500\u2500 cowpy-ciao.txt\n    \u251c\u2500\u2500 cowpy-hola.txt\n    \u2514\u2500\u2500 cowpy-salut.txt\n</code></pre> <p>Success! All our phrases are correctly sorted and we now see which of them correspond to which language.</p> <p>Let's take a look at <code>cowpy-salut.txt</code>:</p> cowpy-salut.txt<pre><code> ____________________\n/ Salut, \u00e7a va?      \\\n\\ \u00e0 plus dans le bus /\n --------------------\n\\                             .       .\n \\                           / `.   .' \"\n  \\                  .---.  &lt;    &gt; &lt;    &gt;  .---.\n   \\                 |    \\  \\ - ~ ~ - /  /    |\n         _____          ..-~             ~-..-~\n        |     |   \\~~~\\.'                    `./~~~/\n       ---------   \\__/                        \\__/\n      .'  O    \\     /               /       \\  \"\n     (_____,    `._.'               |         }  \\/~~~/\n      `----.          /       }     |        /    \\__/\n            `-.      |       /      |       /      `. ,~~|\n                ~-.__|      /_ - ~ ^|      /- _      `..-'\n                     |     /        |     /     ~-.     `-. _  _  _\n                     |_____|        |_____|         ~ - . _ _ _ _ _&gt;\n</code></pre> <p>Look through the other files. All phrases should be spoken by the fashionable stegosaurus.</p> <p>How did this work? The <code>publishDir</code> directive is evaluated at runtime when the process executes. Each process task gets its own meta map from the input tuple When the directive is evaluated, <code>${meta.lang_group}</code> is replaced with the actual group language value for that dataset creating the dynamic paths like <code>results/romance</code>.</p>"},{"location":"side_quests/metadata/#32-customize-the-character","title":"3.2. Customize the character","text":"<p>In our datasheet, we have another column: <code>character</code>. To tailor the tool parameters per file, we can also access information from the <code>meta</code> map in the script section. This is really useful in cases where a tool should have different parameters for each file.</p> <p>Let's customize the characters by changing the <code>cowpy</code> command:</p> AfterBefore main.nf<pre><code>    script:\n    \"\"\"\n    cat $input_file | cowpy -c ${meta.character} &gt; cowpy-${input_file}\n    \"\"\"\n</code></pre> main.nf<pre><code>    script:\n    \"\"\"\n    cat $input_file | cowpy -c \"stegosaurus\" &gt; cowpy-${input_file}\n    \"\"\"\n</code></pre> <p>Let's run this:</p> Use cowpy<pre><code>nextflow run main.nf -resume\n</code></pre> <p>and take another look at our french phrase:</p> romance/cowpy-salut.txt<pre><code> ____________________\n/ Salut, \u00e7a va?      \\\n\\ \u00e0 plus dans le bus /\n --------------------\n  \\\n   \\   \\_\\_    _/_/\n    \\      \\__/\n           (oo)\\_______\n           (__)\\       )\\/\\\n               ||----w |\n               ||     ||\n</code></pre> <p>This approach differs from using pipeline parameters (<code>params</code>), which generally apply the same configuration to all files in your workflow. By leveraging metadata applied to each item in a channel, you can fine-tune process behavior on a per-file basis.</p>"},{"location":"side_quests/metadata/#321-exploiting-metadata-at-the-workflow-level","title":"3.2.1. Exploiting metadata at the workflow level","text":"<p>In the example above, by using a property of the meta map in the script block, we introduce a hard requirement on the properties that must be present. Anyone running with a sample sheet that did not contain the <code>character</code> property would encounter an error. The process <code>input:</code> only says that the <code>meta</code> map is required, so someone trying to use this process in another workflow might not notice immediately that the <code>character</code> property was required.</p> <p>A better approach is to make the required metadata property an explicit input rather than accessing it from within the meta map. This makes the requirement clear and provides better error messages. Here's how to refactor the COWPY process:</p> AfterBefore main.nf<pre><code>    input:\n    tuple val(meta), val(character), path(input_file)\n</code></pre> main.nf<pre><code>    input:\n    tuple val(meta), path(input_file)\n</code></pre> <p>Then, at the workflow level, we extract the <code>character</code> property from the metadata and pass it as a separate input:</p> AfterBefore main.nf<pre><code>    COWPY(ch_languages.map{meta, file -&gt; [meta, meta.character, file] )\n</code></pre> main.nf<pre><code>    COWPY(ch_languages)\n</code></pre> <p>Why this approach is better:</p> <ol> <li>Clear Requirements: The process input explicitly shows that <code>character</code> is required</li> <li>Better Error Messages: If <code>character</code> is missing, Nextflow will fail at the input stage with a clear error</li> <li>Self-Documenting: Anyone using this process can immediately see what inputs are needed</li> <li>Reusable: The process is easier to redeploy in other contexts</li> </ol> <p>This highlights an important design principle: Use the meta map for optional, descriptive information, but extract required values as explicit inputs. The meta map is excellent for keeping channel structures clean and preventing arbitrary channel structures, but for mandatory elements that are directly referenced in a process, extracting them as explicit inputs creates more robust and maintainable code.</p>"},{"location":"side_quests/metadata/#takeaway_2","title":"Takeaway","text":"<p>In this section, you've learned how to:</p> <ul> <li> <p>Tweak directives using meta values: Using meta map values in <code>publishDir</code> directives to create dynamic output paths based on the file's metadata</p> </li> <li> <p>Tweak the script section based on meta values: Customizing tool parameters per file using meta information in the <code>script</code> section</p> </li> </ul>"},{"location":"side_quests/metadata/#summary","title":"Summary","text":"<p>In this side quest, you've explored how to effectively work with metadata in Nextflow workflows. This pattern of keeping metadata explicit and attached to the data is a core best practice in Nextflow that enables building robust, maintainable bioinformatics workflows.</p> <p>Here's what you've learned:</p> <ol> <li> <p>Reading and Structuring Metadata: Reading CSV files and creating organized metadata maps that stay associated with your data files</p> </li> <li> <p>Expanding Metadata During Workflow: Adding new information to your metadata as your pipeline progresses by adding process outputs and deriving values through conditional logic</p> </li> <li> <p>Customizing Process Behavior: Using metadata to adapt how processes handle different files</p> </li> </ol> <p>This approach offers several advantages over hardcoding file information:</p> <ul> <li>File metadata stays associated with files throughout the workflow</li> <li>Process behavior can be customized per file</li> <li>Output organization can reflect file metadata</li> <li>File information can be expanded during pipeline execution</li> </ul>"},{"location":"side_quests/metadata/#key-concepts","title":"Key Concepts","text":"<ul> <li>Reading Datasheets &amp; creating meta maps</li> </ul> <pre><code>Channel.fromPath('samplesheet.csv')\n  .splitCsv(header: true)\n  .map { row -&gt;\n      [ [id:row.id, character:row.character], row.recording ]\n  }\n</code></pre> <ul> <li> <p>Adding new keys to the meta map</p> </li> <li> <p>based on process output:</p> </li> </ul> <pre><code>.map { meta, file, lang -&gt;\n  [ meta + [lang:lang], file ]\n}\n</code></pre> <ol> <li>and using a conditional clause</li> </ol> <pre><code>.map{ meta, file -&gt;\n    if ( meta.lang.equals(\"de\") || meta.lang.equals('en') ){\n        lang_group = \"germanic\"\n    } else if ( meta.lang in [\"fr\", \"es\", \"it\"] ) {\n        lang_group = \"romance\"\n    } else {\n        lang_group = \"unknown\"\n    }\n}\n</code></pre> <ul> <li>Using meta values in Process Directives</li> </ul> <pre><code>publishDir \"results/${meta.lang_group}\", mode: 'copy'\n</code></pre> <ul> <li>Adapting tool parameters for individual files</li> </ul> <pre><code>cat $input_file | cowpy -c ${meta.character} &gt; cowpy-${input_file}\n</code></pre>"},{"location":"side_quests/metadata/#resources","title":"Resources","text":"<ul> <li>map</li> <li>stdout</li> </ul>"},{"location":"side_quests/nf-core/","title":"Introduction to nf-core","text":"<p>nf-core is a community effort to develop and maintain a curated set of analysis pipelines built using Nextflow. It was created by several core facilities wanting to consolidate their analysis development and is governed by community members from academia and industry. It is an open community that anyone can join and contribute to.</p> <p></p> <p>nf-core provides a standardized set of best practices, guidelines, and templates for building and sharing scientific pipelines. These pipelines are designed to be modular, scalable, and portable, allowing researchers to easily adapt and execute them using their own data and compute resources.</p> <p>One of the key benefits of nf-core is that it promotes open development, testing, and peer review, ensuring that the pipelines are robust, well-documented, and validated against real-world datasets. This helps to increase the reliability and reproducibility of scientific analyses and ultimately enables researchers to accelerate their scientific discoveries.</p> <p>nf-core is published in Nature Biotechnology: Nat Biotechnol 38, 276\u2013278 (2020). Nature Biotechnology. An updated preprint is available at bioRxiv.</p> <p>In this tutorial you will explore using and writing nf-core pipelines:</p> <ul> <li>Section 1: Run nf-core pipeline   In the first section, you will learn where you can find information about a particular nf-core pipeline and how to run one with provided test data.</li> <li>Section 2: Develop an nf-core-like pipeline   In second section, you will use a simplified version of the nf-core template to write a nf-core-style pipeline. The pipeline consists of two modules to process FastQ data: <code>fastqe</code> and <code>seqtk</code>. It uses an input from a sample sheet, validates it, and produces a multiqc report.</li> </ul>"},{"location":"side_quests/nf-core/#0-warmup","title":"0. Warmup","text":"<p>Let's move into the project directory.</p> <pre><code>cd side-quests/nf-core\n</code></pre> <p>The <code>nf-core</code> directory has the file content like:</p> Directory contents<pre><code>nf-core\n\u2514\u2500\u2500 data\n  \u2514\u2500\u2500 sequencer_samplesheet.csv\n</code></pre> <p>We will first run a pipeline in this directory and then build our own. We need the <code>sequencer_samplesheet.csv</code> for part 2. For now you can ignore it.</p>"},{"location":"side_quests/nf-core/#1-run-nf-core-pipelines","title":"1. Run nf-core pipelines","text":"<p>nf-core uses their website nf-co.re to centrally display all information such as: general documentation and help articles, documentation for each of its pipelines, blog posts, event annoucenments, etc..</p>"},{"location":"side_quests/nf-core/#11-nf-core-website","title":"1.1 nf-core website","text":"<p>Each released pipeline has a dedicated page that includes 6 documentation sections:</p> <ul> <li>Introduction: An introduction and overview of the pipeline</li> <li>Usage: Descriptions of how to execute the pipeline</li> <li>Parameters: Grouped pipeline parameters with descriptions</li> <li>Output: Descriptions and examples of the expected output files</li> <li>Results: Example output files generated from the full test dataset</li> <li>Releases &amp; Statistics: Pipeline version history and statistics</li> </ul> <p>You should read the pipeline documentation carefully to understand what a given pipeline does and how it can be configured before attempting to run it.</p> <p>Go to the nf-core website and find the documentation for the nf-core/demo pipeline.</p> <p>Find out:</p> <ul> <li>which tools the pipeline will run (Check the tab: <code>Introduction</code>)</li> <li>which parameters the pipeline has (Check the tab: <code>Parameters</code>)</li> <li>what the output files (Check the tab: <code>Output</code>)</li> </ul>"},{"location":"side_quests/nf-core/#takeaway","title":"Takeaway","text":"<p>You know where to find information about a particular nf-core pipeline: where to find general information, where the parameters are described, and where you can find a description on the output that the pipelines produce.</p>"},{"location":"side_quests/nf-core/#whats-next","title":"What's next?","text":"<p>Next, we'll show you how to run your first nf-core pipeline.</p>"},{"location":"side_quests/nf-core/#12-running-an-nf-core-pipeline","title":"1.2 Running an nf-core pipeline","text":"<p>Let's start by creating a new subdirectory to run the pipeline in:</p> <pre><code>mkdir nf-core-demo\ncd nf-core-demo\n</code></pre> <p>Tip</p> <p>You can run this from anywhere, but by creating a new folder all logs and output files that will be generated are bundled in one place.</p> <p>Whenever you're ready, run the command:</p> <pre><code>nextflow pull nf-core/demo\n</code></pre> <p>Nextflow will <code>pull</code> the pipeline code.</p> Output<pre><code>Checking nf-core/demo ...\n downloaded from https://github.com/nf-core/demo.git - revision: 04060b4644 [master]\n</code></pre> <p>To be clear, you can do this with any Nextflow pipeline that is appropriately set up in GitHub, not just nf-core pipelines. However nf-core is the largest open curated collection of Nextflow pipelines.</p> <p>Now that we've got the pipeline pulled, we can try running it!</p>"},{"location":"side_quests/nf-core/#121-trying-out-an-nf-core-pipeline-with-the-test-profile","title":"1.2.1 Trying out an nf-core pipeline with the test profile","text":"<p>Conveniently, every nf-core pipeline comes with a <code>test</code> profile. This is a minimal set of configuration settings for the pipeline to run using a small test dataset that is hosted on the nf-core/test-datasets repository. It's a great way to try out a pipeline at small scale.</p> <p>The <code>test</code> profile for <code>nf-core/demo</code> is shown below:</p> conf/test.config<pre><code>/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    Nextflow config file for running minimal tests\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    Defines input files and everything required to run a fast and simple pipeline test.\n\n    Use as follows:\n        nextflow run nf-core/demo -profile test,&lt;docker/singularity&gt; --outdir &lt;OUTDIR&gt;\n\n----------------------------------------------------------------------------------------\n*/\n\nprocess {\n    resourceLimits = [\n        cpus: 4,\n        memory: '15.GB',\n        time: '1.h'\n    ]\n}\n\nparams {\n    config_profile_name        = 'Test profile'\n    config_profile_description = 'Minimal test dataset to check pipeline function'\n\n    // Input data\n    input  = 'https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/samplesheet/samplesheet_test_illumina_amplicon.csv'\n\n}\n</code></pre> <p>This tells us that the <code>nf-core/demo</code> <code>test</code> profile already specifies the input parameter, so you don't have to provide any input yourself. However, the <code>outdir</code> parameter is not included in the <code>test</code> profile, so you have to add it to the execution command using the <code>--outdir</code> flag.</p> <p>Here, we're also going to specify <code>-profile docker</code>, which by nf-core convention enables the use of Docker.</p> <p>Lets' try it!</p> <pre><code>nextflow run nf-core/demo -profile docker,test --outdir results\n</code></pre> <p>Here's the console output from the pipeline:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `https://github.com/nf-core/demo` [maniac_jones] DSL2 - revision: 04060b4644 [master]\n\n\n------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/demo 1.0.1\n------------------------------------------------------\nInput/output options\n  input                     : https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/samplesheet/samplesheet_test_illumina_amplicon.csv\n  outdir                    : results\n\nInstitutional config options\n  config_profile_name       : Test profile\n  config_profile_description: Minimal test dataset to check pipeline function\n\nCore Nextflow options\n  revision                  : master\n  runName                   : maniac_jones\n  containerEngine           : docker\n  launchDir                 : /workspaces/training/side-quests/nf-core/nf-core-demo\n  workDir                   : /workspaces/training/side-quests/nf-core/nf-core-demo/work\n  projectDir                : /workspaces/.nextflow/assets/nf-core/demo\n  userName                  : gitpod\n  profile                   : docker,test\n  configFiles               :\n\n!! Only displaying parameters that differ from the pipeline defaults !!\n------------------------------------------------------* The pipeline\n  https://doi.org/10.5281/zenodo.12192442\n\n* The nf-core framework\n    https://doi.org/10.1038/s41587-020-0439-x\n\n* Software dependencies\n    https://github.com/nf-core/demo/blob/master/CITATIONS.md\n\nexecutor &gt;  local (7)\n[3c/a00024] NFC\u2026_DEMO:DEMO:FASTQC (SAMPLE2_PE) | 3 of 3 \u2714\n[94/d1d602] NFC\u2026O:DEMO:SEQTK_TRIM (SAMPLE2_PE) | 3 of 3 \u2714\n[ab/460670] NFCORE_DEMO:DEMO:MULTIQC           | 1 of 1 \u2714\n-[nf-core/demo] Pipeline completed successfully-\nCompleted at: 05-Mar-2025 09:46:21\nDuration    : 1m 54s\nCPU hours   : (a few seconds)\nSucceeded   : 7\n</code></pre> <p>Isn't that neat?</p> <p>You can also explore the <code>results</code> directory produced by the pipeline.</p> Output<pre><code>results/\n\u251c\u2500\u2500 fastqc\n\u2502   \u251c\u2500\u2500 SAMPLE1_PE\n\u2502   \u251c\u2500\u2500 SAMPLE2_PE\n\u2502   \u2514\u2500\u2500 SAMPLE3_SE\n\u251c\u2500\u2500 fq\n\u2502   \u251c\u2500\u2500 SAMPLE1_PE\n\u2502   \u251c\u2500\u2500 SAMPLE2_PE\n\u2502   \u2514\u2500\u2500 SAMPLE3_SE\n\u251c\u2500\u2500 multiqc\n\u2502   \u251c\u2500\u2500 multiqc_data\n\u2502   \u251c\u2500\u2500 multiqc_plots\n\u2502   \u2514\u2500\u2500 multiqc_report.html\n\u2514\u2500\u2500 pipeline_info\n    \u251c\u2500\u2500 execution_report_2025-03-05_09-44-26.html\n    \u251c\u2500\u2500 execution_timeline_2025-03-05_09-44-26.html\n    \u251c\u2500\u2500 execution_trace_2025-03-05_09-44-26.txt\n    \u251c\u2500\u2500 nf_core_pipeline_software_mqc_versions.yml\n    \u251c\u2500\u2500 params_2025-03-05_09-44-29.json\n    \u2514\u2500\u2500 pipeline_dag_2025-03-05_09-44-26.html\n</code></pre> <p>If you're curious about what that all means, check out the nf-core/demo pipeline documentation page!</p> <p>And that's all you need to know for now. Congratulations! You have now run your first nf-core pipeline.</p>"},{"location":"side_quests/nf-core/#takeaway_1","title":"Takeaway","text":"<p>You know how to run an nf-core pipeline using its built-in test profile.</p>"},{"location":"side_quests/nf-core/#whats-next_1","title":"What's next?","text":"<p>Celebrate and take a break! Next, we'll show you how to use nf-core tooling to build your own pipeline.</p>"},{"location":"side_quests/nf-core/#2-create-a-basic-pipeline-from-template","title":"2. Create a basic pipeline from template","text":"<p>We will now start developing our own nf-core style pipeline. The nf-core collection currently offers, 72 subworkflows and over 1300 modules that you can use to build your own pipelines. Subworkflows are 'composable' workflows, such as those you may have encountered in the Workflows of workflows side quest, providing ready-made chunks of logic you can you can use in your own worklfows.</p> <p>The nf-core community provides a command line tool with helper functions to use and develop pipelines, including to install those components.</p> <p>We have pre-installed nf-core tools, and here, we will use them to create and develop a new pipeline.</p> <p>View all of the tooling using the <code>nf-core --help</code> argument.</p> <pre><code>nf-core --help\n</code></pre>"},{"location":"side_quests/nf-core/#21-creating-your-pipeline","title":"2.1 Creating your pipeline","text":"<p>Before we start, let's create a new subfolder in the current <code>nf-core</code> directory:</p> <pre><code>cd ..\nmkdir nf-core-pipeline\ncd nf-core-pipeline\n</code></pre> <p>Open a new window in VSCode</p> <p>If you are working with VS Code you can open a new window to reduce visual clutter:</p> <pre><code>code .\n</code></pre> <p>Let's start by creating a new pipeline with the <code>nf-core pipelines create</code> command:</p> <p>All nf-core pipelines are based on a common template, a standardized pipeline skeleton that can be used to streamline development with shared features and components.</p> <p>The <code>nf-core pipelines create</code> command creates a new pipeline using the nf-core base template with a pipeline name, description, and author. It is the first and most important step for creating a pipeline that will integrate with the wider Nextflow ecosystem.</p> <pre><code>nf-core pipelines create\n</code></pre> <p>Running this command will open a Text User Interface (TUI) for pipeline creation.</p> <p>Template features can be flexibly included or excluded at the time of creation, follow these steps create your first pipeline using the <code>nf-core pipelines create</code> TUI:</p> <ol> <li>Run the <code>nf-core pipelines create</code> command</li> <li>Select Let's go! on the welcome screen</li> <li>Select Custom on the Choose pipeline type screen</li> <li> <p>Enter your pipeline details, replacing &lt; YOUR NAME &gt; with your own name, then select Next</p> </li> <li> <p>GitHub organisation: myorg</p> </li> <li>Workflow name: myfirstpipeline</li> <li>A short description of your pipeline: My first pipeline</li> <li> <p>Name of the main author / authors: &lt; YOUR NAME &gt;</p> </li> <li> <p>On the Template features screen, set \"Toggle all features\" to off, then enable:</p> </li> <li> <p><code>Add configuration files</code></p> </li> <li><code>Use multiqc</code></li> <li><code>Use nf-core components</code></li> <li><code>Use nf-schema</code></li> <li><code>Add documentation</code></li> <li> <p><code>Add testing profiles</code></p> </li> <li> <p>Select Finish on the Final details screen</p> </li> <li>Wait for the pipeline to be created, then select Continue</li> <li>Select Finish without creating a repo on the Create GitHub repository screen</li> <li>Select Close on the HowTo create a GitHub repository page</li> </ol> <p>If run successfully, you will see a new folder in your current directory named <code>myorg-myfirstpipeline</code>.</p>"},{"location":"side_quests/nf-core/#211-testing-your-pipeline","title":"2.1.1 Testing your pipeline","text":"<p>Let's try to run our new pipeline:</p> <pre><code>cd myorg-myfirstpipeline\nnextflow run . -profile docker,test --outdir results\n</code></pre> <p>The pipeline should run successfully!</p> <p>Here's the console output from the pipeline:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `./main.nf` [infallible_kilby] DSL2 - revision: fee0bcf390\n\nDownloading plugin nf-schema@2.3.0\nInput/output options\n  input                     : https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/samplesheet/samplesheet_test_illumina_amplicon.csv\n  outdir                    : results\n\nInstitutional config options\n  config_profile_name       : Test profile\n  config_profile_description: Minimal test dataset to check pipeline function\n\nGeneric options\n  trace_report_suffix       : 2025-03-05_10-17-59\n\nCore Nextflow options\n  runName                   : infallible_kilby\n  containerEngine           : docker\n  launchDir                 : /workspaces/training/side-quests/nf-core/nf-core-pipeline/myorg-myfirstpipeline\n  workDir                   : /workspaces/training/side-quests/nf-core/nf-core-pipeline/myorg-myfirstpipeline/work\n  projectDir                : /workspaces/training/side-quests/nf-core/nf-core-pipeline/myorg-myfirstpipeline\n  userName                  : gitpod\n  profile                   : docker,test\n  configFiles               : /workspaces/training/side-quests/nf-core/nf-core-pipeline/myorg-myfirstpipeline/nextflow.config\n\n!! Only displaying parameters that differ from the pipeline defaults !!\n------------------------------------------------------\nexecutor &gt;  local (1)\n[02/510003] MYO\u2026PELINE:MYFIRSTPIPELINE:MULTIQC | 1 of 1 \u2714\n-[myorg/myfirstpipeline] Pipeline completed successfully-\n</code></pre> <p>Let's dissect what we are seeing.</p> <p>The nf-core pipeline template is a working pipeline and comes preconfigured with some modules. Here, we only run MultiQC</p> <p>At the top, you see all parameters displayed that differ from the pipeline defaults. Most of these are default or were set by applying the <code>test</code> profile.</p> <p>Additionally we used the <code>docker</code> profile to use docker for software packaging. nf-core provides this as a profile for convenience to enable the docker feature but we could do it with configuration as we did with the earlier module.</p>"},{"location":"side_quests/nf-core/#212-template-tour","title":"2.1.2 Template tour","text":"<p>The nf-core pipeline template comes packed with a lot of files and folders. While creating the pipeline, we selected a subset of the nf-core features. The features we selected are now included as files and directories in our repository.</p> <p>While the template may feel overwhelming, a complete understanding isn't required to start developing your pipeline. Let's look at the important places that we need to touch during pipeline development.</p>"},{"location":"side_quests/nf-core/#workflows-subworkflows-and-modules","title":"Workflows, subworkflows, and modules","text":"<p>The nf-core pipeline template has a <code>main.nf</code> script that calls <code>myfirstpipeline.nf</code> from the <code>workflows</code> folder. The <code>myfirstpipeline.nf</code> file inside the workflows folder is the central pipeline file that is used to bring everything else together.</p> <p>Instead of having one large monolithic pipeline script, it's broken up into smaller script components, namely, modules and subworkflows:</p> <ul> <li>Modules: Wrappers around a single process</li> <li>Subworkflows: Two or more modules that are packaged together as a mini workflow</li> </ul> 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 InputsMODULE_1SUBWORKFLOW_1OutputsMODULE_4MODULE_2MODULE_3WORKFLOWSUBWORKFLOW_1MODULE_2modules.configtool argumentspublishingoutput namesbase.configcompute resourceserror strategiesprocess MODULE_2 {    label \"process low\"     input:    tuple val(meta), path(fasta)     output:    tuple val(meta), path(fai)     when:    task.ext.when == null || task.ext.when     script:    def args = task.ext.args ?: ''    \"\"\"    my-function $args -i $fasta -o $fai    \"\"\" } <p>Within your pipeline repository, <code>modules</code> and <code>subworkflows</code> are stored within <code>local</code> and <code>nf-core</code> folders. The <code>nf-core</code> folder is for components that have come from the nf-core GitHub repository while the <code>local</code> folder is for components that have been developed independently (usually things very specific to a pipeline):</p> <pre><code>modules/\n\u251c\u2500\u2500 local\n\u2502   \u2514\u2500\u2500 &lt;toolname&gt;.nf\n\u2502   .\n\u2502\n\u2514\u2500\u2500 nf-core\n    \u251c\u2500\u2500 &lt;tool name&gt;\n    \u2502   \u251c\u2500\u2500 environment.yml\n    \u2502   \u251c\u2500\u2500 main.nf\n    \u2502   \u251c\u2500\u2500 meta.yml\n    \u2502   \u2514\u2500\u2500 tests\n    \u2502       \u251c\u2500\u2500 main.nf.test\n    \u2502       \u251c\u2500\u2500 main.nf.test.snap\n    \u2502       \u2514\u2500\u2500 tags.yml\n    .\n</code></pre> <p>Modules from nf-core follow a similar structure and contain a small number of additional files for testing using nf-test and documentation about the module.</p> <p>Note</p> <p>Some nf-core modules are also split into command specific directories:</p> <pre><code>\u2502\n\u2514\u2500\u2500 &lt;tool name&gt;\n    \u2514\u2500\u2500 &lt;command&gt;\n        \u251c\u2500\u2500 environment.yml\n        \u251c\u2500\u2500 main.nf\n        \u251c\u2500\u2500 meta.yml\n        \u2514\u2500\u2500 tests\n            \u251c\u2500\u2500 main.nf.test\n            \u251c\u2500\u2500 main.nf.test.snap\n            \u251c\u2500\u2500 nextflow.config\n            \u2514\u2500\u2500 tags.yml\n</code></pre> <p>Note</p> <p>The nf-core template does not come with a local modules folder by default.</p>"},{"location":"side_quests/nf-core/#configuration-files","title":"Configuration files","text":"<p>The nf-core pipeline template utilizes Nextflow's flexible customization options and has a series of configuration files throughout the template.</p> <p>In the template, the <code>nextflow.config</code> file is a central configuration file and is used to set default values for parameters and other configuration options. The majority of these configuration options are applied by default while others (e.g., software dependency profiles) are included as optional profiles.</p> <p>There are several configuration files that are stored in the <code>conf</code> folder and are added to the configuration by default or optionally as profiles:</p> <ul> <li><code>base.config</code>: A 'blank slate' config file, appropriate for general use on most high-performance computing environments. This defines broad bins of resource usage, for example, which are convenient to apply to modules.</li> <li><code>modules.config</code>: Additional module directives and arguments.</li> <li><code>test.config</code>: A profile to run the pipeline with minimal test data.</li> <li><code>test_full.config</code>: A profile to run the pipeline with a full-sized test dataset.</li> </ul>"},{"location":"side_quests/nf-core/#nextflow_schemajson","title":"<code>nextflow_schema.json</code>","text":"<p>The <code>nextflow_schema.json</code> is a file used to store parameter related information including type, description and help text in a machine readable format. The schema is used for various purposes, including automated parameter validation, help text generation, and interactive parameter form rendering in UI interfaces.</p>"},{"location":"side_quests/nf-core/#assetsschema_inputjson","title":"<code>assets/schema_input.json</code>","text":"<p>The <code>schema_input.json</code> is a file used to define the input samplesheet structure. Each column can have a type, pattern, description and help text in a machine readable format. The schema is used for various purposes, including automated validation, and providing helpful error messages.</p>"},{"location":"side_quests/nf-core/#takeaway_2","title":"Takeaway","text":"<p>You used the nf-core tooling to create a template pipeline. You customized it with components you wanted to use for this pipeline focusing on a handful important ones. You also learned about each of the pieces you have installed and have a general idea of the locations of important files. Lastly, you checked that the template pipeline works by running it as is.</p>"},{"location":"side_quests/nf-core/#whats-next_2","title":"What's next?","text":"<p>Congratulations and take a break! In the next step, we will investigate the default input data, that the pipeline comes with.</p>"},{"location":"side_quests/nf-core/#22-check-the-input-data","title":"2.2 Check the input data","text":"<p>Above, we said that the <code>test</code> profile comes with small test files that are stored in the nf-core. Let's check what type of files we are dealing with to plan our expansion. Remember that we can inspect any channel content using the <code>view</code> operator:</p> workflows/myfirstpipeline.nf<pre><code>ch_samplesheet.view()\n</code></pre> <p>Note</p> <p>nf-core is making heavy use of more complex workflow encapsulation. The <code>main.nf</code> that you used in the hello-series imports and calls the workflow in the file <code>workflows/myfirstpipeline.nf</code>. This is the file we will work in today.</p> <p>and the run command:</p> <pre><code>nextflow run . -profile docker,test --outdir results\n</code></pre> <p>The output should look like the below. We see that we have FASTQ files as input and each set of files is accompanied by some metadata: the <code>id</code> and whether or not they are single end:</p> Output<pre><code>[['id':'SAMPLE1_PE', 'single_end':false], [/nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R1.fastq.gz, /nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R2.fastq.gz]]\n[['id':'SAMPLE2_PE', 'single_end':false], [/nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R1.fastq.gz, /nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R2.fastq.gz]]\n[['id':'SAMPLE3_SE', 'single_end':true], [/nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R1.fastq.gz, /nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R1.fastq.gz]]\n</code></pre> <p>You can comment the <code>view</code> statement for now. We will use it later during this training to inspect the channel content again.</p>"},{"location":"side_quests/nf-core/#takeaway_3","title":"Takeaway","text":"<p>The pipeline template comes with a default samplesheet. You learned what is part of this samplesheet so you can use it in the next steps when we want to add and run modules in the pipeline.</p>"},{"location":"side_quests/nf-core/#whats-next_3","title":"What's next?","text":"<p>In the next step you will start adding your first nf-core module to the pipeline: seqtk.</p>"},{"location":"side_quests/nf-core/#23-add-an-nf-core-module","title":"2.3 Add an nf-core module","text":"<p>nf-core provides a large library of modules and subworkflows: pre-made nextflow wrappers around tools that can be installed into nextflow pipelines. They are designed to be flexible but may require additional configuration to suit different use cases.</p> <p>Currently, there are more than 1400 nf-core modules and 70 nf-core subworkflows (March 2025) available. Modules and subworkflows can be listed, installed, updated, removed, and patched using nf-core tooling.</p> <p>While you could develop a module for this tool independently, you can save a lot of time and effort by leveraging nf-core modules and subworkflows.</p> <p>Let's see which modules are available:</p> <pre><code>nf-core modules list remote\n</code></pre> <p>This command lists all currently available modules, &gt; 1400. An easier way to find them is to go to the nf-core website and visit the modules subpage https://nf-co.re/modules. Here you can search for modules by name or tags, find documentation for each module, and see which nf-core pipeline are using the module:</p> <p></p>"},{"location":"side_quests/nf-core/#231-install-an-nf-core-module","title":"2.3.1 Install an nf-core module","text":"<p>Now let's add another tool to the pipeline.</p> <p><code>Seqtk</code> is a fast and lightweight tool for processing sequences in the FASTA or FASTQ format. Here, you will use the <code>seqtk trim</code> command to trim FASTQ files.</p> <p>In your pipeline, you will add a new step that will take FASTQ files from the sample sheet as inputs and will produce trimmed fastq files that can be used as an input for other tools and version information about the seqtk tools to mix into the inputs for the MultiQC process.</p> eyJ2ZXJzaW9uIjoiMSIsImVuY29kaW5nIjoiYnN0cmluZyIsImNvbXByZXNzZWQiOnRydWUsImVuY29kZWQiOiJ4nOVcXGlT21pcdTAwMTL9zq+geF/DfXdfXtXUXHUwMDE0kJBHgLBcdTAwMGYhM68oYVx1MDAwYlsgW8ZcdTAwMTaLeZX/PudcbrDkRcZcdTAwMDSbmFx1MDAxOacqgNbWvd19zunb8t9cdTAwMGKLi0tpt1x1MDAxNS79sbhcdTAwMTTeVYI4qraD26VcdTAwMGZ++03Y7kRJXHUwMDEzu3j2dye5bleyI+tp2ur88fvvjaB9XHUwMDE5pq04qITkJupcXFx1MDAwN3Enva5GXHSpJI3fozRsdP7p//9cdTAwMWE0wn+0kkY1bZP8JsthNUqT9sO9wjhshM20g6v/XHUwMDFify8u/p39X7AuaLeTXHUwMDA3w7LNuXFMOzO4+WvSzCxljimlnGG9XHUwMDAzos5H3CtccqvYe1x1MDAwZXvDfI/ftHSztrPz59ZVY0verp1vtNXGxf7ubn7b8yiOXHUwMDBm0m6cmdRJ8CT5vk7aTi7D46ia1p/GrLC9d1Y16NTDwmnt5LpWb4ZcdTAwMWT/7LmdSSuoRGnXb6O0tzVo1rKL5Fvu8JdcdTAwMTaCOOEsY1ZqzmV+a38+15RoI7i2wkolXHUwMDA2zFpLYsxcdTAwMDHM+o2F/l9u11lQuazBuGa1d0zaXHUwMDBlmp1W0MZM5cfdPj4wJVTCXHUwMDAw3EVZY7VWxqneQfUwqtVTXHUwMDFjZShhSjMrqFx1MDAxNZJcbpebXHUwMDEzZnOiqNDMyNxMb0Jro5p5xl/5PLThU1x1MDAxYv6E5nVcdTAwMWNcdTAwMTfHsll9XHUwMDFjyydcdTAwMGbKfYg/bvmRP6M//lPB9/I7XFy3qsGDmzBcZlx1MDAxZbOKXHRleW5XXHUwMDFjNS9cdTAwMDdvXHUwMDFmJ5XLXHUwMDExntVJg3a6XHUwMDFhNatRs9Zv2KPbZ1x1MDAwZrJcdTAwMTR/vmx/Pa3RU1x1MDAxYdePrtfa+9zs1nvj7Fx1MDAxZjqpXFx3spGGOdpRozHgmFonbOGoWtDyRlx1MDAwYsKoUVopIbWlhoqhh1x1MDAwZpvV54266nzd//ZFd7nZvErXltdvbKrdKKOWMf9cdTAwMWH+Z1x1MDAxZFdcdTAwMDJRx1x1MDAxZFx1MDAxZrbKXHUwMDExI63TynEpJbfcXHUwMDBlXHUwMDE5XHUwMDE1XHUwMDA3nXQtaTSiXHUwMDE0g7+bRM10cJCz0Vxc8dmgXHUwMDFlXHUwMDA2Q1x1MDAxZYCHKu5cdTAwMWJMXHUwMDFiLX/FPMv4T/7bYlx1MDAxZVfZXHUwMDFmvd//+jD66DFcdTAwMWXvP4O+nl9vofjz8eGHMl47rKRcdTAwMGZcdTAwMDE/XCLrYaTLkp6kljqjae6rzyW99W9cdJd2vfNl9V98a//LrmLV6GqqSW/grNfnPGFccqHOSqaclorqPJH4850mWmppXHJC13ArzYBd08t6imtiXGbCUVwitzI+XCLjMc2IXHUwMDA0XHUwMDBlXHRHKfNGycGcp1x1MDAxMceG2XwgppjzhsP+RTlcdTAwMGbWsnxSn895Y1x1MDAxZDpccu/SUb6sy31cdTAwMTmQxqlcdTAwMTJUuomdebe7kn5u3UfVe9fe+2678cHR3fZ8OzO8h3CjpNDUOimY6XdmR7Rz0lwiw3ChXHUwMDE0UzNzZjgpqFx1MDAwMqKKUiQ0RzVcdTAwMWZ254KLP7qvRYg5VjD6p923b8eEfipcdTAwMTlFLmScUfVcdTAwMDI/za1KmulBdJ/RXHUwMDAz2rd1PWhEcbdvTjNcdTAwMDf2XHUwMDBlXHUwMDEwNFpxXGJcdTAwMWFcdTAwMTempNK5Weo7YCWOas1cZs/D8353TyNw397uNGnleyu4YVx1MDAxMDXD9vBcdTAwMDAl7ahcdTAwMTY1g/jwuZvjqcM/e1mHXHUwMDE0Juos6IR+r99ux4ZoXHUwMDE4x1GrM1x1MDAxMnFcZpODW5+ilFMqXGayMM994Lko/Xh2d7jJ9+5XWlHDhqe39GBn82zOo1RxXHUwMDAwvtBcdTAwMTZU2jLh8qTlz0f0XHUwMDEy51mNXHUwMDAwXHUwMDFliVwi9516lFx1MDAwMt2stVTAZGVVgf3lUaphKuBISMRcdTAwMDdCw1x1MDAxNUbpIWihmij4ipBTXGLbn2LavXPys3PnuF0/Y3rTdG9F7dPOQeVg5ehmp0A6XHUwMDE3XHUwMDA3ZWFvz49cdTAwMGbjrmtqcT3+uL3R3Fxu9r5cdTAwMDabe9/0nlxcnuy6T4xtXGZWgum9XHUwMDA1VrrSKJRcdTAwMWOB6NjkULmR3ESfU324Kb8lSlx1MDAwNbryPVi9mO8g1JRcdTAwMTPDjIGAgFC0WvVcdTAwMDchuJY21Fx1MDAwMVx1MDAxNLSAXHUwMDAw0TNcdTAwMGJCp1xiXHUwMDE4nZNGea6tJkNKXHUwMDBlXHUwMDFkXHUwMDA2Xl6Ywl9cdTAwMDSV+lx1MDAwNW76OqjcPto63Nhbe1uIXHUwMDFjuuk0oDE7akREMltcdTAwMWWRRlJcdTAwMDQlm7z8VKuKQ3axvLd2cbr8sfWtq4Pkjs13RFwizoh3bErBTlxyXHUwMDFiiEjGLVx1MDAxMU5wLqF9tGBcdTAwMDNmTVGIUUOgolx1MDAwMHlcZnpcdTAwMWKoOFx1MDAxY5C53U/M1ZNcXCpmo7xeV22CXHUwMDEw0HZcdTAwMWHVpsHqSMmeKddd+vZNtehSOs9cdTAwMDNnL1x1MDAxNH++XHUwMDE4ZWlpTHtqXHUwMDA39icmp7ryavv6kMru8co+O7tZXHTZVnD/Zd5j2lx1MDAxMm6NocJcdTAwMTmghXH9ipRJSTSUqlx1MDAxNJRZQ5mYXHUwMDFk2ZVSXHUwMDEzyF6tXGbnXHUwMDFlZkdcdTAwMTVY6FBgXHUwMDAz4sCFhJxGXHUwMDFkeTzSjqOb49dcdTAwMTL6XHUwMDFj8UV0c3o4znTf1mcl74dzpImrU/b4k/+nebCyvbv1iZ3ufvrwcFxiO91nJNtNavf5Nt7b9nRcdTAwMGXPz+EjzuGjzlx1MDAxMadcdTAwMDej7zPigP6L/lxujf6/MFpcdTAwMTMxJyfHZtmxhWzGXG7rVYOVbFx1MDAwNnKFND+5ovm81tr73GhcdTAwMDWb3ZN4e3czPemq1f05z7VcdTAwMTa5VoFcdTAwMWM9VrJcdTAwMGL1YX9cdTAwMDGtKFGU+4JcdTAwMWNcdTAwMGVwxVx1MDAxNbOpl7IlgVx1MDAxObS8lC0kcVQzKCwpXHUwMDFjSEpeeHiqZFudVVx1MDAxNaYgcN5nKZtRU1olk1x1MDAxMlxmWSg3uVx1MDAxYdBJrXFG18+3j1x1MDAxYkF3T5zwc3Z0Pt/erJQmXHUwMDA2zMFx8CTIcTXgzIo4oLlTVDBt7Vxmi2TWQHc4i1x0cYyyXHRcdTAwMDW6s84rdDdzgT6ONryi+vVGtGFy+d+4jtPoqnLaXHUwMDBlW0k7JfW0XHUwMDExvy1cdTAwMTKPNWDGXHUwMDE1c850aTJcdTAwMDDqcWR+6SZfpVx1MDAxZL9cdTAwMTQ/n60pSnJcIsDFjYRSk473p1x1MDAwMyE9tlx1MDAxOVx1MDAwMYRHspDUzG6ddrBoXlhyLC2aXHUwMDBmpVx1MDAwN2OUXHUwMDE1ilx1MDAxN0pcdTAwMWPzUzN/RW37w7jr1nhX3F909r7f3G/KLlJp0l3em8J1ZyWOXHUwMDFlwJ47V3CmWYE9d6VlXHUwMDAyhth3RnI6Odqz1vH5d6FuN/d23EZHXHUwMDFkfb28uOdzjvZaXHUwMDEyJrhjRlx1MDAwYufXvvrDW1x1MDAxOWJcdTAwMTEtXHUwMDFjQyEh5OXsolx1MDAxYolcdTAwMDRcdTAwMTnGMb9cblx1MDAwM4NGrIhxRWjfx1xmRrcwWS/V7GtcdTAwMDazh+fSR31cdTAwMDauXHUwMDBmPu1cdTAwMWRunlx1MDAxZe5vbL8tSo+67zTAubRplItCe9Rg6MJcdTAwMDVcdTAwMWNlVk8uO8cn3/mEZi1cdTAwMDWhcDvFhdJa9Ff4gMWEc+0kh1x1MDAwNLeysMQ89dCFs2ZNPiDecP5CRs1cdTAwMGJ8XFxcdTAwMTBJXHUwMDE5RDBVXHUwMDEwXHJuKHIhi7nzrYe/XGKXx3aN9sXf9LtGJ27Q9ElcdTAwMDHTzKDuMe3INflwLT71Z0qinVbUcY2MimF3Q88+UdPo+OaOgk3LlDCpVVY1YExCrskhm5ggXHUwMDE2wVxim8G9nOF0eELeU9Noqb9nO1x1MDAwNz09v9pC8eeL81x1MDAxZEa5nKpcYoheYN9cdTAwMGJ67MaTwvnMd/AywrSG7lx1MDAwMlx1MDAxZlHM9HeMSmqJ8ouBnCmJ8JjlQiV8XjJcdTAwMGLdw2CTXHUwMDFkwVVAXHUwMDFlXHTSslx1MDAwNSORvmGg2Mfw2L5jXHUwMDFj5V5T/Sop8k5SXHUwMDFlJLZcdTAwMTVZU5ZB8lCscNBjdmGEcoNcdTAwMDRcdTAwMDOJaqxfMlx1MDAxYZ/yZrloOtO8s1xc6nf+M+xxL8w8Y2v8IFx1MDAxOaXJxygkQersXHUwMDBiavz1zdpGZ6V7dLL2Kd7fXVx1MDAwZWt69afWU1x1MDAwN2N8djpcdISS+Ndz/GtcdTAwMDBcdTAwMTJxK/t7XHUwMDA3MS1Ecl9Vh0qiSr2qbem38+wzSiRxwlx1MDAxMaKOWk/7RjEtbr1cdTAwMTdcdTAwMDCFXHUwMDE0g1x1MDAxYzHDeVx1MDAwN/ArXHUwMDFkn0av79Tr+29cIvmVLn3XTGpupXNy8oJe5+h653LlurlcdTAwMTHeXHUwMDA2y/r48GTFrK7PuSMrTiCRqVx1MDAwNkORwlxiNeDHXG5u7oCf4I/KvOpds3I3Zr5ZnjmkLF+kXHUwMDEzZsRC1Vx1MDAxOFx1MDAwNfxcdTAwMDSg+Fx1MDAxOG60m1x1MDAwMoD+WrWPzP2Tav+wXHUwMDFkNVx1MDAxYWF1MVv2fVvBX3LryTT/q1abRXlJnnOrhVe9XHUwMDEzR3B9O0wvzz93a8ftdHdZ3IWNi6NgviNYWUNcZtNWcu1hYKCzXHUwMDA3XGJcdTAwMDQ+ZJWxylx1MDAwN4dTr1qhK49hrlxiZFx1MDAxZlx1MDAwMtD64qEtZNU+LEJCtWDr0mnD2TBcdTAwMTiBplglzUxeXHUwMDE2fVx1MDAwZmjkbHlcdTAwMTHLKWC1lS/onThx7HNl5Xh7Izw4452PZnVn40t9vp1ZQ017tayUfyNDu1x1MDAwMU1cdTAwMDfWJVx1MDAwNIi9XHUwMDA04aVKuFctL5U6s1NE+NVuw1x1MDAxNO5GXHUwMDBiPa4vwCPum+yUcVNoRX23cJQk8eKTc78tXHUwMDFhjbzzXHUwMDFigJEuf4lcdTAwMTd5z4FGTc4mL06292tX1TNVW1x0d05O+W5cdTAwMWEud+Y7fJVyxJe5rEBcdTAwMTZ3xVx1MDAxN5lcdTAwMWW+uEBcdTAwMTPrsnZcIuRT6V7V+VRcdTAwMGVFvr9KXG4tXHUwMDAwMlxcXHUwMDAwXHUwMDE5R0OR5pJKqKJcZopcbsT2iU1qpFx1MDAxN1x1MDAwNc77/1xuRWDT5d/CgTGm5lx1MDAwNS9cdTAwMDeO/yaE+XRl55D3pLJCWEMt7adVkN3EWIqdjkE+09lcdTAwMDCRhfpcdTAwMTLOZdLM+Vx1MDAxZb6fXHUwMDEyRtK/bKyknfn7vD0/f6et0z+Lc5nmWYRcdTAwMTOHnbeFuZE3nlx1MDAwNsqVLzuUXHUwMDEzVE6lzd5cYnrBO8NjXHUwMDFi4+ayQUIrScBM4XiSy1x1MDAwMXzTTFx1MDAxMqgsXHUwMDBlbPPF6UGjppNcdTAwMTMoXHUwMDExXHUwMDA2t1x1MDAxN9B6lo1ug1x1MDAxNP6bWyTyXHUwMDEycphyxo1abOBcdTAwMWH01uTPNj+LXHLKXHUwMDE2u8qmv9gw8VomlIihnqeAz1iZvVx1MDAxYlo46mG1wVx1MDAxMqe9sPZrN/57XHUwMDFlhp99ovXV8X3BfTZB9nAmXHJEkoaX8VHfXHUwMDE0xLxI4lx1MDAxNpPv5bZUQza9p2WOUn/3nyFPz6+2UPzpMSS7/lLQalx1MDAxZKTwuN5kwPOj6iNcdTAwMDTkXHUwMDBmuXRcdTAwMTOFt6vlYbnwOJxLXHUwMDBmyVx1MDAxN1x1MDAxN/ux8OO/53dcdTAwMWFcbiJ9 samplesheet.csvMULTIQCsample,fastq_1,fastq_2SAMPLE1_PE,sample1_R1.fastq.gz,sample1_R2.fastq.gzSAMPLE2_PE,sample2_R1.fastq.gz,sample2_R2.fastq.gzSAMPLE3_SE,sample1_R1.fastq.gz,SAMPLE3_SE,sample2_R1.fastq.gz,multiqc_report.htmlSEQTK_TRIMTrimmed fastqTool versionfastq files <p>The <code>nf-core modules install</code> command can be used to install the <code>seqtk/trim</code> module directly from the nf-core repository:</p> <pre><code>nf-core modules install\n</code></pre> <p>Warning</p> <p>You need to be in the myorg-myfirstpipeline directory when executing <code>nf-core modules install</code></p> <p>You can follow the prompts to find and install the module you are interested in:</p> <pre><code>? Tool name: seqtk/trim\n</code></pre> <p>Once selected, the tooling will install the module in the <code>modules/nf-core/</code> folder and suggest code that you can add to your main workflow file (<code>workflows/myfirstpipeline.nf</code>).</p> <pre><code>INFO     Installing 'seqtk/trim'\nINFO     Use the following statement to include this module:\n\ninclude { SEQTK_TRIM } from '../modules/nf-core/seqtk/trim/main'\n</code></pre> <p>To enable reporting and reproducibility, modules and subworkflows from the nf-core repository are tracked using hashes in the <code>modules.json</code> file. When modules are installed or removed using the nf-core tooling the <code>modules.json</code> file will be automatically updated.</p> <p>When you open the <code>modules.json</code>, you will see an entry for each module that is currently installed from the nf-core modules repository. You can open the file with the VS Code user interface by clicking on it in <code>myorg-myfirstpipeline/modules.json</code>:</p> <pre><code>\"nf-core\": {\n    \"multiqc\": {\n        \"branch\": \"master\",\n        \"git_sha\": \"cf17ca47590cc578dfb47db1c2a44ef86f89976d\",\n        \"installed_by\": [\"modules\"]\n    },\n    \"seqtk/trim\": {\n        \"branch\": \"master\",\n        \"git_sha\": \"666652151335353eef2fcd58880bcef5bc2928e1\",\n        \"installed_by\": [\"modules\"]\n    }\n}\n</code></pre>"},{"location":"side_quests/nf-core/#232-add-the-module-to-your-pipeline","title":"2.3.2 Add the module to your pipeline","text":"<p>Although the module has been installed in your local pipeline repository, it is not yet added to your pipeline.</p> <p>The suggested <code>include</code> statement needs to be added to your <code>workflows/myfirstpipeline.nf</code> file and the process call (with inputs) needs to be added to the workflow block.</p> workflows/myfirstpipeline.nf<pre><code>include { SEQTK_TRIM             } from '../modules/nf-core/seqtk/trim/main'\ninclude { MULTIQC                } from '../modules/nf-core/multiqc/main'\n</code></pre> <p>To add the <code>SEQTK_TRIM</code> module to your workflow you will need to check what inputs are required.</p> <p>You can view the input channels for the module by opening the <code>./modules/nf-core/seqtk/trim/main.nf</code> file.</p> modules/nf-core/seqtk/trim/main.nf<pre><code>input:\ntuple val(meta), path(reads)\n</code></pre> <p>Each nf-core module also has a <code>meta.yml</code> file which describes the inputs and outputs. This meta file is rendered on the nf-core website, or can be viewed using the <code>nf-core modules info</code> command:</p> <pre><code>nf-core modules info seqtk/trim\n</code></pre> <p>It outputs a table with all defined inputs and outputs of the module:</p> Output<pre><code>\u256d\u2500 Module: seqtk/trim  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Location: modules/nf-core/seqtk/trim                                                              \u2502\n\u2502 \ud83d\udd27 Tools: seqtk                                                                                   \u2502\n\u2502 \ud83d\udcd6 Description: Trim low quality bases from FastQ files                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n               \u2577                                                                       \u2577\n \ud83d\udce5 Inputs     \u2502Description                                                            \u2502     Pattern\n\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\n input[0]      \u2502                                                                       \u2502\n\u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574\n  meta  (map)  \u2502Groovy Map containing sample information e.g. [ id:'test',             \u2502\n               \u2502single_end:false ]                                                     \u2502\n\u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574\n  reads  (file)\u2502List of input FastQ files                                              \u2502*.{fastq.gz}\n               \u2575                                                                       \u2575\n                      \u2577                                                                \u2577\n \ud83d\udce5 Outputs           \u2502Description                                                     \u2502     Pattern\n\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\n reads                \u2502                                                                \u2502\n\u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574\n  meta  (map)         \u2502Groovy Map containing sample information e.g. [ id:'test',      \u2502\n                      \u2502single_end:false ]                                              \u2502\n\u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574\n  *.fastq.gz  (file)  \u2502Filtered FastQ files                                            \u2502*.{fastq.gz}\n\u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574\n versions             \u2502                                                                \u2502\n\u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574\n  versions.yml  (file)\u2502File containing software versions                               \u2502versions.yml\n                      \u2575                                                                \u2575\n\n Use the following statement to include this module:\n\n include { SEQTK_TRIM } from '../modules/nf-core/seqtk/trim/main'\n</code></pre> <p>Using this module information you can work out what inputs are required for the <code>SEQTK_TRIM</code> process:</p> <ol> <li> <p><code>tuple val(meta), path(reads)</code></p> <ul> <li>A tuple (basically a fixed-length list) with a meta map (we will talk about meta maps more in the next section) and a list of FASTQ files</li> <li>The channel <code>ch_samplesheet</code> used by the <code>FASTQC</code> process can be used as the reads input.</li> </ul> </li> </ol> <p>Only one input channel is required, and it already exists, so it can be added to your <code>firstpipeline.nf</code> file without any additional channel creation or modifications.</p> <p>Before:</p> workflows/myfirstpipeline.nf<pre><code>//\n// Collate and save software versions\n//\n</code></pre> <p>After:</p> workflows/myfirstpipeline.nf<pre><code>//\n// MODULE: Run SEQTK_TRIM\n//\nSEQTK_TRIM (\n    ch_samplesheet\n)\n//\n// Collate and save software versions\n//\n</code></pre> <p>Let's test it:</p> <pre><code>nextflow run . -profile docker,test --outdir results\n</code></pre> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `./main.nf` [admiring_davinci] DSL2 - revision: fee0bcf390\n\nInput/output options\n  input                     : https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/samplesheet/samplesheet_test_illumina_amplicon.csv\n  outdir                    : results\n\nInstitutional config options\n  config_profile_name       : Test profile\n  config_profile_description: Minimal test dataset to check pipeline function\n\nGeneric options\n  trace_report_suffix       : 2025-03-05_10-40-35\n\nCore Nextflow options\n  runName                   : admiring_davinci\n  containerEngine           : docker\n  launchDir                 : /workspaces/training/side-quests/nf-core/nf-core-pipeline/myorg-myfirstpipeline\n  workDir                   : /workspaces/training/side-quests/nf-core/nf-core-pipeline/myorg-myfirstpipeline/work\n  projectDir                : /workspaces/training/side-quests/nf-core/nf-core-pipeline/myorg-myfirstpipeline\n  userName                  : gitpod\n  profile                   : docker,test\n  configFiles               : /workspaces/training/side-quests/nf-core/nf-core-pipeline/myorg-myfirstpipeline/nextflow.config\n\n!! Only displaying parameters that differ from the pipeline defaults !!\n------------------------------------------------------\nexecutor &gt;  local (4)\n[a8/d4ccea] MYO\u2026PELINE:SEQTK_TRIM (SAMPLE1_PE) | 3 of 3 \u2714\n[fb/d907c3] MYO\u2026PELINE:MYFIRSTPIPELINE:MULTIQC | 1 of 1 \u2714\n-[myorg/myfirstpipeline] Pipeline completed successfully-\n</code></pre>"},{"location":"side_quests/nf-core/#234-inspect-results-folder","title":"2.3.4 Inspect results folder","text":"<p>Default nf-core configuration directs the output of each process into the <code>&lt;outdir&gt;/&lt;TOOL&gt;</code>. After running the previous command, you should have a <code>results</code> folder that looks something like this:</p> <pre><code>results/\n\u251c\u2500\u2500 multiqc\n\u2502   \u251c\u2500\u2500 multiqc_data\n\u2502   \u2514\u2500\u2500 multiqc_report.html\n\u251c\u2500\u2500 pipeline_info\n\u2502   \u251c\u2500\u2500 execution_report_2025-03-05_10-17-59.html\n\u2502   \u251c\u2500\u2500 execution_report_2025-03-05_10-28-16.html\n\u2502   \u251c\u2500\u2500 execution_report_2025-03-05_10-40-35.html\n\u2502   \u251c\u2500\u2500 execution_timeline_2025-03-05_10-17-59.html\n\u2502   \u251c\u2500\u2500 execution_timeline_2025-03-05_10-28-16.html\n\u2502   \u251c\u2500\u2500 execution_timeline_2025-03-05_10-40-35.html\n\u2502   \u251c\u2500\u2500 execution_trace_2025-03-05_10-17-59.txt\n\u2502   \u251c\u2500\u2500 execution_trace_2025-03-05_10-28-16.txt\n\u2502   \u251c\u2500\u2500 execution_trace_2025-03-05_10-40-35.txt\n\u2502   \u251c\u2500\u2500 myfirstpipeline_software_mqc_versions.yml\n\u2502   \u251c\u2500\u2500 params_2025-03-05_10-18-03.json\n\u2502   \u251c\u2500\u2500 params_2025-03-05_10-28-19.json\n\u2502   \u251c\u2500\u2500 params_2025-03-05_10-40-37.json\n\u2502   \u251c\u2500\u2500 pipeline_dag_2025-03-05_10-17-59.html\n\u2502   \u251c\u2500\u2500 pipeline_dag_2025-03-05_10-28-16.html\n\u2502   \u2514\u2500\u2500 pipeline_dag_2025-03-05_10-40-35.html\n\u2514\u2500\u2500 seqtk\n    \u251c\u2500\u2500 SAMPLE1_PE_sample1_R1.fastq.gz\n    \u251c\u2500\u2500 SAMPLE1_PE_sample1_R2.fastq.gz\n    \u251c\u2500\u2500 SAMPLE2_PE_sample2_R1.fastq.gz\n    \u251c\u2500\u2500 SAMPLE2_PE_sample2_R2.fastq.gz\n    \u251c\u2500\u2500 SAMPLE3_SE_sample1_R1.fastq.gz\n    \u2514\u2500\u2500 SAMPLE3_SE_sample2_R1.fastq.gz\n</code></pre> <p>The outputs from the <code>multiqc</code> and <code>seqtk</code> modules are published in their respective subdirectories. In addition, by default, nf-core pipelines generate a set of reports. These files are stored in the<code>pipeline_info</code> subdirectory and time-stamped so that runs don't overwrite each other.</p>"},{"location":"side_quests/nf-core/#235-handle-modules-output","title":"2.3.5 Handle modules output","text":"<p>As with the inputs, you can view the outputs for the module by opening the <code>/modules/nf-core/seqtk/trim/main.nf</code> file, use the <code>nf-core modules info seqtk/trim</code>, or check the <code>meta.yml</code>.</p> modules/nf-core/seqtk/trim/main.nf<pre><code>output:\ntuple val(meta), path(\"*.fastq.gz\"), emit: reads\npath \"versions.yml\"                , emit: versions\n</code></pre> <p>To help with organization and readability it is beneficial to create named output channels.</p> <p>For <code>SEQTK_TRIM</code>, the <code>reads</code> output could be put into a channel named <code>ch_trimmed</code>.</p> workflows/myfirstpipeline.nf<pre><code>ch_trimmed  = SEQTK_TRIM.out.reads\n</code></pre> <p>All nf-core modules have a common output channel: <code>versions</code>. The channel contains a file that lists the tool version used in the module. MultiQC can collect all tool versions and print them out in a table in the results folder. This is useful to later track which version was actually run.</p> <p>It is beneficial to immediately mix the tool versions into the <code>ch_versions</code> channel so they can be used as input for the <code>MULTIQC</code> process and passed to the final report.</p> workflows/myfirstpipeline.nf<pre><code>ch_versions = ch_versions.mix(SEQTK_TRIM.out.versions.first())\n</code></pre> <p>Note</p> <p>The <code>first</code> operator is used to emit the first item from <code>SEQTK_TRIM.out.versions</code> to avoid duplication.</p>"},{"location":"side_quests/nf-core/#236-add-a-parameter-to-the-seqtktrim-tool","title":"2.3.6 Add a parameter to the <code>seqtk/trim</code> tool","text":"<p>nf-core modules should be flexible and usable across many different pipelines. Therefore, optional tool parameters are typically not set in an nf-core/module. Instead, additional configuration options on how to run the tool, like its parameters or filename, can be applied to a module using the <code>conf/modules.config</code> file on the pipeline level. Process selectors (e.g., <code>withName</code>) are used to apply configuration options to modules selectively. Process selectors must be used within the <code>process</code> scope.</p> <p>The parameters or arguments of a tool can be changed using the directive <code>args</code>. You can find many examples of how arguments are added to modules in nf-core pipelines, for example, the nf-core/demo modules.config file.</p> <p>Add this snippet to your <code>conf/modules.config</code> file (using the <code>process</code> scope) to call the <code>seqtk/trim</code> tool with the argument <code>-b 5</code> to trim 5 bp from the left end of each read:</p> conf/modules.config<pre><code>withName: 'SEQTK_TRIM' {\n    ext.args = \"-b 5\"\n}\n</code></pre> <p>Run the pipeline again and check if the new parameter is applied:</p> <pre><code>nextflow run . -profile docker,test --outdir results\n</code></pre> Output<pre><code>[67/cc3d2f] process &gt; MYORG_MYFIRSTPIPELINE:MYFIRSTPIPELINE:SEQTK_TRIM (SAMPLE1_PE) [100%] 3 of 3 \u2714\n[b4/a1b41b] process &gt; MYORG_MYFIRSTPIPELINE:MYFIRSTPIPELINE:MULTIQC                 [100%] 1 of 1 \u2714\n</code></pre> <p>Copy the hash you see in your console output (here <code>6c/34e549</code>; it is different for each run). You can <code>ls</code> using tab-completion in your <code>work</code> directory to expand the complete hash. In this folder you will find various log files. The <code>.command.sh</code> file contains the resolved command:</p> <pre><code>less work/6c/34e549912696b6757f551603d135bb/.command.sh\n</code></pre> <p>We can see, that the parameter <code>-b 5</code>, that we set in the <code>modules.config</code> is applied to the task:</p> Output<pre><code>#!/usr/bin/env bash -C -e -u -o pipefail\nprintf \"%s\\n\" sample2_R1.fastq.gz sample2_R2.fastq.gz | while read f;\ndo\n    seqtk \\\n        trimfq \\\n        -b 5 \\\n        $f \\\n        | gzip --no-name &gt; SAMPLE2_PE_$(basename $f)\ndone\n\ncat &lt;&lt;-END_VERSIONS &gt; versions.yml\n\"MYORG_MYFIRSTPIPELINE:MYFIRSTPIPELINE:SEQTK_TRIM\":\n    seqtk: $(echo $(seqtk 2&gt;&amp;1) | sed 's/^.*Version: //; s/ .*$//')\nEND_VERSIONS\n</code></pre>"},{"location":"side_quests/nf-core/#takeaway_4","title":"Takeaway","text":"<p>You changed the pipeline template and added the nf-core/module <code>seqtk</code> to your pipeline. You then changed the default tool command by editing the <code>modules.config</code> for this tool. You also made the output available in the workflow so it can be used by other modules in the pipeline.</p>"},{"location":"side_quests/nf-core/#whats-next_4","title":"What's next?","text":"<p>In the next step we will add a pipeline parameter to allow users to skip the trimming step run by <code>seqtk</code>.</p>"},{"location":"side_quests/nf-core/#24-adding-parameters-to-your-pipeline","title":"2.4 Adding parameters to your pipeline","text":"<p>Any option that a pipeline user may want to configure regularly, whether in the specific modules used or the options passed to them, should be made into a pipeline-level parameter so it can easily be overridden. nf-core defines some standards for providing parameters.</p> <p>Here, as a simple example, you will add a new parameter to your pipeline that will skip the <code>SEQTK_TRIM</code> process. That parameter will be accessible in the pipeline script, and we can use it to control how the pipeline runs.</p>"},{"location":"side_quests/nf-core/#241-default-values","title":"2.4.1 Default values","text":"<p>In the nf-core template the default values for parameters are set in the <code>nextflow.config</code> in the base repository.</p> <p>Any new parameters should be added to the <code>nextflow.config</code> with a default value within the <code>params</code> scope.</p> <p>Parameter names should be unique and easily identifiable.</p> <p>We can add a new parameter <code>skip_trim</code> to your <code>nextflow.config</code> file and set it to <code>false</code>.</p> nextflow.config<pre><code>// Trimming\nskip_trim                   = false\n</code></pre>"},{"location":"side_quests/nf-core/#242-using-the-parameter","title":"2.4.2 Using the parameter","text":"<p>Let's add an <code>if</code> statement that is depended on the <code>skip_trim</code> parameter to control the execution of the <code>SEQTK_TRIM</code> process:</p> workflows/myfirstpipeline.nf<pre><code>    //\n    // MODULE: Run SEQTK_TRIM\n    //\n    if (!params.skip_trim) {\n        SEQTK_TRIM (\n            ch_samplesheet\n        )\n        ch_trimmed  = SEQTK_TRIM.out.reads\n        ch_versions = ch_versions.mix(SEQTK_TRIM.out.versions.first())\n    }\n</code></pre> <p>Here, an <code>if</code> statement that is depended on the <code>skip_trim</code> parameter is used to control the execution of the <code>SEQTK_TRIM</code> process. An <code>!</code> can be used to imply the logical \"not\".</p> <p>Thus, if the <code>skip_trim</code> parameter is not <code>true</code>, the <code>SEQTK_TRIM</code> will be be executed.</p> <p>Now your if statement has been added to your main workflow file and has a default setting in your <code>nextflow.config</code> file, you will be able to flexibly skip the new trimming step using the <code>skip_trim</code> parameter.</p> <p>We can now run the pipeline with the new <code>skip_trim</code> parameter to check it is working:</p> <pre><code>nextflow run . -profile test,docker --outdir results --skip_trim\n</code></pre> <p>You should see that the <code>SEQTK_TRIM</code> process has been skipped in your execution:</p> Output<pre><code>!! Only displaying parameters that differ from the pipeline defaults !!\n------------------------------------------------------\nWARN: The following invalid input values have been detected:\n\n* --skip_trim: true\n\n\nexecutor &gt;  local (1)\n[7b/8b60a0] process &gt; MYORG_MYFIRSTPIPELINE:MYFIRSTPIPELINE:MULTIQC [100%] 1 of 1 \u2714\n-[myorg/myfirstpipeline] Pipeline completed successfully-\n</code></pre>"},{"location":"side_quests/nf-core/#243-validate-input-parameters","title":"2.4.3 Validate input parameters","text":"<p>When we ran the pipeline, we saw a warning message:</p> <pre><code>WARN: The following invalid input values have been detected:\n\n* --skip_trim: true\n</code></pre> <p>Parameters are validated through the <code>nextflow_schema.json</code> file. This file is also used by the nf-core website (for example, in nf-core/mag) to render the parameter documentation and print the pipeline help message (<code>nextflow run . --help</code>). If you have added parameters and they have not been documented in the <code>nextflow_schema.json</code> file, then the input validation does not recognize the parameter.</p> <p>The <code>nextflow_schema.json</code> file can get very big and very complicated very quickly, and is hard to manually edit. Fortunately, the <code>nf-core pipelines schema build</code> command is designed to support developers write, check, validate, and propose additions to your <code>nextflow_schema.json</code> file.</p> <pre><code>nf-core pipelines schema build\n</code></pre> <p>This will enable you to launch a web builder to edit this file in your web browser rather than trying to edit this file manually.</p> <pre><code>INFO     [\u2713] Default parameters match schema validation\nINFO     [\u2713] Pipeline schema looks valid (found 18 params)\n\u2728 Found 'params.skip_trim' in the pipeline config, but not in the schema. Add to pipeline schema? [y/n]: y\nINFO     Writing schema with 19 params: 'nextflow_schema.json'\n\ud83d\ude80  Launch web builder for customization and editing? [y/n]: y\n</code></pre> <p>Using the web builder you can add add details about your new parameters.</p> <p>The parameters that you have added to your pipeline will be added to the bottom of the <code>nf-core pipelines schema build</code> file. Some information about these parameters will be automatically filled based on the default value from your <code>nextflow.config</code>. You will be able to categorize your new parameters into a group, add icons, and add descriptions for each.</p> <p></p> <p>Note</p> <p>Ungrouped parameters in schema will cause a warning.</p> <p>Once you have made your edits you can click <code>Finished</code> and all changes will be automatically added to your <code>nextflow_schema.json</code> file.</p> <p>If you rerun the previous command, the warning should disappear:</p> <pre><code>nextflow run . -profile test,docker --outdir results --skip_trim\n</code></pre> Output<pre><code>!! Only displaying parameters that differ from the pipeline defaults !!\n------------------------------------------------------\nexecutor &gt;  local (1)\n[6c/c78d0c] process &gt; MYORG_MYFIRSTPIPELINE:MYFIRSTPIPELINE:MULTIQC [100%] 1 of 1 \u2714\n-[myorg/myfirstpipeline] Pipeline completed successfully-\n</code></pre>"},{"location":"side_quests/nf-core/#takeaway_5","title":"Takeaway","text":"<p>You added a new parameter to the pipeline. Your pipeline can now run <code>seqtk</code> or the user can decide to skip it. You learned how parameters are handeled in nf-core using the JSON schema and how this gives you additional features, such as help text and validation.</p>"},{"location":"side_quests/nf-core/#whats-next_5","title":"What's next?","text":"<p>In the next step we will take a look at how we track metadata related to an input file.</p>"},{"location":"side_quests/nf-core/#25-meta-maps","title":"2.5 Meta maps","text":"<p>Datasets often contain additional information relevant to the analysis, such as a sample name, information about sequencing protocols, or other conditions needed in the pipeline to process certain samples together, determine their output name, or adjust parameters.</p> <p>By convention, nf-core tracks this information as <code>meta</code> maps. These are <code>key</code>-<code>value</code> pairs that are passed into modules together with the files. We already saw this briefly when inspecting the <code>input</code> for <code>seqtk</code>:</p> modules/nf-core/seqtk/trim/main.nf<pre><code>input:\ntuple val(meta), path(reads)\n</code></pre> <p>If we uncomment our earlier <code>view</code> statement:</p> workflows/myfirstpipeline.nf<pre><code>ch_samplesheet.view()\n</code></pre> <p>and run the pipeline again, we can see the current content of the <code>meta</code> maps:</p> meta map<pre><code>[[id:SAMPLE1_PE, single_end:false], ....]\n</code></pre> <p>You can add any field that you require to the <code>meta</code> map. By default, nf-core modules expect an <code>id</code> field.</p>"},{"location":"side_quests/nf-core/#takeaway_6","title":"Takeaway","text":"<p>In this section you learned, that a <code>meta</code> map is used to pass along additional information for a sample in nf-core. It is a <code>map</code> (or dictionary) that allows you to assign arbitray keys to track any information you require in the workflow.</p>"},{"location":"side_quests/nf-core/#whats-next_6","title":"What's next?","text":"<p>In the next step we will take a look how we can add a new key to the <code>meta</code> map using the samplesheet.</p>"},{"location":"side_quests/nf-core/#26-simple-samplesheet-adaptations","title":"2.6 Simple Samplesheet adaptations","text":"<p>nf-core pipelines typically use samplesheets as inputs to the pipelines. This allows us to:</p> <ul> <li>validate each entry and print specific error messages.</li> <li>attach information to each input file.</li> <li>track which datasets are processed.</li> </ul> <p>Samplesheets are comma-separated text files with a header row specifying the column names, followed by one entry per row. For example, the samplesheet (link) that we have been using during this teaching module looks like this:</p> samplesheet_test_illumina_amplicon.csv<pre><code>sample,fastq_1,fastq_2\nSAMPLE1_PE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R1.fastq.gz,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R2.fastq.gz\nSAMPLE2_PE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R1.fastq.gz,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R2.fastq.gz\nSAMPLE3_SE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R1.fastq.gz,\nSAMPLE3_SE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R1.fastq.gz,\n</code></pre> <p>The structure of the samplesheet is specified in its own schema file in <code>assets/schema_input.json</code>. Each column has its own entry together with information about the column:</p> assets/schema_input.json<pre><code>\"properties\": {\n    \"sample\": {\n        \"type\": \"string\",\n        \"pattern\": \"^\\\\S+$\",\n        \"errorMessage\": \"Sample name must be provided and cannot contain spaces\",\n        \"meta\": [\"id\"]\n    },\n    \"fastq_1\": {\n        \"type\": \"string\",\n        \"format\": \"file-path\",\n        \"exists\": true,\n        \"pattern\": \"^\\\\S+\\\\.f(ast)?q\\\\.gz$\",\n        \"errorMessage\": \"FastQ file for reads 1 must be provided, cannot contain spaces and must have extension '.fq.gz' or '.fastq.gz'\"\n    },\n    \"fastq_2\": {\n        \"type\": \"string\",\n        \"format\": \"file-path\",\n        \"exists\": true,\n        \"pattern\": \"^\\\\S+\\\\.f(ast)?q\\\\.gz$\",\n        \"errorMessage\": \"FastQ file for reads 2 cannot contain spaces and must have extension '.fq.gz' or '.fastq.gz'\"\n    }\n},\n\"required\": [\"sample\", \"fastq_1\"]\n</code></pre> <p>This validates that the samplesheet has at least two columns: <code>sample</code> and <code>fastq1</code> (<code>\"required\": [\"sample\", \"fastq_1\"]</code>). It also checks that <code>fastq1</code> and <code>fastq2</code> are files, and that the file endings match a particular pattern. Lastly, <code>sample</code> is information about the files that we want to attach and pass along the pipeline. nf-core uses <code>meta</code> maps for this: objects that have a key and a value. We can indicate this in the schema file directly by using the meta field:</p> Sample column<pre><code>    \"sample\": {\n        \"type\": \"string\",\n        \"pattern\": \"^\\\\S+$\",\n        \"errorMessage\": \"Sample name must be provided and cannot contain spaces\",\n        \"meta\": [\"id\"]\n    },\n</code></pre> <p>This sets the key name as <code>id</code> and the value that is in the <code>sample</code> column, for example <code>SAMPLE1_PE</code>:</p> meta map<pre><code>[id: SAMPLE1_PE]\n</code></pre> <p>By adding a new entry into the JSON schema, we can attach additional meta information that we want to track. This will automatically validate it for us and add it to the meta map.</p> <p>Let's add some new meta information, like the <code>sequencer</code> as an optional column:</p> assets/schema_input.json<pre><code>\"properties\": {\n    \"sample\": {\n        \"type\": \"string\",\n        \"pattern\": \"^\\\\S+$\",\n        \"errorMessage\": \"Sample name must be provided and cannot contain spaces\",\n        \"meta\": [\"id\"]\n    },\n    \"sequencer\": {\n        \"type\": \"string\",\n        \"pattern\": \"^\\\\S+$\",\n        \"meta\": [\"sequencer\"]\n    },\n    \"fastq_1\": {\n        \"type\": \"string\",\n        \"format\": \"file-path\",\n        \"exists\": true,\n        \"pattern\": \"^\\\\S+\\\\.f(ast)?q\\\\.gz$\",\n        \"errorMessage\": \"FastQ file for reads 1 must be provided, cannot contain spaces and must have extension '.fq.gz' or '.fastq.gz'\"\n    },\n    \"fastq_2\": {\n        \"type\": \"string\",\n        \"format\": \"file-path\",\n        \"exists\": true,\n        \"pattern\": \"^\\\\S+\\\\.f(ast)?q\\\\.gz$\",\n        \"errorMessage\": \"FastQ file for reads 2 cannot contain spaces and must have extension '.fq.gz' or '.fastq.gz'\"\n    }\n},\n\"required\": [\"sample\", \"fastq_1\"]\n</code></pre> <p>We can now run our normal tests with the old samplesheet:</p> <pre><code>nextflow run . -profile docker,test --outdir results\n</code></pre> <p>The meta map now has a new key <code>sequencer</code>, that is empty because we did not specify a value yet:</p> output<pre><code>[['id':'SAMPLE1_PE', 'sequencer':[], 'single_end':false], ... ]\n[['id':'SAMPLE2_PE', 'sequencer':[], 'single_end':false], ... ]\n[['id':'SAMPLE3_SE', 'sequencer':[], 'single_end':true], ... ]\n</code></pre> <p>We have also prepared a new samplesheet, that has the <code>sequencer</code> column. You can overwrite the existing input with this command:</p> <pre><code>nextflow run . -profile docker,test --outdir results --input ../../data/sequencer_samplesheet.csv\n</code></pre> <p>This populates the <code>sequencer</code> and we can see it in the pipeline, when <code>view</code>ing the samplesheet channel:</p> output<pre><code>[['id':'SAMPLE1_PE', 'sequencer':'sequencer1', 'single_end':false], ... ]\n[['id':'SAMPLE2_PE', 'sequencer':'sequencer2', 'single_end':false], ... ]\n[['id':'SAMPLE3_SE', 'sequencer':'sequencer3', 'single_end':true], ... ]\n</code></pre> <p>We can comment the <code>ch_samplesheet.view()</code> line or remove it. We are not going to use it anymore in this training section.</p>"},{"location":"side_quests/nf-core/#261-use-the-new-meta-key-in-the-pipeline","title":"2.6.1 Use the new meta key in the pipeline","text":"<p>We can access this new meta value in the pipeline and use it to, for example, only enable trimming for samples from a particular sequencer. The branch operator let's us split an input channel into several new output channels based on a selection criteria. Let's add this within the <code>if</code> block:</p> workflows/myfirstpipeline.nf<pre><code>    if (!params.skip_trim) {\n\n        ch_seqtk_in = ch_samplesheet.branch { meta, reads -&gt;\n            to_trim: meta[\"sequencer\"] == \"sequencer2\"\n            other: true\n        }\n\n        SEQTK_TRIM (\n            ch_seqtk_in.to_trim\n        )\n        ch_trimmed  = SEQTK_TRIM.out.reads\n        ch_versions = ch_versions.mix(SEQTK_TRIM.out.versions.first())\n    }\n</code></pre> <p>If we now rerun our default test, no reads are being trimmed (even though we did not specify <code>--skip_trim</code>):</p> Output<pre><code>nextflow run . -profile docker,test --outdir results\n\n[-        ] process &gt; MYORG_MYFIRSTPIPELINE:MYFIRSTPIPELINE:SEQTK_TRIM          -\n[5a/f580bc] process &gt; MYORG_MYFIRSTPIPELINE:MYFIRSTPIPELINE:MULTIQC             [100%] 1 of 1 \u2714\n</code></pre> <p>If we use the samplesheet with the <code>sequencer</code> set, only one sample will be trimmed:</p> <pre><code>nextflow run . -profile docker,test --outdir results --input ../../data/sequencer_samplesheet.csv -resume\n</code></pre> Output<pre><code>[47/fdf9de] process &gt; MYORG_MYFIRSTPIPELINE:MYFIRSTPIPELINE:SEQTK_TRIM (SAMPLE2_PE) [100%] 1 of 1 \u2714\n[2a/a742ae] process &gt; MYORG_MYFIRSTPIPELINE:MYFIRSTPIPELINE:MULTIQC                 [100%] 1 of 1 \u2714\n</code></pre> <p>If you want to learn more about how to fine tune and develop the samplesheet schema further, visit nf-schema.</p>"},{"location":"side_quests/nf-core/#takeaway_7","title":"Takeaway","text":"<p>You explored how different samplesheets can provide different sets of additional information to your data files. You know how to adapt the samplesheet validation and how this is reflected in the pipeline in the <code>,meta</code> map.</p>"},{"location":"side_quests/nf-core/#whats-next_7","title":"What's next?","text":"<p>In the next step we will add a module that is not yet in nf-core.</p>"},{"location":"side_quests/nf-core/#27-create-a-custom-module-for-your-pipeline","title":"2.7 Create a custom module for your pipeline","text":"<p>nf-core offers a comprehensive set of modules that have been created and curated by the community. However, as a developer, you may be interested in bespoke pieces of software that are not apart of the nf-core repository or customizing a module that already exists.</p> <p>In this instance, we will write a local module for the QC Tool FastQE, which computes stats for FASTQ files and print those stats as emoji.</p> <p>This section should feel familiar to the <code>hello_modules</code> section.</p>"},{"location":"side_quests/nf-core/#271-create-the-module","title":"2.7.1 Create the module","text":"<p>New module contributions are always welcome and encouraged!</p> <p>If you have a module that you would like to contribute back to the community, reach out on the nf-core slack or open a pull request to the modules repository.</p> <p>Start by using the nf-core tooling to create a sceleton local module:</p> <pre><code>nf-core modules create\n</code></pre> <p>It will ask you to enter the tool name and some configurations for the module. We will use the defaults here:</p> <ul> <li>Specify the tool name: <code>Name of tool/subtool: fastqe</code></li> <li>Add the author name: <code>GitHub Username: (@&lt;your-name&gt;):</code></li> <li>Accept the defaults for the remaining prompts by typing <code>enter</code></li> </ul> <p>This will create a new file in <code>modules/local/fastqe/main.nf</code> that already contains the container and conda definitions, the general structure of the process, and a number of TODO statements to guide you through the adaptation.</p> <p>Warning</p> <p>If the module already exists locally, the command will fail to prevent you from accidentally overwriting existing work:</p> <pre><code>INFO     Repository type: pipeline\nINFO     Press enter to use default values (shown in brackets) or type your own responses. ctrl+click underlined text to open links.\nCRITICAL Module directory exists: 'modules/local/fastqe'. Use '--force' to overwrite\n</code></pre> <p>Let's open the modules file: <code>modules/local/fastqe/main.nf</code>.</p> <p>You will notice, that it still calls <code>samtools</code> and the input are <code>bam</code>.</p> <p>From our sample sheet, we know we have fastq files instead, so let's change the input definition accordingly:</p> modules/local/fastqe/main.nf<pre><code>tuple val(meta), path(reads)\n</code></pre> <p>The output of this tool is a tsv file with the emoji annotation, let's adapt the output as well:</p> modules/local/fastqe/main.nf<pre><code>tuple val(meta), path(\"*.tsv\"), emit: tsv\n</code></pre> <p>The script section still calls <code>samtools</code>. Let's change this to the proper call of the tool:</p> modules/local/fastqe/main.nf<pre><code>    fastqe \\\\\n        $args \\\\\n        $reads \\\\\n        --output ${prefix}.tsv\n</code></pre> <p>And at last, we need to adapt the version retrieval. This tool does not have a version command, so we will add the release number manually:</p> modules/local/fastqe/main.nf<pre><code>    def VERSION = '0.3.3'\n</code></pre> <p>and write it to a file in the script section:</p> modules/local/fastqe/main.nf<pre><code>        fastqe: $VERSION\n</code></pre> <p>We will not cover <code>stubs</code> in this training. They are not necessary to run a module, so let's remove them for now:</p> modules/local/fastqe/main.nf<pre><code>stub:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    // TODO nf-core: A stub section should mimic the execution of the original module as best as possible\n    //               Have a look at the following examples:\n    //               Simple example: https://github.com/nf-core/modules/blob/818474a292b4860ae8ff88e149fbcda68814114d/modules/nf-core/bcftools/annotate/main.nf#L47-L63\n    //               Complex example: https://github.com/nf-core/modules/blob/818474a292b4860ae8ff88e149fbcda68814114d/modules/nf-core/bedtools/split/main.nf#L38-L54\n    \"\"\"\n    touch ${prefix}.bam\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"${task.process}\":\n        fastqe: \\$(samtools --version |&amp; sed '1!d ; s/samtools //')\n    END_VERSIONS\n    \"\"\"\n</code></pre> <p>If you think this looks a bit messy and just want to add a complete final version, here's one we made earlier and we've removed all the commented out instructions:</p> modules/local/fastqe/main.nf<pre><code>process FASTQE {\n    tag \"$meta.id\"\n    label 'process_single'\n\n    conda \"${moduleDir}/environment.yml\"\n    container \"${ workflow.containerEngine == 'singularity' &amp;&amp; !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqe:0.3.3--pyhdfd78af_0':\n        'biocontainers/fastqe:0.3.3--pyhdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def VERSION = '0.3.3'\n    \"\"\"\n    fastqe \\\\\n        $args \\\\\n        $reads \\\\\n        --output ${prefix}.tsv\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"${task.process}\":\n        fastqe: $VERSION\n    END_VERSIONS\n    \"\"\"\n}\n</code></pre>"},{"location":"side_quests/nf-core/#272-include-the-module-into-the-pipeline","title":"2.7.2 Include the module into the pipeline","text":"<p>The module is now ready in your <code>modules/local</code> folder, but not yet included in your pipeline. Similar to <code>seqtk/trim</code> we need to add it to <code>workflows/myfirstpipeline.nf</code>:</p> <p>Before:</p> workflows/myfirstpipeline.nf<pre><code>/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    IMPORT MODULES / SUBWORKFLOWS / FUNCTIONS\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\ninclude { SEQTK_TRIM             } from '../modules/nf-core/seqtk/trim/main'\ninclude { MULTIQC                } from '../modules/nf-core/multiqc/main'\n</code></pre> <p>After:</p> workflows/myfirstpipeline.nf<pre><code>/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    IMPORT MODULES / SUBWORKFLOWS / FUNCTIONS\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\ninclude { FASTQE                 } from '../modules/local/fastqe'\ninclude { SEQTK_TRIM             } from '../modules/nf-core/seqtk/trim/main'\ninclude { MULTIQC                } from '../modules/nf-core/multiqc/main'\n</code></pre> <p>and call it on our input data:</p> workflows/myfirstpipeline.nf<pre><code>    FASTQE(ch_samplesheet)\n    ch_versions = ch_versions.mix(FASTQE.out.versions.first())\n</code></pre> <p>Let's run the pipeline again:</p> <pre><code>nextflow run . -profile docker,test --outdir results\n</code></pre> <p>In the results folder, you should now see a new subdirectory <code>fastqe/</code>, with the mean read qualities:</p> SAMPLE1_PE.tsv<pre><code>Filename\tStatistic\tQualities\nsample1_R1.fastq.gz\tmean\t\ud83d\ude1d \ud83d\ude1d \ud83d\ude1d \ud83d\ude1d \ud83d\ude1d \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude09 \ud83d\ude09 \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude09 \ud83d\ude09 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude09 \ud83d\ude1b \ud83d\ude1c \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude1c \ud83d\ude1c \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude1c \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1b \ud83d\ude1c \ud83d\ude1c \ud83d\ude1b \ud83d\ude1b \ud83d\ude1b \ud83d\ude1a\nsample1_R2.fastq.gz\tmean\t\ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude1d \ud83d\ude1d \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude1c \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude09 \ud83d\ude1c \ud83d\ude09 \ud83d\ude09 \ud83d\ude1c \ud83d\ude1c \ud83d\ude09 \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1b \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1b \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1c \ud83d\ude1b \ud83d\ude1c \ud83d\ude1b \ud83d\ude1b \ud83d\ude1b \ud83d\ude1b \ud83d\ude1b \ud83d\ude1b \ud83d\ude1b \ud83d\ude1b \ud83d\ude1b \ud83d\ude1b \ud83d\ude1b \ud83d\ude1b \ud83d\ude1d \ud83d\ude1b \ud83d\ude1d \ud83d\ude1d \ud83d\ude1d \ud83d\ude1d \ud83d\ude1d \ud83d\ude1d \ud83d\ude1d \ud83d\ude1d \ud83d\ude1d \ud83d\ude1d \ud83d\ude1d \ud83d\ude1d \ud83d\ude1d \ud83d\ude1d \ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude0c \ud83d\ude0b \ud83d\ude0b \ud83d\ude0b \ud83d\ude0b \ud83d\ude0b \ud83d\ude0b \ud83d\ude0b \ud83d\ude0b \ud83d\ude00\n</code></pre>"},{"location":"side_quests/nf-core/#takeaway_8","title":"Takeaway","text":"<p>You added a new local module to the pipeline. We touched on how the module template files in nf-core look like and which aspects you need to adapt to add your own tool.</p>"},{"location":"side_quests/nf-core/#summary","title":"Summary","text":"<p>In this side-quest you got an introduction to nf-core. You've learned:</p> <ul> <li> <p>Section 1: How to run nf-core pipelines</p> </li> <li> <p>Where to find information about nf-core pipelines</p> </li> <li> <p>How to run a nf-core pipelines</p> </li> <li> <p>Section 2: How to create an nf-core pipelines:</p> </li> <li> <p>About nf-core tooling</p> </li> <li> <p>About the nf-core template:</p> <ul> <li>How to create a basic nf-core pipeline</li> <li>What files are in the template</li> </ul> </li> <li> <p>About nf-core/modules:</p> <ul> <li>How to find one</li> <li>How to install it</li> <li>How to configure it in the <code>modules.config</code></li> </ul> </li> <li> <p>About parameters:</p> <ul> <li>Where to add it in the workflow code</li> <li>How to set a default in the <code>nextflow.config</code></li> <li>How to validated the parameter using the <code>nextflow_schema.json</code></li> </ul> </li> <li> <p>About <code>meta</code> maps:</p> <ul> <li>What a <code>meta</code> map is</li> <li>How to access information from it</li> <li>How to add new fields in the <code>assets/schema_input.json</code></li> <li>How to add a column in the samplesheet to track additional <code>meta</code> information</li> </ul> </li> <li> <p>About developping a local module:</p> <ul> <li>How to create a module sceleton file using nf-core tooling</li> <li>How to adapt the sceleton file</li> <li>How to include the module in the pipeline</li> </ul> </li> </ul>"},{"location":"side_quests/nf-core/#whats-next_8","title":"What's next?","text":"<p>Check out the nf-core documentation to learn more. You can join the nf-core community slack where most of the exchange happens. You might want to:</p> <ul> <li>Get involved in the development of an nf-core pipeline</li> <li>Contribute nf-core components</li> <li>Contribute a pipeline to nf-core (before you do, check their guidelines)</li> <li>Start developping your own nf-core style pipeline</li> </ul>"},{"location":"side_quests/nf-test/","title":"Testing with nf-test","text":"<p>Being able to systematically test that every part of your workflow is doing what it's supposed to do is critical for reproducibility and long-term maintenance, and can be a huge help during the development process.</p> <p>Let's take a minute to talk about why testing is so important. If you're developing a workflow, one of the first things you will do is grab some test data that you know is valid and should produce a result. You add the first process to the pipeline and wire it up to your inputs to make it work. Then, to check it's all working, so you run it on the test data. Assuming that works, you move on to the next process and run the test data again. You repeat this process until you have a pipeline that you're happy with.</p> <p>Then, maybe you add a simple true or false parameter such as <code>--skip_process</code>. Now you must run the pipeline twice, once with each parameter to make sure it works as expected. But wait, how do we check if the <code>--skip_process</code> actually skips the process? We have to dig into the outputs or check the log files! This is a pain and prone to error.</p> <p>As you develop your pipeline, it will quickly become so complex that manually testing every iteration is slow and error prone. Furthermore, if you do find an error it will be very difficult to pin down exactly where in your pipeline the error is coming from. This is where testing comes in.</p> <p>Testing allows you to systematically check that every part of your pipeline is working as expected. The benefits to a developer of well written tests are huge:</p> <ul> <li>Confidence: Because the tests cover the entire pipeline, you can be be confident changing something doesn't affect anything else</li> <li>Trust: When multiple developers work on the pipeline, they know the other developers haven't broken the pipeline and every component.</li> <li>Transparency: The tests show where a pipeline is failing and make it easier to track down the problem. They also function as a form of documentation, showing how to run a process or workflow.</li> <li>Speed: Because the tests are automated, they can be run very quickly and repeatedly. You can iterate quickly with less fear of introducing new bugs.</li> </ul> <p>There are lots of different types of tests we can write:</p> <ol> <li>Module-level tests: For individual processes</li> <li>Workflow-level tests: For a single workflow</li> <li>Pipeline-level tests: For the pipeline as a whole</li> <li>Performance tests: For the speed and efficiency of the pipeline</li> <li>Stress tests: Assessing the pipeline's performance under extreme conditions to determine its limits</li> </ol> <p>Testing individual processes is analogous to unit tests in other languages. Testing the workflow or the entire pipeline is analogous to what's called integration tests in other languages, where we test the interactions of the components.</p> <p>nf-test is a tool that allows you to write module, workflow and pipeline level test. In short, it allows you to systematically check every individual part of the pipeline is working as expected, in isolation.</p> <p>In this part of the training, we're going to show you how to use nf-test to write module-level tests for the three processes in our pipeline.</p>"},{"location":"side_quests/nf-test/#0-warmup","title":"0. Warmup","text":"<p>Let's move into the project directory.</p> <pre><code>cd side-quests/nf-test\n</code></pre> <p>The <code>nf-test</code> directory has the file content like:</p> Directory contents<pre><code>nf-test\n\u251c\u2500\u2500 greetings.csv\n\u2514\u2500\u2500main.nf\n</code></pre> <p>For a detailed description of the files, see the warmup from Hello Nextflow. The workflow we'll be testing is part of the workflow built in Hello Workflow, and is composed of two processes: <code>sayHello</code> and <code>convertToUpper</code>:</p> Workflow code<pre><code>/*\n * Pipeline parameters\n */\nparams.input_file = \"greetings.csv\"\n\n/*\n * Use echo to print 'Hello World!' to standard out\n */\nprocess sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        val greeting\n\n    output:\n        path \"${greeting}-output.txt\"\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; '$greeting-output.txt'\n    \"\"\"\n}\n\n/*\n * Use a text replace utility to convert the greeting to uppercase\n */\nprocess convertToUpper {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        path input_file\n\n    output:\n        path \"UPPER-${input_file}\"\n\n    script:\n    \"\"\"\n    cat '$input_file' | tr '[a-z]' '[A-Z]' &gt; UPPER-${input_file}\n    \"\"\"\n}\n\nworkflow {\n\n    // create a channel for inputs from a CSV file\n    greeting_ch = Channel.fromPath(params.input_file).splitCsv().flatten()\n\n    // emit a greeting\n    sayHello(greeting_ch)\n\n    // convert the greeting to uppercase\n    convertToUpper(sayHello.out)\n}\n</code></pre> <p>We're going to assume an understanding of this workflow, but if you're not sure, you can refer back to Hello Workflow.</p>"},{"location":"side_quests/nf-test/#01-run-the-workflow","title":"0.1. Run the workflow","text":"<p>Let's run the workflow to make sure it's working as expected.</p> <pre><code>nextflow run main.nf\n</code></pre> Result of running the workflow<pre><code> N E X T F L O W   ~  version 24.10.2\n\nLaunching `main.nf` [soggy_linnaeus] DSL2 - revision: bbf79d5c31\n\nexecutor &gt;  local (6)\n[f7/c3be66] sayHello (3)       | 3 of 3 \u2714\n[cd/e15303] convertToUpper (3) | 3 of 3 \u2714\n</code></pre> <p>CONGRATULATIONS! You just ran a test!</p> <p>\"Wait, what? I just ran the workflow and it worked! How is that a test?\"</p> <p>Good question!</p> <p>Let's break down what just happened.</p> <p>You ran the workflow with the default parameters, you confirmed it worked and you're happy with the results. This is the essence of testing. If you worked through the Hello Nextflow training course, you'll have noticed we always started every section by running the workflow we were using as a starting point, to confirm everything is set up correctly.</p> <p>Testing software essentially does this process for us. Let's replace our simple <code>nextflow run main.nf</code> with a standardised test provided by nf-test.</p>"},{"location":"side_quests/nf-test/#takeaway","title":"Takeaway","text":"<p>You should be able to 'test' a pipeline by manually running it.</p>"},{"location":"side_quests/nf-test/#whats-next","title":"What's next?","text":"<p>Initialize <code>nf-test</code>.</p>"},{"location":"side_quests/nf-test/#1-initialize-nf-test","title":"1. Initialize <code>nf-test</code>","text":"<p>The <code>nf-test</code> package provides an initialization command that sets up a few things in order for us to start developing tests for our project.</p> <pre><code>nf-test init\n</code></pre> <p>This should produce the following output:</p> <pre><code>\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\nProject configured. Configuration is stored in nf-test.config\n</code></pre> <p>It also creates a <code>tests</code> directory containing a configuration file stub.</p>"},{"location":"side_quests/nf-test/#11-generate-an-nf-test","title":"1.1. Generate an nf-test","text":"<p><code>nf-test</code> comes with a set of tools for building nf-test files, saving us the majority of the work. These come under the subcommand <code>generate</code>. Let's generate a test for the pipeline:</p> <pre><code>nf-test generate pipeline main.nf\n</code></pre> Output<pre><code>&gt; nf-test generate pipeline main.nf\n\nLoad source file '/workspaces/training/side-quests/nf-test/main.nf'\nWrote pipeline test file '/workspaces/training/side-quests/nf-test/tests/main.nf.test\n\nSUCCESS: Generated 1 test files.\n</code></pre> <p>This will create a <code>main.nf.test</code> file within the <code>tests</code> directory. This is our pipeline level test file. If you run <code>tree tests/</code> you should see something like this:</p> Test directory contents<pre><code>tests/\n\u251c\u2500\u2500 main.nf.test\n\u2514\u2500\u2500 nextflow.config\n</code></pre> <p>The <code>main.nf.test</code> file is our pipeline level test file. Let's open it up and take a look at the contents.</p> tests/main.nf.test<pre><code>nextflow_pipeline {\n\n    name \"Test Workflow main.nf\"\n    script \"main.nf\"\n\n    test(\"Should run without failures\") {\n\n        when {\n            params {\n                // define parameters here. Example:\n                // outdir = \"tests/results\"\n            }\n        }\n\n        then {\n            assert workflow.success\n        }\n\n    }\n\n}\n</code></pre> <p>We'll take a second to understand the structure of the test file.</p> <p>The <code>nextflow_pipeline</code> block is the entry point for all pipeline level tests. It contains the following:</p> <ul> <li><code>name</code>: The name of the test.</li> <li><code>script</code>: The path to the pipeline script.</li> </ul> <p>The <code>test</code> block is the actual test. It contains the following:</p> <ul> <li><code>when</code>: The conditions under which the test should be run. This includes the parameters that will be used to run the pipeline.</li> <li><code>then</code>: The assertions that should be made. This includes the expected outcomes of the pipeline.</li> </ul> <p>In plain English, the logic of the test reads as follows: \"When these parameters are provided to this pipeline, then we expect to see these results.\"</p> <p>This isn't a functional test, we will demonstrate how to turn it into one in the next section.</p>"},{"location":"side_quests/nf-test/#a-note-on-test-names","title":"A Note on Test Names","text":"<p>In the example above, we used the default name \"Should run without failures\" which is appropriate for a basic test that just checks if the pipeline runs successfully. However, as we add more specific test cases, we should use more descriptive names that indicate what we're actually testing. For example:</p> <ul> <li>\"Should convert input to uppercase\" - when testing specific functionality</li> <li>\"Should handle empty input gracefully\" - when testing edge cases</li> <li>\"Should respect max memory parameter\" - when testing resource constraints</li> <li>\"Should create expected output files\" - when testing file generation</li> </ul> <p>Good test names should:</p> <ol> <li>Start with \"Should\" to make it clear what the expected behavior is</li> <li>Describe the specific functionality or scenario being tested</li> <li>Be clear enough that if the test fails, you know what functionality is broken</li> </ol> <p>As we add more assertions and specific test cases later, we'll use these more descriptive names to make it clear what each test is verifying.</p>"},{"location":"side_quests/nf-test/#12-run-the-test","title":"1.2. Run the test","text":"<p>Let's run the test to see what happens.</p> <pre><code>nf-test test tests/main.nf.test\n</code></pre> nf-test pipeline fail<pre><code>&gt; nf-test test tests/main.nf.test\n\n\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Workflow main.nf\n\n  Test [693ba951] 'Should run without failures' FAILED (4.652s)\n\n  Assertion failed:\n\n  assert workflow.success\n         |        |\n         workflow false\n\n  Nextflow stdout:\n\n  ERROR ~ No such file or directory: /workspaces/training/side-quests/nf-test/.nf-test/tests/693ba951a20fec36a5a9292ed1cc8a9f/greetings.csv\n\n   -- Check '/workspaces/training/side-quests/nf-test/.nf-test/tests/693ba951a20fec36a5a9292ed1cc8a9f/meta/nextflow.log' file for details\n  Nextflow stderr:\n\nFAILURE: Executed 1 tests in 4.679s (1 failed)\n</code></pre> <p>The test fails! What happened?</p> <ol> <li>nf-test tried to run the pipeline as is, using the settings in the <code>when</code> block:</li> </ol> tests/main.nf.test<pre><code>when {\n    params {\n        // define parameters here. Example:\n        // outdir = \"tests/results\"\n    }\n}\n</code></pre> <ol> <li>nf-test checked the status of the pipeline and compared it to the <code>when</code> block:</li> </ol> tests/main.nf.test<pre><code>then {\n    assert workflow.success\n}\n</code></pre> <p>Note how nf-test has reported the pipeline failed and provided the error message from Nextflow:</p> Error<pre><code>ERROR ~ No such file or directory: /workspaces/training/side-quests/nf-test/.nf-test/tests/693ba951a20fec36a5a9292ed1cc8a9f/greetings.csv\n</code></pre> <p>So what was the issue? Remember the pipeline has a greetings.csv file in the project directory. When nf-test runs the pipeline, it will look for this file, but it can't find it. The file is there, what's happening? Well, if we look at the path we can see the test is occurring in the path <code>./nf-test/tests/longHashString/</code>. Just like Nextflow, nf-test creates a new directory for each test to keep everything isolated. The data file is not located in there so we must correct the path to the file in the original test.</p> <p>Let's go back to the test file and change the path to the file in the <code>when</code> block.</p> <p>You may be wondering how we're going to point to the root of the pipeline in the test. Since this is a common situation, nf-test has a range of global variables that we can use to make our lives easier. You can find the full list here but in the meantime we'll use the <code>projectDir</code> variable, which means the root of the pipeline project.</p> <p>Before:</p> tests/main.nf.test<pre><code>when {\n    params {\n        // define parameters here. Example:\n        // outdir = \"tests/results\"\n    }\n}\n</code></pre> <p>After:</p> tests/main.nf.test<pre><code>when {\n    params {\n        input_file = \"${projectDir}/greetings.csv\"\n    }\n}\n</code></pre> <p>Let's run the test again to see if it works.</p> nf-test pipeline pass<pre><code>nf-test test tests/main.nf.test\n</code></pre> Pipeline passes<pre><code>&gt; nf-test test tests/main.nf.test\n\n\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Workflow main.nf\n\n  Test [1d4aaf12] 'Should run without failures' PASSED (1.619s)\n\n\nSUCCESS: Executed 1 tests in 1.626s\n</code></pre> <p>Success! The pipeline runs successfully and the test passes. Run it as many times as you like and you will always get the same result!</p> <p>By default, the Nextflow output is hidden, but to convince yourself that nf-test is definitely running the workflow, you can use the <code>--verbose</code> flag:</p> <pre><code>nf-test test tests/main.nf.test --verbose\n</code></pre> Pipeline runs all processes<pre><code>&gt; nf-test test tests/main.nf.test\n\n\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Workflow main.nf\n\n  Test [693ba951] 'Should run without failures'\n    &gt; Nextflow 24.10.4 is available - Please consider updating your version to it\n    &gt; N E X T F L O W  ~  version 24.10.0\n    &gt; Launching `/workspaces/training/side-quests/nf-test/main.nf` [zen_ampere] DSL2 - revision: bbf79d5c31\n    &gt; [2b/61e453] Submitted process &gt; sayHello (2)\n    &gt; [31/4e1606] Submitted process &gt; sayHello (1)\n    &gt; [bb/5209ee] Submitted process &gt; sayHello (3)\n    &gt; [83/83db6f] Submitted process &gt; convertToUpper (2)\n    &gt; [9b/3428b1] Submitted process &gt; convertToUpper (1)\n    &gt; [ca/0ba51b] Submitted process &gt; convertToUpper (3)\n    PASSED (5.206s)\n\n\nSUCCESS: Executed 1 tests in 5.239s\n</code></pre>"},{"location":"side_quests/nf-test/#13-add-assertions","title":"1.3. Add assertions","text":"<p>A simple check is to ensure our pipeline is running all the processes we expect and not skipping any silently. Remember our pipeline runs 6 processes, one called <code>sayHello</code> and one called <code>convertToUpper</code> for each of the 3 greetings.</p> <p>Let's add an assertion to our test to check the pipeline runs the expected number of processes. We'll also update our test name to better reflect what we're testing.</p> <p>Before:</p> tests/main.nf.test<pre><code>    test(\"Should run without failures\") {\n\n        when {\n            params {\n                input_file = \"${projectDir}/greetings.csv\"\n            }\n        }\n\n        then {\n            assert workflow.success\n        }\n\n    }\n</code></pre> <p>After:</p> tests/main.nf.test<pre><code>    test(\"Should run successfully with correct number of processes\") {\n\n        when {\n            params {\n                input_file = \"${projectDir}/greetings.csv\"\n            }\n        }\n\n        then {\n            assert workflow.success\n            assert workflow.trace.tasks().size() == 6\n        }\n\n    }\n</code></pre> <p>The test name now better reflects what we're actually verifying - not just that the pipeline runs without failing, but that it runs the expected number of processes.</p> <p>Let's run the test again to see if it works.</p> nf-test pipeline pass<pre><code>nf-test test tests/main.nf.test\n</code></pre> Pipeline passes with assertions<pre><code>\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Workflow main.nf\n\n  Test [1d4aaf12] 'Should run successfully with correct number of processes' PASSED (1.567s)\n\n\nSUCCESS: Executed 1 tests in 1.588s\n</code></pre> <p>Success! The pipeline runs successfully and the test passes. Now we have began to test the details of the pipeline, as well as the overall status.</p>"},{"location":"side_quests/nf-test/#14-test-the-output","title":"1.4. Test the output","text":"<p>Let's add an assertion to our test to check the output file was created. We'll add it as a separate test, with an informative name, to make the results easier to interpret.</p> <p>Before:</p> tests/main.nf.test<pre><code>    test(\"Should run successfully with correct number of processes\") {\n\n        when {\n            params {\n                input_file = \"${projectDir}/greetings.csv\"\n            }\n        }\n\n        then {\n            assert workflow.success\n            assert workflow.trace.tasks().size() == 6\n        }\n\n    }\n</code></pre> <p>After:</p> tests/main.nf.test<pre><code>    test(\"Should run successfully with correct number of processes\") {\n\n        when {\n            params {\n                input_file = \"${projectDir}/greetings.csv\"\n            }\n        }\n\n        then {\n            assert workflow.success\n            assert workflow.trace.tasks().size() == 6\n        }\n\n    }\n\n    test(\"Should produce correct output files\") {\n\n        when {\n            params {\n                input_file = \"${projectDir}/greetings.csv\"\n            }\n        }\n\n        then {\n            assert file(\"$launchDir/results/Bonjour-output.txt\").exists()\n            assert file(\"$launchDir/results/Hello-output.txt\").exists()\n            assert file(\"$launchDir/results/Hol\u00e0-output.txt\").exists()\n            assert file(\"$launchDir/results/UPPER-Bonjour-output.txt\").exists()\n            assert file(\"$launchDir/results/UPPER-Hello-output.txt\").exists()\n            assert file(\"$launchDir/results/UPPER-Hol\u00e0-output.txt\").exists()\n        }\n\n    }\n</code></pre> <p>Run the test again to see if it works.</p> nf-test pipeline pass<pre><code>nf-test test tests/main.nf.test\n</code></pre> Pipeline passes with file assertions<pre><code>&gt; nf-test test tests/main.nf.test\n\n\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Workflow main.nf\n\n  Test [f0e08a68] 'Should run successfully with correct number of processes' PASSED (8.144s)\n  Test [d7e32a32] 'Should produce correct output files' PASSED (6.994s)\n\n\nSUCCESS: Executed 2 tests in 15.165s\n</code></pre> <p>Success! The tests pass because the pipeline completed successfully, the correct number of processes ran and the output files were created. This should also show you how useful it is to provide those informative names for your tests.</p> <p>This is just the surface, we can keep writing assertions to check the details of the pipeline, but for now let's move on to testing the internals of the pipeline.</p>"},{"location":"side_quests/nf-test/#takeaway_1","title":"Takeaway","text":"<p>You know how to write an nf-test for a pipeline.</p>"},{"location":"side_quests/nf-test/#whats-next_1","title":"What's next?","text":"<p>Learn how to test a Nextflow process.</p>"},{"location":"side_quests/nf-test/#2-test-a-nextflow-process","title":"2. Test a Nextflow process","text":"<p>We don't have to write tests for every part of the pipeline, but the more tests we have the more comprehensive we can be about the pipeline and the more confident we can be that it's working as expected. In this section we're going to test both processes in the pipeline as individual units.</p>"},{"location":"side_quests/nf-test/#21-test-the-sayhello-process","title":"2.1. Test the <code>sayHello</code> process","text":"<p>Let's start with the <code>sayHello</code> process.</p> <p>Let's use the <code>nf-test generate</code> command again to generate tests for the process.</p> <pre><code>nf-test generate process main.nf\n</code></pre> Output<pre><code>&gt; nf-test generate process main.nf\n\nLoad source file '/workspaces/training/side-quests/nf-test/main.nf'\nWrote process test file '/workspaces/training/side-quests/nf-test/tests/main.sayhello.nf.test\nWrote process test file '/workspaces/training/side-quests/nf-test/tests/main.converttoupper.nf.test\n\nSUCCESS: Generated 2 test files.\n</code></pre> <p>Let's focus for now on the <code>sayhello</code> process in the <code>main.sayhello.nf.test</code> file.</p> <p>Let's open the file and take a look at the contents.</p> tests/main.sayhello.nf.test<pre><code>nextflow_process {\n\n    name \"Test Process sayHello\"\n    script \"main.nf\"\n    process \"sayHello\"\n\n    test(\"Should run without failures\") {\n\n        when {\n            params {\n                // define parameters here. Example:\n                // outdir = \"tests/results\"\n            }\n            process {\n                \"\"\"\n                // define inputs of the process here. Example:\n                // input[0] = file(\"test-file.txt\")\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert snapshot(process.out).match()\n        }\n\n    }\n\n}\n</code></pre> <p>As before, we start with the test details, followed by the <code>when</code> and <code>then</code> blocks. However, we also have an additional <code>process</code> block which allows us to define the inputs to the process.</p> <p>Let's run the test to see if it works.</p> nf-test pipeline pass<pre><code>nf-test test tests/main.sayhello.nf.test\n</code></pre> Process test fails<pre><code>&gt; nf-test test tests/main.sayhello.nf.test\n\n\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Process sayHello\n\n  Test [1eaad118] 'Should run without failures' FAILED (4.876s)\n\n  Assertion failed:\n\n  assert process.success\n         |       |\n         |       false\n         sayHello\n\n  Nextflow stdout:\n\n  Process `sayHello` declares 1 input channel but 0 were specified\n\n   -- Check script '/workspaces/training/side-quests/nf-test/.nf-test-1eaad118145a1fd798cb07e7dd75d087.nf' at line: 38 or see '/workspaces/training/side-quests/nf-test/.nf-test/tests/1eaad118145a1fd798cb07e7dd75d087/meta/nextflow.log' file for more details\n  Nextflow stderr:\n\nFAILURE: Executed 1 tests in 4.884s (1 failed)\n</code></pre> <p>The test fails because the <code>sayHello</code> process declares 1 input channel but 0 were specified. Let's fix that by adding an input to the process. Remember from Hello Workflow (and the warmup section above) that our <code>sayHello</code> process takes a single value input, which we will need to provide. We should also fix the test name to better reflect what we're testing.</p> <p>Before:</p> tests/main.sayhello.nf.test<pre><code>    test(\"Should run without failures\") {\n\n        when {\n            params {\n                // define parameters here. Example:\n                // outdir = \"tests/results\"\n            }\n            process {\n                \"\"\"\n                // define inputs of the process here. Example:\n                // input[0] = file(\"test-file.txt\")\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert snapshot(process.out).match()\n        }\n\n    }\n</code></pre> <p>After:</p> tests/main.sayhello.nf.test<pre><code>    test(\"Should run without failures and produce correct output\") {\n\n        when {\n            params {\n                // define parameters here. Example:\n                // outdir = \"tests/results\"\n            }\n            process {\n                \"\"\"\n                input[0] = \"hello\"\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert snapshot(process.out).match()\n        }\n\n    }\n</code></pre> <p>Let's run the test again to see if it works.</p> nf-test pipeline pass<pre><code>&gt; nf-test test tests/main.sayhello.nf.test\n\n\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Process sayHello\n\n  Test [f91a1bcd] 'Should run without failures and produce correct output' PASSED (1.604s)\n  Snapshots:\n    1 created [Should run without failures and produce correct output]\n\n\nSnapshot Summary:\n  1 created\n\nSUCCESS: Executed 1 tests in 1.611s\n</code></pre> <p>Success! The test passes because the <code>sayHello</code> process ran successfully and the output was created.</p>"},{"location":"side_quests/nf-test/#22-check-out-the-snapshot-created-by-the-test","title":"2.2. Check out the snapshot created by the test","text":"<p>If we look at the <code>tests/main.sayhello.nf.test</code> file, we can see it uses a method <code>snapshot()</code> in the assertion block:</p> tests/main.sayhello.nf.test<pre><code>assert snapshot(process.out).match()\n</code></pre> <p>This is telling nf-test to create a snapshot of the output of the <code>sayHello</code> process. Let's take a look at the contents of the snapshot file.</p> Snapshot file contents<pre><code>code tests/main.sayhello.nf.test.snap\n</code></pre> <p>We won't print it here, but you should see a JSON file containing details of the process and process outputs. In particular, we can see a line that looks like this:</p> Snapshot file contents<pre><code>\"0\": [\n    \"hello-output.txt:md5,b1946ac92492d2347c6235b4d2611184\"\n]\n</code></pre> <p>This represents the outputs created by the <code>sayHello</code> process, which we are testing explicitly. If we re-run the test, the program will check that the new output matches the output that was originally recorded. This is a quick, simple way of testing that process outputs don't change, which is why nf-test provides it as a default.</p> <p>Warning</p> <p>That means we have to be sure that the output we record in the original run is correct!</p> <p>If, in the course of future development, something in the code changes that causes the output to be different, the test will fail and we will have to determine whether the change is expected or not.</p> <ul> <li>If it turns out that something in the code broke, we will have to fix it, with the expectation that the fixed code will pass the test.</li> <li>If it is an expected change (e.g., the tool has been improved and the results are better) then we will need to update the snapshot to accept the new output as the reference to match. nf-test has a parameter <code>--update-snapshot</code> for this purpose.</li> </ul> <p>We can run the test again and see the test should pass:</p> nf-test process pass with snapshot<pre><code>&gt; nf-test test tests/main.sayhello.nf.test\n\n\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Process sayHello\n\n  Test [f91a1bcd] 'Should run without failures and produce correct output' PASSED (1.675s)\n\n\nSUCCESS: Executed 1 tests in 1.685s\n</code></pre> <p>Success! The test passes because the <code>sayHello</code> process ran successfully and the output matched the snapshot.</p>"},{"location":"side_quests/nf-test/#23-alternative-to-snapshots-direct-content-assertions","title":"2.3. Alternative to Snapshots: Direct Content Assertions","text":"<p>While snapshots are great for catching any changes in output, sometimes you want to verify specific content without being so strict about the entire file matching. For example:</p> <ul> <li>When parts of the output might change (timestamps, random IDs, etc.) but certain key content must be present</li> <li>When you want to check for specific patterns or values in the output</li> <li>When you want to make the test more explicit about what constitutes success</li> </ul> <p>Here's how we could modify our test to check specific content:</p> <p>Before:</p> tests/main.sayhello.nf.test<pre><code>    test(\"Should run without failures and produce correct output\") {\n\n        when {\n            params {\n                // define parameters here. Example:\n                // outdir = \"tests/results\"\n            }\n            process {\n                \"\"\"\n                input[0] = \"hello\"\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert snapshot(process.out).match()\n        }\n\n    }\n</code></pre> <p>After:</p> tests/main.sayhello.nf.test<pre><code>     test(\"Should run without failures and contain expected greeting\") {\n\n        when {\n            params {\n                // define parameters here\n            }\n            process {\n                \"\"\"\n                input[0] = \"hello\"\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert path(process.out[0][0]).readLines().contains('hello')\n            assert !path(process.out[0][0]).readLines().contains('HELLO')\n        }\n\n    }\n</code></pre> <p>Note that nf-test sees the process outputs as a list of lists, so <code>process.out[0][0]</code> is fetching the first part of the first channel item (or 'emission') from this process.</p> <p>This approach:</p> <ul> <li>Makes it clear exactly what we expect in the output</li> <li>Is more resilient to irrelevant changes in the output</li> <li>Provides better error messages when tests fail</li> <li>Allows for more complex validations (regex patterns, numerical comparisons, etc.)</li> </ul> <p>Let's run the test to see if it works.</p> nf-test pipeline pass<pre><code>nf-test test tests/main.sayhello.nf.test\n</code></pre> Process test fails<pre><code>&gt; nf-test test tests/main.sayhello.nf.test\n\n\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Process sayHello\n\n  Test [58df4e4b] 'Should run without failures and contain expected greeting' PASSED (7.196s)\n\n\nSUCCESS: Executed 1 tests in 7.208s\n</code></pre>"},{"location":"side_quests/nf-test/#24-test-the-converttoupper-process","title":"2.4. Test the <code>convertToUpper</code> process","text":"<p>Let's open the <code>tests/main.converttoupper.nf.test</code> file and take a look at the contents:</p> tests/main.converttoupper.nf.test<pre><code>nextflow_process {\n\n    name \"Test Process convertToUpper\"\n    script \"main.nf\"\n    process \"convertToUpper\"\n\n    test(\"Should run without failures\") {\n\n        when {\n            params {\n                // define parameters here. Example:\n                // outdir = \"tests/results\"\n            }\n            process {\n                \"\"\"\n                // define inputs of the process here. Example:\n                // input[0] = file(\"test-file.txt\")\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert snapshot(process.out).match()\n        }\n\n    }\n\n}\n</code></pre> <p>This is a similar test to the <code>sayHello</code> process, but it's testing the <code>convertToUpper</code> process. We know this one will fail because just like with <code>sayHello</code>, the <code>convertToUpper</code> process takes a single path input, but we haven't specified one.</p> <p>We now need to supply a single input file to the convertToUpper process, which includes some text that we want to convert to uppercase. There are lots of ways we could do this:</p> <ul> <li>We could create a dedicated file to test</li> <li>We could re-use the existing data/greetings.csv file</li> <li>We could create it on the fly within the test</li> </ul> <p>For now, let's re-use the existing data/greetings.csv file using the example we used with the pipeline level test. As before, we can name the test to better reflect what we're testing, but this time let's leave it to 'snapshot' the content rather than checking for specific strings (as we did in the other process).</p> <p>Before:</p> tests/main.converttoupper.nf.test<pre><code>    test(\"Should run without failures\") {\n\n        when {\n            params {\n                // define parameters here. Example:\n                // outdir = \"tests/results\"\n            }\n            process {\n                \"\"\"\n                // define inputs of the process here. Example:\n                // input[0] = file(\"test-file.txt\")\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert snapshot(process.out).match()\n        }\n\n    }\n</code></pre> <p>After:</p> tests/main.converttoupper.nf.test<pre><code>    test(\"Should run without failures and produce correct output\") {\n\n        when {\n            params {\n                // define parameters here. Example:\n                // outdir = \"tests/results\"\n            }\n            process {\n                \"\"\"\n                input[0] = \"${projectDir}/greetings.csv\"\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert snapshot(process.out).match()\n        }\n\n    }\n</code></pre> <p>And run the test!</p> nf-test pipeline pass<pre><code>nf-test test tests/main.converttoupper.nf.test\n</code></pre> nf-test process convertToUpper pass<pre><code>&gt; nf-test test tests/main.converttoupper.nf.test\n\n\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Process convertToUpper\n\n  Test [c59b6044] 'Should run without failures and produce correct output' PASSED (1.755s)\n  Snapshots:\n    1 created [Should run without failures and produce correct output]\n\n\nSnapshot Summary:\n  1 created\n\nSUCCESS: Executed 1 tests in 1.764s\n</code></pre> <p>Note, we have created a snapshot file for the <code>convertToUpper</code> process at <code>tests/main.converttoupper.nf.test.snap</code>. If we run the test again, we should see the nf-test passes again.</p> nf-test process convertToUpper pass<pre><code>nf-test test tests/main.converttoupper.nf.test\n</code></pre> nf-test process convertToUpper pass<pre><code>&gt; nf-test test tests/main.converttoupper.nf.test\n\n\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Process convertToUpper\n\n  Test [c59b6044] 'Should run without failures and produce correct output' PASSED (1.798s)\n\n\nSUCCESS: Executed 1 tests in 1.811s\n</code></pre>"},{"location":"side_quests/nf-test/#takeaway_2","title":"Takeaway","text":"<p>You know how to write tests for a Nextflow process and run them.</p>"},{"location":"side_quests/nf-test/#whats-next_2","title":"What's next?","text":"<p>Learn how to run tests for everything at once!</p>"},{"location":"side_quests/nf-test/#3-run-tests-for-the-entire-repository","title":"3. Run tests for the entire repository","text":"<p>Running nf-test on each component is fine, but laborious and error prone. Can't we just test everything at once?</p> <p>Yes we can!</p> <p>Let's run nf-test on the entire repo.</p>"},{"location":"side_quests/nf-test/#31-run-nf-test-on-the-entire-repo","title":"3.1. Run nf-test on the entire repo","text":"<p>We can run nf-test on the entire repo by running the <code>nf-test test</code> command.</p> <pre><code>nf-test test .\n</code></pre> <p>Note, we are just using the <code>.</code> to run everything from our current directory. This will include every test!</p> nf-test repo pass<pre><code>&gt; nf-test test tests/main.converttoupper.nf.test\n\n\ud83d\ude80 nf-test 0.9.2\nhttps://www.nf-test.com\n(c) 2021 - 2024 Lukas Forer and Sebastian Schoenherr\n\n\nTest Process convertToUpper\n\n  Test [c59b6044] 'Should run without failures and produce correct output' PASSED (1.798s)\n\nTest Workflow main.nf\n\n  Test [f0e08a68] 'Should run successfully with correct number of processes' PASSED (8.144s)\n  Test [d7e32a32] 'Should produce correct output files' PASSED (6.994s)\n\nTest Process sayHello\n\n  Test [f91a1bcd] 'Should run without failures and contain expected greeting' PASSED (1.664s)\n\n\nSUCCESS: Executed 3 tests in 5.007s\n</code></pre> <p>Check that out! We ran 3 tests, 1 for each process and 1 for the whole pipeline with a single command. Imagine how powerful this is on a large codebase!</p>"},{"location":"side_quests/nf-test/#4-summary","title":"4. Summary","text":"<p>In this side quest, we've learned:</p> <ol> <li>How to initialize nf-test in a Nextflow project</li> <li>How to write and run pipeline-level tests:</li> <li>Basic success testing</li> <li>Process count verification</li> <li>Output file existence checks</li> <li>How to write and run process-level tests</li> <li>Two approaches to output validation:</li> <li>Using snapshots for complete output verification</li> <li>Using direct content assertions for specific content checks</li> <li>Best practices for test naming and organization</li> <li>How to run all tests in a repository with a single command</li> </ol> <p>Testing is a critical part of pipeline development that helps ensure:</p> <ul> <li>Your code works as expected</li> <li>Changes don't break existing functionality</li> <li>Other developers can contribute with confidence</li> <li>Problems can be identified and fixed quickly</li> <li>Output content matches expectations</li> </ul>"},{"location":"side_quests/nf-test/#whats-next_3","title":"What's next?","text":"<p>Check out the nf-test documentation for more advanced testing features and best practices. You might want to:</p> <ul> <li>Add more comprehensive assertions to your tests</li> <li>Write tests for edge cases and error conditions</li> <li>Set up continuous integration to run tests automatically</li> <li>Learn about other types of tests like workflow and module tests</li> <li>Explore more advanced content validation techniques</li> </ul> <p>Remember: Tests are living documentation of how your code should behave. The more tests you write, and the more specific your assertions are, the more confident you can be in your pipeline's reliability.</p>"},{"location":"side_quests/orientation/","title":"Orientation","text":"<p>The GitHub Codespaces environment contains all the software, code and data necessary to work through this training course, so you don't need to install anything yourself. However, you do need a (free) account to log in, and you should take a few minutes to familiarize yourself with the interface.</p> <p>If you have not yet done so, please follow this link before going any further.</p>"},{"location":"side_quests/orientation/#materials-provided","title":"Materials provided","text":"<p>Throughout this training course, we'll be working in the <code>side-quests/</code> directory. This directory contains all the code files, test data and accessory files you will need.</p> <p>Feel free to explore the contents of this directory; the easiest way to do so is to use the file explorer on the left-hand side of the GitHub Codespaces workspace. Alternatively, you can use the <code>tree</code> command. Throughout the course, we use the output of <code>tree</code> to represent directory structure and contents in a readable form, sometimes with minor modifications for clarity.</p> <p>Here we generate a table of contents to the second level down:</p> <pre><code>tree . -L 2\n</code></pre> <p>If you run this inside <code>side-quests</code>, you should see the following output:</p> Directory contents<pre><code>.\n\u251c\u2500\u2500 metadata\n\u251c\u2500\u2500 nf-core\n\u251c\u2500\u2500 nf-test\n\u251c\u2500\u2500 solutions\n\u251c\u2500\u2500 splitting_and_grouping\n\u2514\u2500\u2500 workflows_of_workflows\n</code></pre> <p>Here's a summary of what you should know to get started:</p> <ul> <li> <p>Each directory corresponds to an individual side quest.   Their contents are detailed on the corresponding side quest's page.</p> </li> <li> <p>The <code>solutions</code> directory contains the completed workflow and/or module scripts that result from running through various steps of each side quest.   They are intended to be used as a reference to check your work and troubleshoot any issues.</p> </li> </ul> <p>Tip</p> <p>If for whatever reason you move out of this directory, you can always run this command to return to it:</p> <pre><code>cd /workspaces/training/side-quests\n</code></pre> <p>Now, to begin the course, click on the arrow in the bottom right corner of this page.</p>"},{"location":"side_quests/splitting_and_grouping/","title":"Splitting and Grouping","text":"<p>Nextflow provides powerful tools for working with data flexibly. A key capability is splitting data into different streams and then grouping related items back together. This is especially valuable in bioinformatics workflows where you need to process different types of samples separately before combining results for analysis.</p> <p>Think of it like sorting mail: you separate letters by destination, process each pile differently, then recombine items going to the same person. Nextflow uses special operators to accomplish this with scientific data. This approach is also commonly known as the scatter/gather pattern in distributed computing and bioinformatics workflows.</p> <p>Nextflow's channel system is at the heart of this flexibility. Channels connect different parts of your workflow, allowing data to flow through your analysis. You can create multiple channels from a single data source, process each channel differently, and then merge channels back together when needed. This approach lets you design workflows that naturally mirror the branching and converging paths of complex bioinformatics analyses.</p> <p>In this side quest, you'll learn to split and group data using Nextflow's channel operators. We'll start with a CSV file containing sample information and associated data files, then manipulate and reorganize this data. By the end, you'll be able to separate and combine data streams effectively, creating more efficient and understandable workflows.</p> <p>You will:</p> <ul> <li>Read data from files using <code>splitCsv</code></li> <li>Filter and transform data with <code>filter</code> and <code>map</code></li> <li>Combine related data using <code>join</code> and <code>groupTuple</code></li> <li>Create data combinations with <code>combine</code> for parallel processing</li> <li>Optimize data structure using <code>subMap</code> and deduplication strategies</li> <li>Build reusable functions with named closures to help you manipulate channel structures</li> </ul> <p>These skills will help you build workflows that can handle multiple input files and different types of data efficiently, while maintaining clean, maintainable code structure.</p>"},{"location":"side_quests/splitting_and_grouping/#0-warmup","title":"0. Warmup","text":""},{"location":"side_quests/splitting_and_grouping/#01-prerequisites","title":"0.1. Prerequisites","text":"<p>Before taking on this side quest you should:</p> <ul> <li>Complete the Hello Nextflow tutorial</li> <li>Understand basic Nextflow concepts (processes, channels, operators, working with files, meta data)</li> </ul> <p>You may also find it useful to review Working with metadata before starting here, as it covers in detail how to work with metadata associated with files in your workflows.</p>"},{"location":"side_quests/splitting_and_grouping/#02-starting-point","title":"0.2. Starting Point","text":"<p>Let's move into the project directory.</p> <pre><code>cd side-quests/splitting_and_grouping\n</code></pre> <p>You'll find a <code>data</code> directory containing a samplesheet and a main workflow file.</p> Directory contents<pre><code>&gt; tree\n.\n\u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 samplesheet.csv\n\u2514\u2500\u2500 main.nf\n</code></pre> <p><code>samplesheet.csv</code> contains information about samples from different patients, including the patient ID, sample repeat number, type (normal or tumor), and paths to BAM files (which don't actually exist, but we will pretend they do).</p> samplesheet.csv<pre><code>id,repeat,type,bam\npatientA,1,normal,patientA_rep1_normal.bam\npatientA,1,tumor,patientA_rep1_tumor.bam\npatientA,2,normal,patientA_rep2_normal.bam\npatientA,2,tumor,patientA_rep2_tumor.bam\npatientB,1,normal,patientB_rep1_normal.bam\npatientB,1,tumor,patientB_rep1_tumor.bam\npatientC,1,normal,patientC_rep1_normal.bam\npatientC,1,tumor,patientC_rep1_tumor.bam\n</code></pre> <p>Note there are 8 samples in total from 3 patients (patientA has 2 repeats), 4 normal and 4 tumor.</p> <p>We're going to read in samplesheet.csv, then group and split the samples based on their data.</p>"},{"location":"side_quests/splitting_and_grouping/#1-read-in-sample-data","title":"1. Read in sample data","text":""},{"location":"side_quests/splitting_and_grouping/#11-read-in-sample-data-with-splitcsv","title":"1.1. Read in sample data with splitCsv","text":"<p>Let's start by reading in the sample data with <code>splitCsv</code>. In the <code>main.nf</code>, you'll see that we've already started the workflow.</p> main.nf<pre><code>workflow {\n    ch_samplesheet = Channel.fromPath(\"./data/samplesheet.csv\")\n}\n</code></pre> <p>Note</p> <p>Throughout this tutorial, we'll use the <code>ch_</code> prefix for all channel variables to clearly indicate they are Nextflow channels.</p> <p>We can use the <code>splitCsv</code> operator to split the data into a channel of maps (key/ value pairs), where each map represents a row from the CSV file.</p> <p>Note</p> <p>We'll encounter two different concepts called <code>map</code> in this training:</p> <ul> <li>Data structure: The Groovy map (equivalent to dictionaries/hashes in other languages) that stores key-value pairs</li> <li>Channel operator: The <code>.map()</code> operator that transforms items in a channel</li> </ul> <p>We'll clarify which one we mean in context, but this distinction is important to understand when working with Nextflow.</p> <p>Apply these changes to <code>main.nf</code>:</p> AfterBefore main.nf<pre><code>    ch_samples = Channel.fromPath(\"./data/samplesheet.csv\")\n        .splitCsv(header: true)\n        .view()\n</code></pre> main.nf<pre><code>    ch_samplesheet = Channel.fromPath(\"./data/samplesheet.csv\")\n</code></pre> <p><code>splitCsv</code> takes the file passed to it from the channel factory and the <code>header: true</code> option tells Nextflow to use the first row of the CSV file as the header row, which will be used as keys for the values. We're using the <code>view</code> operator you should have encountered before to examine the output this gives us.</p> <p>Run the pipeline:</p> Test the splitCsv operation<pre><code>nextflow run main.nf\n</code></pre> Read data with splitCsv<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [deadly_mercator] DSL2 - revision: bd6b0224e9\n\n[id:patientA, repeat:1, type:normal, bam:patientA_rep1_normal.bam]\n[id:patientA, repeat:1, type:tumor, bam:patientA_rep1_tumor.bam]\n[id:patientA, repeat:2, type:normal, bam:patientB_rep1_normal.bam]\n[id:patientA, repeat:2, type:tumor, bam:patientB_rep1_tumor.bam]\n[id:patientB, repeat:1, type:normal, bam:patientC_rep1_normal.bam]\n[id:patientB, repeat:1, type:tumor, bam:patientC_rep1_tumor.bam]\n[id:patientC, repeat:1, type:normal, bam:patientD_rep1_normal.bam]\n[id:patientC, repeat:1, type:tumor, bam:patientD_rep1_tumor.bam]\n</code></pre> <p>Each row from the CSV file has become a single item in the channel, with each item being a map with keys matching the header row.</p> <p>You should be able to see that each map contains:</p> <ul> <li><code>id</code>: The patient identifier (patientA, patientB, patientC)</li> <li><code>repeat</code>: The replicate number (1 or 2)</li> <li><code>type</code>: The sample type (normal or tumor)</li> <li><code>bam</code>: Path to the BAM file</li> </ul> <p>This format makes it easy to access specific fields from each sample via their keys in the map. We can access the BAM file path with the <code>bam</code> key, but also any of the 'metadata' fields that describe the file via <code>id</code>, <code>repeat</code>, <code>type</code>.</p> <p>Note</p> <p>For a more extensive introduction on working with metadata, you can work through the training Working with metadata</p> <p>Let's separate the metadata from the files. We can do this with a <code>map</code> operation:</p> AfterBefore main.nf<pre><code>    ch_samples = Channel.fromPath(\"./data/samplesheet.csv\")\n        .splitCsv(header: true)\n        .map{ row -&gt;\n          [[id:row.id, repeat:row.repeat, type:row.type], row.bam]\n        }\n        .view()\n</code></pre> main.nf<pre><code>    ch_samples = Channel.fromPath(\"./data/samplesheet.csv\")\n        .splitCsv(header: true)\n        .view()\n</code></pre> <p>Apply that change and re-run the pipeline:</p> Test the metadata separation<pre><code>nextflow run main.nf\n</code></pre> Sample data with separated metadata<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [deadly_mercator] DSL2 - revision: bd6b0224e9\n\n[[id:patientA, repeat:1, type:normal], patientA_rep1_normal.bam]\n[[id:patientA, repeat:1, type:tumor], patientA_rep1_tumor.bam]\n[[id:patientA, repeat:2, type:normal], patientA_rep2_normal.bam]\n[[id:patientA, repeat:2, type:tumor], patientA_rep2_tumor.bam]\n[[id:patientB, repeat:1, type:normal], patientB_rep1_normal.bam]\n[[id:patientB, repeat:1, type:tumor], patientB_rep1_tumor.bam]\n[[id:patientC, repeat:1, type:normal], patientC_rep1_normal.bam]\n[[id:patientC, repeat:1, type:tumor], patientC_rep1_tumor.bam]\n</code></pre> <p>We separated the sample meta data from the files into a map. We now have a channel of maps and files, each representing a row from the input sample sheet, which we will use in this training to split and group our workload.</p>"},{"location":"side_quests/splitting_and_grouping/#takeaway","title":"Takeaway","text":"<p>In this section, you've learned:</p> <ul> <li>Reading in a data sheet: How to read in data sheet with <code>splitCsv</code></li> <li>Combining patient-specific information: Using groovy maps to hold information about a patient</li> </ul>"},{"location":"side_quests/splitting_and_grouping/#2-filter-and-transform-data","title":"2. Filter and transform data","text":""},{"location":"side_quests/splitting_and_grouping/#21-filter-data-with-filter","title":"2.1. Filter data with <code>filter</code>","text":"<p>We can use the <code>filter</code> operator to filter the data based on a condition. Let's say we only want to process normal samples. We can do this by filtering the data based on the <code>type</code> field. Let's insert this before the <code>view</code> operator.</p> AfterBefore main.nf<pre><code>    ch_samples = Channel.fromPath(\"./data/samplesheet.csv\")\n        .splitCsv(header: true)\n        .map{ row -&gt;\n          [[id:row.id, repeat:row.repeat, type:row.type], row.bam]\n        }\n        .filter { meta, file -&gt; meta.type == 'normal' }\n        .view()\n</code></pre> main.nf<pre><code>    ch_samples = Channel.fromPath(\"./data/samplesheet.csv\")\n        .splitCsv(header: true)\n        .map{ row -&gt;\n          [[id:row.id, repeat:row.repeat, type:row.type], row.bam]\n        }\n        .view()\n</code></pre> <p>Run the workflow again to see the filtered result:</p> Test the filter operation<pre><code>nextflow run main.nf\n</code></pre> Filtered normal samples<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [admiring_brown] DSL2 - revision: 194d61704d\n\n[[id:patientA, repeat:1, type:normal], patientA_rep1_normal.bam]\n[[id:patientA, repeat:2, type:normal], patientA_rep2_normal.bam]\n[[id:patientB, repeat:1, type:normal], patientB_rep1_normal.bam]\n[[id:patientC, repeat:1, type:normal], patientC_rep1_normal.bam]\n</code></pre> <p>We have successfully filtered the data to only include normal samples. Let's recap how this works.</p> <p>The <code>filter</code> operator takes a closure that is applied to each element in the channel. If the closure returns <code>true</code>, the element is included; if it returns <code>false</code>, the element is excluded.</p> <p>In our case, we want to keep only samples where <code>meta.type == 'normal'</code>. The closure uses the tuple <code>meta,file</code> to refer to each sample, accesses the sample type with <code>meta.type</code>, and checks if it equals <code>'normal'</code>.</p> <p>This is accomplished with the single closure we introduced above:</p> main.nf<pre><code>    .filter { meta, file -&gt; meta.type == 'normal' }\n</code></pre>"},{"location":"side_quests/splitting_and_grouping/#22-create-separate-filtered-channels","title":"2.2. Create separate filtered channels","text":"<p>Currently we're applying the filter to the channel created directly from the CSV, but we want to filter this in more ways than one, so let's re-write the logic to create a separate filtered channel for normal samples:</p> AfterBefore main.nf<pre><code>    ch_samples = Channel.fromPath(\"./data/samplesheet.csv\")\n        .splitCsv(header: true)\n        .map{ row -&gt;\n            [[id:row.id, repeat:row.repeat, type:row.type], row.bam]\n        }\n    ch_normal_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'normal' }\n    ch_normal_samples\n        .view()\n</code></pre> main.nf<pre><code>    ch_samples = Channel.fromPath(\"./data/samplesheet.csv\")\n        .splitCsv(header: true)\n        .map{ row -&gt;\n          [[id:row.id, repeat:row.repeat, type:row.type], row.bam]\n        }\n        .filter { meta, file -&gt; meta.type == 'normal' }\n        .view()\n</code></pre> <p>Once again, run the pipeline to see the results:</p> Test separate channel creation<pre><code>nextflow run main.nf\n</code></pre> Filtered normal samples<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [trusting_poisson] DSL2 - revision: 639186ee74\n\n[[id:patientA, repeat:1, type:normal], patientA_rep1_normal.bam]\n[[id:patientA, repeat:2, type:normal], patientA_rep2_normal.bam]\n[[id:patientB, repeat:1, type:normal], patientB_rep1_normal.bam]\n[[id:patientC, repeat:1, type:normal], patientC_rep1_normal.bam]\n</code></pre> <p>We've successfully filtered the data and created a separate channel for normal samples. Let's create a filtered channel for the tumor samples as well:</p> AfterBefore main.nf<pre><code>    ch_normal_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'normal' }\n    ch_tumor_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'tumor' }\n    ch_normal_samples\n        .view{'Normal sample: ' + it}\n    ch_tumor_samples\n        .view{'Tumor sample: ' + it}\n</code></pre> main.nf<pre><code>    ch_normal_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'normal' }\n    ch_normal_samples\n        .view()\n</code></pre> Test filtering both sample types<pre><code>nextflow run main.nf\n</code></pre> Normal and tumor samples<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [maniac_boltzmann] DSL2 - revision: 3636b6576b\n\nTumor sample: [[id:patientA, repeat:1, type:tumor], patientA_rep1_tumor.bam]\nTumor sample: [[id:patientA, repeat:2, type:tumor], patientA_rep2_tumor.bam]\nNormal sample: [[id:patientA, repeat:1, type:normal], patientA_rep1_normal.bam]\nNormal sample: [[id:patientA, repeat:2, type:normal], patientA_rep2_normal.bam]\nNormal sample: [[id:patientB, repeat:1, type:normal], patientB_rep1_normal.bam]\nNormal sample: [[id:patientC, repeat:1, type:normal], patientC_rep1_normal.bam]\nTumor sample: [[id:patientB, repeat:1, type:tumor], patientB_rep1_tumor.bam]\nTumor sample: [[id:patientC, repeat:1, type:tumor], patientC_rep1_tumor.bam]\n</code></pre> <p>We've separated out the normal and tumor samples into two different channels, and used a closure supplied to <code>view()</code> to label them differently in the output: <code>ch_tumor_samples.view{'Tumor sample: ' + it}</code>.</p>"},{"location":"side_quests/splitting_and_grouping/#takeaway_1","title":"Takeaway","text":"<p>In this section, you've learned:</p> <ul> <li>Filtering data: How to filter data with <code>filter</code></li> <li>Splitting data: How to split data into different channels based on a condition</li> <li>Viewing data: How to use <code>view</code> to print the data and label output from different channels</li> </ul> <p>We've now separated out the normal and tumor samples into two different channels. Next, we'll join the normal and tumor samples on the <code>id</code> field.</p>"},{"location":"side_quests/splitting_and_grouping/#3-joining-channels-by-identifiers","title":"3. Joining channels by identifiers","text":"<p>In the previous section, we separated out the normal and tumor samples into two different channels. These could be processed independently using specific processes or workflows based on their type. But what happens when we want to compare the normal and tumor samples from the same patient? At this point, we need to join them back together making sure to match the samples based on their <code>id</code> field.</p> <p>Nextflow includes many methods for combining channels, but in this case the most appropriate operator is <code>join</code>. If you are familiar with SQL, it acts like the <code>JOIN</code> operation, where we specify the key to join on and the type of join to perform.</p>"},{"location":"side_quests/splitting_and_grouping/#31-use-map-and-join-to-combine-based-on-patient-id","title":"3.1. Use <code>map</code> and <code>join</code> to combine based on patient ID","text":"<p>If we check the <code>join</code> documentation, we can see that by default it joins two channels based on the first item in each tuple. If you don't have the console output still available, let's run the pipeline to check our data structure and see how we need to modify it to join on the <code>id</code> field.</p> Check current data structure<pre><code>nextflow run main.nf\n</code></pre> Normal and tumor samples<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [maniac_boltzmann] DSL2 - revision: 3636b6576b\n\nTumour sample: [[id:patientA, repeat:1, type:tumor], patientA_rep1_tumor.bam]\nTumour sample: [[id:patientA, repeat:2, type:tumor], patientA_rep2_tumor.bam]\nNormal sample: [[id:patientA, repeat:1, type:normal], patientA_rep1_normal.bam]\nNormal sample: [[id:patientA, repeat:2, type:normal], patientA_rep2_normal.bam]\nNormal sample: [[id:patientB, repeat:1, type:normal], patientB_rep1_normal.bam]\nNormal sample: [[id:patientC, repeat:1, type:normal], patientC_rep1_normal.bam]\nTumour sample: [[id:patientB, repeat:1, type:tumor], patientB_rep1_tumor.bam]\nTumour sample: [[id:patientC, repeat:1, type:tumor], patientC_rep1_tumor.bam]\n</code></pre> <p>We can see that the <code>id</code> field is the first element in each meta map. For <code>join</code> to work, we should isolate the <code>id</code> field in each tuple. After that, we can simply use the <code>join</code> operator to combine the two channels.</p> <p>To isolate the <code>id</code> field, we can use the <code>map</code> operator to create a new tuple with the <code>id</code> field as the first element.</p> AfterBefore main.nf<pre><code>    ch_normal_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'normal' }\n        .map { meta, file -&gt; [meta.id, meta, file] }\n    ch_tumor_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'tumor' }\n        .map { meta, file -&gt; [meta.id, meta, file] }\n    ch_normal_samples\n        .view{'Normal sample: ' + it}\n    ch_tumor_samples\n        .view{'Tumor sample: ' + it}\n</code></pre> main.nf<pre><code>    ch_normal_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'normal' }\n    ch_tumor_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'tumor' }\n    ch_normal_samples\n        .view{'Normal sample: ' + it}\n    ch_tumor_samples\n        .view{'Tumor sample: ' + it}\n</code></pre> Test the map transformation<pre><code>nextflow run main.nf\n</code></pre> Samples with ID as first element<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [mad_lagrange] DSL2 - revision: 9940b3f23d\n\nTumour sample: [patientA, [id:patientA, repeat:1, type:tumor], patientA_rep1_tumor.bam]\nTumour sample: [patientA, [id:patientA, repeat:2, type:tumor], patientA_rep2_tumor.bam]\nNormal sample: [patientA, [id:patientA, repeat:1, type:normal], patientA_rep1_normal.bam]\nNormal sample: [patientA, [id:patientA, repeat:2, type:normal], patientA_rep2_normal.bam]\nTumour sample: [patientB, [id:patientB, repeat:1, type:tumor], patientB_rep1_tumor.bam]\nTumour sample: [patientC, [id:patientC, repeat:1, type:tumor], patientC_rep1_tumor.bam]\nNormal sample: [patientB, [id:patientB, repeat:1, type:normal], patientB_rep1_normal.bam]\nNormal sample: [patientC, [id:patientC, repeat:1, type:normal], patientC_rep1_normal.bam]\n</code></pre> <p>It might be subtle, but you should be able to see the first element in each tuple is the <code>id</code> field. Now we can use the <code>join</code> operator to combine the two channels based on the <code>id</code> field.</p> <p>Once again, we will use <code>view</code> to print the joined outputs.</p> AfterBefore main.nf<pre><code>    ch_normal_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'normal' }\n        .map { meta, file -&gt; [meta.id, meta, file] }\n    ch_tumor_samples = ch_sample\n        .filter { meta, file -&gt; meta.type == 'tumor' }\n        .map { meta, file -&gt; [meta.id, meta, file] }\n    ch_joined_samples = ch_normal_samples\n        .join(ch_tumor_samples)\n    ch_joined_samples.view()\n</code></pre> main.nf<pre><code>    ch_normal_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'normal' }\n        .map { meta, file -&gt; [meta.id, meta, file] }\n    ch_tumor_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'tumor' }\n        .map { meta, file -&gt; [meta.id, meta, file] }\n    ch_normal_samples\n        .view{'Normal sample: ' + it}\n    ch_tumor_samples\n        .view{'Tumor sample: ' + it}\n</code></pre> Test the join operation<pre><code>nextflow run main.nf\n</code></pre> Joined normal and tumor samples<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [soggy_wiles] DSL2 - revision: 3bc1979889\n\n[patientA, [id:patientA, repeat:1, type:normal], patientA_rep1_normal.bam, [id:patientA, repeat:1, type:tumor], patientA_rep1_tumor.bam]\n[patientA, [id:patientA, repeat:2, type:normal], patientA_rep2_normal.bam, [id:patientA, repeat:2, type:tumor], patientA_rep2_tumor.bam]\n[patientB, [id:patientB, repeat:1, type:normal], patientB_rep1_normal.bam, [id:patientB, repeat:1, type:tumor], patientB_rep1_tumor.bam]\n[patientC, [id:patientC, repeat:1, type:normal], patientC_rep1_normal.bam, [id:patientC, repeat:1, type:tumor], patientC_rep1_tumor.bam]\n</code></pre> <p>It's a little hard to tell because it's so wide, but you should be able to see the samples have been joined by the <code>id</code> field. Each tuple now has the format:</p> <ul> <li><code>id</code>: The sample ID</li> <li><code>normal_meta_map</code>: The normal sample meta data including type, replicate and path to bam file</li> <li><code>normal_sample_file</code>: The normal sample file</li> <li><code>tumor_meta_map</code>: The tumor sample meta data including type, replicate and path to bam file</li> <li><code>tumor_sample</code>: The tumor sample including type, replicate and path to bam file</li> </ul> <p>Warning</p> <p>The <code>join</code> operator will discard any un-matched tuples. In this example, we made sure all samples were matched for tumor and normal but if this is not true you must use the parameter <code>remainder: true</code> to keep the unmatched tuples. Check the documentation for more details.</p>"},{"location":"side_quests/splitting_and_grouping/#takeaway_2","title":"Takeaway","text":"<p>In this section, you've learned:</p> <ul> <li>How to use <code>map</code> to isolate a field in a tuple</li> <li>How to use <code>join</code> to combine tuples based on the first field</li> </ul> <p>With this knowledge, we can successfully combine channels based on a shared field. Next, we'll consider the situation where you want to join on multiple fields.</p>"},{"location":"side_quests/splitting_and_grouping/#32-join-on-multiple-fields","title":"3.2. Join on multiple fields","text":"<p>We have 2 replicates for sampleA, but only 1 for sampleB and sampleC. In this case we were able to join them effectively by using the <code>id</code> field, but what would happen if they were out of sync? We could mix up the normal and tumor samples from different replicates!</p> <p>To avoid this, we can join on multiple fields. There are actually multiple ways to achieve this but we are going to focus on creating a new joining key which includes both the sample <code>id</code> and <code>replicate</code> number.</p> <p>Let's start by creating a new joining key. We can do this in the same way as before by using the <code>map</code> operator to create a new tuple with the <code>id</code> and <code>repeat</code> fields as the first element.</p> AfterBefore main.nf<pre><code>    ch_normal_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'normal' }\n        .map { meta, file -&gt; [[meta.id, meta.repeat], meta, file] }\n    ch_tumor_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'tumor' }\n        .map { meta, file -&gt; [[meta.id, meta.repeat], meta, file] }\n</code></pre> main.nf<pre><code>    ch_normal_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'normal' }\n        .map { meta, file -&gt; [meta.id, meta, file] }\n    ch_tumor_samples = ch_sample\n        .filter { meta, file -&gt; meta.type == 'tumor' }\n        .map { meta, file -&gt; [meta.id, meta, file] }\n</code></pre> <p>Now we should see the join is occurring but using both the <code>id</code> and <code>repeat</code> fields. Run the workflow:</p> Test multi-field joining<pre><code>nextflow run main.nf\n</code></pre> Samples joined on multiple fields<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [prickly_wing] DSL2 - revision: 3bebf22dee\n\n[[patientA, 1], [id:patientA, repeat:1, type:normal], patientA_rep1_normal.bam, [id:patientA, repeat:1, type:tumor], patientA_rep1_tumor.bam]\n[[patientA, 2], [id:patientA, repeat:2, type:normal], patientA_rep2_normal.bam, [id:patientA, repeat:2, type:tumor], patientA_rep2_tumor.bam]\n[[patientB, 1], [id:patientB, repeat:1, type:normal], patientB_rep1_normal.bam, [id:patientB, repeat:1, type:tumor], patientB_rep1_tumor.bam]\n[[patientC, 1], [id:patientC, repeat:1, type:normal], patientC_rep1_normal.bam, [id:patientC, repeat:1, type:tumor], patientC_rep1_tumor.bam]\n</code></pre> <p>Note how we have a tuple of two elements (<code>id</code> and <code>repeat</code> fields) as the first element of each joined result. This demonstrates how complex items can be used as a joining key, enabling fairly intricate matching between samples from the same conditions.</p> <p>If you want to explore more ways to join on different keys, check out the join operator documentation for additional options and examples.</p>"},{"location":"side_quests/splitting_and_grouping/#33-use-submap-to-create-a-new-joining-key","title":"3.3. Use subMap to create a new joining key","text":"<p>The previous approach loses the field names from our joining key - the <code>id</code> and <code>repeat</code> fields become just a list of values. To retain the field names for later access, we can use the <code>subMap</code> method.</p> <p>The <code>subMap</code> method extracts only the specified key-value pairs from a map. Here we'll extract just the <code>id</code> and <code>repeat</code> fields to create our joining key.</p> AfterBefore main.nf<pre><code>    ch_normal_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'normal' }\n        .map { meta, file -&gt; [meta.subMap(['id', 'repeat']), meta, file] }\n    ch_tumor_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'tumor' }\n        .map { meta, file -&gt; [meta.subMap(['id', 'repeat']), meta, file] }\n</code></pre> main.nf<pre><code>    ch_normal_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'normal' }\n        .map { meta, file -&gt; [[meta.id, meta.repeat], meta, file] }\n    ch_tumor_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'tumor' }\n        .map { meta, file -&gt; [[meta.id, meta.repeat], meta, file] }\n</code></pre> Test subMap joining keys<pre><code>nextflow run main.nf\n</code></pre> Samples with subMap joining keys<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [reverent_wing] DSL2 - revision: 847016c3b7\n\n[[id:patientA, repeat:1], [id:patientA, repeat:1, type:normal], patientA_rep1_normal.bam, [id:patientA, repeat:1, type:tumor], patientA_rep1_tumor.bam]\n[[id:patientA, repeat:2], [id:patientA, repeat:2, type:normal], patientA_rep2_normal.bam, [id:patientA, repeat:2, type:tumor], patientA_rep2_tumor.bam]\n[[id:patientB, repeat:1], [id:patientB, repeat:1, type:normal], patientB_rep1_normal.bam, [id:patientB, repeat:1, type:tumor], patientB_rep1_tumor.bam]\n[[id:patientC, repeat:1], [id:patientC, repeat:1, type:normal], patientC_rep1_normal.bam, [id:patientC, repeat:1, type:tumor], patientC_rep1_tumor.bam]\n</code></pre> <p>Now we have a new joining key that not only includes the <code>id</code> and <code>repeat</code> fields but also retains the field names so we can access them later by name, e.g. <code>meta.id</code> and <code>meta.repeat</code>.</p>"},{"location":"side_quests/splitting_and_grouping/#34-use-a-named-closure-in-map","title":"3.4. Use a named closure in map","text":"<p>To avoid duplication and reduce errors, we can use a named closure. A named closure allows us to create a reusable function that we can call in multiple places.</p> <p>To do so, first we define the closure as a new variable:</p> AfterBefore main.nf<pre><code>    ch_samples = Channel.fromPath(\"./data/samplesheet.csv\")\n        .splitCsv(header: true)\n        .map{ row -&gt;\n            [[id:row.id, repeat:row.repeat, type:row.type], row.bam]\n        }\n\n    getSampleIdAndReplicate = { meta, bam -&gt; [ meta.subMap(['id', 'repeat']), meta, file(bam) ] }\n\n    ch_normal_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'normal' }\n</code></pre> main.nf<pre><code>    ch_samples = Channel.fromPath(\"./data/samplesheet.csv\")\n        .splitCsv(header: true)\n        .map{ row -&gt;\n            [[id:row.id, repeat:row.repeat, type:row.type], row.bam]\n        }\n    ch_normal_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'normal' }\n</code></pre> <p>We've defined the map transformation as a named variable that we can reuse. Note that we also convert the file path to a Path object using <code>file()</code> so that any process receiving this channel can handle the file correctly (for more information see Working with files).</p> <p>Let's implement the closure in our workflow:</p> AfterBefore main.nf<pre><code>    ch_normal_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'normal' }\n         .map ( getSampleIdAndReplicate )\n    ch_tumor_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'tumor' }\n         .map ( getSampleIdAndReplicate )\n</code></pre> main.nf<pre><code>    ch_normal_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'normal' }\n        .map { meta, file -&gt; [meta.subMap(['id', 'repeat'], meta, file] }\n    ch_tumor_samples = ch_samples\n        .filter { meta, file -&gt; meta.type == 'tumor' }\n        .map { meta, file -&gt; [meta.subMap(['id', 'repeat'], meta, file] }\n</code></pre> <p>Note</p> <p>The <code>map</code> operator has switched from using <code>{ }</code> to using <code>( )</code> to pass the closure as an argument. This is because the <code>map</code> operator expects a closure as an argument and <code>{ }</code> is used to define an anonymous closure. When calling a named closure, use the <code>( )</code> syntax.</p> <p>Just run the workflow once more to check everything is still working:</p> Test the named closure<pre><code>nextflow run main.nf\n</code></pre> Samples using named closure<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [angry_meninsky] DSL2 - revision: 2edc226b1d\n\n[[id:patientA, repeat:1], [id:patientA, repeat:1, type:normal], patientA_rep1_normal.bam, [id:patientA, repeat:1, type:tumor], patientA_rep1_tumor.bam]\n[[id:patientA, repeat:2], [id:patientA, repeat:2, type:normal], patientA_rep2_normal.bam, [id:patientA, repeat:2, type:tumor], patientA_rep2_tumor.bam]\n[[id:patientB, repeat:1], [id:patientB, repeat:1, type:normal], patientB_rep1_normal.bam, [id:patientB, repeat:1, type:tumor], patientB_rep1_tumor.bam]\n[[id:patientC, repeat:1], [id:patientC, repeat:1, type:normal], patientC_rep1_normal.bam, [id:patientC, repeat:1, type:tumor], patientC_rep1_tumor.bam]\n</code></pre> <p>Using a named closure allows us to reuse the same transformation in multiple places, reducing the risk of errors and making the code more readable and maintainable.</p>"},{"location":"side_quests/splitting_and_grouping/#35-reduce-duplication-of-data","title":"3.5. Reduce duplication of data","text":"<p>We have a lot of duplicated data in our workflow. Each item in the joined samples repeats the <code>id</code> and <code>repeat</code> fields. Since this information is already available in the grouping key, we can avoid this redundancy. As a reminder, our current data structure looks like this:</p> <pre><code>[\n  [\n    \"id\": \"sampleC\",\n    \"repeat\": \"1\",\n  ],\n  [\n    \"id\": \"sampleC\",\n    \"repeat\": \"1\",\n    \"type\": \"normal\",\n  ],\n  \"sampleC_rep1_normal.bam\"\n  [\n    \"id\": \"sampleC\",\n    \"repeat\": \"1\",\n    \"type\": \"tumor\",\n  ],\n  \"sampleC_rep1_tumor.bam\"\n]\n</code></pre> <p>Since the <code>id</code> and <code>repeat</code> fields are available in the grouping key, let's remove them from the rest of each channel item to avoid duplication. We can do this by using the <code>subMap</code> method to create a new map with only the <code>type</code> field. This approach allows us to maintain all necessary information while eliminating redundancy in our data structure.</p> AfterBefore main.nf<pre><code>    getSampleIdAndReplicate = { meta, bam -&gt; [ meta.subMap(['id', 'repeat']), meta.subMap(['type']), file(bam) ] }\n</code></pre> main.nf<pre><code>    getSampleIdAndReplicate = { meta, bam -&gt; [ meta.subMap(['id', 'repeat']), meta, file(bam) ] }\n</code></pre> <p>Now the closure returns a tuple where the first element contains the <code>id</code> and <code>repeat</code> fields, and the second element contains only the <code>type</code> field. This eliminates redundancy by storing the <code>id</code> and <code>repeat</code> information once in the grouping key, while maintaining all necessary information.</p> <p>Run the workflow to see what this looks like:</p> Test data deduplication<pre><code>nextflow run main.nf\n</code></pre> Deduplicated sample data<pre><code>[[id:patientA, repeat:1], [type:normal], /workspaces/training/side-quests/splitting_and_grouping/patientA_rep1_normal.bam, [type:tumor], /workspaces/training/side-quests/splitting_and_grouping/patientA_rep1_tumor.bam]\n[[id:patientA, repeat:2], [type:normal], /workspaces/training/side-quests/splitting_and_grouping/patientA_rep2_normal.bam, [type:tumor], /workspaces/training/side-quests/splitting_and_grouping/patientA_rep2_tumor.bam]\n[[id:patientB, repeat:1], [type:normal], /workspaces/training/side-quests/splitting_and_grouping/patientB_rep1_normal.bam, [type:tumor], /workspaces/training/side-quests/splitting_and_grouping/patientB_rep1_tumor.bam]\n[[id:patientC, repeat:1], [type:normal], /workspaces/training/side-quests/splitting_and_grouping/patientC_rep1_normal.bam, [type:tumor], /workspaces/training/side-quests/splitting_and_grouping/patientC_rep1_tumor.bam]\n</code></pre> <p>We can see we only state the <code>id</code> and <code>repeat</code> fields once in the grouping key and we have the <code>type</code> field in the sample data. We haven't lost any information but we managed to make our channel contents more succinct.</p>"},{"location":"side_quests/splitting_and_grouping/#36-remove-redundant-information","title":"3.6. Remove redundant information","text":"<p>We removed duplicated information above, but we still have some other redundant information in our channels.</p> <p>In the beginning, we separated the normal and tumor samples using <code>filter</code>, then joined them based on <code>id</code> and <code>repeat</code> keys. The <code>join</code> operator preserves the order in which tuples are merged, so in our case, with normal samples on the left side and tumor samples on the right, the resulting channel maintains this structure: <code>id, &lt;normal elements&gt;, &lt;tumor elements&gt;</code>.</p> <p>Since we know the position of each element in our channel, we can simplify the structure further by dropping the <code>[type:normal]</code> and <code>[type:tumor]</code> metadata.</p> AfterBefore main.nf<pre><code>    getSampleIdAndReplicate = { meta, file -&gt; [ meta.subMap(['id', 'repeat']), file ] }\n</code></pre> main.nf<pre><code>    getSampleIdAndReplicate = { meta, file -&gt; [ meta.subMap(['id', 'repeat']), meta.subMap(['type']), file ] }\n</code></pre> <p>Run again to see the result:</p> Test streamlined data structure<pre><code>nextflow run main.nf\n</code></pre> Streamlined sample data<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [confident_leavitt] DSL2 - revision: a2303895bd\n\n[[id:patientA, repeat:1], patientA_rep1_normal.bam, patientA_rep1_tumor.bam]\n[[id:patientA, repeat:2], patientA_rep2_normal.bam, patientA_rep2_tumor.bam]\n[[id:patientB, repeat:1], patientB_rep1_normal.bam, patientB_rep1_tumor.bam]\n[[id:patientC, repeat:1], patientC_rep1_normal.bam, patientC_rep1_tumor.bam]\n</code></pre>"},{"location":"side_quests/splitting_and_grouping/#takeaway_3","title":"Takeaway","text":"<p>In this section, you've learned:</p> <ul> <li>Manipulating Tuples: How to use <code>map</code> to isolate a field in a tuple</li> <li>Joining Tuples: How to use <code>join</code> to combine tuples based on the first field</li> <li>Creating Joining Keys: How to use <code>subMap</code> to create a new joining key</li> <li>Named Closures: How to use a named closure in map</li> <li>Multiple Field Joining: How to join on multiple fields for more precise matching</li> <li>Data Structure Optimization: How to streamline channel structure by removing redundant information</li> </ul> <p>You now have a workflow that can split a samplesheet, filter the normal and tumor samples, join them together by sample ID and replicate number, then print the results.</p> <p>This is a common pattern in bioinformatics workflows where you need to match up samples or other types of data after processing independently, so it is a useful skill. Next, we will look at repeating a sample multiple times.</p>"},{"location":"side_quests/splitting_and_grouping/#4-spread-samples-intervals","title":"4. Spread samples intervals","text":"<p>A key pattern in bioinformatics workflows is distributing analysis across genomic regions. For instance, variant calling can be parallelized by dividing the genome into intervals (like chromosomes or smaller regions). This parallelization strategy significantly improves pipeline efficiency by distributing computational load across multiple cores or nodes, reducing overall execution time.</p> <p>In the following section, we'll demonstrate how to distribute our sample data across multiple genomic intervals. We'll pair each sample with every interval, allowing parallel processing of different genomic regions. This will multiply our dataset size by the number of intervals, creating multiple independent analysis units that can be brought back together later.</p>"},{"location":"side_quests/splitting_and_grouping/#41-spread-samples-over-intervals-using-combine","title":"4.1. Spread samples over intervals using <code>combine</code>","text":"<p>Let's start by creating a channel of intervals. To keep life simple, we will just use 3 intervals we will manually define. In a real workflow, you could read these in from a file input or even create a channel with lots of interval files.</p> AfterBefore main.nf<pre><code>        .join(ch_tumor_samples)\n    ch_intervals = Channel.of('chr1', 'chr2', 'chr3')\n</code></pre> main.nf<pre><code>        .join(ch_tumor_samples)\n    ch_joined_samples.view()\n</code></pre> <p>Now remember, we want to repeat each sample for each interval. This is sometimes referred to as the Cartesian product of the samples and intervals. We can achieve this by using the <code>combine</code> operator. This will take every item from channel 1 and repeat it for each item in channel 2. Let's add a combine operator to our workflow:</p> AfterBefore main.nf<pre><code>    ch_intervals = Channel.of('chr1', 'chr2', 'chr3')\n\n    ch_combined_samples = ch_joined_samples\n        .combine(ch_intervals)\n        .view()\n</code></pre> main.nf<pre><code>    ch_intervals = Channel.of('chr1', 'chr2', 'chr3')\n</code></pre> <p>Now let's run it and see what happens:</p> Test the combine operation<pre><code>nextflow run main.nf\n</code></pre> Samples combined with intervals<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [mighty_tesla] DSL2 - revision: ae013ab70b\n\n[[id:patientA, repeat:1], patientA_rep1_normal.bam, patientA_rep1_tumor.bam, chr1]\n[[id:patientA, repeat:1], patientA_rep1_normal.bam, patientA_rep1_tumor.bam, chr2]\n[[id:patientA, repeat:1], patientA_rep1_normal.bam, patientA_rep1_tumor.bam, chr3]\n[[id:patientA, repeat:2], patientA_rep2_normal.bam, patientA_rep2_tumor.bam, chr1]\n[[id:patientA, repeat:2], patientA_rep2_normal.bam, patientA_rep2_tumor.bam, chr2]\n[[id:patientA, repeat:2], patientA_rep2_normal.bam, patientA_rep2_tumor.bam, chr3]\n[[id:patientB, repeat:1], patientB_rep1_normal.bam, patientB_rep1_tumor.bam, chr1]\n[[id:patientB, repeat:1], patientB_rep1_normal.bam, patientB_rep1_tumor.bam, chr2]\n[[id:patientB, repeat:1], patientB_rep1_normal.bam, patientB_rep1_tumor.bam, chr3]\n[[id:patientC, repeat:1], patientC_rep1_normal.bam, patientC_rep1_tumor.bam, chr1]\n[[id:patientC, repeat:1], patientC_rep1_normal.bam, patientC_rep1_tumor.bam, chr2]\n[[id:patientC, repeat:1], patientC_rep1_normal.bam, patientC_rep1_tumor.bam, chr3]\n</code></pre> <p>Success! We have repeated every sample for every single interval in our 3 interval list. We've effectively tripled the number of items in our channel. It's a little hard to read though, so in the next section we will tidy it up.</p>"},{"location":"side_quests/splitting_and_grouping/#42-organise-the-channel","title":"4.2. Organise the channel","text":"<p>We can use the <code>map</code> operator to tidy and refactor our sample data so it's easier to understand. Let's move the intervals string to the joining map at the first element.</p> AfterBefore main.nf<pre><code>    ch_combined_samples = ch_joined_samples\n        .combine(ch_intervals)\n        .map { grouping_key, normal, tumor, interval -&gt;\n            [\n                grouping_key + [interval: interval],\n                normal,\n                tumor\n            ]\n        }\n        .view()\n</code></pre> main.nf<pre><code>    ch_combined_samples = ch_joined_samples\n        .combine(ch_intervals)\n        .view()\n</code></pre> <p>Let's break down what this map operation does step by step.</p> <p>First, we use named parameters to make the code more readable. By using the names <code>grouping_key</code>, <code>normal</code>, <code>tumor</code> and <code>interval</code>, we can refer to the elements in the tuple by name instead of by index:</p> <pre><code>        .map { grouping_key, normal, tumor, interval -&gt;\n</code></pre> <p>Next, we combine the <code>grouping_key</code> with the <code>interval</code> field. The <code>grouping_key</code> is a map containing <code>id</code> and <code>repeat</code> fields. We create a new map with the <code>interval</code> and merge them using Groovy's map addition (<code>+</code>):</p> <pre><code>                grouping_key + [interval: interval],\n</code></pre> <p>Finally, we return this as a tuple with three elements: the combined metadata map, the normal sample file, and the tumor sample file:</p> <pre><code>            [\n                grouping_key + [interval: interval],\n                normal,\n                tumor\n            ]\n</code></pre> <p>Let's run it again and check the channel contents:</p> Test the reorganized structure<pre><code>nextflow run main.nf\n</code></pre> Samples combined with intervals<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [sad_hawking] DSL2 - revision: 1f6f6250cd\n\n[[id:patientA, interval:chr1], patientA_rep1_normal.bam, patientA_rep1_tumor.bam]\n[[id:patientA, interval:chr2], patientA_rep1_normal.bam, patientA_rep1_tumor.bam]\n[[id:patientA, interval:chr3], patientA_rep1_normal.bam, patientA_rep1_tumor.bam]\n[[id:patientA, interval:chr1], patientA_rep2_normal.bam, patientA_rep2_tumor.bam]\n[[id:patientA, interval:chr2], patientA_rep2_normal.bam, patientA_rep2_tumor.bam]\n[[id:patientA, interval:chr3], patientA_rep2_normal.bam, patientA_rep2_tumor.bam]\n[[id:patientB, interval:chr1], patientB_rep1_normal.bam, patientB_rep1_tumor.bam]\n[[id:patientB, interval:chr2], patientB_rep1_normal.bam, patientB_rep1_tumor.bam]\n[[id:patientB, interval:chr3], patientB_rep1_normal.bam, patientB_rep1_tumor.bam]\n[[id:patientC, interval:chr1], patientC_rep1_normal.bam, patientC_rep1_tumor.bam]\n[[id:patientC, interval:chr2], patientC_rep1_normal.bam, patientC_rep1_tumor.bam]\n[[id:patientC, interval:chr3], patientC_rep1_normal.bam, patientC_rep1_tumor.bam]\n</code></pre> <p>Using <code>map</code> to coerce your data into the correct structure can be tricky, but it's crucial for effective data manipulation.</p> <p>We now have every sample repeated across all genomic intervals, creating multiple independent analysis units that can be processed in parallel. But what if we want to bring related samples back together? In the next section, we'll learn how to group samples that share common attributes.</p>"},{"location":"side_quests/splitting_and_grouping/#takeaway_4","title":"Takeaway","text":"<p>In this section, you've learned:</p> <ul> <li>Spreading samples over intervals: How to use <code>combine</code> to repeat samples over intervals</li> <li>Creating Cartesian products: How to generate all combinations of samples and intervals</li> <li>Organizing channel structure: How to use <code>map</code> to restructure data for better readability</li> <li>Parallel processing preparation: How to set up data for distributed analysis</li> </ul>"},{"location":"side_quests/splitting_and_grouping/#5-aggregating-samples-using-grouptuple","title":"5. Aggregating samples using <code>groupTuple</code>","text":"<p>In the previous sections, we learned how to split data from an input file and filter by specific fields (in our case normal and tumor samples). But this only covers a single type of joining. What if we want to group samples by a specific attribute? For example, instead of joining matched normal-tumor pairs, we might want to process all samples from \"sampleA\" together regardless of their type. This pattern is common in bioinformatics workflows where you may want to process related samples separately for efficiency reasons before comparing or combining the results at the end.</p> <p>Nextflow includes built in methods to do this, the main one we will look at is <code>groupTuple</code>.</p> <p>Let's start by grouping all of our samples that have the same <code>id</code> and <code>interval</code> fields, this would be typical of an analysis where we wanted to group technical replicates but keep meaningfully different samples separated.</p> <p>To do this, we should separate out our grouping variables so we can use them in isolation.</p> <p>The first step is similar to what we did in the previous section. We must isolate our grouping variable as the first element of the tuple. Remember, our first element is currently a map of <code>id</code>, <code>repeat</code> and <code>interval</code> fields:</p> main.nf<pre><code>{\n  \"id\": \"sampleA\",\n  \"repeat\": \"1\",\n  \"interval\": \"chr1\"\n}\n</code></pre> <p>We can reuse the <code>subMap</code> method from before to isolate our <code>id</code> and <code>interval</code> fields from the map. Like before, we will use <code>map</code> operator to apply the <code>subMap</code> method to the first element of the tuple for each sample.</p> AfterBefore main.nf<pre><code>    ch_combined_samples = ch_joined_samples\n        .combine(ch_intervals)\n        .map { grouping_key, normal, tumor, interval -&gt;\n            [\n                grouping_key + [interval: interval],\n                normal,\n                tumor\n            ]\n        }\n\n    ch_grouped_samples = ch_combined_samples\n        .map { grouping_key, normal, tumor -&gt;\n            [\n                grouping_key.subMap('id', 'interval'),\n                normal,\n                tumor\n            ]\n          }\n          .view()\n</code></pre> main.nf<pre><code>    ch_combined_samples = ch_joined_samples\n        .combine(ch_intervals)\n        .map { grouping_key, normal, tumor, interval -&gt;\n            [\n                grouping_key + [interval: interval],\n                normal,\n                tumor\n            ]\n        }\n        .view()\n</code></pre> <p>Let's run it again and check the channel contents:</p> Test grouping key isolation<pre><code>nextflow run main.nf\n</code></pre> Samples prepared for grouping<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [hopeful_brenner] DSL2 - revision: 7f4f7fea76\n\n[[id:patientA, interval:chr1], patientA_rep1_normal.bam, patientA_rep1_tumor.bam]\n[[id:patientA, interval:chr2], patientA_rep1_normal.bam, patientA_rep1_tumor.bam]\n[[id:patientA, interval:chr3], patientA_rep1_normal.bam, patientA_rep1_tumor.bam]\n[[id:patientA, interval:chr1], patientA_rep2_normal.bam, patientA_rep2_tumor.bam]\n[[id:patientA, interval:chr2], patientA_rep2_normal.bam, patientA_rep2_tumor.bam]\n[[id:patientA, interval:chr3], patientA_rep2_normal.bam, patientA_rep2_tumor.bam]\n[[id:patientB, interval:chr1], patientB_rep1_normal.bam, patientB_rep1_tumor.bam]\n[[id:patientB, interval:chr2], patientB_rep1_normal.bam, patientB_rep1_tumor.bam]\n[[id:patientB, interval:chr3], patientB_rep1_normal.bam, patientB_rep1_tumor.bam]\n[[id:patientC, interval:chr1], patientC_rep1_normal.bam, patientC_rep1_tumor.bam]\n[[id:patientC, interval:chr2], patientC_rep1_normal.bam, patientC_rep1_tumor.bam]\n[[id:patientC, interval:chr3], patientC_rep1_normal.bam, patientC_rep1_tumor.bam]\n</code></pre> <p>We can see that we have successfully isolated the <code>id</code> and <code>interval</code> fields, but not grouped the samples yet.</p> <p>Note</p> <p>We are discarding the <code>replicate</code> field here. This is because we don't need it for further downstream processing. After completing this tutorial, see if you can include it without affecting the later grouping!</p> <p>Let's now group the samples by this new grouping element, using the <code>groupTuple</code> operator.</p> AfterBefore main.nf<pre><code>    ch_grouped_samples = ch_combined_samples\n        .map { grouping_key, normal, tumor -&gt;\n            [\n                grouping_key.subMap('id', 'interval'),\n                normal,\n                tumor\n            ]\n          }\n          .groupTuple()\n          .view()\n</code></pre> main.nf<pre><code>    ch_grouped_samples = ch_combined_samples\n        .map { grouping_key, normal, tumor -&gt;\n            [\n                grouping_key.subMap('id', 'interval'),\n                normal,\n                tumor\n            ]\n          }\n          .view()\n</code></pre> <p>That's all there is to it! We just added a single line of code. Let's see what happens when we run it:</p> Test the groupTuple operation<pre><code>nextflow run main.nf\n</code></pre> Grouped samples by ID and interval<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `main.nf` [friendly_jang] DSL2 - revision: a1bee1c55d\n\n[[id:patientA, interval:chr1], [patientA_rep1_normal.bam, patientA_rep2_normal.bam], [patientA_rep1_tumor.bam, patientA_rep2_tumor.bam]]\n[[id:patientA, interval:chr2], [patientA_rep1_normal.bam, patientA_rep2_normal.bam], [patientA_rep1_tumor.bam, patientA_rep2_tumor.bam]]\n[[id:patientA, interval:chr3], [patientA_rep1_normal.bam, patientA_rep2_normal.bam], [patientA_rep1_tumor.bam, patientA_rep2_tumor.bam]]\n[[id:patientB, interval:chr1], [patientB_rep1_normal.bam], [patientB_rep1_tumor.bam]]\n[[id:patientB, interval:chr2], [patientB_rep1_normal.bam], [patientB_rep1_tumor.bam]]\n[[id:patientB, interval:chr3], [patientB_rep1_normal.bam], [patientB_rep1_tumor.bam]]\n[[id:patientC, interval:chr1], [patientC_rep1_normal.bam], [patientC_rep1_tumor.bam]]\n[[id:patientC, interval:chr2], [patientC_rep1_normal.bam], [patientC_rep1_tumor.bam]]\n[[id:patientC, interval:chr3], [patientC_rep1_normal.bam], [patientC_rep1_tumor.bam]]\n</code></pre> <p>Note our data has changed structure and within each channel element the files now contained in tuples like <code>[patientA_rep1_normal.bam, patientA_rep2_normal.bam]</code>. This is because when we use <code>groupTuple</code>, Nextflow combines the single files for each sample of a group. This is important to remember when trying to handle the data downstream.</p> <p>Note</p> <p><code>transpose</code> is the opposite of groupTuple. It unpacks the items in a channel and flattens them. Try and add <code>transpose</code> and undo the grouping we performed above!</p>"},{"location":"side_quests/splitting_and_grouping/#takeaway_5","title":"Takeaway","text":"<p>In this section, you've learned:</p> <ul> <li>Grouping related samples: How to use <code>groupTuple</code> to aggregate samples by common attributes</li> <li>Isolating grouping keys: How to use <code>subMap</code> to extract specific fields for grouping</li> <li>Handling grouped data structures: How to work with the nested structure created by <code>groupTuple</code></li> <li>Technical replicate handling: How to group samples that share the same experimental conditions</li> </ul>"},{"location":"side_quests/splitting_and_grouping/#summary","title":"Summary","text":"<p>In this side quest, you've learned how to split and group data using channels. By modifying the data as it flows through the pipeline, you can construct a pipeline that handles as many items as possible with no loops or while statements. It gracefully scales to large numbers of items. Here's what we achieved:</p> <ol> <li> <p>Read in samplesheet with splitCsv: We read in a CSV file with sample data and viewed the contents.</p> </li> <li> <p>Use filter (and/or map) to manipulate into 2 separate channels: We used <code>filter</code> to split the data into two channels based on the <code>type</code> field.</p> </li> <li> <p>Join on ID and repeat: We used <code>join</code> to join the two channels on the <code>id</code> and <code>repeat</code> fields.</p> </li> <li> <p>Combine by intervals: We used <code>combine</code> to create Cartesian products of samples with genomic intervals.</p> </li> <li> <p>Group by ID and interval: We used <code>groupTuple</code> to group samples by the <code>id</code> and <code>interval</code> fields, aggregating technical replicates.</p> </li> </ol> <p>This approach offers several advantages over writing a pipeline as more standard code, such as using for and while loops:</p> <ul> <li>We can scale to as many or as few inputs as we want with no additional code</li> <li>We focus on handling the flow of data through the pipeline, instead of iteration</li> <li>We can be as complex or simple as required</li> <li>The pipeline becomes more declarative, focusing on what should happen rather than how it should happen</li> <li>Nextflow will optimize execution for us by running independent operations in parallel</li> </ul> <p>By mastering these channel operations, you can build flexible, scalable pipelines that handle complex data relationships without resorting to loops or iterative programming. This declarative approach allows Nextflow to optimize execution and parallelize independent operations automatically.</p>"},{"location":"side_quests/splitting_and_grouping/#key-concepts","title":"Key Concepts","text":"<ul> <li>Reading data sheets</li> </ul> <pre><code>// Read CSV with header\nChannel.fromPath('samplesheet.csv')\n    .splitCsv(header: true)\n</code></pre> <ul> <li>Filtering</li> </ul> <pre><code>// Filter channel based on condition\nchannel.filter { it.type == 'tumor' }\n</code></pre> <ul> <li>Joining Channels</li> </ul> <pre><code>// Join two channels by key (first element of tuple)\ntumor_ch.join(normal_ch)\n\n// Extract joining key and join by this value\ntumor_ch.map { meta, file -&gt; [meta.id, meta, file] }\n    .join(\n       normal_ch.map { meta, file -&gt; [meta.id, meta, file] }\n     )\n\n// Join on multiple fields using subMap\ntumor_ch.map { meta, file -&gt; [meta.subMap(['id', 'repeat']), meta, file] }\n    .join(\n       normal_ch.map { meta, file -&gt; [meta.subMap(['id', 'repeat']), meta, file] }\n     )\n</code></pre> <ul> <li>Grouping Data</li> </ul> <pre><code>// Group by the first element in each tuple\nchannel.groupTuple()\n</code></pre> <ul> <li>Combining Channels</li> </ul> <pre><code>// Combine with Cartesian product\nsamples_ch.combine(intervals_ch)\n</code></pre> <ul> <li>Data Structure Optimization</li> </ul> <pre><code>// Extract specific fields using subMap\nmeta.subMap(['id', 'repeat'])\n\n// Named closures for reusable transformations\ngetSampleIdAndReplicate = { meta, file -&gt; [meta.subMap(['id', 'repeat']), file] }\nchannel.map(getSampleIdAndReplicate)\n</code></pre>"},{"location":"side_quests/splitting_and_grouping/#resources","title":"Resources","text":"<ul> <li>filter</li> <li>map</li> <li>join</li> <li>groupTuple</li> <li>combine</li> </ul>"},{"location":"side_quests/workflows_of_workflows/","title":"Workflows of Workflows","text":"<p>One of the most powerful features of Nextflow is its ability to compose complex pipelines from smaller, reusable workflow modules. This modular approach makes pipelines easier to develop, test, and maintain.</p> <p>Let's explore why workflow composition is so important. When you're developing a pipeline, you often find yourself creating similar sequences of processes for different data types or analysis steps. Without workflow composition, you might end up copying and pasting these process sequences, leading to duplicated code that's hard to maintain. Or you might create one massive workflow that's difficult to understand and modify.</p> <p>With workflow composition, you can:</p> <ul> <li>Break down complex pipelines into logical, reusable units</li> <li>Test each workflow module independently</li> <li>Mix and match workflows to create new pipelines</li> <li>Share common workflow modules across different pipelines</li> <li>Make your code more maintainable and easier to understand</li> </ul> <p>In this side quest, we'll create a pipeline that demonstrates these benefits by:</p> <ol> <li>Creating independent workflow modules that can be tested and used separately</li> <li>Composing these modules into a larger pipeline</li> <li>Using Nextflow's workflow composition features to manage data flow between modules</li> </ol>"},{"location":"side_quests/workflows_of_workflows/#0-warmup","title":"0. Warmup","text":""},{"location":"side_quests/workflows_of_workflows/#01-prerequisites","title":"0.1. Prerequisites","text":"<p>Before taking on this side quest you should:</p> <ul> <li>Complete the Hello Nextflow tutorial</li> <li>Understand basic Nextflow concepts (processes, channels, operators)</li> </ul>"},{"location":"side_quests/workflows_of_workflows/#02-starting-point","title":"0.2. Starting Point","text":"<p>Let's move into the project directory.</p> <pre><code>cd side-quests/workflows_of_workflows\n</code></pre> <p>You'll find a <code>modules</code> directory containing several process definitions that build upon what you learned in 'Hello Nextflow':</p> Directory contents<pre><code>modules/\n\u251c\u2500\u2500 say_hello.nf             # Creates a greeting (from Hello Nextflow)\n\u251c\u2500\u2500 say_hello_upper.nf       # Converts to uppercase (from Hello Nextflow)\n\u251c\u2500\u2500 timestamp_greeting.nf    # Adds timestamps to greetings\n\u251c\u2500\u2500 validate_name.nf         # Validates input names\n\u2514\u2500\u2500 reverse_text.nf          # Reverses text content\n</code></pre> <p>We're going to compose these modules into two separate workflows that we will then compose into a main workflow.</p>"},{"location":"side_quests/workflows_of_workflows/#1-create-the-greeting-workflow","title":"1. Create the Greeting Workflow","text":"<p>Let's start by creating a workflow that validates names and generates timestamped greetings.</p>"},{"location":"side_quests/workflows_of_workflows/#11-create-the-workflow-structure","title":"1.1. Create the workflow structure","text":"Create workflow directory and file<pre><code>mkdir -p workflows\ntouch workflows/greeting.nf\n</code></pre>"},{"location":"side_quests/workflows_of_workflows/#12-add-the-first-subworkflow-code","title":"1.2. Add the first (sub)workflow code","text":"<p>Add this code to <code>workflows/greeting.nf</code>:</p> workflows/greeting.nf<pre><code>include { VALIDATE_NAME } from '../modules/validate_name'\ninclude { SAY_HELLO } from '../modules/say_hello'\ninclude { TIMESTAMP_GREETING } from '../modules/timestamp_greeting'\n\nworkflow {\n\n    names_ch = Channel.from('Alice', 'Bob', 'Charlie')\n\n    // Chain processes: validate -&gt; create greeting -&gt; add timestamp\n    validated_ch = VALIDATE_NAME(names_ch)\n    greetings_ch = SAY_HELLO(validated_ch)\n    timestamped_ch = TIMESTAMP_GREETING(greetings_ch)\n}\n</code></pre> <p>This is a complete workflow, with a structure similar to the ones you saw in the 'Hello Nextflow' tutorial, that we can test independently. Let's try that now:</p> Run the greeting workflow<pre><code>nextflow run workflows/greeting.nf\n</code></pre> Expected output<pre><code>N E X T F L O W  ~  version 24.10.0\nLaunching `workflows/greeting.nf` [peaceful_montalcini] DSL2 - revision: 90f61b7093\nexecutor &gt;  local (9)\n[51/4f980f] process &gt; VALIDATE_NAME (validating Bob)                    [100%] 3 of 3 \u2714\n[2b/dd8dc2] process &gt; SAY_HELLO (greeting Bob)                          [100%] 3 of 3 \u2714\n[8e/882565] process &gt; TIMESTAMP_GREETING (adding timestamp to greeting) [100%] 3 of 3 \u2714\n</code></pre> <p>This works as expected, but to make it composable there's a few things we need to change.</p>"},{"location":"side_quests/workflows_of_workflows/#13-make-the-workflow-composable","title":"1.3. Make the workflow composable","text":"<p>Composable workflows have some differences from the ones you saw in the 'Hello Nextflow' tutorial:</p> <ul> <li>The workflow block needs to be named</li> <li>Inputs are declared using the <code>take:</code> keyword</li> <li>Workflow content is placed inside the <code>main:</code> block</li> <li>Outputs are declared using the <code>emit:</code> keyword</li> </ul> <p>Let's update the greeting workflow to match this structure. Change the code to the following:</p> workflows/greeting.nf<pre><code>include { VALIDATE_NAME } from '../modules/validate_name'\ninclude { SAY_HELLO } from '../modules/say_hello'\ninclude { TIMESTAMP_GREETING } from '../modules/timestamp_greeting'\n\nworkflow GREETING_WORKFLOW {\n    take:\n        names_ch        // Input channel with names\n\n    main:\n        // Chain processes: validate -&gt; create greeting -&gt; add timestamp\n        validated_ch = VALIDATE_NAME(names_ch)\n        greetings_ch = SAY_HELLO(validated_ch)\n        timestamped_ch = TIMESTAMP_GREETING(greetings_ch)\n\n    emit:\n        greetings = greetings_ch      // Original greetings\n        timestamped = timestamped_ch  // Timestamped greetings\n}\n</code></pre> <p>You can see that the workflow is now named and has a <code>take:</code> and <code>emit:</code> block, and these are the connections we will use to compose a higher level workflow. The workflow content is also placed inside the <code>main:</code> block. Note also that we have removed the <code>names_ch</code> input channel declaration, as it's now passed as an argument to the workflow.</p> <p>Let's test the workflow again to see if it works as expected:</p> Run the greeting workflow<pre><code>nextflow run workflows/greeting.nf\n</code></pre> <p>What you'll actually see in response is:</p> Expected output<pre><code>N E X T F L O W  ~  version 24.10.0\nLaunching `workflows/greeting.nf` [high_brahmagupta] DSL2 - revision: 8f5857af25\nWARN: No entry workflow specified\n</code></pre> <p>This tells you about another new concept, an 'entry workflow'. The entry workflow is the workflow that gets called when you run a Nextflow script. By default, Nextflow will use an un-named workflow as the entry workflow, when present, and that's what you've been doing so far, with workflow blocks starting like this:</p> hello.nf<pre><code>workflow {\n</code></pre> <p>But our greeting workflow doesn't have an un-named workflow, rather we have a named workflow:</p> workflows/greeting.nf<pre><code>workflow GREETING_WORKFLOW {\n</code></pre> <p>... so Nextflow will throw an error. We can actually tell Nextflow to use our named workflow as the entry workflow by adding this line to Nextflow's command line:</p> Run the greeting workflow<pre><code>nextflow run workflows/greeting.nf -entry GREETING_WORKFLOW\n</code></pre> <p>This will also throw an error, because the workflow is expecting an input channel:</p> Expected output<pre><code>N E X T F L O W  ~  version 24.10.0\nLaunching `workflows/greeting.nf` [compassionate_fermi] DSL2 - revision: 8f5857af25\nERROR ~ Workflow `GREETING_WORKFLOW` declares 1 input channels but 0 were given\n\n -- Check '.nextflow.log' file for details\n</code></pre> <p>... but if you wanted to call a named workflow that didn't require inputs, you could call it this way.</p> <p>But we didn't add that syntax so we could call the workflow directly, we did it so we could compose it with other workflows. Let's start by creating a main workflow that imports and uses the <code>greeting</code> workflow.</p>"},{"location":"side_quests/workflows_of_workflows/#14-create-and-test-the-main-workflow","title":"1.4. Create and test the main workflow","text":"<p>Now we will create a main workflow that imports and uses the <code>greeting</code> workflow.</p> <p>Create <code>main.nf</code>:</p> main.nf<pre><code>include { GREETING_WORKFLOW } from './workflows/greeting'\n\nworkflow {\n    names = Channel.from('Alice', 'Bob', 'Charlie')\n    GREETING_WORKFLOW(names)\n\n    GREETING_WORKFLOW.out.greetings.view { \"Original: $it\" }\n    GREETING_WORKFLOW.out.timestamped.view { \"Timestamped: $it\" }\n}\n</code></pre> <p>Note that our workflow entry in this file is un-named, and that's because we're going to use it as an entry workflow.</p> <p>Run this and see the output:</p> Run the workflow<pre><code>nextflow run main.nf\n</code></pre> Expected output<pre><code>N E X T F L O W  ~  version 24.10.0\nLaunching `main.nf` [goofy_mayer] DSL2 - revision: 543f8742fe\nexecutor &gt;  local (9)\n[05/3cc752] process &gt; GREETING_WORKFLOW:VALIDATE_NAME (validating Char... [100%] 3 of 3 \u2714\n[b1/b56ecf] process &gt; GREETING_WORKFLOW:SAY_HELLO (greeting Charlie)      [100%] 3 of 3 \u2714\n[ea/342168] process &gt; GREETING_WORKFLOW:TIMESTAMP_GREETING (adding tim... [100%] 3 of 3 \u2714\nOriginal: /workspaces/training/side_quests/workflows_of_workflows/work/bb/c8aff3df0ebc15a4d7d35f736db44c/Alice-output.txt\nOriginal: /workspaces/training/side_quests/workflows_of_workflows/work/fb/fa877776e8a5d90b537b1bcd3b6f5b/Bob-output.txt\nOriginal: /workspaces/training/side_quests/workflows_of_workflows/work/b1/b56ecf938fda8bcbec211847c8f0be/Charlie-output.txt\nTimestamped: /workspaces/training/side_quests/workflows_of_workflows/work/06/877bc909f140bbf8223343450cea36/timestamped_Alice-output.txt\nTimestamped: /workspaces/training/side_quests/workflows_of_workflows/work/aa/bd31b71cdb745b7c155ca7f8837b8a/timestamped_Bob-output.txt\nTimestamped: /workspaces/training/side_quests/workflows_of_workflows/work/ea/342168d4ba04cc899a89c56cbfd9b0/timestamped_Charlie-output.txt\n</code></pre> <p>It works! We've wrapped the named greeting workflow in a main workflow with an un-named entry <code>workflow</code> block. The main workflow is using the <code>GREETING_WORKFLOW</code> workflow almost (not quite) like a process, and passing the <code>names</code> channel as an argument.</p>"},{"location":"side_quests/workflows_of_workflows/#takeaway","title":"Takeaway","text":"<p>In this section, you've learned several important concepts:</p> <ul> <li>Named Workflows: Creating a named workflow (<code>GREETING_WORKFLOW</code>) that can be imported and reused</li> <li>Workflow Interfaces: Defining clear inputs with <code>take:</code> and outputs with <code>emit:</code> to create a composable workflow</li> <li>Entry Points: Understanding that Nextflow needs an entry workflow (either unnamed or specified with <code>-entry</code>)</li> <li>Workflow Composition: Importing and using a named workflow within another workflow</li> <li>Workflow Namespaces: Accessing workflow outputs using the <code>.out</code> namespace (<code>GREETING_WORKFLOW.out.greetings</code>)</li> </ul> <p>You now have a working greeting workflow that:</p> <ul> <li>Takes a channel of names as input</li> <li>Validates each name</li> <li>Creates a greeting for each valid name</li> <li>Adds timestamps to the greetings</li> <li>Exposes both original and timestamped greetings as outputs</li> </ul> <p>This modular approach allows you to test the greeting workflow independently or use it as a component in larger pipelines.</p>"},{"location":"side_quests/workflows_of_workflows/#2-add-the-transform-workflow","title":"2. Add the Transform Workflow","text":"<p>Now let's create a workflow that applies text transformations to the greetings.</p>"},{"location":"side_quests/workflows_of_workflows/#21-create-the-workflow-file","title":"2.1. Create the workflow file","text":"Create transform workflow<pre><code>touch workflows/transform.nf\n</code></pre>"},{"location":"side_quests/workflows_of_workflows/#22-add-the-workflow-code","title":"2.2. Add the workflow code","text":"<p>Add this code to <code>workflows/transform.nf</code>:</p> workflows/transform.nf<pre><code>include { SAY_HELLO_UPPER } from '../modules/say_hello_upper'\ninclude { REVERSE_TEXT } from '../modules/reverse_text'\n\nworkflow TRANSFORM_WORKFLOW {\n    take:\n        input_ch         // Input channel with messages\n\n    main:\n        // Apply transformations in sequence\n        upper_ch = SAY_HELLO_UPPER(input_ch)\n        reversed_ch = REVERSE_TEXT(upper_ch)\n\n    emit:\n        upper = upper_ch        // Uppercase greetings\n        reversed = reversed_ch  // Reversed uppercase greetings\n}\n</code></pre> <p>We won't repeat the explanation of the composable syntax here, but note the named workflow is again declared with a <code>take:</code> and <code>emit:</code> block, and the workflow content is placed inside the <code>main:</code> block.</p>"},{"location":"side_quests/workflows_of_workflows/#23-update-the-main-workflow","title":"2.3. Update the main workflow","text":"<p>Update <code>main.nf</code> to use both workflows:</p> main.nf<pre><code>include { GREETING_WORKFLOW } from './workflows/greeting'\ninclude { TRANSFORM_WORKFLOW } from './workflows/transform'\n\nworkflow {\n    names = Channel.from('Alice', 'Bob', 'Charlie')\n\n    // Run the greeting workflow\n    GREETING_WORKFLOW(names)\n\n    // Run the transform workflow\n    TRANSFORM_WORKFLOW(GREETING_WORKFLOW.out.timestamped)\n\n    // View results\n    TRANSFORM_WORKFLOW.out.upper.view { \"Uppercase: $it\" }\n    TRANSFORM_WORKFLOW.out.reversed.view { \"Reversed: $it\" }\n}\n</code></pre> <p>Run the complete pipeline:</p> Run the workflow<pre><code>nextflow run main.nf\n</code></pre> Expected output<pre><code>N E X T F L O W  ~  version 24.10.0\nLaunching `main.nf` [sick_kimura] DSL2 - revision: 8dc45fc6a8\nexecutor &gt;  local (13)\nexecutor &gt;  local (15)\n[83/1b51f4] process &gt; GREETING_WORKFLOW:VALIDATE_NAME (validating Alice)  [100%] 3 of 3 \u2714\n[68/556150] process &gt; GREETING_WORKFLOW:SAY_HELLO (greeting Alice)        [100%] 3 of 3 \u2714\n[de/511abd] process &gt; GREETING_WORKFLOW:TIMESTAMP_GREETING (adding tim... [100%] 3 of 3 \u2714\n[cd/e6a7e0] process &gt; TRANSFORM_WORKFLOW:SAY_HELLO_UPPER (converting t... [100%] 3 of 3 \u2714\n[f0/74ba4a] process &gt; TRANSFORM_WORKFLOW:REVERSE_TEXT (reversing UPPER... [100%] 3 of 3 \u2714\nUppercase: /workspaces/training/side_quests/workflows_of_workflows/work/a0/d4f5df4d6344604498fa47a6084a11/UPPER-timestamped_Bob-output.txt\nUppercase: /workspaces/training/side_quests/workflows_of_workflows/work/69/b5e37f6c79c2fd38adb75d0eca8f87/UPPER-timestamped_Charlie-output.txt\nUppercase: /workspaces/training/side_quests/workflows_of_workflows/work/cd/e6a7e0b17e7d5a2f71bb8123cd53a7/UPPER-timestamped_Alice-output.txt\nReversed: /workspaces/training/side_quests/workflows_of_workflows/work/7a/7a222f7957b35d1d121338566a24ac/REVERSED-UPPER-timestamped_Bob-output.txt\nReversed: /workspaces/training/side_quests/workflows_of_workflows/work/46/8d19af62e33a5a6417c773496e0f90/REVERSED-UPPER-timestamped_Charlie-output.txt\nReversed: /workspaces/training/side_quests/workflows_of_workflows/work/f0/74ba4a10d9ef5c82f829d1c154d0f6/REVERSED-UPPER-timestamped_Alice-output.txt\n</code></pre> <p>If you take a look at one of those reversed files, you'll see that it's the uppercase version of the greeting reversed:</p> Check a final output file<pre><code>cat /workspaces/training/side_quests/workflows_of_workflows/work/f0/74ba4a10d9ef5c82f829d1c154d0f6/REVERSED-UPPER-timestamped_Alice-output.txt\n</code></pre> Reversed file content<pre><code>!ECILA ,OLLEH ]04:50:71 60-30-5202[\n</code></pre>"},{"location":"side_quests/workflows_of_workflows/#takeaway_1","title":"Takeaway","text":"<p>You should now have a complete pipeline that:</p> <ul> <li>Processes names through the greeting workflow</li> <li>Feeds the timestamped greetings into the transform workflow</li> <li>Produces both uppercase and reversed versions of the greetings</li> </ul>"},{"location":"side_quests/workflows_of_workflows/#summary","title":"Summary","text":"<p>In this side quest, we've explored the powerful concept of workflow composition in Nextflow, which allows us to build complex pipelines from smaller, reusable components. Here's what we've accomplished:</p> <ol> <li> <p>Created Modular Workflows: We built two independent workflow modules:</p> </li> <li> <p>A <code>GREETING_WORKFLOW</code> that validates names, creates greetings, and adds timestamps</p> </li> <li> <p>A <code>TRANSFORM_WORKFLOW</code> that converts text to uppercase and reverses it</p> </li> <li> <p>Composed Workflows Together: We connected these workflows in a main pipeline, demonstrating how data can flow from one workflow to another.</p> </li> <li> <p>Used Workflow Interfaces: We defined clear inputs and outputs for each workflow using the <code>take:</code> and <code>emit:</code> syntax, creating well-defined interfaces between components.</p> </li> <li> <p>Managed Data Flow: We learned how to access workflow outputs using the namespace notation (<code>WORKFLOW_NAME.out.channel_name</code>) and pass them to other workflows.</p> </li> <li> <p>Practiced Modular Design: We experienced firsthand how breaking a pipeline into logical components makes the code more maintainable and easier to understand.</p> </li> <li> <p>Worked with Entry Points: We learned that Nextflow requires an entry workflow (either unnamed or specified with <code>-entry</code>) to know where to start execution.</p> </li> <li> <p>Structured Workflow Content: We wrapped workflow logic within the <code>main:</code> block.</p> </li> </ol> <p>This modular approach offers several advantages over monolithic pipelines:</p> <ul> <li>Each workflow can be developed, tested, and debugged independently</li> <li>Workflows can be reused across different pipelines</li> <li>The overall pipeline structure becomes more readable and maintainable</li> <li>Changes to one workflow don't necessarily affect others if the interfaces remain consistent</li> <li>Entry points can be configured to run different parts of your pipeline as needed</li> </ul> <p>It's important to note that while calling workflows is a bit like calling processes, it's not the same. You can't, for example, run a workflow n times by calling it with a channel of size n - you would need to pass a channel of size n to the workflow and iterate internally.</p> <p>By mastering workflow composition, you're now equipped to build more sophisticated Nextflow pipelines that can handle complex bioinformatics tasks while remaining maintainable and scalable.</p>"},{"location":"side_quests/workflows_of_workflows/#key-concepts","title":"Key Concepts","text":"<ol> <li>Workflow Inclusion</li> </ol> <pre><code>// Include a single workflow\ninclude { WORKFLOW_NAME } from './path/to/workflow'\n\n// Include multiple workflows\ninclude { WORKFLOW_A; WORKFLOW_B } from './path/to/workflows'\n\n// Include with alias to avoid name conflicts\ninclude { WORKFLOW_A as WORKFLOW_A_ALIAS } from './path/to/workflow'\n</code></pre> <ol> <li>Workflow Inputs and Outputs</li> </ol> <pre><code>workflow EXAMPLE {\n    take:\n        input_ch    // Declare inputs\n    emit:\n        output_ch   // Declare outputs\n}\n</code></pre> <ol> <li>Main Block Structure</li> </ol> <pre><code>workflow EXAMPLE_WORKFLOW {\n    take:\n        // Input channels are declared here\n        input_ch\n\n    main:\n        // Workflow logic goes here\n        // This is where processes are called and channels are manipulated\n        result_ch = SOME_PROCESS(input_ch)\n\n    emit:\n        // Output channels are declared here\n        output_ch = result_ch\n}\n</code></pre> <ol> <li>Workflow Composition</li> </ol> <pre><code>// Using explicit connections\nWORKFLOW_A(input_ch)\nWORKFLOW_B(WORKFLOW_A.out.some_channel)\n</code></pre> <ol> <li>Entry Points</li> </ol> <pre><code>// Unnamed workflow (default entry point)\nworkflow {\n    // This is automatically the entry point when the script is run\n}\n\n// Named workflow (not an entry point by default)\nworkflow NAMED_WORKFLOW {\n    // This is not automatically run\n}\n\n// Running a named workflow as entry point\n// nextflow run script.nf -entry NAMED_WORKFLOW\n</code></pre>"},{"location":"side_quests/workflows_of_workflows/#resources","title":"Resources","text":"<ul> <li>Nextflow Workflow Documentation</li> <li>Channel Operators Reference</li> <li>Error Strategy Documentation</li> </ul>"},{"location":"side_quests/working_with_files/","title":"Working with Files","text":"<p>Bioinformatics workflows often involve processing large numbers of files. Nextflow provides powerful tools to handle files efficiently, helping you organize and process your data with minimal code.</p> <p>In this side quest, we'll explore how Nextflow handles files, from basic file operations to more advanced techniques for working with file collections. You'll learn how to extract metadata from filenames - a common requirement in bioinformatics pipelines.</p> <p>By the end of this side quest, you'll be able to:</p> <ul> <li>Create Path objects from file path strings using Nextflow's <code>file()</code> method</li> <li>Access file attributes such as name, extension, and parent directory</li> <li>Handle both local and remote files transparently using URIs</li> <li>Use channels to automate file handling with <code>Channel.fromPath()</code> and <code>Channel.fromFilePairs()</code></li> <li>Extract and structure metadata from filenames using string manipulation</li> <li>Group related files using pattern matching and glob expressions</li> <li>Integrate file operations into Nextflow processes with proper input handling</li> <li>Organize process outputs using metadata-driven directory structures</li> </ul>"},{"location":"side_quests/working_with_files/#0-warmup","title":"0. Warmup","text":""},{"location":"side_quests/working_with_files/#01-prerequisites","title":"0.1. Prerequisites","text":"<p>Before taking on this side quest you should:</p> <ul> <li>Complete the Hello Nextflow tutorial</li> <li>Understand basic Nextflow concepts (processes, channels, operators)</li> <li>Basic understanding of using sample-specific data (meta data): Working with sample-specific data</li> </ul>"},{"location":"side_quests/working_with_files/#02-starting-point","title":"0.2. Starting Point","text":"<p>Let's move into the project directory:</p> Navigate to project directory<pre><code>cd side-quests/working_with_files\n</code></pre> <p>You can set VSCode to focus on this directory:</p> Open directory in VSCode<pre><code>code .\n</code></pre> <p>You'll find a simple workflow file (<code>file_operations.nf</code>) and a data directory containing some example files.</p> Directory contents<pre><code>&gt; tree\n.\n\u251c\u2500\u2500 count_lines.nf\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 patientA_rep1_normal_R1_001.fastq.gz\n\u2502   \u251c\u2500\u2500 patientA_rep1_normal_R2_001.fastq.gz\n\u2502   \u251c\u2500\u2500 patientA_rep1_tumor_R1_001.fastq.gz\n\u2502   \u251c\u2500\u2500 patientA_rep1_tumor_R2_001.fastq.gz\n\u2502   \u251c\u2500\u2500 patientA_rep2_normal_R1_001.fastq.gz\n\u2502   \u251c\u2500\u2500 patientA_rep2_normal_R2_001.fastq.gz\n\u2502   \u251c\u2500\u2500 patientA_rep2_tumor_R1_001.fastq.gz\n\u2502   \u251c\u2500\u2500 patientA_rep2_tumor_R2_001.fastq.gz\n\u2502   \u251c\u2500\u2500 patientB_rep1_normal_R1_001.fastq.gz\n\u2502   \u251c\u2500\u2500 patientB_rep1_normal_R2_001.fastq.gz\n\u2502   \u251c\u2500\u2500 patientB_rep1_tumor_R1_001.fastq.gz\n\u2502   \u251c\u2500\u2500 patientB_rep1_tumor_R2_001.fastq.gz\n\u2502   \u251c\u2500\u2500 patientC_rep1_normal_R1_001.fastq.gz\n\u2502   \u251c\u2500\u2500 patientC_rep1_normal_R2_001.fastq.gz\n\u2502   \u251c\u2500\u2500 patientC_rep1_tumor_R1_001.fastq.gz\n\u2502   \u2514\u2500\u2500 patientC_rep1_tumor_R2_001.fastq.gz\n\u2514\u2500\u2500 file_operations.nf\n\n2 directories, 18 files\n</code></pre> <p>This directory contains paired-end sequencing data for three patients (A, B, C), with a typical <code>_R1_</code> and <code>_R2_</code> naming convention for what are known as 'forward' and 'reverse reads' (don't worry if you don't know what this means, it's not important for this session). Each patient has normal and tumor tissue types, and patient A has two replicates.</p>"},{"location":"side_quests/working_with_files/#03-running-the-workflow","title":"0.3. Running the Workflow","text":"<p>Take a look at the workflow file <code>file_operations.nf</code>:</p> file_operations.nf<pre><code>workflow {\n    // Create a Path object from a string path\n    myFile = 'data/patientA_rep1_normal_R1_001.fastq.gz'\n    println \"${myFile} is of class ${myFile.class}\"\n}\n</code></pre> <p>We have a mini-workflow that refers to a single file path in it's workflow, then prints it to the console, along with its class.</p> <p>Note</p> <p>What is <code>.class</code>?</p> <p>In Groovy (the language Nextflow uses), <code>.class</code> tells us what type of object we're working with. It's like asking \"what kind of thing is this?\" - whether it's a string, a number, a file, or something else. This will help us see the difference between a plain string and a Path object in the next sections.</p> <p>Run the workflow:</p> Run the workflow<pre><code>nextflow run file_operations.nf\n</code></pre> Starting Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `file_operations.nf` [romantic_chandrasekhar] DSL2 - revision: 5a4a89bc3a\n\ndata/patientA_rep1_normal_R1_001.fastq.gz is of class java.lang.String\n</code></pre> <p>We printed the string path exactly as we wrote it. This is just text output - Nextflow hasn't done anything special with it yet. We've also confirmed that, so far, to Nextflow this is only a string (of class <code>java.lang.String</code>), we haven't yet told Nextflow about its file nature.</p>"},{"location":"side_quests/working_with_files/#1-basic-file-operations","title":"1. Basic File Operations","text":""},{"location":"side_quests/working_with_files/#11-creating-path-objects","title":"1.1. Creating Path Objects","text":"<p>We can tell Nextflow how to handle files by creating Path objects from path strings. In our workflow, we have a string path <code>data/patientA_rep1_normal_R1_001.fastq.gz</code>, and we covert that to a Path object using the <code>file()</code> method, which provides access to file properties and operations.</p> <p>Edit the <code>file_operations.nf</code> to wrap the string with <code>file()</code> as follows:</p> AfterBefore file_operations.nf<pre><code>// Create a Path object from a string path\n    myFile = file('data/patientA_rep1_normal_R1_001.fastq.gz')\n    println \"${myFile} is of class ${myFile.class}\"\n</code></pre> file_operations.nf<pre><code>// Create a Path object from a string path\n    myFile = 'data/patientA_rep1_normal_R1_001.fastq.gz'\n    println \"${myFile} is of class ${myFile.class}\"\n</code></pre> <p>Run the workflow:</p> Test Path object creation<pre><code>nextflow run file_operations.nf\n</code></pre> Path object output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `file_operations.nf` [kickass_coulomb] DSL2 - revision: 5af44b1b59\n\n/workspaces/training/side-quests/working_with_files/data/patientA_rep1_normal_R1_001.fastq.gz is of class class sun.nio.fs.UnixPath\n</code></pre> <p>Now we see the full absolute path instead of the relative path we wrote. Nextflow has converted our string into a Path object and resolved it to the actual file location on the system. The file path will now be absolute, like <code>/workspaces/training/side-quests/working_with_files/data/patientA_rep1_normal_R1_001.fastq.gz</code>, and notice that the Path object class is <code>sun.nio.fs.UnixPath</code>, this is Nextflow's way of representing local files. As we'll see later, remote files will have different class names (like <code>nextflow.file.http.XPath</code> for HTTP files), but they all work exactly the same way and can be used identically in your workflows.</p> <p>Note</p> <p>The key difference:</p> <ul> <li>Path string: Just text that Nextflow treats as characters</li> <li>Path object: A smart file reference that Nextflow can work with</li> </ul> <p>Think of it like this: a path string is like writing an address on paper, while a Path object is like having a GPS device that knows how to navigate and can tell you details about the journey.</p>"},{"location":"side_quests/working_with_files/#12-file-attributes","title":"1.2. File Attributes","text":"<p>Why is this helpful? Well now Nextflow understands that <code>myFile</code> is a Path object and not just a string, we can access the various attributes of the Path object.</p> <p>Let's update our workflow to print out the file attributes:</p> AfterBefore file_operations.nf<pre><code>    // Create a Path object from a string path\n    myFile = file('data/patientA_rep1_normal_R1_001.fastq.gz')\n\n    // Print file attributes\n    println \"File object class: ${myFile.class}\"\n    println \"File name: ${myFile.name}\"\n    println \"Simple name: ${myFile.simpleName}\"\n    println \"Extension: ${myFile.extension}\"\n    println \"Parent directory: ${myFile.parent}\"\n</code></pre> file_operations.nf<pre><code>    // Create a file object from a string path\n    myFile = file('data/patientA_rep1_normal_R1_001.fastq.gz')\n    println \"${myFile} is of class ${myFile.class}\"\n</code></pre> <p>Run the workflow:</p> Test file attributes<pre><code>nextflow run file_operations.nf\n</code></pre> <p>You'll see various file attributes printed to the console:</p> File Attributes Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `file_operations.nf` [ecstatic_ampere] DSL2 - revision: f3fa3dcb48\n\nFile object class: sun.nio.fs.UnixPath\nFile name: patientA_rep1_normal_R1_001.fastq.gz\nSimple name: patientA_rep1_normal_R1_001\nExtension: gz\nParent directory: /workspaces/training/side-quests/working_with_files/data\n</code></pre>"},{"location":"side_quests/working_with_files/#13-why-proper-file-handling-matters","title":"1.3. Why proper file handling matters","text":"<p>The difference between strings and Path objects becomes critical when you start building actual workflows with processes. Let's side-step for a moment to take a look at a workflow where this has been done wrong.</p> <p><code>count_lines.nf</code> contains a process that takes a <code>val</code> input and tries to treat it as a file:</p> count_lines.nf<pre><code>process COUNT_LINES {\n    debug true\n\n    input:\n    val fastq_file\n\n    script:\n    \"\"\"\n    set -o pipefail\n    echo \"Processing file: $fastq_file\"\n    gzip -dc $fastq_file | wc -l\n    \"\"\"\n}\n\nworkflow {\n    myFile = 'data/patientA_rep1_normal_R1_001.fastq.gz'\n    COUNT_LINES(myFile)\n}\n</code></pre> <p>Note</p> <p>About <code>debug = true</code>:</p> <p>The <code>debug = true</code> directive in the process definition causes Nextflow to print the output from your script (like the line count \"40\") directly in the execution log. Without this, you would only see the process execution status but not the actual output from your script.</p> <p>For more information on debugging Nextflow processes, see the Debugging Nextflow Workflows side quest.</p> <p>Run this workflow to see the error:</p> Test val input with string error<pre><code>nextflow run count_lines.nf\n</code></pre> <p>You'll get an error like this:</p> Val input with string error<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `count_lines.nf` [goofy_koch] DSL2 - revision: 4d9e909d80\n\nexecutor &gt;  local (1)\n[7f/c22b7f] COUNT_LINES [  0%] 0 of 1\nERROR ~ Error executing process &gt; 'COUNT_LINES'\n\nCaused by:\n  Process `COUNT_LINES` terminated with an error exit status (1)\n\n\nCommand executed:\n\nexecutor &gt;  local (1)\n[7f/c22b7f] COUNT_LINES [  0%] 0 of 1 \u2718\nWARN: Got an interrupted exception while taking agent result | java.lang.InterruptedException\nERROR ~ Error executing process &gt; 'COUNT_LINES'\n\nCaused by:\n  Process `COUNT_LINES` terminated with an error exit status (1)\n\n\nCommand executed:\n\n  set -o pipefail\n  echo \"Processing file: data/patientA_rep1_normal_R1_001.fastq.gz\"\n  gzip -dc data/patientA_rep1_normal_R1_001.fastq.gz | wc -l\n\nCommand exit status:\n  1\n\nCommand output:\n  Processing file: data/patientA_rep1_normal_R1_001.fastq.gz\n  0\n\nCommand error:\n  Processing file: data/patientA_rep1_normal_R1_001.fastq.gz\n  gzip: data/patientA_rep1_normal_R1_001.fastq.gz: No such file or directory\n  0\n\nWork dir:\n  /workspaces/training/side-quests/working_with_files/work/7f/c22b7f6f86c81f14d53de15584fdd5\n\nTip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`\n\n -- Check '.nextflow.log' file for details\n</code></pre> <p>The process failed because the file <code>data/patientA_rep1_normal_R1_001.fastq.gz</code> doesn't exist in the process working directory. When you use <code>val</code> input, Nextflow passes the string value through to your script, but it doesn't stage the actual file. The process tries to use the string as a file path, but the file isn't there.</p> <p>Now let's fix this by changing the process to use a <code>path</code> input:</p> AfterBefore file_operations.nf<pre><code>process COUNT_LINES {\n    debug true\n\n    input:\n    path fastq_file\n\n    script:\n    \"\"\"\n    set -o pipefail\n    echo \"Processing file: $fastq_file\"\n    gzip -dc $fastq_file | wc -l\n    \"\"\"\n}\n\nworkflow {\n    myFile = 'data/patientA_rep1_normal_R1_001.fastq.gz'\n    COUNT_LINES(myFile)\n}\n</code></pre> file_operations.nf<pre><code>process COUNT_LINES {\n    debug true\n\n    input:\n    val fastq_file\n\n    script:\n    \"\"\"\n    set -o pipefail\n    echo \"Processing file: $fastq_file\"\n    gzip -dc $fastq_file | wc -l\n    \"\"\"\n}\n\nworkflow {\n    myFile = 'data/patientA_rep1_normal_R1_001.fastq.gz'\n    COUNT_LINES(myFile)\n}\n</code></pre> <p>Run this updated version and you'll get a different error:</p> Path input with string error<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `file_operations.nf` [mighty_poitras] DSL2 - revision: e996edfc53\n\n[-        ] COUNT_LINES -\nERROR ~ Error executing process &gt; 'COUNT_LINES'\n\nCaused by:\n  Not a valid path value: 'data/patientA_rep1_normal_R1_001.fastq.gz'\n\nTip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line\n</code></pre> <p>What this error means:</p> <p>This is much better! Nextflow immediately detected the problem and failed before even starting the process. When you use <code>path</code> input, Nextflow validates that you're passing actual file references, not just strings. It's telling you that <code>'data/patientA_rep1_normal_R1_001.fastq.gz'</code> is not a valid path value because it's a string, not a Path object.</p> <p>Now let's fix this properly by using the <code>file()</code> method to create a Path object:</p> AfterBefore file_operations.nf<pre><code>process COUNT_LINES {\n    debug true\n\n    input:\n    path fastq_file\n\n    script:\n    \"\"\"\n    set -o pipefail\n    echo \"Processing file: $fastq_file\"\n    gzip -dc $fastq_file | wc -l\n    \"\"\"\n}\n\nworkflow {\n    myFile = file('data/patientA_rep1_normal_R1_001.fastq.gz')\n    COUNT_LINES(myFile)\n}\n</code></pre> file_operations.nf<pre><code>process COUNT_LINES {\n    debug true\n\n    input:\n    path fastq_file\n\n    script:\n    \"\"\"\n    set -o pipefail\n    echo \"Processing file: $fastq_file\"\n    gzip -dc $fastq_file | wc -l\n    \"\"\"\n}\n\nworkflow {\n    myFile = 'data/patientA_rep1_normal_R1_001.fastq.gz'\n    COUNT_LINES(myFile)\n}\n</code></pre> <p>Now when you run this, it should work correctly! The file will be staged into the process working directory and the <code>wc -l</code> command will succeed.</p> Test successful execution<pre><code>nextflow run count_lines.nf\n</code></pre> <p>You should see output like this:</p> Successful execution<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `count_lines.nf` [astonishing_tesla] DSL2 - revision: ee38f96485\n\nexecutor &gt;  local (1)\n[8a/3f23d5] COUNT_LINES [100%] 1 of 1 \u2714\nProcessing file: patientA_rep1_normal_R1_001.fastq.gz\n      40\n</code></pre> <p>The process successfully:</p> <ul> <li>Staged the file into the working directory</li> <li>Decompressed the .gz file</li> <li>Counted the lines (40 lines in this case)</li> <li>Completed without errors</li> </ul>"},{"location":"side_quests/working_with_files/#takeaway","title":"Takeaway","text":"<ul> <li>Path strings vs Path objects: Strings are just text, Path objects are smart file references</li> <li>The <code>file()</code> method converts a string path into a Path object that Nextflow can work with</li> <li>You can access file properties like <code>name</code>, <code>simpleName</code>, <code>extension</code>, and <code>parent</code> using file attributes</li> <li>Using Path objects instead of strings allows Nextflow to properly manage files in your workflow</li> <li>Process Input Outcomes: Proper file handling requires Path objects, not strings, to ensure files are correctly staged and accessible in processes.</li> </ul>"},{"location":"side_quests/working_with_files/#2-using-remote-files","title":"2. Using Remote Files","text":"<p>One of the key features of Nextflow is the ability to transparently switch between local files (on the same machine) to remote files accessible over the internet. To do this, all you need to do as a user is switch the file path you supply to the workflow from a normal file path (e.g. <code>/path/to/data</code>) to a file path with a remote protocol at the start. Importantly, you should never have to change the workflow logic to accomodate files coming from different locations.</p> <p>For example, replacing <code>/path/to/data</code> with <code>s3://path/to/data</code> in your inputs will switch to using the S3 protocol. Many different protocols are supported:</p> <ul> <li>HTTP(S)/FTP (http://, https://, ftp://)</li> <li>Amazon S3 (s3://)</li> <li>Azure Blob Storage (az://)</li> <li>Google Cloud Storage (gs://)</li> </ul> <p>To use these, simply replace the string and Nextflow will handle authentication and staging the files to the right place, downloading or uploading and all other file operations you would expect. We call this string the Uniform Resource Identifier (URI).</p> <p>The key strength of this is we can switch between environments without changing any pipeline logic. For example, you can develop with a small, local test set before moving to a remote set of data by changing the URI.</p>"},{"location":"side_quests/working_with_files/#21-using-a-file-from-the-internet","title":"2.1. Using a file from the internet","text":"<p>Warning</p> <p>Accessing remote data requires an internet connection!</p> <p>In your workflow, you can replace the string path with an HTTPS one to download this file from the internet. We are going to swap the relative path of the FASTQ files with the remote one. This is the same data as we have been previously using.</p> <p>Open <code>file_operations.nf</code> again and make changes like this:</p> AfterBefore file_operations.nf<pre><code>// Using a remote file from the internet\n    myFile = file('https://github.com/nextflow-io/training/blob/bb187e3bfdf4eec2c53b3b08d2b60fdd7003b763/side-quests/working_with_files/data/patientA_rep1_normal_R1_001.fastq.gz')\n\n    // Print file attributes\n    println \"File object class: ${myFile.class}\"\n    println \"File name: ${myFile.name}\"\n    println \"Simple name: ${myFile.simpleName}\"\n    println \"Extension: ${myFile.extension}\"\n    println \"Parent directory: ${myFile.parent}\"\n</code></pre> file_operations.nf<pre><code>    // Create a file object from a string path\n    myFile = file('data/patientA_rep1_normal_R1_001.fastq.gz')\n\n    // Print file attributes\n    println \"File object class: ${myFile.class}\"\n    println \"File name: ${myFile.name}\"\n    println \"Simple name: ${myFile.simpleName}\"\n    println \"Extension: ${myFile.extension}\"\n    println \"Parent directory: ${myFile.parent}\"\n</code></pre> <p>Note</p> <p>HTTPS remote data does not accept globs because HTTPS cannot list multiple files, and similarly cannot be used with directory paths (you must specify exact file URLs). However, other storage protocols such as blob storage (<code>s3://</code>, <code>az://</code>, <code>gs://</code>) can use both globs and directory paths.</p> <p>Run the workflow and it will automatically pull the data from the internet:</p> Test remote file access<pre><code>nextflow run file_operations.nf\n</code></pre> Remote file output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `file_operations.nf` [insane_swartz] DSL2 - revision: fff18abe6d\n\nFile object class: class nextflow.file.http.XPath\nFile name: patientA_rep1_normal_R1_001.fastq.gz\nSimple name: patientA_rep1_normal_R1_001\nExtension: gz\nParent directory: /nextflow-io/training/blob/bb187e3bfdf4eec2c53b3b08d2b60fdd7003b763/side-quests/working_with_files/data\n</code></pre> <p>In this example very little has changed! This shows how easy it is to switch between local and remote data using Nextflow.</p> <p>To see the difference, check the working directory located at the hash value of the process. The file will be downloaded to a staging directory located within the work directory, then symlinked into the relevant process directory. If the file was used again, Nextflow would only download it once.</p> <p>In this way, you can replace local with remote data without changing any pipeline logic.</p>"},{"location":"side_quests/working_with_files/#cloud-storage-with-glob-patterns","title":"Cloud Storage with Glob Patterns","text":"<p>While HTTP doesn't support globs, cloud storage protocols do. Here's how you could use glob patterns with cloud storage:</p> Cloud storage examples (not runnable in this environment)<pre><code>// S3 with glob patterns - would match multiple files\nch_s3_files = Channel.fromPath('s3://my-bucket/data/*.fastq.gz')\n\n// Azure Blob Storage with glob patterns\nch_azure_files = Channel.fromPath('az://container/data/patient*_R{1,2}.fastq.gz')\n\n// Google Cloud Storage with glob patterns\nch_gcs_files = Channel.fromPath('gs://bucket/data/sample_*.fastq.gz')\n</code></pre> <p>These examples show the power of Nextflow's unified file handling - the same code works whether files are local or in the cloud, as long as the protocol supports the operations you need.</p>"},{"location":"side_quests/working_with_files/#takeaway_1","title":"Takeaway","text":"<ul> <li>Remote data is accessed using a URI (HTTP, FTP, S3, Azure, Google Cloud)</li> <li>Nextflow will automatically download and stage the data to the right place</li> <li>Do not write logic to download or upload remote files!</li> <li>Local and remote files produce different object types but work identically</li> <li>Important: HTTP/HTTPS only work with single files (no glob patterns)</li> <li>Cloud storage (S3, Azure, GCS) supports both single files and glob patterns</li> <li>You can seamlessly switch between local and remote data sources without changing code logic (as long as the protocol supports your required operations)</li> </ul> <p>Note</p> <p>Note on Object Types: Notice that local files produce <code>sun.nio.fs.UnixPath</code> objects while remote files produce <code>nextflow.file.http.XPath</code> objects. Despite these different class names, both work exactly the same way and can be used identically in your workflows. This is a key feature of Nextflow - you can seamlessly switch between local and remote data sources without changing your code logic.</p>"},{"location":"side_quests/working_with_files/#22-switching-back-to-local-files","title":"2.2. Switching Back to Local Files","text":"<p>For the remainder of this side quest, we'll use local files in our examples. This allows us to demonstrate powerful features like glob patterns and batch processing that aren't available with HTTP URLs. Remember: the same concepts apply to cloud storage (S3, Azure, GCS) where glob patterns are fully supported.</p> <p>Let's update our workflow to use local files again:</p> AfterBefore file_operations.nf<pre><code>    // Create a file object from a string path\n    myFile = file('data/patientA_rep1_normal_R1_001.fastq.gz')\n\n    // Print file attributes\n    println \"File object class: ${myFile.class}\"\n    println \"File name: ${myFile.name}\"\n    println \"Simple name: ${myFile.simpleName}\"\n    println \"Extension: ${myFile.extension}\"\n    println \"Parent directory: ${myFile.parent}\"\n</code></pre> file_operations.nf<pre><code>    // Create a Path object from a string path\n    myFile = file('https://github.com/nextflow-io/training/blob/bb187e3bfdf4eec2c53b3b08d2b60fdd7003b763/side-quests/working_with_files/data/patientA_rep1_normal_R1_001.fastq.gz')\n\n    // Print file attributes\n    println \"File object class: ${myFile.class}\"\n    println \"File name: ${myFile.name}\"\n    println \"Simple name: ${myFile.simpleName}\"\n    println \"Extension: ${myFile.extension}\"\n    println \"Parent directory: ${myFile.parent}\"\n</code></pre>"},{"location":"side_quests/working_with_files/#3-reading-files-using-the-frompath-channel-factory","title":"3. Reading files using the <code>fromPath()</code> channel factory","text":"<p>The <code>file()</code> method is useful for simple file operations, and we can combine that with <code>Channel.of()</code> to build channels from files like:</p> Channel.of() with file()<pre><code>    ch_fastq = Channel.of([file('data/patientA_rep1_normal_R1_001.fastq.gz')])\n</code></pre> <p>But we have a much more convenient tool called <code>Channel.fromPath()</code> which generates a channel from static file strings as well as glob patterns.</p>"},{"location":"side_quests/working_with_files/#31-reading-files-with-channelfrompath","title":"3.1. Reading Files with Channel.fromPath","text":"<p>Update your <code>file_operations.nf</code> file:</p> AfterBefore file_operations.nf<pre><code>    // Reading files with Channel.fromPath\n    ch_fastq = Channel.fromPath('data/patientA_rep1_normal_R1_001.fastq.gz')\n    ch_fastq.view { \"Found file: $it of type ${it.class}\" }\n\n    // // Print file attributes\n    // Comment these out for now, we'll come back to them!\n    // println \"File object class: ${myFile.class}\"\n    // println \"File name: ${myFile.name}\"\n    // println \"Simple name: ${myFile.simpleName}\"\n    // println \"Extension: ${myFile.extension}\"\n    // println \"Parent directory: ${myFile.parent}\"\n</code></pre> file_operations.nf<pre><code>// Create a Path object from a string path\n    // Create a file object from a string path\n    myFile = file('data/patientA_rep1_normal_R1_001.fastq.gz')\n\n    // Print file attributes\n    println \"File object class: ${myFile.class}\"\n    println \"File name: ${myFile.name}\"\n    println \"Simple name: ${myFile.simpleName}\"\n    println \"Extension: ${myFile.extension}\"\n    println \"Parent directory: ${myFile.parent}\"\n</code></pre> <p>Run the workflow:</p> Test Channel.fromPath<pre><code>nextflow run file_operations.nf\n</code></pre> <p>You'll see each file path being emitted as a separate element in the channel:</p> Channel.fromPath Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `file_operations.nf` [grave_meucci] DSL2 - revision: b09964a583\n\nFound file: /workspaces/training/side-quests/working_with_files/data/patientA_rep1_normal_R1_001.fastq.gz of type class sun.nio.fs.UnixPath\n</code></pre> <p>Note how Nextflow has grabbed the file we specified and turned it into a <code>Path</code> type object, in exactly the same way that <code>file()</code> would have done. <code>Channel.fromPath()</code> is just a convenient way of creating a new channel populated by a list of files.</p>"},{"location":"side_quests/working_with_files/#32-viewing-channel-contents","title":"3.2. Viewing Channel Contents","text":"<p>In our first version, we use <code>.view()</code> to print the file name. Let's update our workflow to print out the file attributes:</p> AfterBefore file_operations.nf<pre><code>    // Reading files with Channel.fromPath\n    ch_fastq = Channel.fromPath('data/patientA_rep1_normal_R1_001.fastq.gz')\n    ch_fastq.view { myFile -&gt;\n        println \"File object class: ${myFile.class}\"\n        println \"File name: ${myFile.name}\"\n        println \"Simple name: ${myFile.simpleName}\"\n        println \"Extension: ${myFile.extension}\"\n        println \"Parent directory: ${myFile.parent}\"\n    }\n</code></pre> file_operations.nf<pre><code>    // Reading files with Channel.fromPath\n    ch_fastq = Channel.fromPath('data/patientA_rep1_normal_R1_001.fastq.gz')\n    ch_fastq.view { myFile -&gt; \"Found file: $myFile\" }\n\n    // // Print file attributes\n    // Comment these out for now, we'll come back to them!\n    // println \"File name: ${myFile.name}\"\n    // println \"Simple name: ${myFile.simpleName}\"\n    // println \"Extension: ${myFile.extension}\"\n    // println \"Parent directory: ${myFile.parent}\"\n</code></pre> <p>Run the workflow:</p> Test file attributes with Channel.fromPath<pre><code>nextflow run file_operations.nf\n</code></pre> Channel.fromPath Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `file_operations.nf` [furious_swanson] DSL2 - revision: c35c34950d\n\nFile object class: sun.nio.fs.UnixPath\nFile name: patientA_rep1_normal_R1_001.fastq.gz\nSimple name: patientA_rep1_normal_R1_001\nExtension: gz\nParent directory: /workspaces/training/side-quests/working_with_files/data\n</code></pre>"},{"location":"side_quests/working_with_files/#33-using-a-glob-to-match-multiple-files","title":"3.3. Using a glob to match multiple files","text":"<p><code>Channel.fromPath()</code> can take a glob pattern as an argument, which will match all files in the directory that match the pattern. Let's grab both of the pair of FASTQs associated with this patient.</p> <p>A glob pattern is a pattern that matches one or more characters in a string. The <code>*</code> wildcard is the most common glob pattern, which will match any character in it's place. To do this, we replace the full path with a <code>*</code> wildcard, which will match any character in it's place. In this case, we will replace the read number from <code>R1</code> to <code>R*</code>.</p> AfterBefore file_operations.nf<pre><code>    ch_fastq = Channel.fromPath('data/patientA_rep1_normal_R*_001.fastq.gz')\n</code></pre> file_operations.nf<pre><code>    ch_fastq = Channel.fromPath('data/patientA_rep1_normal_R1_001.fastq.gz')\n</code></pre> <p>Run the workflow:</p> Test glob pattern matching<pre><code>nextflow run file_operations.nf\n</code></pre> Channel.fromPath Glob Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `file_operations.nf` [boring_sammet] DSL2 - revision: d2aa789c9a\n\nFile object class: sun.nio.fs.UnixPath\nFile name: patientA_rep1_normal_R1_001.fastq.gz\nSimple name: patientA_rep1_normal_R1_001\nExtension: gz\nParent directory: /workspaces/training/side-quests/working_with_files/data\nFile object class: sun.nio.fs.UnixPath\nFile name: patientA_rep1_normal_R2_001.fastq.gz\nSimple name: patientA_rep1_normal_R2_001\nExtension: gz\nParent directory: /workspaces/training/side-quests/working_with_files/data\n</code></pre> <p>Using this method, we could grab as many or as few files as we want just by changing the glob pattern. If we made it more generous, we could grab all the files in the <code>data</code> directory, but we'll come back to that later.</p>"},{"location":"side_quests/working_with_files/#takeaway_2","title":"Takeaway","text":"<ul> <li><code>Channel.fromPath()</code> creates a channel with files matching a pattern</li> <li>Each file is emitted as a separate element in the channel</li> <li>We can use a glob pattern to match multiple files</li> <li>Files are automatically converted to Path objects with full attributes</li> <li>The <code>.view()</code> method allows inspection of channel contents</li> </ul>"},{"location":"side_quests/working_with_files/#4-extracting-patient-metadata-from-filenames","title":"4. Extracting Patient Metadata from Filenames","text":"<p>One of the most common tasks in bioinformatics workflows is extracting metadata from filenames. This is usually feasible when working with sequencing data, where filenames often contain information about the sample, condition, replicate, and read number.</p> <p>This isn't ideal - metadata should never be embedded in filenames, but it's a common reality. We want to extract that metadata in a standardised manner so we can use it later.</p> <p>Let's explore how to extract metadata from our FASTQ filenames using Nextflow's powerful data transformation capabilities.</p>"},{"location":"side_quests/working_with_files/#41-basic-metadata-extraction","title":"4.1. Basic Metadata Extraction","text":"<p>First, let's modify our workflow to extract metadata from the filenames.</p> <p>First we will grab the simpleName of the file, which includes the metadata, and return with the file. Then, we will separate out the metadata by underscores using tokenize. Finally, we will use string handling to remove additional text like \"rep\" which aren't required right now.</p> AfterBefore file_operations.nf<pre><code>    ch_fastq.map { myFile -&gt;\n        [ myFile.simpleName, myFile ]\n    }\n    .view()\n</code></pre> file_operations.nf<pre><code>    ch_fastq.view {\n        println \"File object class: ${myFile.class}\"\n        println \"File name: ${it.name}\"\n        println \"Simple name: ${it.simpleName}\"\n        println \"Extension: ${it.extension}\"\n        println \"Parent directory: ${it.parent}\"\n    }\n</code></pre> Test filename metadata extraction<pre><code>nextflow run file_operations.nf\n</code></pre> Sample Metadata Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `file_operations.nf` [suspicious_mahavira] DSL2 - revision: ae8edc4e48\n\n[patientA_rep1_normal_R2_001, /workspaces/training/side-quests/working_with_files/data/patientA_rep1_normal_R2_001.fastq.gz]\n[patientA_rep1_normal_R1_001, /workspaces/training/side-quests/working_with_files/data/patientA_rep1_normal_R1_001.fastq.gz]\n</code></pre> <p>Note how we have separated the patient <code>simpleName</code>, which includes the metadata, from the <code>file</code> object. This is useful if we want to use the patient metadata in a later process.</p>"},{"location":"side_quests/working_with_files/#42-extracting-metadata-from-filenames","title":"4.2. Extracting Metadata from Filenames","text":"<p>Our metadata is embedded in the filename, but it's not in a standard format. We need to split up the filename into it's components which are separated by underscores.</p> <p>Groovy includes a method called <code>tokenize()</code> which is perfect for this task.</p> AfterBefore file_operations.nf<pre><code>    ch_fastq.map { myFile -&gt;\n        [ myFile.simpleName.tokenize('_'), myFile ]\n    }\n</code></pre> file_operations.nf<pre><code>    ch_fastq.map { myFile -&gt;\n        [ myFile.simpleName, myFile ]\n    }\n</code></pre> <p>Once we run this, we should see the patient metadata as a list of strings, and the Path object as the second element in the tuple.</p> Test filename tokenization<pre><code>nextflow run file_operations.nf\n</code></pre> Sample Tokenize Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `file_operations.nf` [gigantic_gauss] DSL2 - revision: a39baabb57\n\n[[patientA, rep1, normal, R1, 001], /workspaces/training/side-quests/working_with_files/data/patientA_rep1_normal_R1_001.fastq.gz]\n[[patientA, rep1, normal, R2, 001], /workspaces/training/side-quests/working_with_files/data/patientA_rep1_normal_R2_001.fastq.gz]\n</code></pre> <p>Success! We've broken down our patient information from a single string into a list of strings. We can now handle each part of the patient information separately.</p>"},{"location":"side_quests/working_with_files/#43-using-a-map-to-organise-the-data","title":"4.3. Using a map to organise the data","text":"<p>Our meta data is just a flat list at the moment. It's easy to use but hard to read. What is the item at index 3? Can you tell without checking?</p> <p>A map is Groovy's version of a key-value store. Every item has a key and a value and we can refer to each key to get the value. This will make our code much easier to read, i.e. we go from this:</p> <pre><code>data = [patientA, 1, normal, R1]\n\nprintln data[3]\n</code></pre> <p>to this:</p> <pre><code>data = [id: patientA, replicate: 1, type: normal, readNum: 1]\n\nprintln data.readNum\n</code></pre> <p>Let's convert our flat list into a map now.</p> AfterBefore file_operations.nf<pre><code>    ch_fastq.map { myFile -&gt;\n        def (patient, replicate, type, readNum) = myFile.simpleName.tokenize('_')\n        [\n          [\n            id: patient,\n            replicate: replicate.replace('rep', ''),\n            type: type,\n            readNum: readNum.replace('rep', ''),\n          ],\n          myFile\n        ]\n    }\n</code></pre> file_operations.nf<pre><code>    ch_fastq.map { myFile -&gt;\n        [ myFile.simpleName.tokenize('_'), myFile ]\n    }\n</code></pre> <p>Notice that we're simplifying a couple of the meta data items as we go (e.g. <code>readNum.replace('rep', '')</code>).</p> <p>Now re-run the workflow:</p> Test metadata map structure<pre><code>nextflow run file_operations.nf\n</code></pre> Map Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `file_operations.nf` [infallible_swartz] DSL2 - revision: 7f4e68c0cb\n\n[[id:patientA, replicate:rep1, type:normal, readNum:R2], /workspaces/training/side-quests/working_with_files/data/patientA_rep1_normal_R2_001.fastq.gz]\n[[id:patientA, replicate:rep1, type:normal, readNum:R1], /workspaces/training/side-quests/working_with_files/data/patientA_rep1_normal_R1_001.fastq.gz]\n</code></pre> <p>We have converted our flat list into a map, and now we can refer to each bit of sample data by name instead of by index. This makes our code easier to read and more maintainable.</p>"},{"location":"side_quests/working_with_files/#takeaway_3","title":"Takeaway","text":"<ul> <li>We can handle filenames in Nextflow with the power of a full programming language</li> <li>We can treat the filenames as strings to extract relevant information</li> <li>Use of methods like <code>tokenize()</code> and <code>replace()</code> allows us to manipulate strings in the filename</li> <li>The <code>.map()</code> operation transforms channel elements while preserving structure</li> <li>Structured metadata (maps) makes code more readable and maintainable than positional lists</li> </ul> <p>Next up, we will look at how to handle paired-end reads.</p>"},{"location":"side_quests/working_with_files/#5-simplifying-with-channelfromfilepairs","title":"5. Simplifying with Channel.fromFilePairs","text":"<p>Nextflow provides a specialized channel factory method for working with paired files: <code>Channel.fromFilePairs()</code>. This method automatically groups files that share a common prefix. This is particularly useful for paired-end sequencing data, where you have two files (e.g., R1 and R2) for each sample.</p>"},{"location":"side_quests/working_with_files/#51-basic-usage-of-fromfilepairs","title":"5.1. Basic Usage of fromFilePairs","text":"<p>Complete your <code>file_operations.nf</code> file with the following (deleting the map operation):</p> AfterBefore file_operations.nf<pre><code>    ch_fastq = Channel.fromFilePairs('data/patientA_rep1_normal_R{1,2}_001.fastq.gz')\n        .view()\n</code></pre> file_operations.nf<pre><code>    ch_fastq = Channel.fromPath('data/patientA_rep1_normal_R*_001.fastq.gz')\n    ch_fastq.map { myFile -&gt;\n        def (sample, replicate, type, readNum) = myFile.simpleName.tokenize('_')\n        [\n            [\n                id: sample,\n                replicate: replicate.replace('rep', ''),\n                type: type,\n                readNum: readNum,\n            ],\n            myFile\n        ]\n    }\n    .view()\n</code></pre> <p>Run the workflow:</p> Test Channel.fromFilePairs<pre><code>nextflow run file_operations.nf\n</code></pre> <p>The output will show the paired files grouped together:</p> Channel.fromFilePairs Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `file_operations.nf` [chaotic_cuvier] DSL2 - revision: 472265a440\n\n[patientA_rep1_normal_R, [/workspaces/training/side-quests/working_with_files/data/patientA_rep1_normal_R1_001.fastq.gz, /workspaces/training/side-quests/working_with_files/data/patientA_rep1_normal_R2_001.fastq.gz]]\n</code></pre> <p>Note the difference in data structure. Rather than being a list of results, we have a single result in the format <code>id, [ fastq1, fastq2 ]</code>. Nextflow has done the hard work of extracting the patient name by examining the shared prefix and using it as a patient id.</p>"},{"location":"side_quests/working_with_files/#52-extract-metadata-from-file-pairs","title":"5.2. Extract metadata from file pairs","text":"<p>We still need the metadata. Our <code>map</code> operation from before won't work because it doesn't match the data structure, but we can modify it to work. We already have access to the patient name in the <code>id</code> variable, so we can use that to extract the metadata without grabbing the <code>simpleName</code> from the Path object like before.</p> AfterBefore file_operations.nf<pre><code>    ch_fastq = Channel.fromFilePairs('data/patientA_rep1_normal_R{1,2}_001.fastq.gz')\n    ch_fastq.map { id, fastqs -&gt;\n        def (sample, replicate, type, readNum) = id.tokenize('_')\n        [\n            [\n                id: sample,\n                replicate: replicate.replace('rep', ''),\n                type: type\n            ],\n            fastqs\n        ]\n    }\n    .view()\n</code></pre> file_operations.nf<pre><code>    ch_fastq = Channel.fromFilePairs('data/patientA_rep1_normal_R{1,2}_001.fastq.gz')\n        .view()\n</code></pre> Test file pairs metadata extraction<pre><code>nextflow run file_operations.nf\n</code></pre> File Pairs Output parsed<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `file_operations.nf` [prickly_stonebraker] DSL2 - revision: f62ab10a3f\n\n[[id:patientA, replicate:1, type:normal], [/workspaces/training/side-quests/working_with_files/data/patientA_rep1_normal_R1_001.fastq.gz, /workspaces/training/side-quests/working_with_files/data/patientA_rep1_normal_R2_001.fastq.gz]]\n</code></pre> <p>Notice, that this time we don't have a <code>readNum</code>. Because read 1 and read 2 are kept together, we do not need to track this in the meta data.</p> <p>Well done! We have grabbed the metadata from the filenames and used them as values in the tuple.</p>"},{"location":"side_quests/working_with_files/#takeaway_4","title":"Takeaway","text":"<ul> <li><code>Channel.fromFilePairs()</code> automatically finds and pairs related files</li> <li>This simplifies handling paired-end reads in your pipeline</li> <li>Paired files can be grouped as <code>[id, [file1, file2]]</code> tuples</li> <li>Metadata extraction can be done from the paired file ID rather than individual files</li> </ul>"},{"location":"side_quests/working_with_files/#6-using-file-operations-in-processes","title":"6. Using File Operations in Processes","text":"<p>Now let's put all this together in a simple process to reinforce how to use file operations in a Nextflow process.</p>"},{"location":"side_quests/working_with_files/#61-create-the-process","title":"6.1. Create the process","text":"<p>We'll keep it simple and make a process called <code>ANALYZE_READS</code> that takes in a tuple of metadata and a pair of fastq files and analyses them. We could imagine this is an alignment, or variant calling or any other step.</p> <p>Add the following to the top of your <code>file_operations.nf</code> file:</p> AfterBefore file_operations.nf - process example<pre><code>process ANALYZE_READS {\n    tag \"${meta.id}\"\n\n    publishDir \"results/${meta.id}\", mode: 'copy'\n\n    input:\n    tuple val(meta), path(fastqs)\n\n    output:\n    tuple val(meta.id), path(\"${meta.id}_stats.txt\")\n\n    script:\n    \"\"\"\n    echo \"Sample metadata: ${meta.id}\" &gt; ${meta.id}_stats.txt\n    echo \"Replicate: ${meta.replicate}\" &gt;&gt; ${meta.id}_stats.txt\n    echo \"Type: ${meta.type}\" &gt;&gt; ${meta.id}_stats.txt\n    echo \"Read 1: ${fastqs[0]}\" &gt;&gt; ${meta.id}_stats.txt\n    echo \"Read 2: ${fastqs[1]}\" &gt;&gt; ${meta.id}_stats.txt\n    echo \"Read 1 size: \\$(gunzip -dc ${fastqs[0]} | wc -l | awk '{print \\$1/4}') reads\" &gt;&gt; ${meta.id}_stats.txt\n    echo \"Read 2 size: \\$(gunzip -dc ${fastqs[1]} | wc -l | awk '{print \\$1/4}') reads\" &gt;&gt; ${meta.id}_stats.txt\n    \"\"\"\n}\n\nworkflow {\n</code></pre> file_operations.nf<pre><code>workflow {\n</code></pre> <p>Note</p> <p>We are calling our map '<code>meta</code>'. For a more in-depth introduction to meta maps, see Working with metadata.</p>"},{"location":"side_quests/working_with_files/#62-implement-the-process-in-the-workflow","title":"6.2. Implement the process in the workflow","text":"<p>Then implement the process in the workflow:</p> AfterBefore file_operations.nf<pre><code>    ch_fastq = Channel.fromFilePairs('data/patientA_rep1_normal_R{1,2}_001.fastq.gz')\n    ch_samples = ch_fastq.map { id, fastqs -&gt;\n        def (sample, replicate, type, readNum) = id.tokenize('_')\n        [\n            [\n                id: sample,\n                replicate: replicate.replace('rep', ''),\n                type: type\n            ],\n            fastqs\n        ]\n    }\n    ANALYZE_READS(ch_samples)\n}\n</code></pre> file_operations.nf<pre><code>    ch_fastq = Channel.fromFilePairs('data/patientA_rep1_normal_R{1,2}_001.fastq.gz')\n    ch_fastq.map { id, fastqs -&gt;\n        def (sample, replicate, type, readNum) = id.tokenize('_')\n        [\n            [\n                id: sample,\n                replicate: replicate.replace('rep', ''),\n                type: type\n            ],\n            fastqs\n        ]\n    }\n    .view()\n}\n</code></pre> Test ANALYZE_READS process<pre><code>nextflow run file_operations.nf\n</code></pre> ANALYZE_READS Output<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `./file_operations.nf` [shrivelled_cori] DSL2 - revision: b546a31769\n\nexecutor &gt;  local (1)\n[b5/110360] process &gt; ANALYZE_READS (patientA) [100%] 1 of 1 \u2714\n</code></pre> <p>We should see the following files in the <code>results/patientA</code> directory:</p> Results Directory<pre><code>&gt; tree results/patientA\nresults/patientA\n\u2514\u2500\u2500 patientA_stats.txt\n</code></pre> <p>The process took our inputs and created a new file with the patient metadata. Based on what you learned in hello-nextflow, what occurred in the working directory?</p>"},{"location":"side_quests/working_with_files/#63-include-many-more-patients","title":"6.3. Include many more patients","text":"<p>Remember Channel.fromPath() accepts a glob as input, which means it can accept any number of files that match the pattern. Therefore if we want to include all the patients we can just modify the input string to include more patients.</p> AfterBefore file_operations.nf<pre><code>    ch_fastq = Channel.fromFilePairs('data/*_R{1,2}_001.fastq.gz')\n</code></pre> file_operations.nf<pre><code>    ch_fastq = Channel.fromFilePairs('data/patientA_rep1_normal_R{1,2}_001.fastq.gz')\n</code></pre> <p>Run the pipeline now and see all the results:</p> Test processing multiple samples<pre><code>nextflow run file_operations.nf\n</code></pre> ANALYZE_READS Multiple Samples<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `./file_operations.nf` [big_stonebraker] DSL2 - revision: f7f9b8a76c\n\nexecutor &gt;  local (8)\n[d5/441891] process &gt; ANALYZE_READS (patientC) [100%] 8 of 8 \u2714\n</code></pre> <p>Check the results directory now:</p> Results Directory<pre><code>&gt; tree results\nresults\n\u251c\u2500\u2500 patientA\n\u2502   \u2514\u2500\u2500 patientA_stats.txt\n\u251c\u2500\u2500 patientB\n\u2502   \u2514\u2500\u2500 patientB_stats.txt\n\u2514\u2500\u2500 patientC\n    \u2514\u2500\u2500 patientC_stats.txt\n</code></pre> <p>See how we have analyzed all the patients in one go!</p> <p>Wait, we have a problem. We have 2 replicates for patientA, but only 1 output file! We are overwriting the output file each time.</p>"},{"location":"side_quests/working_with_files/#64-make-the-published-files-unique","title":"6.4. Make the published files unique","text":"<p>Since we have access to the patient metadata, we can use it to make the output files unique.</p> AfterBefore file_operations.nf<pre><code>    publishDir \"results/${meta.type}/${meta.id}/${meta.replicate}\", mode: 'copy'\n</code></pre> file_operations.nf<pre><code>    publishDir \"results/${id}\", mode: 'copy'\n</code></pre> <p>We have grabbed the metadata from the patients and used it to construct an output directory for each patient.</p> <p>Run the pipeline now and see all the results. Remove the results directory first to give yourself a clean workspace:</p> Test unique published files<pre><code>rm -r results\nnextflow run file_operations.nf\n</code></pre> Results Directory<pre><code> N E X T F L O W   ~  version 25.04.3\n\nLaunching `./file_operations.nf` [insane_swartz] DSL2 - revision: fff18abe6d\n\nexecutor &gt;  local (8)\n[e3/449081] process &gt; ANALYZE_READS (patientC) [100%] 8 of 8 \u2714\n</code></pre> <p>Check the results directory now:</p> Results Directory<pre><code>&gt; tree results\nresults/\n\u251c\u2500\u2500 normal\n\u2502   \u251c\u2500\u2500 patientA\n\u2502   \u2502   \u251c\u2500\u2500 1\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 patientA_stats.txt\n\u2502   \u2502   \u2514\u2500\u2500 2\n\u2502   \u2502       \u2514\u2500\u2500 patientA_stats.txt\n\u2502   \u251c\u2500\u2500 patientB\n\u2502   \u2502   \u2514\u2500\u2500 1\n\u2502   \u2502       \u2514\u2500\u2500 patientB_stats.txt\n\u2502   \u2514\u2500\u2500 patientC\n\u2502       \u2514\u2500\u2500 1\n\u2502           \u2514\u2500\u2500 patientC_stats.txt\n\u2514\u2500\u2500 tumor\n    \u251c\u2500\u2500 patientA\n    \u2502   \u251c\u2500\u2500 1\n    \u2502   \u2502   \u2514\u2500\u2500 patientA_stats.txt\n    \u2502   \u2514\u2500\u2500 2\n    \u2502       \u2514\u2500\u2500 patientA_stats.txt\n    \u251c\u2500\u2500 patientB\n    \u2502   \u2514\u2500\u2500 1\n    \u2502       \u2514\u2500\u2500 patientB_stats.txt\n    \u2514\u2500\u2500 patientC\n        \u2514\u2500\u2500 1\n            \u2514\u2500\u2500 patientC_stats.txt\n</code></pre> <p>See how using patient metadata as values gives us powerful flexibility in our pipeline. By propagating metadata alongside our data in tuples, we can:</p> <ol> <li>Create organized output directories based on patient attributes</li> <li>Make decisions in processes based on patient properties</li> <li>Split, join, and recombine data based on metadata values</li> </ol> <p>This pattern of keeping metadata explicit and attached to the data (rather than encoded in filenames) is a core best practice in Nextflow that enables building robust, maintainable bioinformatics workflows. Learn more about this in Working with metadata</p>"},{"location":"side_quests/working_with_files/#takeaway_5","title":"Takeaway","text":"<ul> <li>The <code>publishDir</code> directive can organize outputs based on metadata values</li> <li>Metadata in tuples enables structured organization of results</li> <li>This approach creates maintainable workflows with clear data provenance</li> <li>Processes can take tuples of metadata and files as input</li> <li>The <code>tag</code> directive provides process identification in execution logs</li> <li>Workflow structure separates channel creation from process execution</li> </ul>"},{"location":"side_quests/working_with_files/#summary","title":"Summary","text":"<p>In this side quest, you've learned how to work with files in Nextflow, from basic operations to more advanced techniques for handling collections of files. Here's a summary of what we covered:</p> <ol> <li> <p>Basic File Operations: We created Path objects with <code>file()</code> and accessed file attributes like name, extension, and parent directory, learning the difference between strings and Path objects.</p> </li> <li> <p>Using Remote Files: We learned how to transparently switch between local and remote files using URIs, demonstrating Nextflow's ability to handle files from various sources without changing workflow logic.</p> </li> <li> <p>Reading files using the <code>fromPath()</code> channel factory: We created channels from file patterns with <code>Channel.fromPath()</code> and viewed their file attributes, including object types.</p> </li> <li> <p>Extracting Patient Metadata from Filenames: We used <code>tokenize()</code> and <code>replace()</code> to extract and structure metadata from filenames, converting them to organized maps.</p> </li> <li> <p>Simplifying with Channel.fromFilePairs: We used <code>Channel.fromFilePairs()</code> to automatically pair related files and extract metadata from paired file IDs.</p> </li> <li> <p>Using File Operations in Processes: We integrated file operations into Nextflow processes with proper input handling, using <code>publishDir</code> to organize outputs based on metadata.</p> </li> </ol> <p>These techniques will help you build more efficient and maintainable workflows, especially when working with large numbers of files with complex naming conventions.</p>"},{"location":"side_quests/working_with_files/#key-concepts","title":"Key Concepts","text":"<ul> <li>Path Object Creation</li> </ul> <pre><code>// Create a Path object from a string path\nmyFile = file('path/to/file.txt')\n</code></pre> <ul> <li>File Attributes</li> </ul> <pre><code>// Get file attributes\nprintln myFile.name       // file.txt\nprintln myFile.baseName   // file\nprintln myFile.extension  // txt\nprintln myFile.parent     // path/to\n</code></pre> <ul> <li>Channel Creation from Files</li> </ul> <pre><code>// Create a channel from a file pattern\nch_fastq = Channel.fromPath('data/*.fastq.gz')\n\n// Create a channel from paired files\nch_pairs = Channel.fromFilePairs('data/*_R{1,2}_001.fastq.gz')\n</code></pre> <ul> <li>Extracting Metadata</li> </ul> <pre><code>// Extract metadata with tokenize\ndef name = file.name.tokenize('_')\ndef patientId = name[0]\ndef replicate = name[1].replace('rep', '')\ndef type = name[2]\ndef readNum = name[3].replace('R', '')\n</code></pre> <ul> <li>Using remote files</li> </ul> <pre><code>// Use a local file\nmyFile = file('path/to/file.txt')\n\n// Use a file on FTP\nmyFile = file('ftp://path/to/file.txt')\n\n// Use a file on HTTPS\nmyFile = file('https://path/to/file.txt')\n\n// Use a file on S3\nmyFile = file('s3://path/to/file.txt')\n\n// Use a file on Azure Blob Storage\nmyFile = file('az://path/to/file.txt')\n\n// Use a file on Google Cloud Storage\nmyFile = file('gs://path/to/file.txt')\n</code></pre>"},{"location":"side_quests/working_with_files/#resources","title":"Resources","text":"<ul> <li>Nextflow Documentation: Working with Files</li> <li>Channel.fromPath</li> <li>Channel.fromFilePairs</li> </ul>"},{"location":"training_collections/","title":"Training Collections","text":"<p>This section contains curated collections of training modules called Side Quests that aim to provide a comprehensive learning experience around a particular theme or use case.</p>"},{"location":"training_collections/#prerequisites","title":"Prerequisites","text":"<p>Each collection has specific prerequisites documented on its index page. However, most collections assume:</p> <ul> <li>Experience with the command line</li> <li>Foundational Nextflow concepts and tooling covered in the Hello Nextflow beginner training course</li> </ul> <p>For technical requirements and environment setup, see the Environment Setup mini-course.</p>"},{"location":"training_collections/#available-collections","title":"Available collections","text":"<ul> <li>The Architect's Toolkit I - A collection of four Side Quests covering workflow architecture patterns for assembling complex pipelines, implementing testing strategies, managing metadata management, and grouping and splitting data. Estimated duration: 4 hours in group training.</li> </ul>"},{"location":"training_collections/#suggesting-new-collections","title":"Suggesting new collections","text":"<p>We are actively working on developing additional Side Quests and Collections. Please feel free to suggest topics that you think would make sense to cover in a Collection by posting in the Training section of the community forum.</p>"},{"location":"training_collections/architects_toolkit_1/","title":"The Architect's Toolkit I","text":"<p>Our Training Collections provide curated learning paths through our advanced training materials (called Side Quests). This collection covers four essential topics that are frequently used together for building robust and scalable workflows.</p>"},{"location":"training_collections/architects_toolkit_1/#learning-objectives","title":"Learning objectives","text":"<p>By the end of this collection, you'll have experience with:</p> <ul> <li>Complex modular workflow architectures - Combining multiple workflows into cohesive pipelines</li> <li>Comprehensive testing strategies - Ensuring your workflows are reliable and maintainable</li> <li>Metadata management - Handling sample-specific metadata throughout your workflows effectively</li> <li>Advanced data processing - Implementing efficient data splitting and grouping patterns</li> </ul> <p>These skills will enable you to build robust, scalable, and maintainable Nextflow workflows for real-world applications.</p>"},{"location":"training_collections/architects_toolkit_1/#audience-prerequisites","title":"Audience &amp; prerequisites","text":"<p>This collection is designed for users who have completed the basic Nextflow training and want to dive deeper into advanced workflow patterns, testing strategies, and data and metadata handling techniques.</p> <p>Prerequisites</p> <ul> <li>Completion of Hello Nextflow training or equivalent experience</li> <li>Basic familiarity with Nextflow syntax and concepts</li> <li>Understanding of basic workflow development patterns</li> <li>Experience with command-line tools</li> </ul>"},{"location":"training_collections/architects_toolkit_1/#collection-contents","title":"Collection contents","text":"<p>This collection consists of four Side Quests that cover complementary workflow engineering topics:</p> <ol> <li>Workflows of Workflows - Complex workflow architecture and composition</li> <li>Testing with nf-test - Testing strategies for Nextflow workflows</li> <li>Metadata - Handling metadata for items in Nextflow channels</li> <li>Splitting and Grouping - Advanced data processing patterns</li> </ol> <p>Each Side Quest is self-contained and covers independent concepts, but we recommend completing them in the order listed above for a logical progression through the topics.</p>"},{"location":"training_collections/architects_toolkit_1/#how-to-use-this-collection","title":"How to use this collection","text":"<p>First, command-click on the \"Open in GitHub Codespaces\" button below to launch the training environment in a separate tab, then read on while it loads.</p> <p></p> <p>Once your environment is running, work through the collection as follows:</p> <ol> <li>In this tab: Navigate to the first Side Quest listed above, which describes step-by-step development exercises.</li> <li>In your Codespaces tab: Work through the exercises for the Side Quest.</li> <li>When you complete a Side Quest, return to this page and navigate to the next one in the list above.</li> <li>When you have completed the collection, click the button below to fill out a very short survey. Your feedback allows us to continue improving the training materials for everyone.</li> </ol> <p></p> <p>Ready to begin? Start with the first module above!</p>"},{"location":"pt/","title":"Treinamentos Nextflow","text":"<p>Seja bem vindo ao portal de treinamentos da comunidade do Nextflow!</p> <p>Temos v\u00e1rios treinamentos distintos dispon\u00edveis neste site. Role para baixo para encontrar o que se adequa a suas necessidades.</p> <p>Os treinamentos listados abaixo foram constru\u00eddos de modo que voc\u00ea possa faz\u00ea-los de forma independente; voc\u00ea pode trabalhar neles sozinho a qualquer momento (veja Configura\u00e7\u00e3o do Ambiente para detalhes pr\u00e1ticos). No entanto, voc\u00ea pode usufruir ainda mais desses materiais ao participar de um evento de treinamento em grupo.</p> <ul> <li>Eventos online gratuitos s\u00e3o realizados regularmente pela comunidade nf-core, veja a p\u00e1gina de eventos nf-core para mais informa\u00e7\u00f5es.</li> <li>A Seqera (a empresa que desenvolve o Nextflow) realiza uma variedade de eventos de treinamento. Veja a p\u00e1gina Seqera Events e procure por 'Seqera Sessions' e 'Nextflow Summit'.</li> <li>Nosso time de Comunidade tamb\u00e9m ministra regularmente treinamentos hospedados por outras organiza\u00e7\u00f5es; an\u00fancios e inscri\u00e7\u00f5es para eles s\u00e3o normalmente gerenciados por essas organiza\u00e7\u00f5es.</li> </ul> <p>Quando decidir come\u00e7ar o treinamento, clique no bot\u00e3o 'Open in GitHub Codespaces', nesta p\u00e1gina ou na p\u00e1gina de \u00edndice do treinamento escolhido, para abrir um ambiente de treinamento dentro do seu pr\u00f3porio navegador (requer uma conta gratuita no GitHub).</p> <p></p> <p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/#configuracao-do-ambiente","title":"Configura\u00e7\u00e3o do Ambiente","text":"<p>Configura\u00e7\u00e3o do Ambiente</p> <p> Essencial para configurar seu ambiente pela primeira vez.</p> <p>Instru\u00e7\u00f5es para configurar seu ambiente para trabalhar por meio dos materiais de treinamento desse portal. Fornece uma orienta\u00e7\u00e3o para o Gitpod, bem como instru\u00e7\u00f5es de instala\u00e7\u00e3o alternativas para trabalhar em sua pr\u00f3pria m\u00e1quina local.</p> <p>Inicie o treinamento de Configura\u00e7\u00e3o do Ambiente </p>"},{"location":"pt/#nextflow-para-iniciantes","title":"Nextflow para iniciantes","text":"<p>Hello Nextflow</p> <p> Uma s\u00e9rie de treinamento modular para come\u00e7ar a usar o Nextflow.</p> <p>Este \u00e9 um treinamento b\u00e1sico para aqueles que s\u00e3o completamente novos no Nextflow. Ele consiste em uma s\u00e9rie de m\u00f3dulos de treinamento que s\u00e3o projetados para ajudar os alunos a desenvolver suas habilidades progressivamente. A s\u00e9rie abrange os principais componentes da linguagem Nextflow, bem como pr\u00e1ticas essenciais de design e desenvolvimento de pipeline e uso eficaz de recursos de terceiros.</p> <p>Inicie o treinamento Hello Nextflow </p>"},{"location":"pt/#treinamento-aprofundado-em-nextflow","title":"Treinamento aprofundado em Nextflow","text":"<p>Fundamentals Training</p> <p> Material de treinamento abrangente para explorar todo o escopo dos recursos do Nextflow.</p> <p>O treinamento Fundamentals Training abrange tudo sobre o Nextflow. Excelente material de refer\u00eancia para qualquer um que queira construir fluxos de trabalho complexos com o Nextflow.</p> <p>Inicie o Fundamentals Training </p> <p>Advanced Training</p> <p> Material de treinamento avan\u00e7ado para dominar o Nextflow.</p> <p>Material avan\u00e7ado explorando os recursos mais avan\u00e7ados da linguagem e da ferramenta Nextflow e como us\u00e1-los para escrever fluxos de trabalho intensivos em dados, eficientes e escal\u00e1veis.</p> <p>Inicie o Advanced Training </p>"},{"location":"pt/#outrosem-desenvolvimento","title":"Outros/Em desenvolvimento","text":"<p>Configure a execu\u00e7\u00e3o de um pipeline nf-core</p> <p> Este treinamento ajudar\u00e1 voc\u00ea a entender como personalizar a execu\u00e7\u00e3o de um pipeline nf-core.</p> <p>Um tutorial \"aprenda fazendo\" com foco na configura\u00e7\u00e3o de pipelines nf-core.</p> <p>Inicie o treinamento de configura\u00e7\u00e3o do nf-core </p> <p>Desenvolva um pipeline com o modelo nf-core</p> <p> Este treinamento ajudar\u00e1 voc\u00ea a entender a estrutura do modelo de pipeline nf-core.</p> <p>Um tutorial de desenvolvimento de pipeline \"aprenda fazendo\" com foco no desenvolvimento de um pipeline com o modelo nf-core.</p> <p>Inicie o treinamento de desenvolvimento com nf-core </p> <p>Troubleshooting exercises</p> <p> Este treinamento ajudar\u00e1 voc\u00ea a solucionar erros comuns em pipeline.</p> <p>Um tutorial de solu\u00e7\u00e3o de problemas \"aprenda fazendo\" para desenvolvedores e usu\u00e1rios de pipeline.</p> <p>Inicie o treinamento de solucionamento de problemas </p>"},{"location":"pt/#obsoleto","title":"Obsoleto","text":"<p>Simple RNA-seq variant calling</p> <p> Um breve tutorial pr\u00e1tico focado em um exemplo concreto de pipeline de an\u00e1lise.</p> <p>Este curso foi desenvolvido como um tutorial \"aprender fazendo\", com a inten\u00e7\u00e3o de ser uma maneira r\u00e1pida e pr\u00e1tica de entender o Nextflow usando um exemplo de pipeline de an\u00e1lise muito concreto. Voc\u00ea ainda pode encontrar os materiais no reposit\u00f3rio do GitHub, mas ele n\u00e3o est\u00e1 mais sendo mantido e n\u00e3o pode mais ser iniciado no Gitpod ou no portal de treinamento.</p>"},{"location":"pt/#recursos","title":"Recursos","text":"<p>Refer\u00eancia r\u00e1pida para alguns links \u00fateis:</p> Reference \u00a0Community Documenta\u00e7\u00e3o do Nextflow Slack do Nextflow P\u00e1gina do Nextflow P\u00e1gina do nf-core P\u00e1gina da Seqera Seqera Community Forum <p>N\u00e3o sabe para onde ir? Confira a p\u00e1gina Obtendo ajuda.</p>"},{"location":"pt/#creditos-e-contribuicoes","title":"Cr\u00e9ditos e contribui\u00e7\u00f5es","text":"<p>Este material de treinamento \u00e9 desenvolvido e mantido pela Seqera e lan\u00e7ado sob uma licen\u00e7a de c\u00f3digo aberto (CC BY-NC-ND) para o benef\u00edcio da comunidade. Sinta-se \u00e0 vontade para reutilizar esses materiais de acordo com os termos da licen\u00e7a. Se voc\u00ea administra seus pr\u00f3prios treinamentos, ensinando terceiros, adorar\u00edamos saber como est\u00e1 indo e o que poder\u00edamos fazer para facilitar.</p> <p>Aceitamos corre\u00e7\u00f5es e melhorias da comunidade. Cada p\u00e1gina tem um \u00edcone  no canto superior direito da p\u00e1gina, que te levar\u00e1 ao GitHub, onde voc\u00ea pode propor altera\u00e7\u00f5es no material de treinamento por meio de uma solicita\u00e7\u00e3o de pull.</p> <p></p> <p></p>"},{"location":"pt/help/","title":"Conseguindo ajuda","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/help/#documentacao-do-nextflow","title":"Documenta\u00e7\u00e3o do Nextflow","text":"<p>Mesmo os desenvolvedores mais proficientes precisam de documenta\u00e7\u00e3o. Com o Nextflow n\u00e3o funciona diferente, e ele possui uma excelente documenta\u00e7\u00e3o. Voc\u00ea pode encontrar a documenta\u00e7\u00e3o do Nextflow em https://nextflow.io/docs/latest/ - recomendamos mant\u00ea-la aberta em uma aba enquanto voc\u00ea segue o treinamento!</p>"},{"location":"pt/help/#slack","title":"Slack","text":"<p>Se voc\u00ea est\u00e1 tendo dificuldades com o treinamento, n\u00e3o hesite em pedir ajuda. Nossa incr\u00edvel comunidade \u00e9 um dos grandes pontos fortes do Nextflow!</p> <p>Existem duas inst\u00e2ncias relevantes do Slack:</p> <ul> <li>Nextflow (inscreva-se aqui)</li> <li>nf-core (inscreva-se aqui)</li> </ul> <p>Geralmente, o Slack do Nextflow \u00e9 melhor, pois atende a toda a comunidade. A exce\u00e7\u00e3o \u00e9 se voc\u00ea estiver seguindo o treinamento atrav\u00e9s de um workshop organizado pelo projeto nf-core. Nesse caso, voc\u00ea deve ter sido informado sobre onde fazer perguntas.</p>"},{"location":"pt/help/#pergunte-aos-profissionais","title":"Pergunte aos profissionais","text":"<p>O Nextflow \u00e9 um software gratuito e de c\u00f3digo aberto, desenvolvido pela Seqera. A Seqera oferece um servi\u00e7o de suporte profissional para o Nextflow e produtos associados, al\u00e9m de realizar sess\u00f5es de treinamento sob medida.</p> <p>Se isso soa como algo que pode ser do seu interesse, por favor entre em contato conosco.</p>"},{"location":"pt/archive/basic_training/","title":"Index","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/archive/basic_training/#bem-vindo-ao-treinamento-basico-do-nextflow","title":"Bem vindo ao treinamento b\u00e1sico do Nextflow","text":"<p>Estamos entusiasmados em t\u00ea-lo no caminho para escrever fluxos de trabalho cient\u00edficos reprodut\u00edveis e escal\u00e1veis usando o Nextflow. Este guia complementa a documenta\u00e7\u00e3o oficial do Nextflow - se voc\u00ea tiver alguma d\u00favida, acesse a documenta\u00e7\u00e3o oficial localizada aqui.</p>"},{"location":"pt/archive/basic_training/#objetivos","title":"Objetivos","text":"<p>Ao final deste curso voc\u00ea dever\u00e1:</p> <ol> <li>Ser proficiente em escrever fluxos de trabalho com o Nextflow</li> <li>Conhecer os conceitos b\u00e1sicos de Canais, Processos e Operadores no Nextflow</li> <li>Ter uma compreens\u00e3o dos fluxos de trabalho usando cont\u00eaineres</li> <li>Entender as diferentes plataformas de execu\u00e7\u00e3o suportadas pelo Nextflow</li> <li>Sentir-se apresentado \u00e0 comunidade e ao ecossistema do Nextflow</li> </ol>"},{"location":"pt/archive/basic_training/#acompanhe-os-videos-de-treinamento","title":"Acompanhe os v\u00eddeos de treinamento","text":"<p>Realizamos um evento de treinamento online gratuito para este curso aproximadamente a cada seis meses. Os v\u00eddeos s\u00e3o transmitidos no YouTube e as perguntas s\u00e3o respondidas na comunidade nf-core do Slack. Voc\u00ea pode assistir \u00e0 grava\u00e7\u00e3o do treinamento mais recente (mar\u00e7o de 2024) na Playlist do YouTube abaixo:</p> <p>Se o ingl\u00eas n\u00e3o for seu idioma preferido, pode ser \u00fatil seguir o treinamento do evento de mar\u00e7o de 2023, que realizamos em m\u00faltiplos idiomas. Observe que algumas partes do material de treinamento podem ter sido atualizadas desde que foi gravado.</p> <ul> <li> Em Ingl\u00eas</li> <li> Em Hindu</li> <li> Em Espanhol</li> <li> Em Portugu\u00eas</li> <li> Em Franc\u00eas</li> </ul>"},{"location":"pt/archive/basic_training/#visao-geral","title":"Vis\u00e3o geral","text":"<p>Para come\u00e7ar a usar o Nextflow o mais r\u00e1pido poss\u00edvel, seguiremos as seguintes etapas:</p> <ol> <li>Configurar um ambiente de desenvolvimento para executar o Nextflow</li> <li>Explorar os conceitos do Nextflow usando alguns fluxos de trabalho b\u00e1sicos, incluindo uma an\u00e1lise de RNA-Seq com v\u00e1rias etapas</li> <li>Criar e usar cont\u00eaineres do Docker para encapsular todas as depend\u00eancias do fluxo de trabalho</li> <li>Mergulhar mais fundo na sintaxe principal do Nextflow, incluindo Canais, Processos e Operadores</li> <li>Cobrir cen\u00e1rios de implanta\u00e7\u00e3o na nuvem e em clusters e explorar os recursos do Nextflow Tower</li> </ol> <p>Isso lhe dar\u00e1 uma ampla compreens\u00e3o do Nextflow para come\u00e7ar a escrever seus pr\u00f3prios fluxos de trabalho. Esperamos que goste do curso! Este \u00e9 um documento em constante evolu\u00e7\u00e3o - feedback \u00e9 sempre bem-vindo.</p>"},{"location":"pt/archive/basic_training/cache_and_resume/","title":"Cache e reentr\u00e2ncia","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/archive/basic_training/cache_and_resume/#execucao-de-cache-e-de-reentrancia","title":"Execu\u00e7\u00e3o de cache e de reentr\u00e2ncia","text":"<p>O mecanismo de caching do Nextflow funciona atribuindo uma ID \u00fanica para cada tarefa que \u00e9 usada para criar um diret\u00f3rio de execu\u00e7\u00e3o separado onde as tarefas s\u00e3o executadas e os resultados guardados.</p> <p>A ID \u00fanica de tarefa \u00e9 gerada como uma hash de 128-bit compondo os valores de entrada da tarefa, arquivos e a string de comando.</p> <p>O diret\u00f3rio de trabalho do fluxo de trabalho \u00e9 organizado como mostrado abaixo:</p> <pre><code>work/\n\u251c\u2500\u2500 12\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 1adacb582d2198cd32db0e6f808bce\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 genome.fa -&gt; /data/../genome.fa\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 index\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 hash.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 header.json\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 indexing.log\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 quasi_index.log\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 refInfo.json\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 rsd.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 sa.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 txpInfo.bin\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 versionInfo.json\n\u251c\u2500\u2500 19\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 663679d1d87bfeafacf30c1deaf81b\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 aux_info\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ambig_info.tsv\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 expected_bias.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fld.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta_info.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 observed_bias.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 observed_bias_3p.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 cmd_info.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 libParams\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 flenDist.txt\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 lib_format_counts.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 salmon_quant.log\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 quant.sf\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut_1.fq -&gt; /data/../ggal_gut_1.fq\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut_2.fq -&gt; /data/../ggal_gut_2.fq\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 index -&gt; /data/../asciidocs/day2/work/12/1adacb582d2198cd32db0e6f808bce/index\n</code></pre> <p>Info</p> <p>Voc\u00ea pode criar esse plot usando a fun\u00e7\u00e3o <code>tree</code> se voc\u00ea a tiver instalada. No unix, simplesmente use <code>sudo apt install -y tree</code> ou com Homebrew: <code>brew install tree</code></p>"},{"location":"pt/archive/basic_training/cache_and_resume/#como-funciona-a-reentrancia","title":"Como funciona a reentr\u00e2ncia","text":"<p>A op\u00e7\u00e3o de linha de comando <code>-resume</code> permite a continua\u00e7\u00e3o da execu\u00e7\u00e3o do fluxo de trabalho pelo \u00faltimo passo que foi completado com sucesso:</p> <pre><code>nextflow run &lt;script&gt; -resume\n</code></pre> <p>Em termos pr\u00e1ticos, o fluxo de trabalho \u00e9 executado do in\u00edcio. Entretanto, antes do lan\u00e7amento da execu\u00e7\u00e3o de um processo, o Nextflow usa a ID \u00fanica da tarefa para checar se o diret\u00f3rio de trabalho existe e se ele cont\u00e9m um estado de sa\u00edda v\u00e1lido do comando com os esperados arquivos de sa\u00edda.</p> <p>Se esta condi\u00e7\u00e3o for satisfeita a tarefa \u00e9 ignorada e os resultados computados previamente s\u00e3o usados como resultados do processo.</p> <p>A primeira tarefa que tem uma nova sa\u00edda computada invalida todas execu\u00e7\u00f5es posteriores no que resta do Grafo Ac\u00edclico Direcionado (DAG, do ingl\u00eas Directed Acyclic Graph).</p>"},{"location":"pt/archive/basic_training/cache_and_resume/#diretorio-de-trabalho","title":"Diret\u00f3rio de trabalho","text":"<p>O diret\u00f3rio de trabalho das tarefas \u00e9 criado por padr\u00e3o na pasta <code>work</code> no mesmo diret\u00f3rio onde o fluxo de trabalho foi executado. Essa localiza\u00e7\u00e3o \u00e9 supostamente uma \u00e1rea de armazenamento provis\u00f3ria que pode ser limpada quando a execu\u00e7\u00e3o do fluxo de trabalho for finalizado.</p> <p>Note</p> <p>As sa\u00eddas finais do fluxo de trabalho geralmente s\u00e3o guardadas em uma localiza\u00e7\u00e3o diferente especificada usando uma ou mais diretivas publishDir.</p> <p>Warning</p> <p>Certifique-se de deletar o diret\u00f3rio de trabalho ocasionalmente, se n\u00e3o sua m\u00e1quina ou ambiente estar\u00e1 cheia de arquivos sem uso.</p> <p>Uma localiza\u00e7\u00e3o diferente para o diret\u00f3rio de trabalho pode ser especificada usando a op\u00e7\u00e3o <code>-w</code>. Por exemplo:</p> <pre><code>nextflow run &lt;script&gt; -w /algum/diretorio/de/scratch\n</code></pre> <p>Warning</p> <p>Se voc\u00ea deletar ou mover o diret\u00f3rio de trabalho do fluxo de trabalho, isso ir\u00e1 impedir que voc\u00ea use o recurso de reentr\u00e2ncia nas execu\u00e7\u00f5es posteriores.</p> <p>O c\u00f3digo hash para os arquivos de entrada s\u00e3o computados usando:</p> <ul> <li>O caminho completo do arquivo</li> <li>O tamanho do arquivo</li> <li>A \u00faltima marca\u00e7\u00e3o de tempo de modifica\u00e7\u00e3o</li> </ul> <p>Portanto, o simples uso do touch em um arquivo ir\u00e1 invalidar a execu\u00e7\u00e3o da tarefa relacionada.</p>"},{"location":"pt/archive/basic_training/cache_and_resume/#como-organizar-experimentos-in-silico","title":"Como organizar experimentos in-silico","text":"<p>\u00c9 uma boa pr\u00e1tica organizar cada experimento em sua pr\u00f3pria pasta. Os par\u00e2metros de entrada do experimento principal devem ser especificados usando o arquivo de configura\u00e7\u00e3o do Nextflow. Isso torna simples acompanhar e replicar o experimento ao longo do tempo.</p> <p>Note</p> <p>No mesmo experimento, o mesmo fluxo de trabalho pode ser executado diversas vezes, entretanto, iniciar duas (ou mais) inst\u00e2ncias do Nextflow no mesmo diret\u00f3rio ao mesmo tempo deve ser evitado.</p> <p>O comando <code>nextflow log</code> lista todas as execu\u00e7\u00f5es na pasta atual:</p> <pre><code>$ nextflow log\n\nTIMESTAMP            DURATION  RUN NAME          STATUS  REVISION ID  SESSION ID                            COMMAND\n2019-05-06 12:07:32  1.2s      focused_carson    ERR     a9012339ce   7363b3f0-09ac-495b-a947-28cf430d0b85  nextflow run hello\n2019-05-06 12:08:33  21.1s     mighty_boyd       OK      a9012339ce   7363b3f0-09ac-495b-a947-28cf430d0b85  nextflow run rnaseq-nf -with-docker\n2019-05-06 12:31:15  1.2s      insane_celsius    ERR     b9aefc67b4   4dc656d2-c410-44c8-bc32-7dd0ea87bebf  nextflow run rnaseq-nf\n2019-05-06 12:31:24  17s       stupefied_euclid  OK      b9aefc67b4   4dc656d2-c410-44c8-bc32-7dd0ea87bebf  nextflow run rnaseq-nf -resume -with-docker\n</code></pre> <p>Voc\u00ea pode usar tanto o ID da sess\u00e3o ou o nome da execu\u00e7\u00e3o para recuperar uma execu\u00e7\u00e3o espec\u00edfica. Por exemplo:</p> <pre><code>nextflow run rnaseq-nf -resume mighty_boyd\n</code></pre>"},{"location":"pt/archive/basic_training/cache_and_resume/#proveniencia-da-execucao","title":"Proveni\u00eancia da execu\u00e7\u00e3o","text":"<p>O comando <code>log</code>, quando provido do nome da execu\u00e7\u00e3o ou ID da sess\u00e3o, pode retornar algumas informa\u00e7\u00f5es importantes sobre um fluxo de trabalho em execu\u00e7\u00e3o que podem ser usadas para criar um relat\u00f3rio de proveni\u00eancia.</p> <p>Por padr\u00e3o, o comando ir\u00e1 listar todos diret\u00f3rios de trabalho usados em cada tarefa. Por exemplo:</p> <pre><code>$ nextflow log tiny_fermat\n\n/data/.../work/7b/3753ff13b1fa5348d2d9b6f512153a\n/data/.../work/c1/56a36d8f498c99ac6cba31e85b3e0c\n/data/.../work/f7/659c65ef60582d9713252bcfbcc310\n/data/.../work/82/ba67e3175bd9e6479d4310e5a92f99\n/data/.../work/e5/2816b9d4e7b402bfdd6597c2c2403d\n/data/.../work/3b/3485d00b0115f89e4c202eacf82eba\n</code></pre> <p>A op\u00e7\u00e3o <code>-f</code> (do ingl\u00eas, fields) pode ser usada para especificar qual metadado deve ser impresso pelo comando <code>log</code>. Por exemplo:</p> <pre><code>$ nextflow log tiny_fermat -f 'process,exit,hash,duration'\n\nindex    0   7b/3753ff  2.0s\nfastqc   0   c1/56a36d  9.3s\nfastqc   0   f7/659c65  9.1s\nquant    0   82/ba67e3  2.7s\nquant    0   e5/2816b9  3.2s\nmultiqc  0   3b/3485d0  6.3s\n</code></pre> <p>A lista completa dos campos dispon\u00edveis pode ser recuperada com o comando:</p> <pre><code>nextflow log -l\n</code></pre> <p>A op\u00e7\u00e3o <code>-F</code> permite a especifica\u00e7\u00e3o de um crit\u00e9rio de filtro para imprimir apenas um subconjunto de tarefas. Por exemplo:</p> <pre><code>$ nextflow log tiny_fermat -F 'process =~ /fastqc/'\n\n/data/.../work/c1/56a36d8f498c99ac6cba31e85b3e0c\n/data/.../work/f7/659c65ef60582d9713252bcfbcc310\n</code></pre> <p>Isso pode ser \u00fatil para localizar um diret\u00f3rio de trabalho de uma tarefa espec\u00edfica.</p> <p>Finalmente, a op\u00e7\u00e3o <code>-t</code> permite a cria\u00e7\u00e3o de um relat\u00f3rio b\u00e1sico e customiz\u00e1vel de proveni\u00eancia, mostrando um modelo de arquivo em qualquer formato de sua escolha. Por exemplo:</p> <pre><code>&lt;div&gt;\n  &lt;h2&gt;${name}&lt;/h2&gt;\n  &lt;div&gt;\n    Script:\n    &lt;pre&gt;${script}&lt;/pre&gt;\n  &lt;/div&gt;\n\n  &lt;ul&gt;\n    &lt;li&gt;Exit: ${exit}&lt;/li&gt;\n    &lt;li&gt;Status: ${status}&lt;/li&gt;\n    &lt;li&gt;Work dir: ${workdir}&lt;/li&gt;\n    &lt;li&gt;Cont\u00eainer: ${container}&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n</code></pre> <p>Exercise</p> <p>Salve o trecho acima em um arquivo chamado <code>template.html</code>. Ent\u00e3o execute o comando (usando o ID correto para sua execu\u00e7\u00e3o, ex. <code>tiny_fermat</code>):</p> <pre><code>nextflow log tiny_fermat -t template.html &gt; prov.html\n</code></pre> <p>Finalmente, abra o arquivo <code>prov.html</code> com um navegador.</p>"},{"location":"pt/archive/basic_training/cache_and_resume/#resolucao-de-problemas-de-reentrancia","title":"Resolu\u00e7\u00e3o de problemas de reentr\u00e2ncia","text":"<p>Ser capaz de retomar fluxos de trabalho \u00e9 um recurso importante do Nextflow, mas nem sempre funciona como voc\u00ea espera. Nesta se\u00e7\u00e3o, analisamos alguns motivos comuns pelos quais o Nextflow pode estar ignorando seus resultados em cache.</p> <p>Tip</p> <p>Para saber mais detalhes sobre o mecanismo de reentr\u00e2ncia e como solucionar problemas, consulte as tr\u00eas postagens de blog a seguir:</p> <ol> <li>Demystifying Nextflow resume</li> <li>Troubleshooting Nextflow resume</li> <li>Analyzing caching behavior of pipelines</li> </ol>"},{"location":"pt/archive/basic_training/cache_and_resume/#arquivos-de-entrada-mudados","title":"Arquivos de entrada mudados","text":"<p>Tenha certeza que n\u00e3o h\u00e1 nenhuma mudan\u00e7a no(s) arquivo(s) de entrada. N\u00e3o esque\u00e7a que cada tarefa tem seu hash \u00fanico que \u00e9 computado levando em conta o caminho completo do arquivo, a \u00faltima marca\u00e7\u00e3o de tempo de modifica\u00e7\u00e3o e o tamanho do arquivo. Se alguma dessas informa\u00e7\u00f5es foi alterada, o fluxo de trabalho deve ser re-executado mesmo que o conte\u00fado do arquivo seja o mesmo.</p>"},{"location":"pt/archive/basic_training/cache_and_resume/#um-processo-modifica-uma-entrada","title":"Um processo modifica uma entrada","text":"<p>Um processo nunca deve alterar os arquivos de entrada, se n\u00e3o a fun\u00e7\u00e3o <code>resume</code> em execu\u00e7\u00f5es futuras ser\u00e1 invalidada pela mesma raz\u00e3o explicada no ponto anterior.</p>"},{"location":"pt/archive/basic_training/cache_and_resume/#atributos-de-arquivos-inconsistentes","title":"Atributos de arquivos inconsistentes","text":"<p>Alguns sistemas de arquivos compartilhados, como o NFS, podem reportar uma marca\u00e7\u00e3o de tempo inconsistente para os arquivos (por exemplo, uma diferente marca\u00e7\u00e3o de tempo para um mesmo arquivo) at\u00e9 mesmo quando este n\u00e3o foi modificado. Para prevenir esse problema use a estrat\u00e9gia de cache leniente.</p>"},{"location":"pt/archive/basic_training/cache_and_resume/#condicao-de-corrida-em-uma-variavel-global","title":"Condi\u00e7\u00e3o de corrida em uma vari\u00e1vel global","text":"<p>O Nextflow \u00e9 desenvolvido para simplificar programa\u00e7\u00e3o paralela, de modo que voc\u00ea n\u00e3o precise se preocupar com condi\u00e7\u00f5es de corrida e acesso a recursos compartilhados. Um dos poucos casos que uma condi\u00e7\u00e3o de corrida pode surgir \u00e9 quando uma vari\u00e1vel global \u00e9 utilizada com dois (ou mais) operadores. Por exemplo:</p> <pre><code>Channel\n    .of(1, 2, 3)\n    .map { it -&gt; X = it; X += 2 }\n    .view { \"canal1 = $it\" }\n\nChannel\n    .of(1, 2, 3)\n    .map { it -&gt; X = it; X *= 2 }\n    .view { \"canal2 = $it\" }\n</code></pre> <p>O problema desse trecho \u00e9 que a vari\u00e1vel <code>X</code> na clausura \u00e9 definida no escopo global. Portanto, como operadores s\u00e3o executados em paralelo, o valor de <code>X</code> pode ser sobrescrito pela outra invoca\u00e7\u00e3o do <code>map</code>.</p> <p>A implementa\u00e7\u00e3o correta requer o uso da palavra chave <code>def</code> para declarar a vari\u00e1vel local.</p> <pre><code>Channel\n    .of(1, 2, 3)\n    .map { it -&gt; def X = it; X += 2 }\n    .println { \"canal1 = $it\" }\n\nChannel\n    .of(1, 2, 3)\n    .map { it -&gt; def X = it; X *= 2 }\n    .println { \"canal2 = $it\" }\n</code></pre>"},{"location":"pt/archive/basic_training/cache_and_resume/#canais-de-entrada-nao-deterministicos","title":"Canais de entrada n\u00e3o determin\u00edsticos.","text":"<p>Embora a ordem de elementos em canais dataflow seja garantida \u2013 os dados s\u00e3o lidos na mesma ordem em que s\u00e3o escritos no canal \u2013 saiba que n\u00e3o h\u00e1 garantia de que os elementos manter\u00e3o sua ordem no canal de sa\u00edda do processo. Isso ocorre porque um processo pode gerar v\u00e1rias tarefas, que podem ser executadas em paralelo. Por exemplo, a opera\u00e7\u00e3o no segundo elemento pode terminar antes da opera\u00e7\u00e3o no primeiro elemento, alterando a ordem dos elementos no canal de sa\u00edda.</p> <p>Em termos pr\u00e1ticos, considere o trecho a seguir:</p> <pre><code>process FOO {\n    input:\n    val x\n\n    output:\n    tuple val(task.index), val(x)\n\n    script:\n    \"\"\"\n    sleep \\$((RANDOM % 3))\n    \"\"\"\n}\n\nworkflow {\n    channel.of('A', 'B', 'C', 'D') | FOO | view\n}\n</code></pre> <p>Assim como vimos no in\u00edcio deste tutorial com HELLO WORLD ou WORLD HELLO, a sa\u00edda do trecho acima pode ser:</p> <pre><code>[3, C]\n[4, D]\n[2, B]\n[1, A]\n</code></pre> <p>..e essa ordem provavelmente ser\u00e1 diferente toda vez que o fluxo de trabalho for executado.</p> <p>Imagine agora que temos dois processos como este, cujos canais de sa\u00edda est\u00e3o atuando como canais de entrada para um terceiro processo. Ambos os canais ser\u00e3o aleat\u00f3rios de forma independente, portanto, o terceiro processo n\u00e3o deve esperar que eles retenham uma sequ\u00eancia pareada. Se assumir que o primeiro elemento no primeiro canal de sa\u00edda do processo est\u00e1 relacionado ao primeiro elemento no segundo canal de sa\u00edda do processo, haver\u00e1 uma incompatibilidade.</p> <p>Uma solu\u00e7\u00e3o comum para isso \u00e9 usar o que \u00e9 comumente chamado de meta mapa (meta map). Um objeto groovy com informa\u00e7\u00f5es de amostra \u00e9 distribu\u00eddo junto com os resultados do arquivo em um canal de sa\u00edda como uma tupla. Isso pode ent\u00e3o ser usado para emparelhar amostras que est\u00e3o em canais separados para uso em processos posteriores. Por exemplo, em vez de colocar apenas <code>/algum/caminho/minhasaida.bam</code> em um canal, voc\u00ea pode usar <code>['SRR123', '/algum/caminho/minhasaida.bam']</code> para garantir que os processos n\u00e3o incorram em uma incompatibilidade. Verifique o exemplo abaixo:</p> <pre><code>// Apenas para fins de exemplos.\n// Estes abaixo seriam normalmente as sa\u00eddas de processos anteriores\nChannel\n    .of(\n        [[id: 'amostra_1'], '/caminho/para/amostra_1.bam'],\n        [[id: 'amostra_2'], '/caminho/para/amostra_2.bam']\n    )\n    .set { bam }\n\n// Nota: amostra_2 \u00e9 agora o primeiro elemento, em vez de amostra_1\nChannel\n    .of(\n        [[id: 'amostra_2'], '/caminho/para/amostra_2.bai'],\n        [[id: 'amostra_1'], '/caminho/para/amostra_1.bai']\n    )\n    .set { bai }\n\n// Em vez de alimentar o processo posterior com esses dois canais separadamente,\n// n\u00f3s podemos un\u00ed-los com o operador `join` e entregar um \u00fanico canal onde o\n// meta mapa de amostra \u00e9 correspondido implicitamente:\nbam\n    .join(bai)\n    | PROCESSO_C\n</code></pre> <p>Se os meta mapas n\u00e3o forem poss\u00edveis, uma alternativa \u00e9 usar a diretiva de processo <code>fair</code>. Quando especificada, o Nextflow garantir\u00e1 que a ordem dos elementos nos canais de sa\u00edda corresponder\u00e1 \u00e0 ordem dos respectivos elementos nos canais de entrada. \u00c9 importante deixar claro que a ordem em que as tarefas ser\u00e3o conclu\u00eddas n\u00e3o ser\u00e1 necessariamente a ordem dos elementos no canal entrada, mas Nextflow garante que, ao final do processamento, os elementos no canal de sa\u00edda estar\u00e3o na ordem correta.</p> <p>Warning</p> <p>Dependendo da sua situa\u00e7\u00e3o, usar a diretiva <code>fair</code> levar\u00e1 a uma queda de desempenho.</p>"},{"location":"pt/archive/basic_training/channels/","title":"Channels","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/archive/basic_training/channels/#canais","title":"Canais","text":"<p>Canais s\u00e3o uma estrutura de dados chave do Nextflow que permite a implementa\u00e7\u00e3o de fluxos de trabalho computacionais utilizando paradigmas funcional e reativo com base no paradigma de programa\u00e7\u00e3o Dataflow.</p> <p>Eles s\u00e3o usados para conectar logicamente tarefas entre si ou para implementar transforma\u00e7\u00f5es de dados de estilo funcional.</p>"},{"location":"pt/archive/basic_training/channels/#tipos-de-canais","title":"Tipos de canais","text":"<p>O Nextflow distingue dois tipos diferentes de canais: canais de fila e canais de valor.</p>"},{"location":"pt/archive/basic_training/channels/#canal-de-fila","title":"Canal de fila","text":"<p>Um canal de fila \u00e9 uma fila ass\u00edncrona unidirecional FIFO (First-in-First-out, o primeiro a entrar, \u00e9 o primeiro a sair) que conecta dois processos ou operadores.</p> <ul> <li>ass\u00edncrono significa que as opera\u00e7\u00f5es ocorrem sem bloqueio.</li> <li>unidirecional significa que os dados fluem do gerador para o consumidor.</li> <li>FIFO significa que os dados s\u00e3o entregues na mesma ordem em que s\u00e3o produzidos. Primeiro a entrar, primeiro a sair.</li> </ul> <p>Um canal de fila \u00e9 criado implicitamente por defini\u00e7\u00f5es de sa\u00edda de um processo ou usando f\u00e1bricas de canal, como o Channel.of ou Channel.fromPath.</p> <p>Tente os seguintes trechos de c\u00f3digo:</p> <p>Clique no \u00edcone  no c\u00f3digo para ver explica\u00e7\u00f5es.</p> <pre><code>canal = Channel.of(1, 2, 3)\nprintln(canal) // (1)!\ncanal.view() // (2)!\n</code></pre> <ol> <li>Use a fun\u00e7\u00e3o <code>println</code> embutida no Nextflow por padr\u00e3o para imprimir o conte\u00fado do canal <code>canal</code></li> <li>Aplique o operador <code>view</code> no canal <code>canal</code> para imprimir cada emiss\u00e3o desse canal</li> </ol> <p>Exercise</p> <p>Tente executar este trecho de c\u00f3digo. Voc\u00ea pode fazer isso criando um novo arquivo <code>.nf</code> ou editando um arquivo <code>.nf</code> j\u00e1 existente.</p> <pre><code>canal = Channel.of(1, 2, 3)\ncanal.view()\n</code></pre>"},{"location":"pt/archive/basic_training/channels/#canais-de-valor","title":"Canais de valor","text":"<p>Um canal de valor (tamb\u00e9m conhecido como canal singleton), por defini\u00e7\u00e3o, est\u00e1 vinculado a um \u00fanico valor e pode ser lido quantas vezes for necess\u00e1rio sem consumir seu conte\u00fado. Um canal de <code>valor</code> \u00e9 criado usando a f\u00e1brica de canal value ou por operadores que retornam um valor apenas, como first, last, collect, count, min, max, reduce, e sum.</p> <p>Para entender melhor a diferen\u00e7a entre canais de valor e de fila, salve o trecho abaixo como <code>exemplo.nf</code>.</p> exemplo.nf<pre><code>canal1 = Channel.of(1, 2, 3)\ncanal2 = Channel.of(1)\n\nprocess SUM {\n    input:\n    val x\n    val y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo \\$(($x+$y))\n    \"\"\"\n}\n\nworkflow {\n    SUM(canal1, canal2).view()\n}\n</code></pre> <p>Ao rodar o script, ele imprime apenas 2, como voc\u00ea pode ver abaixo:</p> <pre><code>2\n</code></pre> <p>Um processo s\u00f3 instanciar\u00e1 uma tarefa quando houver elementos a serem consumidos de todos os canais fornecidos como entrada para ele. Como <code>canal1</code> e <code>canal2</code> s\u00e3o canais de fila, e o \u00fanico elemento de <code>canal2</code> foi consumido, nenhuma nova inst\u00e2ncia de processo ser\u00e1 iniciada, mesmo se houver outros elementos a serem consumidos em <code>canal1</code>.</p> <p>Para usar o \u00fanico elemento em <code>canal2</code> v\u00e1rias vezes, podemos usar <code>Channel.value</code> como mencionado acima, ou usar um operador de canal que retorna um \u00fanico elemento como <code>first()</code> abaixo:</p> <pre><code>canal1 = Channel.of(1, 2, 3)\ncanal2 = Channel.of(1)\n\nprocess SUM {\n    input:\n    val x\n    val y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo \\$(($x+$y))\n    \"\"\"\n}\n\nworkflow {\n    SUM(canal1, canal2.first()).view()\n}\n</code></pre> Output<pre><code>4\n\n3\n\n2\n</code></pre> <p>Al\u00e9m disso, em muitas situa\u00e7\u00f5es, o Nextflow converter\u00e1 implicitamente vari\u00e1veis em canais de valor quando forem usadas em uma chamada de processo. Por exemplo, quando voc\u00ea chama um processo com um par\u00e2metro de fluxo de trabalho (<code>params.exemplo</code>) que possui um valor de string, ele \u00e9 automaticamente convertido em um canal de valor.</p>"},{"location":"pt/archive/basic_training/channels/#fabricas-de-canal","title":"F\u00e1bricas de canal","text":"<p>Estes s\u00e3o comandos do Nextflow para criar canais que possuem entradas e fun\u00e7\u00f5es impl\u00edcitas esperadas.</p>"},{"location":"pt/archive/basic_training/channels/#value","title":"<code>value()</code>","text":"<p>A f\u00e1brica de canal <code>value</code> \u00e9 utilizada para criar um canal de valor. Um argumento opcional n\u00e3o <code>nulo</code> pode ser especificado para vincular o canal a um valor espec\u00edfico. Por exemplo:</p> <pre><code>canal1 = Channel.value() // (1)!\ncanal2 = Channel.value('Ol\u00e1, voc\u00ea!') // (2)!\ncanal3 = Channel.value([1, 2, 3, 4, 5]) // (3)!\n</code></pre> <ol> <li>Cria um canal de valor vazio</li> <li>Cria um canal de valor e vincula uma string a ele</li> <li>Cria um canal de valor e vincula a ele um objeto de lista que ser\u00e1 emitido como uma \u00fanica emiss\u00e3o</li> </ol>"},{"location":"pt/archive/basic_training/channels/#of","title":"<code>of()</code>","text":"<p>A f\u00e1brica <code>Channel.of</code> permite a cria\u00e7\u00e3o de um canal de fila com os valores especificados como argumentos.</p> <pre><code>canal = Channel.of(1, 3, 5, 7)\ncanal.view { \"numero: $it\" }\n</code></pre> <p>A primeira linha neste exemplo cria uma vari\u00e1vel <code>canal</code> que cont\u00e9m um objeto de canal. Este canal emite os valores especificados como par\u00e2metro na f\u00e1brica de canal <code>of</code>. Assim, a segunda linha imprimir\u00e1 o seguinte:</p> <pre><code>numero: 1\nnumero: 3\nnumero: 5\nnumero: 7\n</code></pre> <p>A f\u00e1brica de canal <code>Channel.of</code> funciona de maneira semelhante ao <code>Channel.from</code> (que foi descontinuado), corrigindo alguns comportamentos inconsistentes do \u00faltimo e fornecendo um melhor manuseio quando um intervalo de valores \u00e9 especificado. Por exemplo, o seguinte funciona com um intervalo de 1 a 23:</p> <pre><code>Channel\n    .of(1..23, 'X', 'Y')\n    .view()\n</code></pre>"},{"location":"pt/archive/basic_training/channels/#fromlist","title":"<code>fromList()</code>","text":"<p>A f\u00e1brica de canal <code>Channel.fromList</code> cria um canal emitindo os elementos fornecidos por um objeto de lista especificado como um argumento:</p> <pre><code>list = ['ol\u00e1', 'mundo']\n\nChannel\n    .fromList(list)\n    .view()\n</code></pre>"},{"location":"pt/archive/basic_training/channels/#frompath","title":"<code>fromPath()</code>","text":"<p>A f\u00e1brica de canal <code>fromPath</code> cria um canal de fila emitindo um ou mais arquivos correspondentes ao padr\u00e3o glob especificado.</p> <pre><code>Channel.fromPath('./data/meta/*.csv')\n</code></pre> <p>Este exemplo cria um canal e emite tantos itens quanto arquivos com extens\u00e3o <code>csv</code> existirem na pasta <code>./data/meta</code>. Cada elemento \u00e9 um objeto de arquivo implementando a interface Path do Java.</p> <p>Tip</p> <p>Dois asteriscos, ou seja, <code>**</code>, funcionam como <code>*</code>, mas cruzam os limites do diret\u00f3rio. Essa sintaxe geralmente \u00e9 usada para percorrer caminhos completos. Os colchetes especificam uma cole\u00e7\u00e3o de subpadr\u00f5es.</p> Nome Descri\u00e7\u00e3o glob Quando <code>true</code> interpreta caracteres <code>*</code>, <code>?</code>, <code>[]</code> e <code>{}</code> como glob wildcards, caso contr\u00e1rio, os trata como caracteres normais (padr\u00e3o: <code>true</code>) type Tipo de caminho retornado, ou <code>file</code>, <code>dir</code> ou <code>any</code> (padr\u00e3o: <code>file</code>) hidden Quando <code>true</code> inclui arquivos ocultos nos caminhos resultantes (padr\u00e3o: <code>false</code>) maxDepth N\u00famero m\u00e1ximo de n\u00edveis de diret\u00f3rio a serem visitados (padr\u00e3o: <code>no limit</code>) followLinks Quando <code>true</code> links simb\u00f3licos s\u00e3o seguidos durante a travessia da \u00e1rvore de diret\u00f3rios, caso contr\u00e1rio, eles s\u00e3o gerenciados como arquivos (padr\u00e3o: <code>true</code>) relative Quando <code>true</code> os caminhos de retorno s\u00e3o relativos ao diret\u00f3rio de topo mais comum (padr\u00e3o: <code>false</code>) checkIfExists Quando <code>true</code> lan\u00e7a uma exce\u00e7\u00e3o quando o caminho especificado n\u00e3o existe no sistema de arquivos (padr\u00e3o: <code>false</code>) <p>Saiba mais sobre a sintaxe dos padr\u00f5es glob neste link.</p> <p>Exercise</p> <p>Use a f\u00e1brica de canal <code>Channel.fromPath</code> para criar um canal emitindo todos os arquivos com o sufixo <code>.fq</code> no diret\u00f3rio <code>data/ggal/</code> e qualquer subdiret\u00f3rio, al\u00e9m dos arquivos ocultos. Em seguida, imprima os nomes dos arquivos.</p> Solution <pre><code>Channel\n    .fromPath('./data/ggal/**.fq', hidden: true)\n    .view()\n</code></pre>"},{"location":"pt/archive/basic_training/channels/#fromfilepairs","title":"<code>fromFilePairs()</code>","text":"<p>A f\u00e1brica de canal <code>fromFilePairs</code> cria um canal emitindo os pares de arquivos correspondentes a um padr\u00e3o glob fornecido pelo usu\u00e1rio. Os arquivos correspondentes s\u00e3o emitidos como tuplas, nas quais o primeiro elemento \u00e9 a chave de agrupamento do par correspondente e o segundo elemento \u00e9 a lista de arquivos (classificados em ordem lexicogr\u00e1fica).</p> <pre><code>Channel\n    .fromFilePairs('./data/ggal/*_{1,2}.fq')\n    .view()\n</code></pre> <p>Ele produzir\u00e1 uma sa\u00edda semelhante \u00e0 seguinte:</p> <pre><code>[liver, [/workspaces/training/nf-training/data/ggal/liver_1.fq, /workspaces/training/nf-training/data/ggal/liver_2.fq]]\n[gut, [/workspaces/training/nf-training/data/ggal/gut_1.fq, /workspaces/training/nf-training/data/ggal/gut_2.fq]]\n[lung, [/workspaces/training/nf-training/data/ggal/lung_1.fq, /workspaces/training/nf-training/data/ggal/lung_2.fq]]\n</code></pre> <p>Warning</p> <p>O padr\u00e3o glob precisa conter pelo menos um caractere curinga de estrela (<code>*</code>).</p> Nome Descri\u00e7\u00e3o type Tipo de caminhos retornados, ou <code>file</code>, <code>dir</code> ou <code>any</code> (padr\u00e3o: <code>file</code>) hidden Quando <code>true</code> inclui arquivos ocultos nos caminhos resultantes (padr\u00e3o: <code>false</code>) maxDepth N\u00famero m\u00e1ximo de n\u00edveis de diret\u00f3rio a serem visitados (padr\u00e3o: <code>no limit</code>) followLinks Quando <code>true</code> links simb\u00f3licos s\u00e3o seguidos durante a travessia da \u00e1rvore de diret\u00f3rios, caso contr\u00e1rio, eles s\u00e3o gerenciados como arquivos (padr\u00e3o: <code>true</code>) size Define o n\u00famero de arquivos que cada item emitido deve conter (padr\u00e3o: <code>2</code>). Use <code>-1</code> para qualquer n\u00famero flat Quando <code>true</code> os arquivos correspondentes s\u00e3o produzidos como \u00fanicos elementos nas tuplas emitidas (padr\u00e3o: <code>false</code>) checkIfExists Quando <code>true</code> lan\u00e7a uma exce\u00e7\u00e3o quando o caminho especificado n\u00e3o existe no sistema de arquivos (padr\u00e3o: <code>false</code>) <p>Exercise</p> <p>Use a f\u00e1brica de canal <code>fromFilePairs</code> para criar um canal emitindo todos os pares de leituras em fastq no diret\u00f3rio <code>data/ggal/</code> e imprima-os. Em seguida, use a op\u00e7\u00e3o <code>flat: true</code> e compare a sa\u00edda com a execu\u00e7\u00e3o anterior.</p> Solution <p>Use o seguinte, com ou sem <code>flat: true</code>:</p> <pre><code>Channel\n    .fromFilePairs('./data/ggal/*_{1,2}.fq', flat: true)\n    .view()\n</code></pre> <p>Em seguida, verifique os colchetes ao redor dos nomes dos arquivos, para ver a diferen\u00e7a com <code>flat</code>.</p>"},{"location":"pt/archive/basic_training/channels/#fromsra","title":"<code>fromSRA()</code>","text":"<p>A f\u00e1brica de canal <code>Channel.fromSRA</code> permite consultar o banco de dados NCBI SRA e retorna um canal que emite os arquivos FASTQ correspondentes aos crit\u00e9rios de sele\u00e7\u00e3o especificados.</p> <p>A consulta pode ser ID(s) de projeto(s) ou n\u00famero(s) de acesso suportado(s) pela API do NCBI ESearch.</p> <p>Info</p> <p>Esta fun\u00e7\u00e3o agora requer uma chave de API que voc\u00ea s\u00f3 pode obter fazendo login em sua conta NCBI.</p> Instru\u00e7\u00f5es para login do NCBI e aquisi\u00e7\u00e3o de chave <ol> <li>V\u00e1 para: https://www.ncbi.nlm.nih.gov/</li> <li>Clique no bot\u00e3o \"Login\" no canto superior direito para entrar no NCBI. Siga suas instru\u00e7\u00f5es.</li> <li>Uma vez em sua conta, clique no bot\u00e3o no canto superior direito, geralmente seu ID.</li> <li>V\u00e1 para Account settings</li> <li>Role para baixo at\u00e9 a se\u00e7\u00e3o \"API Key Management\".</li> <li>Clique em \"Create an API Key\".</li> <li>A p\u00e1gina ser\u00e1 atualizada e a chave ser\u00e1 exibida onde estava o bot\u00e3o. Copie sua chave.</li> </ol> <p>Por exemplo, o trecho a seguir imprimir\u00e1 o conte\u00fado de um ID de projeto NCBI:</p> <pre><code>params.ncbi_api_key = '&lt;Sua chave da API aqui&gt;'\n\nChannel\n    .fromSRA(['SRP073307'], apiKey: params.ncbi_api_key)\n    .view()\n</code></pre> <p> Substitua <code>&lt;Sua chave de API aqui&gt;</code> com sua chave de API.</p> <p>Isso deve imprimir:</p> <pre><code>[SRR3383346, [/vol1/fastq/SRR338/006/SRR3383346/SRR3383346_1.fastq.gz, /vol1/fastq/SRR338/006/SRR3383346/SRR3383346_2.fastq.gz]]\n[SRR3383347, [/vol1/fastq/SRR338/007/SRR3383347/SRR3383347_1.fastq.gz, /vol1/fastq/SRR338/007/SRR3383347/SRR3383347_2.fastq.gz]]\n[SRR3383344, [/vol1/fastq/SRR338/004/SRR3383344/SRR3383344_1.fastq.gz, /vol1/fastq/SRR338/004/SRR3383344/SRR3383344_2.fastq.gz]]\n[SRR3383345, [/vol1/fastq/SRR338/005/SRR3383345/SRR3383345_1.fastq.gz, /vol1/fastq/SRR338/005/SRR3383345/SRR3383345_2.fastq.gz]]\n// (o restante foi omitido)\n</code></pre> <p>V\u00e1rios IDs de acesso podem ser especificados usando um objeto lista:</p> <pre><code>ids = ['ERR908507', 'ERR908506', 'ERR908505']\nChannel\n    .fromSRA(ids, apiKey: params.ncbi_api_key)\n    .view()\n</code></pre> <pre><code>[ERR908507, [/vol1/fastq/ERR908/ERR908507/ERR908507_1.fastq.gz, /vol1/fastq/ERR908/ERR908507/ERR908507_2.fastq.gz]]\n[ERR908506, [/vol1/fastq/ERR908/ERR908506/ERR908506_1.fastq.gz, /vol1/fastq/ERR908/ERR908506/ERR908506_2.fastq.gz]]\n[ERR908505, [/vol1/fastq/ERR908/ERR908505/ERR908505_1.fastq.gz, /vol1/fastq/ERR908/ERR908505/ERR908505_2.fastq.gz]]\n</code></pre> <p>Info</p> <p>Os pares de leituras s\u00e3o gerenciados implicitamente e s\u00e3o retornados como uma lista de arquivos.</p> <p>\u00c9 f\u00e1cil usar este canal como uma entrada usando a sintaxe usual do Nextflow. O c\u00f3digo abaixo cria um canal contendo duas amostras de um estudo SRA p\u00fablico e executa o FASTQC nos arquivos resultantes. Veja:</p> <pre><code>params.ncbi_chave_api = '&lt;Sua chave de API aqui&gt;'\n\nparams.accession = ['ERR908507', 'ERR908506']\n\nprocess FASTQC {\n    input:\n    tuple val(id_amostra), path(arquivo_de_leituras)\n\n    output:\n    path(\"fastqc_${id_amostra}_logs\")\n\n    script:\n    \"\"\"\n    mkdir fastqc_${id_amostra}_logs\n    fastqc -o fastqc_${id_amostra}_logs -f fastq -q ${arquivo_de_leituras}\n    \"\"\"\n}\n\nworkflow {\n    leituras = Channel.fromSRA(params.accession, apiKey: params.ncbi_chave_api)\n    FASTQC(leituras)\n}\n</code></pre> <p>Se voc\u00ea deseja executar o fluxo de trabalho acima e n\u00e3o possui o fastqc instalado em sua m\u00e1quina, n\u00e3o esque\u00e7a o que aprendeu na se\u00e7\u00e3o anterior. Execute este fluxo de trabalho com <code>-with-docker biocontainers/fastqc:v0.11.5</code>, por exemplo.</p>"},{"location":"pt/archive/basic_training/channels/#arquivos-de-texto","title":"Arquivos de texto","text":"<p>O operador <code>splitText</code> permite dividir strings de v\u00e1rias linhas ou itens de arquivo de texto, emitidos por um canal de origem em blocos contendo n linhas, que ser\u00e3o emitidos pelo canal resultante. Veja:</p> <pre><code>Channel\n    .fromPath('data/meta/random.txt') // (1)!\n    .splitText() // (2)!\n    .view() // (3)!\n</code></pre> <ol> <li>Instrui o Nextflow a criar um canal a partir do caminho <code>data/meta/random.txt</code></li> <li>O operador <code>splitText</code> divide cada item em peda\u00e7os de uma linha por padr\u00e3o.</li> <li>Veja o conte\u00fado do canal.</li> </ol> <p>Voc\u00ea pode definir o n\u00famero de linhas em cada bloco usando o par\u00e2metro <code>by</code>, conforme mostrado no exemplo a seguir:</p> <pre><code>Channel\n    .fromPath('data/meta/random.txt')\n    .splitText(by: 2)\n    .subscribe {\n        print it;\n        print \"--- fim do bloco ---\\n\"\n    }\n</code></pre> <p>Info</p> <p>O operador <code>subscribe</code> permite a execu\u00e7\u00e3o de fun\u00e7\u00f5es definidas pelo usu\u00e1rio cada vez que um novo valor \u00e9 emitido pelo canal de origem.</p> <p>Uma clausura opcional pode ser especificada para transformar os blocos de texto produzidos pelo operador. O exemplo a seguir mostra como dividir arquivos de texto em blocos de 10 linhas e transform\u00e1-los em letras mai\u00fasculas:</p> <pre><code>Channel\n    .fromPath('data/meta/random.txt')\n    .splitText(by: 10) { it.toUpperCase() }\n    .view()\n</code></pre> <p>Voc\u00ea tamb\u00e9m pode fazer contagens para cada linha:</p> <pre><code>contador = 0\n\nChannel\n    .fromPath('data/meta/random.txt')\n    .splitText()\n    .view { \"${contador++}: ${it.toUpperCase().trim()}\" }\n</code></pre> <p>Por fim, voc\u00ea tamb\u00e9m pode usar o operador em arquivos simples (fora do contexto do canal):</p> <pre><code>def f = file('data/meta/random.txt')\ndef linhas = f.splitText()\ndef contador = 0\nfor (String linha : linhas) {\n    log.info \"${contador++} ${linha.toUpperCase()}\"\n}\n</code></pre>"},{"location":"pt/archive/basic_training/channels/#valores-separados-por-virgula-csv","title":"Valores separados por v\u00edrgula (.csv)","text":"<p>O operador <code>splitCsv</code> permite analisar itens de texto formatados em CSV (Comma-separated value) emitidos por um canal.</p> <p>Em seguida, ele os divide em registros ou os agrupa como uma lista de registros com um comprimento especificado.</p> <p>No caso mais simples, basta aplicar o operador <code>splitCsv</code> a um canal que emite arquivos de texto ou entradas de texto no formato CSV. Por exemplo, para visualizar apenas a primeira e a quarta colunas:</p> <pre><code>Channel\n    .fromPath(\"data/meta/patients_1.csv\")\n    .splitCsv()\n    // linha \u00e9 um objeto de lista\n    .view { linha -&gt; \"${linha[0]}, ${linha[3]}\" }\n</code></pre> <p>Quando o CSV come\u00e7a com uma linha de cabe\u00e7alho definindo os nomes das colunas, voc\u00ea pode especificar o par\u00e2metro <code>header: true</code> que permite referenciar cada valor pelo nome da coluna, conforme mostrado no exemplo a seguir:</p> <pre><code>Channel\n    .fromPath(\"data/meta/patients_1.csv\")\n    .splitCsv(header: true)\n    // linha \u00e9 um objeto de lista\n    .view { linha -&gt; \"${linha.patient_id}, ${linha.num_samples}\" }\n</code></pre> <p>Como alternativa, voc\u00ea pode fornecer nomes de cabe\u00e7alho personalizados especificando uma lista de strings no par\u00e2metro de cabe\u00e7alho, conforme mostrado abaixo:</p> <pre><code>Channel\n    .fromPath(\"data/meta/patients_1.csv\")\n    .splitCsv(header: ['col1', 'col2', 'col3', 'col4', 'col5'])\n    // linha \u00e9 um objeto de lista\n    .view { linha -&gt; \"${linha.col1}, ${linha.col4}\" }\n</code></pre> <p>Voc\u00ea tamb\u00e9m pode processar v\u00e1rios arquivos CSV ao mesmo tempo:</p> <pre><code>Channel\n    .fromPath(\"data/meta/patients_*.csv\") // &lt;-- use um padr\u00e3o de captura\n    .splitCsv(header: true)\n    .view { linha -&gt; \"${linha.patient_id}\\t${linha.num_samples}\" }\n</code></pre> <p>Tip</p> <p>Observe que voc\u00ea pode alterar o formato de sa\u00edda simplesmente adicionando um delimitador diferente.</p> <p>Por fim, voc\u00ea tamb\u00e9m pode operar em arquivos CSV fora do contexto do canal:</p> <pre><code>def f = file('data/meta/patients_1.csv')\ndef linhas = f.splitCsv()\nfor (List linha : linhas) {\n    log.info \"${linha[0]} -- ${linha[2]}\"\n}\n</code></pre> <p>Exercise</p> <p>Tente inserir leituras fastq no fluxo de trabalho do RNA-Seq anterior usando <code>.splitCsv</code>.</p> Solution <p>Adicione um arquivo de texto CSV contendo o seguinte, como uma entrada de exemplo com o nome \"fastq.csv\":</p> <pre><code>gut,/workspaces/training/nf-training/data/ggal/gut_1.fq,/workspaces/training/nf-training/data/ggal/gut_2.fq\n</code></pre> <p>Em seguida, substitua o canal de entrada para as leituras em <code>script7.nf</code>, alterando as seguintes linhas:</p> <pre><code>Channel\n    .fromFilePairs(params.reads, checkIfExists: true)\n    .set { read_pairs_ch }\n</code></pre> <p>Para uma entrada de f\u00e1brica de canal splitCsv:</p> <pre><code>Channel\n    .fromPath(\"fastq.csv\")\n    .splitCsv()\n    .view { linha -&gt; \"${linha[0]}, ${linha[1]}, ${linha[2]}\" }\n    .set { read_pairs_ch }\n</code></pre> <p>Por fim, altere a cardinalidade dos processos que usam os dados de entrada. Por exemplo, para o processo de quantifica\u00e7\u00e3o, mude de:</p> <pre><code>process QUANTIFICATION {\n    tag \"$sample_id\"\n\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path sample_id, emit: quant_ch\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\n</code></pre> <p>Para:</p> <pre><code>process QUANTIFICATION {\n    tag \"$sample_id\"\n\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads1), path(reads2)\n\n    output:\n    path sample_id, emit: quant_ch\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U -i $salmon_index -1 ${reads1} -2 ${reads2} -o $sample_id\n    \"\"\"\n}\n</code></pre> <p>Repita o procedimento acima para a etapa fastqc.</p> <pre><code>process FASTQC {\n    tag \"FASTQC on $sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads1), path(reads2)\n\n    output:\n    path \"fastqc_${sample_id}_logs\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads1} ${reads2}\n    \"\"\"\n}\n</code></pre> <p>Agora o fluxo de trabalho deve ser executado a partir de um arquivo CSV.</p>"},{"location":"pt/archive/basic_training/channels/#valores-separados-por-tabulacao-tsv","title":"Valores separados por tabula\u00e7\u00e3o (.tsv)","text":"<p>A an\u00e1lise de arquivos TSV funciona de maneira semelhante, basta adicionar a op\u00e7\u00e3o <code>sep: '\\t'</code> no contexto do <code>splitCsv</code>:</p> <pre><code>Channel\n    .fromPath(\"data/meta/regions.tsv\", checkIfExists: true)\n    // Use a op\u00e7\u00e3o `sep` para analisar arquivos com tabula\u00e7\u00e3o como separador\n    .splitCsv(sep: '\\t')\n    .view()\n</code></pre> <p>Exercise</p> <p>Tente usar a t\u00e9cnica de separa\u00e7\u00e3o por tabula\u00e7\u00e3o no arquivo <code>data/meta/regions.tsv</code>, mas imprima apenas a primeira coluna e remova o cabe\u00e7alho.</p> Solution <pre><code>Channel\n    .fromPath(\"data/meta/regions.tsv\", checkIfExists: true)\n    // Use a op\u00e7\u00e3o `sep` para analisar arquivos com tabula\u00e7\u00e3o como separador\n    .splitCsv(sep: '\\t', header: true)\n    // linha \u00e9 um objeto de lista\n    .view { linha -&gt; \"${linha.patient_id}\" }\n</code></pre>"},{"location":"pt/archive/basic_training/channels/#formatos-de-arquivo-mais-complexos","title":"Formatos de arquivo mais complexos","text":""},{"location":"pt/archive/basic_training/channels/#json","title":"JSON","text":"<p>Tamb\u00e9m podemos analisar facilmente o formato de arquivo JSON usando o oeprador de canal <code>splitJson</code>.</p> <p>O operador <code>splitJson</code> suporta arranjos JSON:</p> C\u00f3digo-fonteSa\u00edda <pre><code>Channel\n    .of('[\"Domingo\", \"Segunda\", \"Ter\u00e7a\", \"Quarta\", \"Quinta\", \"Sexta\", \"S\u00e1bado\"]')\n    .splitJson()\n    .view { \"Item: ${it}\" }\n</code></pre> <pre><code>Item: Domingo\nItem: Segunda\nItem: Ter\u00e7a\nItem: Quarta\nItem: Quinta\nItem: Sexta\nItem: S\u00e1bado\n</code></pre> <p>Objetos JSON:</p> C\u00f3digo-fonteSa\u00edda <pre><code>Channel\n    .of('{\"jogador\": {\"nome\": \"Bob\", \"altura\": 180, \"venceu_campeonato\": false}}')\n    .splitJson()\n    .view { \"Item: ${it}\" }\n</code></pre> <pre><code>Item: [key:jogador, value:[nome:Bob, altura:180, venceu_campeonato:false]]\n</code></pre> <p>E inclusive arranjos JSON com objetos JSON!</p> C\u00f3digo-fonteSa\u00edda <pre><code>Channel\n    .of('[{\"nome\": \"Bob\", \"altura\": 180, \"venceu_campeonato\": false}, \\\n        {\"nome\": \"Alice\", \"height\": 170, \"venceu_campeonato\": false}]')\n    .splitJson()\n    .view { \"Item: ${it}\" }\n</code></pre> <pre><code>Item: [nome:Bob, altura:180, venceu_campeonato:false]\nItem: [nome:Alice, altura:170, venceu_campeonato:false]\n</code></pre> <p>Arquivos contendo dados em formato JSON tamb\u00e9m podem ser analisados:</p> C\u00f3digo-fontearquivo.jsonSa\u00edda <pre><code>Channel\n    .fromPath('arquivo.json')\n    .splitJson()\n    .view { \"Item: ${it}\" }\n</code></pre> <pre><code>[{\"nome\": \"Bob\", \"altura\": 180, \"venceu_campeonato\": false}, {\"nome\": \"Alice\", \"altura\": 170, \"venceu_campeonato\": false}]\n</code></pre> <pre><code>Item: [nome:Bob, altura:180, venceu_campeonato:false]\nItem: [nome:Alice, altura:170, venceu_campeonato:false]\n</code></pre>"},{"location":"pt/archive/basic_training/channels/#yaml","title":"YAML","text":"<p>Isso tamb\u00e9m pode ser usado como uma forma de analisar arquivos YAML:</p> C\u00f3digo-fontedata/meta/regions.ymlSa\u00edda <pre><code>import org.yaml.snakeyaml.Yaml\n\ndef f = file('data/meta/regions.yml')\ndef registros = new Yaml().load(f)\n\nfor (def entrada : registros) {\n    log.info \"$entrada.patient_id -- $entrada.feature\"\n}\n</code></pre> <pre><code>- patient_id: ATX-TBL-001-GB-01-105\n  region_id: R1\n  feature: pass_vafqc_flag\n  pass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\n  region_id: R1\n  feature: pass_stripy_flag\n  pass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\n  region_id: R1\n  feature: pass_manual_flag\n  pass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\n  region_id: R1\n  feature: other_region_selection_flag\n  pass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\n  region_id: R1\n  feature: ace_information_gained\n  pass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\n  region_id: R1\n  feature: concordance_flag\n  pass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\n  region_id: R2\n  feature: pass_vafqc_flag\n  pass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\n  region_id: R2\n  feature: pass_stripy_flag\n  pass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\n  region_id: R2\n  feature: pass_manual_flag\n  pass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\n  region_id: R2\n  feature: other_region_selection_flag\n  pass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\n  region_id: R2\n  feature: ace_information_gained\n  pass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\n  region_id: R2\n  feature: concordance_flag\n  pass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\n  region_id: R3\n  feature: pass_vafqc_flag\n  pass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\n  region_id: R3\n  feature: pass_stripy_flag\n  pass_flag: \"FALSE\"\n</code></pre> <pre><code>ATX-TBL-001-GB-01-105 -- pass_vafqc_flag\nATX-TBL-001-GB-01-105 -- pass_stripy_flag\nATX-TBL-001-GB-01-105 -- pass_manual_flag\nATX-TBL-001-GB-01-105 -- other_region_selection_flag\nATX-TBL-001-GB-01-105 -- ace_information_gained\nATX-TBL-001-GB-01-105 -- concordance_flag\nATX-TBL-001-GB-01-105 -- pass_vafqc_flag\nATX-TBL-001-GB-01-105 -- pass_stripy_flag\nATX-TBL-001-GB-01-105 -- pass_manual_flag\nATX-TBL-001-GB-01-105 -- other_region_selection_flag\nATX-TBL-001-GB-01-105 -- ace_information_gained\nATX-TBL-001-GB-01-105 -- concordance_flag\nATX-TBL-001-GB-01-105 -- pass_vafqc_flag\nATX-TBL-001-GB-01-105 -- pass_stripy_flag\n</code></pre>"},{"location":"pt/archive/basic_training/channels/#armazenamento-em-modulos-de-analisadores-sintaticos","title":"Armazenamento em m\u00f3dulos de analisadores sint\u00e1ticos","text":"<p>A melhor maneira de armazenar scripts com analisadores \u00e9 mant\u00ea-los em um arquivo de m\u00f3dulo Nextflow.</p> <p>Digamos que n\u00e3o temos um operador de canal JSON, mas criamos uma fun\u00e7\u00e3o para isso. O arquivo <code>parsers.nf</code> deve conter a fun\u00e7\u00e3o <code>parseArquivoJson</code>. Veja o conte\u00fado abaixo:</p> C\u00f3digo-fonte./modules/parsers.nfSa\u00edda <pre><code>include { parseArquivoJson } from './modules/parsers.nf'\n\nprocess FOO {\n    input:\n    tuple val(id_paciente), val(caracteristica)\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo $id_paciente tem $caracteristica como coluna\n    \"\"\"\n}\n\nworkflow {\n    Channel\n        .fromPath('data/meta/regions*.json')\n        | flatMap { parseArquivoJson(it) }\n        | map { registro -&gt; [registro.patient_id, registro.feature] }\n        | unique\n        | FOO\n        | view\n}\n</code></pre> <pre><code>import groovy.json.JsonSlurper\n\ndef parseArquivoJson(arquivo_json) {\n    def f = file(arquivo_json)\n    def registros = new JsonSlurper().parse(f)\n    return registros\n}\n</code></pre> <pre><code>ATX-TBL-001-GB-01-105 tem pass_stripy_flag como coluna\n\nATX-TBL-001-GB-01-105 tem ace_information_gained como coluna\n\nATX-TBL-001-GB-01-105 tem concordance_flag como coluna\n\nATX-TBL-001-GB-01-105 tem pass_vafqc_flag como coluna\n\nATX-TBL-001-GB-01-105 tem pass_manual_flag como coluna\n\nATX-TBL-001-GB-01-105 tem other_region_selection_flag como coluna\n</code></pre> <p>O Nextflow usar\u00e1 isso como uma fun\u00e7\u00e3o personalizada dentro do escopo <code>workflow</code>.</p> <p>Tip</p> <p>Voc\u00ea aprender\u00e1 mais sobre arquivos de m\u00f3dulo posteriormente na se\u00e7\u00e3o de Modulariza\u00e7\u00e3o desse tutorial.</p>"},{"location":"pt/archive/basic_training/config/","title":"Configura\u00e7\u00e3o","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/archive/basic_training/config/#configuracao-do-nextflow","title":"Configura\u00e7\u00e3o do Nextflow","text":"<p>Um recurso importante do Nextflow \u00e9 a capacidade de desacoplar a implementa\u00e7\u00e3o do fluxo de trabalho e as configura\u00e7\u00f5es exigidas pela plataforma onde ele ser\u00e1 executado.</p> <p>Isso permite a portabilidade da aplica\u00e7\u00e3o sem a necessidade de modificar seu c\u00f3digo quando se faz necess\u00e1ria a execu\u00e7\u00e3o em diferentes ambientes.</p>"},{"location":"pt/archive/basic_training/config/#arquivo-de-configuracao","title":"Arquivo de configura\u00e7\u00e3o","text":"<p>Quando um script de fluxo de trabalho \u00e9 executado, o Nextflow procura um arquivo chamado <code>nextflow.config</code> no diret\u00f3rio atual e no diret\u00f3rio base do script (se n\u00e3o for o mesmo que o diret\u00f3rio atual). Caso n\u00e3o encontre, o Nextflow verifica o arquivo: <code>$HOME/.nextflow/config</code>.</p> <p>Quando existir mais de um dos arquivos acima, eles ser\u00e3o mesclados, de modo que as configura\u00e7\u00f5es do primeiro substituam as mesmas configura\u00e7\u00f5es que podem aparecer no segundo e assim por diante.</p> <p>O mecanismo de pesquisa padr\u00e3o do arquivo de configura\u00e7\u00e3o pode ser estendido fornecendo um arquivo de configura\u00e7\u00e3o extra usando a op\u00e7\u00e3o de linha de comando: <code>-c &lt;arquivo de configura\u00e7\u00e3o&gt;</code>.</p>"},{"location":"pt/archive/basic_training/config/#sintaxe-do-arquivo-de-configuracao","title":"Sintaxe do arquivo de configura\u00e7\u00e3o","text":"<p>Um arquivo de configura\u00e7\u00e3o do Nextflow \u00e9 um arquivo de texto simples contendo um conjunto de propriedades definidas usando a sintaxe:</p> <pre><code>nome = valor\n</code></pre> <p>Info</p> <p>Observe que strings precisam ser colocadas entre aspas, enquanto n\u00fameros e valores booleanos (<code>true</code>, <code>false</code>) n\u00e3o. Al\u00e9m disso, observe que os valores s\u00e3o tipados, o que significa, por exemplo, que <code>1</code> \u00e9 diferente de <code>'1'</code>, pois o primeiro \u00e9 interpretado como o n\u00famero um, enquanto o \u00faltimo \u00e9 interpretado como uma string.</p>"},{"location":"pt/archive/basic_training/config/#variaveis-de-configuracao","title":"Vari\u00e1veis de configura\u00e7\u00e3o","text":"<p>As propriedades de configura\u00e7\u00e3o podem ser usadas como vari\u00e1veis no pr\u00f3prio arquivo de configura\u00e7\u00e3o, usando a sintaxe usual <code>$nomePropriedade</code> ou <code>${expressao}</code>.</p> <pre><code>propriedadeUm = 'mundo'\numaOutraPropriedade = \"Ol\u00e1 $propriedadeUm\"\ncaminhoCustomizado = \"$PATH:/pasta/da/minha/app\"\n</code></pre> <p>Tip</p> <p>No arquivo de configura\u00e7\u00e3o \u00e9 poss\u00edvel acessar qualquer vari\u00e1vel definida no ambiente de execu\u00e7\u00e3o, como <code>$PATH</code>, <code>$HOME</code>, <code>$PWD</code>, etc.</p>"},{"location":"pt/archive/basic_training/config/#comentarios-no-arquivo-de-configuracao","title":"Coment\u00e1rios no arquivo de configura\u00e7\u00e3o","text":"<p>Os arquivos de configura\u00e7\u00e3o usam as mesmas conven\u00e7\u00f5es para coment\u00e1rios usados no script Nextflow:</p> <pre><code>// comentar uma \u00fanica linha\n\n/*\n    um coment\u00e1rio abrangendo\n    v\u00e1rias linhas\n  */\n</code></pre>"},{"location":"pt/archive/basic_training/config/#escopos-de-configuracao","title":"Escopos de configura\u00e7\u00e3o","text":"<p>As defini\u00e7\u00f5es de configura\u00e7\u00e3o podem ser organizadas em diferentes escopos. Pode-se utilizar a nota\u00e7\u00e3o <code>escopo.propriedade</code> ou agrupar as propriedades no mesmo escopo usando a nota\u00e7\u00e3o de chaves, como mostrado a seguir:</p> <pre><code>alfa.x = 1\nalfa.y = 'valor da string..'\n\nbeta {\n    p = 2\n    q = 'outra string ..'\n}\n</code></pre>"},{"location":"pt/archive/basic_training/config/#configurar-parametros","title":"Configurar par\u00e2metros","text":"<p>O escopo <code>params</code> permite a defini\u00e7\u00e3o de par\u00e2metros que substituem os valores definidos no script principal do fluxo de trabalho.</p> <p>Isso \u00e9 \u00fatil para refor\u00e7ar o uso de um ou mais par\u00e2metros de execu\u00e7\u00e3o em um arquivo separado.</p> Arquivo de configura\u00e7\u00e3o<pre><code>params.foo = 'Bonjour'\nparams.bar = 'le monde!'\n</code></pre> Script do fluxo de trabalho<pre><code>params.foo = 'Ol\u00e1'\nparams.bar = 'mundo!'\n\n// imprime ambos os par\u00e2metros\nprintln \"$params.foo $params.bar\"\n</code></pre> <p>Exercise</p> <p>Salve o primeiro trecho como <code>nextflow.config</code> e o segundo como <code>params.nf</code>. Em seguida, execute:</p> <pre><code>nextflow run params.nf\n</code></pre> Solution <pre><code>Bonjour le monde!\n</code></pre> <p>Execute novamente o comando anterior especificando o par\u00e2metro <code>foo</code> na linha de comando:</p> <pre><code>nextflow run params.nf --foo Ol\u00e1\n</code></pre> Solution <pre><code>Ol\u00e1 le monde!\n</code></pre> <p>Compare o resultado das duas execu\u00e7\u00f5es.</p>"},{"location":"pt/archive/basic_training/config/#configurar-ambiente","title":"Configurar ambiente","text":"<p>O escopo <code>env</code> permite a defini\u00e7\u00e3o de uma ou mais vari\u00e1veis que ser\u00e3o exportadas para o ambiente onde ser\u00e3o executadas as tarefas do fluxo de trabalho.</p> <pre><code>env.ALFA = 'algum valor'\nenv.BETA = \"$HOME/algum/caminho\"\n</code></pre> <p>Salve o trecho acima como um arquivo chamado <code>meu-env.config</code>. Em seguida, salve o trecho abaixo em um arquivo chamado <code>foo.nf</code>:</p> <pre><code>process FOO {\n    debug true\n\n    script:\n    '''\n    env | egrep 'ALFA|BETA'\n    '''\n}\n\nworkflow {\n    FOO()\n}\n</code></pre> <p>Por fim, execute o seguinte comando:</p> <pre><code>nextflow run foo.nf -c meu-env.config\n</code></pre> Solution <pre><code>BETA=/home/usuario/algum/caminho\nALFA=algum valor\n</code></pre>"},{"location":"pt/archive/basic_training/config/#configurar-processos","title":"Configurar processos","text":"<p>As diretivas de processos permite a especifica\u00e7\u00e3o de configura\u00e7\u00f5es para a execu\u00e7\u00e3o de uma tarefa, como <code>cpus</code>, <code>memory</code>, <code>container</code>, al\u00e9m de outros recursos, no script do fluxo de trabalho.</p> <p>Isso \u00e9 \u00fatil ao criarmos um exemplo-teste ou um prot\u00f3tipo para o nosso fluxo de trabalho.</p> <p>No entanto, \u00e9 sempre uma boa pr\u00e1tica desassociar a l\u00f3gica de execu\u00e7\u00e3o do fluxo de trabalho das configura\u00e7\u00f5es que ser\u00e3o utilizadas por ele. Assim, \u00e9 altamente recomend\u00e1vel definir as configura\u00e7\u00f5es do processo no arquivo de configura\u00e7\u00e3o do fluxo de trabalho em vez de no script do fluxo de trabalho.</p> <p>Al\u00e9m disso, quaisquer diretivas do escopo dos processos (<code>process</code>) podem ser usadas no arquivo de configura\u00e7\u00e3o.</p> <pre><code>process {\n    cpus = 10\n    memory = 8.GB\n    container = 'biocontainers/bamtools:v2.4.0_cv3'\n}\n</code></pre> <p>O trecho de configura\u00e7\u00e3o acima define as diretivas <code>cpus</code>, <code>memory</code> e <code>container</code> para todos os processos em seu fluxo de trabalho.</p> <p>O seletor de processo pode ser usado para aplicar a configura\u00e7\u00e3o a um processo espec\u00edfico ou grupo de processos (discutido posteriormente).</p> <p>Info</p> <p>As unidades de mem\u00f3ria e a dura\u00e7\u00e3o de tempo podem ser especificadas usando uma nota\u00e7\u00e3o baseada em string na qual o(s) d\u00edgito(s) e a unidade podem ser separados por um espa\u00e7o em branco. Tamb\u00e9m pode-se usar a nota\u00e7\u00e3o num\u00e9rica na qual o(s) d\u00edgito(s) e a(s) unidade(s) s\u00e3o separados por um ponto e n\u00e3o s\u00e3o colocados entre aspas.</p> Sintaxe de string Sintaxe num\u00e9rica Valor <code>'10 KB'</code> <code>10.KB</code> 10240 bytes <code>'500 MB'</code> <code>500.MB</code> 524288000 bytes <code>'1 min'</code> <code>1.min</code> 60 segundos <code>'1 hour 25 sec'</code> - 1 hora e 25 segundos <p>A sintaxe para definir as diretivas de processo no arquivo de configura\u00e7\u00e3o requer <code>=</code> (ou seja, operador de atribui\u00e7\u00e3o). Esta nota\u00e7\u00e3o n\u00e3o deve ser usada para definir as diretivas do processo no script do fluxo de trabalho.</p> Example <pre><code>process FOO {\n    cpus 4\n    memory 2.GB\n    time 1.hour\n    maxRetries 3\n\n    script:\n    \"\"\"\n    seu_comando --cpus $task.cpus --mem $task.memory\n    \"\"\"\n}\n</code></pre> <p>Isso \u00e9 especialmente importante quando voc\u00ea deseja criar uma defini\u00e7\u00e3o de configura\u00e7\u00e3o usando uma express\u00e3o din\u00e2mica com uma clausura. Por exemplo, em um arquivo de fluxo de trabalho:</p> <pre><code>process FOO {\n    memory { 4.GB * task.cpus }\n}\n</code></pre> <p>E o equivalente em um arquivo de configura\u00e7\u00e3o, se voc\u00ea preferir configurar isso l\u00e1:</p> <pre><code>process {\n    withName: FOO {\n        memory = { 4.GB * task.cpus }\n    }\n}\n</code></pre> <p>Diretivas que exigem mais de um valor, como a diretiva pod, precisam ser expressas como um objeto Map no arquivo de configura\u00e7\u00e3o.</p> <pre><code>process {\n    pod = [ambiente: 'FOO', valor: '123']\n}\n</code></pre> <p>Por fim, as diretivas que devem ser repetidas na defini\u00e7\u00e3o do processo e nos arquivos de configura\u00e7\u00e3o precisam ser definidas como um objeto List. Por exemplo:</p> <pre><code>process {\n    pod = [[ambiente: 'FOO', valor: '123'],\n           [ambiente: 'BAR', valor: '456']]\n}\n</code></pre>"},{"location":"pt/archive/basic_training/config/#configurar-execucao-do-docker","title":"Configurar execu\u00e7\u00e3o do Docker","text":"<p>A imagem do cont\u00eainer a ser utilizada para a execu\u00e7\u00e3o do processo pode ser especificada no arquivo <code>nextflow.config</code>:</p> <pre><code>process.container = 'nextflow/rnaseq-nf'\ndocker.enabled = true\n</code></pre> <p>O uso de IDs \u00fanicos com \"SHA256\" para imagens Docker garante que o conte\u00fado da imagem n\u00e3o mude com o tempo, por exemplo:</p> <pre><code>process.container = 'nextflow/rnaseq-nf@sha256:aeacbd7ea1154f263cda972a96920fb228b2033544c2641476350b9317dab266'\ndocker.enabled = true\n</code></pre>"},{"location":"pt/archive/basic_training/config/#configurar-execucao-do-singularity","title":"Configurar execu\u00e7\u00e3o do Singularity","text":"<p>Para rodar um fluxo de trabalho com Singularity, \u00e9 necess\u00e1rio fornecer o caminho para o arquivo de imagem do cont\u00eainer usando a diretiva de cont\u00eainer (<code>container</code>):</p> <pre><code>process.container = '/alguma/imagem/singularity/imagem.sif'\nsingularity.enabled = true\n</code></pre> <p>Info</p> <p>O arquivo de imagem do cont\u00eainer deve ser um caminho absoluto: deve come\u00e7ar com <code>/</code>.</p> <p>Os seguintes protocolos s\u00e3o suportados:</p> <ul> <li><code>library://</code> baixe a imagem do cont\u00eainer do Singularity Library service.</li> <li><code>shub://</code> baixe a imagem do cont\u00eainer do Singularity Hub.</li> <li><code>docker://</code> baixe a imagem do cont\u00eainer do Docker Hub e a converta para o formato Singularity.</li> <li><code>docker-daemon://</code> extraia a imagem do cont\u00eainer de uma instala\u00e7\u00e3o local do Docker e a converta em um arquivo de imagem Singularity.</li> </ul> <p>Tip</p> <p>O hub do Singularity <code>shub://</code> n\u00e3o est\u00e1 mais dispon\u00edvel como servi\u00e7o de constru\u00e7\u00e3o de cont\u00eaineres. Entretanto, as imagens existentes anteriores a 19 de abril de 2021 ainda funcionam.</p> <p>Tip</p> <p>Ao especificar um nome simples de imagem de cont\u00eainer do Docker, o Nextflow faz o download e a converte implicitamente em uma imagem do Singularity quando a execu\u00e7\u00e3o do Singularity est\u00e1 habilitada.</p> <pre><code>process.container = 'nextflow/rnaseq-nf'\nsingularity.enabled = true\n</code></pre> <p>A configura\u00e7\u00e3o acima instrui o Nextflow a usar o mecanismo Singularity para executar seus processos. O cont\u00eainer \u00e9 extra\u00eddo do reposit\u00f3rio de imagens do Docker e armazenado em cache no diret\u00f3rio atual para ser usado em outras execu\u00e7\u00f5es.</p> <p>Como alternativa, se voc\u00ea tiver uma imagem Singularity, seu caminho absoluto pode ser especificado por meio do nome do cont\u00eainer usando a op\u00e7\u00e3o <code>-with-singularity</code> ou a configura\u00e7\u00e3o <code>process.container</code> no arquivo de configura\u00e7\u00e3o.</p> <p>Exercise</p> <p>Tente executar o script conforme mostrado abaixo, alterando o arquivo <code>nextflow.config</code> para ser usado com o Singularity:</p> <pre><code>nextflow run script7.nf\n</code></pre> <p>Note</p> <p>O Nextflow ir\u00e1 baixar a imagem do cont\u00eainer automaticamente. Isto levar\u00e1 alguns segundos, dependendo da velocidade da sua conex\u00e3o.</p>"},{"location":"pt/archive/basic_training/config/#configurar-execucao-com-ambientes-conda","title":"Configurar execu\u00e7\u00e3o com ambientes Conda","text":"<p>Ambientes Conda tamb\u00e9m podem ser fornecidos no arquivo de configura\u00e7\u00e3o. Basta adicionar a seguinte configura\u00e7\u00e3o no arquivo <code>nextflow.config</code>:</p> <pre><code>process.conda = \"/home/ubuntu/miniconda2/envs/nf-tutorial\"\n</code></pre> <p>Voc\u00ea pode especificar o caminho de um ambiente Conda existente como diret\u00f3rio ou o caminho do arquivo YAML do ambiente Conda.</p>"},{"location":"pt/archive/basic_training/containers/","title":"Containers","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/archive/basic_training/containers/#gerencie-dependencias-e-conteineres","title":"Gerencie depend\u00eancias e cont\u00eaineres","text":"<p>Fluxos de trabalhos computacionais s\u00e3o raramente compostos de um s\u00f3 script ou ferramenta. Muitas vezes, eles dependem de d\u00fazias de componentes de softwares ou bibliotecas.</p> <p>Instalar e manter tais depend\u00eancias \u00e9 uma tarefa desafiadora e uma fonte comum de irreprodutibilidade em aplica\u00e7\u00f5es cient\u00edficas.</p> <p>Para superar esses problemas, n\u00f3s utilizamos cont\u00eaineres que habilitam essas depend\u00eancias de softwares, isto \u00e9 ferramentas e bibliotecas necess\u00e1rias para uma an\u00e1lise de dados, para estar encapsuladas em uma ou mais imagens de cont\u00eainer Linux independentes, prontas para serem executadas e imut\u00e1veis. Elas podem facilmente ser implementadas em qualquer plataforma que suporta o motor de conteineriza\u00e7\u00e3o.</p> <p>Cont\u00eaineres podem ser executados de uma forma isolada pelo sistema do hospedeiro. Elas tem sua pr\u00f3pria c\u00f3pia do sistema de arquivos, espa\u00e7o de processamento, gerenciamento de mem\u00f3ria, etc.</p> <p>Info</p> <p>Cont\u00eaineres foram introduzidos no kernel 2.6 como um recurso do Linux conhecido como Control Groups ou Cgroups.</p>"},{"location":"pt/archive/basic_training/containers/#docker","title":"Docker","text":"<p>Docker \u00e9 uma ferramenta \u00fatil para o gerenciamento, execu\u00e7\u00e3o e compartilhamento de imagens de cont\u00eaineres.</p> <p>Essas imagens can podem ser carregadas e publicadas em um reposit\u00f3rio centralizado conhecido como Docker Hub, ou hospedadas por terceiros como o Quay.</p>"},{"location":"pt/archive/basic_training/containers/#execute-um-conteiner","title":"Execute um cont\u00eainer","text":"<p>Um cont\u00eainer pode ser executado com o seguinte comando:</p> <pre><code>docker run &lt;nome-do-cont\u00eainer&gt;\n</code></pre> <p>Tente executar o seguinte cont\u00eainer p\u00fablico (se voc\u00ea tiver o Docker instalado), por exemplo:</p> <pre><code>docker run hello-world\n</code></pre>"},{"location":"pt/archive/basic_training/containers/#baixe-um-conteiner","title":"Baixe um cont\u00eainer","text":"<p>O comando pull possibilita que voc\u00ea baixe uma imagem Docker sem que a execute. Por exemplo:</p> <pre><code>docker pull debian:bullseye-slim\n</code></pre> <p>O comando acima baixa uma imagem Debian Linux. Voc\u00ea pode checar se ela existe usando:</p> <pre><code>docker images\n</code></pre>"},{"location":"pt/archive/basic_training/containers/#execute-um-conteiner-em-mode-interativo","title":"Execute um cont\u00eainer em mode interativo","text":"<p>Iniciar uma shell BASH em um cont\u00eainer permite que voc\u00ea opere em modo interativo no sistema operacional conteinerizado. Por exemplo:</p> <pre><code>docker run -it debian:bullseye-slim bash\n</code></pre> <p>Uma vez iniciado, voc\u00ea vai notar que est\u00e1 como root (!). Use os comandos usuais para navegar pelo sistema de arquivos. Isso \u00e9 \u00fatil para checar se os programas necess\u00e1rios est\u00e3o presentes no cont\u00eainer.</p> <p>Para sair do cont\u00eainer, pare a sess\u00e3o BASH com o comando <code>exit</code>.</p>"},{"location":"pt/archive/basic_training/containers/#seu-primeiro-dockerfile","title":"Seu primeiro Dockerfile","text":"<p>Imagens Docker s\u00e3o criadas utilizando um arquivo chamado <code>Dockerfile</code>, que \u00e9 um simples arquivo de texto contendo uma lista de comandos para montar e configurar uma imagem com os pacotes de programas necess\u00e1rios.</p> <p>Aqui, voc\u00ea criar\u00e1 uma imagem Docker contendo o cowsay e a ferramenta Salmon</p> <p>Warning</p> <p>O processo de montagem do Docker automaticamente copia todos os arquivos que est\u00e3o no diret\u00f3rio atual para o Docker daemon para que ele possa criar a imagem. Isso pode custar muito tempo quando existem v\u00e1rios ou grandes arquivos. Por essa raz\u00e3o, \u00e9 importante que sempre se trabalhe em um diret\u00f3rio contendo apenas os arquivos que voc\u00ea realmente precisa incluir em sua imagem Docker. Alternativamente, voc\u00ea pode usar o arquivo <code>.dockerignore</code> para selecionar os aquivos que ser\u00e3o exclu\u00eddos da montagem.</p> <p>Use seu editor favorito (ex.: <code>vim</code> ou <code>nano</code>) para criar um arquivo chamado <code>Dockerfile</code> e copiar o seguinte conte\u00fado:</p> <pre><code>FROM debian:bullseye-slim\n\nLABEL image.author.name \"Seu nome aqui\"\nLABEL image.author.email \"seu@email.aqui\"\n\nRUN apt-get update &amp;&amp; apt-get install -y curl cowsay\n\nENV PATH=$PATH:/usr/games/\n</code></pre>"},{"location":"pt/archive/basic_training/containers/#monte-a-imagem","title":"Monte a imagem","text":"<p>Monte a imagem do Docker com base no Dockerfile utilizando o seguinte comando:</p> <pre><code>docker build -t minha-imagem .\n</code></pre> <p>Onde \"minha-imagem\" \u00e9 o nome que o usu\u00e1rio especificou para a imagem que ser\u00e1 criada.</p> <p>Tip</p> <p>N\u00e3o esque\u00e7a do ponto no comando acima.</p> <p>Quando completo, verifique se a imagem foi criada listando todas imagens dispon\u00edveis:</p> <pre><code>docker images\n</code></pre> <p>Voc\u00ea pode testar seu novo cont\u00eainer executando esse comando:</p> <pre><code>docker run minha-imagem cowsay Ol\u00e1 Docker!\n</code></pre>"},{"location":"pt/archive/basic_training/containers/#adicione-um-pacote-de-programa-a-imagem","title":"Adicione um pacote de programa a imagem","text":"<p>Adicione o pacote Salmon para a imagem Docker adicionando o seguinte trecho para o <code>Dockerfile</code>:</p> <pre><code>RUN curl -sSL https://github.com/COMBINE-lab/salmon/releases/download/v1.5.2/salmon-1.5.2_linux_x86_64.tar.gz | tar xz \\\n&amp;&amp; mv /salmon-*/bin/* /usr/bin/ \\\n&amp;&amp; mv /salmon-*/lib/* /usr/lib/\n</code></pre> <p>Salve o arquivo e monte a imagem novamente utilizando o mesmo comando anterior:</p> <pre><code>docker build -t minha-imagem .\n</code></pre> <p>Voc\u00ea perceber\u00e1 que isso cria uma nova imagem Docker mas com um ID de imagem diferente.</p>"},{"location":"pt/archive/basic_training/containers/#execute-salmon-no-conteiner","title":"Execute Salmon no cont\u00eainer","text":"<p>Cheque se o Salmon est\u00e1 executando corretamente no cont\u00eainer como mostrado abaixo:</p> <pre><code>docker run minha-imagem salmon --version\n</code></pre> <p>Voc\u00ea pode at\u00e9 iniciar o cont\u00eainer no modo interativo utilizando o seguinte comando:</p> <pre><code>docker run -it minha-imagem bash\n</code></pre> <p>Use o comando <code>exit</code> para finalizar a sess\u00e3o interativa.</p>"},{"location":"pt/archive/basic_training/containers/#montagem-do-sistema-de-arquivos","title":"Montagem do sistema de arquivos","text":"<p>Crie um arquivo \u00edndice de genoma utilizando o Salmon no cont\u00eainer.</p> <p>Tente executar o Salmon no cont\u00eainer com o seguinte comando:</p> <pre><code>docker run minha-imagem \\\n    salmon index -t $PWD/data/ggal/transcriptome.fa -i transcript-index\n</code></pre> <p>O comando acima falha porque o Salmon n\u00e3o pode acessar o arquivo de entrada.</p> <p>Isso acontece porque o cont\u00eainer executa em um sistema de arquivos totalmente diferente e n\u00e3o pode acessar o arquivo no sistema de arquivo do hospedeiro por padr\u00e3o.</p> <p>Voc\u00ea precisar\u00e1 usar a op\u00e7\u00e3o de linha de comando <code>--volume</code> para montar o(s) arquivo(s) de entrada, por exemplo</p> <pre><code>docker run --volume $PWD/data/ggal/transcriptome.fa:/transcriptome.fa minha-imagem \\\n    salmon index -t /transcriptome.fa -i transcript-index\n</code></pre> <p>Warning</p> <p>O diret\u00f3rio <code>transcript-index</code> gerado ainda est\u00e1 inacess\u00edvel no sistema de arquivo do sistema operacional hospedeiro.</p> <p>Um jeito mais f\u00e1cil \u00e9 montar o diret\u00f3rio original em um id\u00eantico no cont\u00eainer. Isso permite que voc\u00ea utilize o mesmo caminho durante a execu\u00e7\u00e3o dentro do cont\u00eainer, por exemplo</p> <pre><code>docker run --volume $PWD:$PWD --workdir $PWD minha-imagem \\\n    salmon index -t $PWD/data/ggal/transcriptome.fa -i transcript-index\n</code></pre> <p>Ou definir uma pasta que voc\u00ea queira montar como uma vari\u00e1vel de ambiente, chamada <code>DATA</code>:</p> <pre><code>DATA=/workspaces/training/nf-training/data\ndocker run --volume $DATA:$DATA --workdir $PWD minha-imagem \\\n    salmon index -t $PWD/data/ggal/transcriptome.fa -i transcript-index\n</code></pre> <p>Agora cheque o conte\u00fado da pasta <code>transcript-index</code> utilizando o comando:</p> <pre><code>ls -la transcript-index\n</code></pre> <p>Note</p> <p>Note que as permiss\u00f5es para cria\u00e7\u00e3o dos arquivos utilizado pelo Docker s\u00e3o <code>root</code>.</p>"},{"location":"pt/archive/basic_training/containers/#disponibilize-o-conteiner-no-docker-hub-bonus","title":"Disponibilize o cont\u00eainer no Docker Hub (b\u00f4nus)","text":"<p>Publique seu cont\u00eainer no Docker Hub para compartilh\u00e1-lo com outras pessoas.</p> <p>Crie uma conta no site https://hub.docker.com. Ent\u00e3o no seu terminal shell execute o seguinte comando, utilizando seu usu\u00e1rio e senha que criou quando se registrou no Hub:</p> <pre><code>docker login\n</code></pre> <p>Renomeie a imagem para incluir seu nome de usu\u00e1rio Docker:</p> <pre><code>docker tag minha-imagem &lt;nome-de-usuario&gt;/minha-imagem\n</code></pre> <p>Finalmente mande para o Docker Hub:</p> <pre><code>docker push &lt;nome-de-usuario&gt;/minha-imagem\n</code></pre> <p>Depois disso, qualquer um conseguir\u00e1 baixar a imagem utilizando o comando:</p> <pre><code>docker pull &lt;nome-de-usuario&gt;/minha-imagem\n</code></pre> <p>Note que depois de uma opera\u00e7\u00e3o push e pull, o Docker imprime na tela o n\u00famero de registro do cont\u00eainer, por exemplo:</p> <pre><code>Digest: sha256:aeacbd7ea1154f263cda972a96920fb228b2033544c2641476350b9317dab266\nStatus: Downloaded newer image for nextflow/rnaseq-nf:latest\n</code></pre> <p>Isso \u00e9 um identificador imut\u00e1vel e \u00fanico que pode ser usado para referenciar a imagem de cont\u00eainer de uma forma \u00fanica. Por exemplo:</p> <pre><code>docker pull nextflow/rnaseq-nf@sha256:aeacbd7ea1154f263cda972a96920fb228b2033544c2641476350b9317dab266\n</code></pre>"},{"location":"pt/archive/basic_training/containers/#execute-um-script-do-nextflow-utilizando-um-conteiner-docker","title":"Execute um script do Nextflow utilizando um cont\u00eainer Docker","text":"<p>A maneira mais simples de rodar um script Nextflow \u00e9 usando a op\u00e7\u00e3o de linha de comando <code>-with-docker</code>:</p> <pre><code>nextflow run script2.nf -with-docker minha-imagem\n</code></pre> <p>Como visto na \u00faltima se\u00e7\u00e3o, voc\u00ea tamb\u00e9m pode configurar o arquivo config (<code>nextflow.config</code>) para selecionar qual cont\u00eainer utilizar inv\u00e9s de ter que especificar como um argumento de linha de comando toda vez.</p>"},{"location":"pt/archive/basic_training/containers/#singularity","title":"Singularity","text":"<p>Singularity \u00e9 um motor de conteineriza\u00e7\u00e3o desenvolvido para trabalhar com computa\u00e7\u00e3o de alta performance em centro de dados, onde geralmente o Docker n\u00e3o \u00e9 permitido por motivos de restri\u00e7\u00f5es de seguran\u00e7a.</p> <p>O Singularity implementa um modelo de execu\u00e7\u00e3o de cont\u00eainer similar ao Docker. Entretanto, ele usa um design de implementa\u00e7\u00e3o completamente diferente.</p> <p>Uma imagem de cont\u00eainer do Singularity \u00e9 arquivada como um arquivo de texto simples que pode ser armazenado em um sistema de arquivo compartilhado e acessado por muitos n\u00f3s computacionais gerenciados usando um escalonador de lote.</p> <p>Warning</p> <p>O Singularity n\u00e3o ir\u00e1 funcionar com Gitpod. Se voc\u00ea quer testar essa se\u00e7\u00e3o, por favor fa\u00e7a localmente, ou em um cluster.</p>"},{"location":"pt/archive/basic_training/containers/#crie-imagens-do-singularity","title":"Crie imagens do Singularity","text":"<p>Imagens do Singularity s\u00e3o criadas utilizando um arquivo <code>Singularity</code> de uma forma similar ao Docker mas utilizando uma sintaxe diferente.</p> <pre><code>Bootstrap: docker\nFrom: debian:bullseye-slim\n\n%environment\nexport PATH=$PATH:/usr/games/\n\n%labels\nAUTHOR &lt;seu nome&gt;\n\n%post\n\napt-get update &amp;&amp; apt-get install -y locales-all curl cowsay\ncurl -sSL https://github.com/COMBINE-lab/salmon/releases/download/v1.0.0/salmon-1.0.0_linux_x86_64.tar.gz | tar xz \\\n&amp;&amp; mv /salmon-*/bin/* /usr/bin/ \\\n&amp;&amp; mv /salmon-*/lib/* /usr/lib/\n</code></pre> <p>Uma vez que voc\u00ea salvou o arquivo <code>Singularity</code>, voc\u00ea pode criar uma imagem utilizando esses comandos:</p> <pre><code>sudo singularity build minha-imagem.sif Singularity\n</code></pre> <p>Nota: O comando <code>build</code> requer permiss\u00f5es <code>sudo</code>. Uma forma de contornar isso consiste em criar a imagem em uma esta\u00e7\u00e3o de trabalho local e ent\u00e3o implementar no cluster copiando o arquivo imagem.</p>"},{"location":"pt/archive/basic_training/containers/#executando-um-conteiner","title":"Executando um cont\u00eainer","text":"<p>Quando terminar, voc\u00ea pode executar o cont\u00eainer com o seguinte comando:</p> <pre><code>singularity exec minha-imagem.sif cowsay 'Ol\u00e1 Singularity'\n</code></pre> <p>Utilizando o comando <code>shell</code> voc\u00ea pode entrar no cont\u00eainer utilizando o modo interativo. Por exemplo:</p> <pre><code>singularity shell minha-imagem.sif\n</code></pre> <p>Uma vez dentro da inst\u00e2ncia do cont\u00eainer execute o comando:</p> <pre><code>touch ola.txt\nls -la\n</code></pre> <p>Info</p> <p>Note como os arquivos do sistema de arquivos hospedeiro s\u00e3o mostrados. O Singularity automaticamente monta o diret\u00f3rio do hospedeiro <code>$HOME</code> e usa como diret\u00f3rio de trabalho.</p>"},{"location":"pt/archive/basic_training/containers/#importe-uma-imagem-do-docker","title":"Importe uma imagem do Docker","text":"<p>Uma forma mais f\u00e1cil de criar um cont\u00eainer com o Singularity n\u00e3o necessitando da permiss\u00e3o <code>sudo</code> e melhorando a interoperabilidade dos cont\u00eaineres \u00e9 importando uma imagem de cont\u00eainer do Docker puxando diretamente do reposit\u00f3rio de imagens do Docker. Por exemplo:</p> <pre><code>singularity pull docker://debian:bullseye-slim\n</code></pre> <p>O comando acima automaticamente baixa uma imagem Docker do Debian e converte para uma imagem Singularity no diret\u00f3rio atual com o nome <code>debian-jessie.simg</code>.</p>"},{"location":"pt/archive/basic_training/containers/#execute-um-script-do-nextflow-utilizando-um-conteiner-singularity","title":"Execute um script do Nextflow utilizando um cont\u00eainer Singularity","text":"<p>O Nextflow permite o uso de cont\u00eaineres Singularity de forma t\u00e3o f\u00e1cil e transparente quanto com o Docker.</p> <p>Simplesmente ative o uso do motor Singularity no lugar do Docker na linha de comando do Nextflow utilizando a op\u00e7\u00e3o de linha de comando <code>-with-singularity</code>:</p> <pre><code>nextflow run script7.nf -with-singularity nextflow/rnaseq-nf\n</code></pre> <p>Como antes, o cont\u00eainer Singularity tamb\u00e9m pode ser disponibilizado no arquivo de configura\u00e7\u00e3o do Nextflow. N\u00f3s iremos ver como funciona isso mais tarde.</p>"},{"location":"pt/archive/basic_training/containers/#a-biblioteca-de-conteineres-singularity","title":"A Biblioteca de Cont\u00eaineres Singularity","text":"<p>Os autores do Singularity, SyLabs tem o seu pr\u00f3prio reposit\u00f3rio de cont\u00eaineres Singularity.</p> <p>Da mesma forma que disponibilizamos as imagens Docker no Docker Hub, n\u00f3s podemos disponibilizar as imagens Singularity na Singularity Library.</p>"},{"location":"pt/archive/basic_training/containers/#pacotes-condabioconda","title":"Pacotes Conda/Bioconda","text":"<p>O Conda \u00e9 um popular gerenciador de pacotes e ambientes. O suporte a Conda permite que fluxos de trabalho Nextflow automaticamente criem e ativem ambientes Conda, dadas as depend\u00eancias especificadas de cada processo.</p> <p>Neste ambiente Gitpod, o conda j\u00e1 est\u00e1 instalado.</p>"},{"location":"pt/archive/basic_training/containers/#usando-conda","title":"Usando conda","text":"<p>Um ambiente Conda \u00e9 definido utilizando um arquivo YAML, que lista todos os pacotes de programas. A primeira coisa que voc\u00ea precisa fazer \u00e9 iniciar o conda para uma intera\u00e7\u00e3o shell, e da\u00ed abrir um novo terminal utilizando bash.</p> <pre><code>conda init\nbash\n</code></pre> <p>Com isso, escreva seu arquivo YAML (<code>env.yml</code>). J\u00e1 existe um arquivo <code>env.yml</code> na pasta <code>nf-training</code> como um exemplo. O seu conte\u00fado \u00e9 mostrado abaixo.</p> <pre><code>name: nf-tutorial\nchannels:\n  - conda-forge\n  - defaults\n  - bioconda\ndependencies:\n  - bioconda::salmon=1.5.1\n  - bioconda::fastqc=0.11.9\n  - bioconda::multiqc=1.12\n  - conda-forge::tbb=2020.2\n</code></pre> <p>Dado o arquivo de receita, o ambiente \u00e9 criado utilizando o comando abaixo. O comando <code>conda env create</code> deve demorar v\u00e1rios minutos, pois o conda tenta resolver todas as depend\u00eancias dos pacotes desejados durante a execu\u00e7\u00e3o, e ent\u00e3o baixa tudo que \u00e9 requerido.</p> <pre><code>conda env create --file env.yml\n</code></pre> <p>Voc\u00ea pode checar se o ambiente foi criado com \u00eaxito com o comando abaixo:</p> <pre><code>conda env list\n</code></pre> <p>Voc\u00ea deve ver algo similar ao mostrado abaixo:</p> <pre><code># conda environments:\n#\nbase                  *  /opt/conda\nnf-tutorial              /opt/conda/envs/nf-tutorial\n</code></pre> <p>Para habilitar o ambiente, voc\u00ea pode usar o comando <code>activate</code>:</p> <pre><code>conda activate nf-tutorial\n</code></pre> <p>O Nextflow consegue gerenciar a ativa\u00e7\u00e3o de um ambiente Conda quando seu diret\u00f3rio \u00e9 especificado utilizando a op\u00e7\u00e3o <code>-with-conda</code> (usando o mesmo caminho mostrado com a fun\u00e7\u00e3o <code>list</code>). Por exemplo:</p> <pre><code>nextflow run script7.nf -with-conda /opt/conda/envs/nf-tutorial\n</code></pre> <p>Info</p> <p>Quando criar um ambiente Conda com o arquivo de receita YAML, o Nextflow automaticamente baixar\u00e1 todas depend\u00eancias necess\u00e1rias, montar\u00e1 o ambiente e o ativar\u00e1.</p> <p>Isso torna f\u00e1cil gerenciar diferentes ambientes para os processos no fluxo de trabalho do script.</p> <p>Veja a documenta\u00e7\u00e3o para mais detalhes.</p>"},{"location":"pt/archive/basic_training/containers/#crie-e-utilize-ambientes-no-estilo-do-conda-com-o-micromamba","title":"Crie e utilize ambientes no estilo do conda com o micromamba","text":"<p>Outra forma de construir um ambiente no estilo do conda \u00e9 com o <code>Dockerfile</code> e o <code>micromamba</code>.</p> <p>O <code>micromamba</code> \u00e9 um pacote r\u00e1pido e robusto para montar pequenos ambientes baseados no conda.</p> <p>Isso economiza tempo ao montar um ambiente conda toda vez que quiser utiliz\u00e1-lo (como delineado nas se\u00e7\u00f5es anteriores).</p> <p>Para fazer isso, voc\u00ea simplesmente precisa de um <code>Dockerfile</code> e utilizar o micromamba para instalar os pacotes. Por\u00e9m, uma boa pr\u00e1tica \u00e9 ter o arquivo de receita YAML como nas se\u00e7\u00f5es anteriores, ent\u00e3o n\u00f3s iremos fazer isso aqui tamb\u00e9m, utilizando o mesmo <code>env.yml</code> utilizado anteriormente.</p> <pre><code>name: nf-tutorial\nchannels:\n  - conda-forge\n  - defaults\n  - bioconda\ndependencies:\n  - bioconda::salmon=1.5.1\n  - bioconda::fastqc=0.11.9\n  - bioconda::multiqc=1.12\n  - conda-forge::tbb=2020.2\n</code></pre> <p>Ent\u00e3o, n\u00f3s podemos escrever nosso Dockerfile com o micromamba instalando os pacotes por esse arquivo de receita.</p> <pre><code>FROM mambaorg/micromamba:0.25.1\n\nLABEL image.author.name \"Seu Nome Aqui\"\nLABEL image.author.email \"seu@email.aqui\"\n\nCOPY --chown=$MAMBA_USER:$MAMBA_USER env.yml /tmp/env.yml\n\nRUN micromamba create -n nf-tutorial\n\nRUN micromamba install -y -n nf-tutorial -f /tmp/env.yml &amp;&amp; \\\n    micromamba clean --all --yes\n\nENV PATH /opt/conda/envs/nf-tutorial/bin:$PATH\n</code></pre> <p>O <code>Dockerfile</code> acima pega a imagem pai mambaorg/micromamba, e instala um ambiente <code>conda</code> utilizando <code>micromamba</code>, e ent\u00e3o instala o <code>salmon</code>, o <code>fastqc</code> e o <code>multiqc</code>.</p> <p>Tente executar o fluxo de trabalho RNA-seq visto anteriormente (script7.nf). Comece montando seu pr\u00f3prio <code>Dockerfile</code> micromamba (como mostrado acima), salve no seu reposit\u00f3rio no Docker Hub, e oriente o Nextflow a rodar por esse cont\u00eainer (mudando seu <code>nextflow.config</code>).</p> <p>Warning</p> <p>Montar um cont\u00eainer Docker e disponibilizar no seu reposit\u00f3rio pessoal pode levar &gt;10 minutos.</p> Para um resumo dos passos a tomar, clique aqui: <ol> <li> <p>Crie um arquivo chamado <code>Dockerfile</code> no diret\u00f3rio atual (com os c\u00f3digo acima).</p> </li> <li> <p>Monte a imagem: <code>docker build -t minha-imagem .</code> (n\u00e3o esque\u00e7a o .).</p> </li> <li> <p>Publique a imagem Docker na sua conta do Docker Hub.</p> <p>Algo parecido como o seguinte, com <code>&lt;meurepo&gt;</code> substitu\u00eddo para seu pr\u00f3prio ID do Docker, sem &lt; e &gt; caracteres!</p> <p><code>minha-imagem</code> pode ser substitu\u00eddo por qualquer nome que voc\u00ea escolher. Como boa pr\u00e1tica, escolha algo memor\u00e1vel e certifique-se de que o nome combine com o nome usado no comando anterior.</p> <pre><code>docker login\ndocker tag minha-imagem &lt;meurepo&gt;/minha-imagem\ndocker push &lt;meurepo&gt;/minha-imagem\n</code></pre> </li> <li> <p>Adicione a imagem do cont\u00eainer no arquivo <code>nextflow.config</code>.</p> <p>ex. remova o seguinte do arquivo <code>nextflow.config</code>:</p> <pre><code>process.container = 'nextflow/rnaseq-nf'\n</code></pre> <p>e adicione:</p> <pre><code>process.container = '&lt;meurepo&gt;/minha-imagem'\n</code></pre> </li> <li> <p>Tente executar o Nextflow, por exemplo:</p> <pre><code>nextflow run script7.nf -with-docker\n</code></pre> </li> </ol> <p>Agora, o Nextflow deve conseguir achar <code>salmon</code> pra rodar o processo.</p>"},{"location":"pt/archive/basic_training/containers/#biocontainers","title":"BioContainers","text":"<p>Outro recurso \u00fatil para conectar Bioconda e cont\u00eaineres \u00e9 o projeto BioContainers. BioContainers \u00e9 uma iniciativa da comunidade para prover um reposit\u00f3rio de imagens de cont\u00eainer para cada receita do Bioconda.</p> <p>At\u00e9 agora, n\u00f3s vimos como instalar pacotes com conda e micromamba, ambos localmente e com cont\u00eainer. Com o BioContainers, voc\u00ea n\u00e3o precisa criar sua pr\u00f3pria imagem de cont\u00eainer para as ferramentas que voc\u00ea quiser, e n\u00e3o precisa utilizar conda ou micromamba para instalar pacotes. O BioContainers j\u00e1 disponibiliza uma imagem Docker contendo os programas que voc\u00ea quer instalado. Por exemplo, voc\u00ea pode adquirir a imagem de cont\u00eainer do fastqc utilizando BioContainers:</p> <pre><code>docker pull biocontainers/fastqc:v0.11.5\n</code></pre> <p>Voc\u00ea pode checar o reposit\u00f3rio dos pacotes que quer no site oficial do BioContainers. Para encontrar imagens de container com v\u00e1rias ferramentas, confira a p\u00e1gina Multi-package images.</p> <p>Diferente de outros reposit\u00f3rios que ir\u00e3o puxar a imagem mais recente quando nenhum r\u00f3tulo (vers\u00e3o) \u00e9 especificado, voc\u00ea precisa especificar um r\u00f3tulo quando for baixar do BioContainers (depois de dois pontos <code>:</code>, por exemplo fastqc:v0.11.5). Cheque os r\u00f3tulos com o registro e escolha o que melhor se ad\u00e9qua a suas necessidades.</p> <p>Voc\u00ea tamb\u00e9m pode instalar o pacote <code>galaxy-util-tools</code> e procurar por imagens de container mulled atrav\u00e9s da linha de comando. Veja as instru\u00e7\u00f5es abaixo, usando o <code>conda</code> para instalar o pacote.</p> <pre><code>conda activate um-ambiente-conda-que-voce-ja-criou\nconda install galaxy-tool-util\nmulled-search --destination quay singularity --channel bioconda --search bowtie samtools | grep mulled\n</code></pre> <p>Tip</p> <p>Voc\u00ea pode ter defini\u00e7\u00f5es mais complexas dentro de seu bloco de processo deixando a imagem de cont\u00eainer apropriada ou o pacote conda para serem usadas dependendo se o usu\u00e1rio selecionou singularity, Docker ou conda. Voc\u00ea pode clicar aqui para mais informa\u00e7\u00f5es e aqui para um exemplo.</p>"},{"location":"pt/archive/basic_training/containers/#exercises","title":"Exercises","text":"<p>Exercise</p> <p>Durante a se\u00e7\u00e3o onde constru\u00edmos o fluxo de trabalho de RNA-Seq, n\u00f3s criamos um \u00edndice (script2.nf). Dado que n\u00f3s n\u00e3o temos Salmon instalado localmente na m\u00e1quina provida pelo Gitpod, n\u00f3s temos que ou executar com <code>-with-conda</code> ou <code>-with-docker</code>. Sua tarefa agora \u00e9 executar novamente com <code>-with-docker</code>, mas sem ter que criar sua pr\u00f3pria imagem de cont\u00eainer Docker. Inv\u00e9s disso, use a imagem do BioContainers para Salmon 1.7.0.</p> Solution <pre><code>nextflow run script2.nf -with-docker quay.io/biocontainers/salmon:1.7.0--h84f40af_0\n</code></pre> <p>Bonus Exercise</p> <p>Mude as diretivas do processo no <code>script5.nf</code> ou no arquivo <code>nextflow.config</code> para fazer o fluxo de trabalho utilizar automaticamente BioContainers quando usando salmon, ou fastqc.</p> <p>Dica</p> <p>temporariamente comente a linha <code>process.container = 'nextflow/rnaseq-nf'</code> no arquivo <code>nextflow.config</code> para ter certeza que o processo est\u00e1 utilizando o cont\u00eainer do BioContainers que voc\u00ea configurou, e n\u00e3o a imagem de cont\u00eainer que est\u00e1vamos usando durante esse treinamento.</p> Solution <p>Com essas mudan\u00e7as, voc\u00ea deve ser capaz de executar o fluxo de trabalho com BioContainers executando a seguinte linha de comando:</p> <pre><code>nextflow run script5.nf\n</code></pre> <p>com as seguintes diretivas de cont\u00eainer para cada processo:</p> <pre><code>process FASTQC {\n    container 'biocontainers/fastqc:v0.11.5'\n    tag \"FASTQC on $sample_id\"\n...\n</code></pre> <p>e</p> <pre><code>process QUANTIFICATION {\n    tag \"Salmon on $sample_id\"\n    container 'quay.io/biocontainers/salmon:1.7.0--h84f40af_0'\n    publishDir params.outdir, mode: 'copy'\n...\n</code></pre> <p>Cheque o arquivo <code>.command.run</code> no diret\u00f3rio de trabalho e certifique-se que a linha de execu\u00e7\u00e3o cont\u00e9m o Biocontainers correto.</p>"},{"location":"pt/archive/basic_training/debugging/","title":"Resolu\u00e7\u00e3o de problemas","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/archive/basic_training/debugging/#tratamento-de-erros-e-resolucao-de-problemas","title":"Tratamento de erros e resolu\u00e7\u00e3o de problemas","text":""},{"location":"pt/archive/basic_training/debugging/#depuracao-de-erros-de-execucao","title":"Depura\u00e7\u00e3o de erros de execu\u00e7\u00e3o","text":"<p>Quando a execu\u00e7\u00e3o de um processo termina com um status de sa\u00edda diferente de zero, o Nextflow encerra a execu\u00e7\u00e3o e informa sobre a tarefa com falhas:</p> <p>Clique no \u00edcone  no c\u00f3digo para ver explica\u00e7\u00f5es.</p> <pre><code>ERROR ~ Error executing process &gt; 'INDEX'\n\nCaused by: # (1)!\n  Process `INDEX` terminated with an error exit status (127)\n\nCommand executed: # (2)!\n\n  salmon index --threads 1 -t transcriptome.fa -i index\n\nCommand exit status: # (3)!\n  127\n\nCommand output: # (4)!\n  (empty)\n\nCommand error: # (5)!\n  .command.sh: line 2: salmon: command not found\n\nWork dir: # (6)!\n  /Users/pditommaso/work/0b/b59f362980defd7376ee0a75b41f62\n</code></pre> <ol> <li>Uma descri\u00e7\u00e3o da causa do erro</li> <li>O comando executado</li> <li>O status de sa\u00edda do comando</li> <li>A sa\u00edda padr\u00e3o do comando, quando dispon\u00edvel</li> <li>O erro padr\u00e3o do comando</li> <li>O diret\u00f3rio de trabalho do comando</li> </ol> <p>Analise cuidadosamente os dados do erro, j\u00e1 que eles podem fornecer informa\u00e7\u00f5es valiosas para a depura\u00e7\u00e3o.</p> <p>Se isso n\u00e3o for o suficiente, use <code>cd</code> para entrar no diret\u00f3rio de trabalho da tarefa. Ele contem todos os arquivos necess\u00e1rios para reproduzir o erro de forma isolada.</p> <p>O diret\u00f3rio de execu\u00e7\u00e3o da tarefa possui os seguintes arquivos:</p> <ul> <li><code>.command.sh</code>: O script do comando.</li> <li><code>.command.run</code>: Um wrapper do comando usado para executar a tarefa.</li> <li><code>.command.out</code>: A sa\u00edda padr\u00e3o completa da tarefa.</li> <li><code>.command.err</code>: O erro padr\u00e3o completo da tarefa.</li> <li><code>.command.log</code>: A sa\u00edda do wrapper de execu\u00e7\u00e3o.</li> <li><code>.command.begin</code>: Um arquivo sentinela criado no momento que a tarefa \u00e9 iniciada.</li> <li><code>.exitcode</code>: Um arquivo contendo o c\u00f3digo de sa\u00edda da tarefa.</li> <li>Os arquivos de entrada da tarefa (links simb\u00f3licas)</li> <li>Os arquivos de sa\u00edda da tarefa</li> </ul> <p>Certifique-se que o arquivo <code>.command.sh</code> cont\u00e9m o comando esperado e que todas as vari\u00e1veis foram substitu\u00eddas pelos valores desejados.</p> <p>Tamb\u00e9m verifique a exist\u00eancia dos arquivos <code>.exitcode</code> e <code>.command.begin</code>, que, se ausentes, sugerem que a tarefa nunca foi executada pelo subsistema (o escalonador de lotes, por exemplo). Se o arquivo <code>.command.begin</code> existir, a tarefa foi iniciada mas foi provavelmente encerrada abruptamente.</p> <p>Para verificar a causa do erro, voc\u00ea pode replicar a execu\u00e7\u00e3o com falhas usando <code>bash .command.run</code>.</p>"},{"location":"pt/archive/basic_training/debugging/#ignorando-erros","title":"Ignorando erros","text":"<p>Existem casos em que um erro em um processo \u00e9 esperado e n\u00e3o deve encerrar a execu\u00e7\u00e3o do fluxo de trabalho.</p> <p>Para lidar com isso, forne\u00e7a o valor <code>ignore</code> a <code>errorStrategy</code>:</p> <pre><code>process FOO {\n    errorStrategy 'ignore'\n\n    script:\n    \"\"\"\n    seu_comando --isso --aquilo\n    \"\"\"\n}\n</code></pre> <p>Se voc\u00ea deseja ignorar qualquer erro, voc\u00ea pode especificar a mesma diretiva em um arquivo de configura\u00e7\u00e3o:</p> <pre><code>process.errorStrategy = 'ignore'\n</code></pre>"},{"location":"pt/archive/basic_training/debugging/#tolerancia-automatica-a-falhas","title":"Toler\u00e2ncia autom\u00e1tica a falhas","text":"<p>Em casos mais raros, erros podem surgir por causa de condi\u00e7\u00f5es transit\u00f3rias. Nessas situa\u00e7\u00f5es, uma estrat\u00e9gia eficaz \u00e9 re-executar a tarefa com falhas.</p> <pre><code>process FOO {\n    errorStrategy 'retry'\n\n    script:\n    \"\"\"\n    seu_comando --isso --aquilo\n    \"\"\"\n}\n</code></pre> <p>Ao usar a estrat\u00e9gia de erro <code>retry</code> a tarefa \u00e9 re-executada uma segunda vez se ela retornar um status de sa\u00edda diferente de zero antes de encerrar a execu\u00e7\u00e3o completa do fluxo de trabalho.</p> <p>A diretiva maxRetries pode ser utilizada para configurar o n\u00famero de tentativas que uma tarefa pode ser re-executada antes de declarar que ela falhou com uma condi\u00e7\u00e3o de erro.</p>"},{"location":"pt/archive/basic_training/debugging/#re-execucao-com-atraso","title":"Re-execu\u00e7\u00e3o com atraso","text":"<p>Existem casos em que os recursos necess\u00e1rios para a execu\u00e7\u00e3o est\u00e3o temporariamente indispon\u00edveis (por exemplo, congestionamento de rede). Nesses casos apenas re-executar a tarefa provavelmente levar\u00e1 a um erro id\u00eantico. Uma re-execu\u00e7\u00e3o com um atraso exponencial pode contribuir de uma melhor forma para a resolu\u00e7\u00e3o desses erros.</p> <pre><code>process FOO {\n    errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n    maxRetries 5\n\n    script:\n    '''\n    seu_comando --aqui\n    '''\n}\n</code></pre>"},{"location":"pt/archive/basic_training/debugging/#alocacao-dinamica-de-recursos","title":"Aloca\u00e7\u00e3o din\u00e2mica de recursos","text":"<p>Uma situa\u00e7\u00e3o bastante comum \u00e9 que diferentes inst\u00e2ncias de um mesmo processo podem ter necessidades diferentes de recursos computacionais. Nessas situa\u00e7\u00f5es, solicitar uma quantidade de mem\u00f3ria muito baixa, por exemplo, ir\u00e1 levar algumas tarefas a falharem. Por outro lado, usar um limite mais elevado que abrange todas as suas tarefas pode reduzir significativamente a prioridade de execu\u00e7\u00e3o delas em um sistema de escalonamento de tarefas.</p> <p>Para lidar com isso, voc\u00ea pode utilizar uma estrat\u00e9gia de erro <code>retry</code> e aumentar os recursos computacionais alocados pela tarefa em cada tentativa consecutiva.</p> <pre><code>process FOO {\n    cpus 4\n    memory { 2.GB * task.attempt } // (1)!\n    time { 1.hour * task.attempt } // (2)!\n    errorStrategy { task.exitStatus == 140 ? 'retry' : 'terminate' } // (3)!\n    maxRetries 3 // (4)!\n\n    script:\n    \"\"\"\n    seu_comando --cpus $task.cpus --mem $task.memory\n    \"\"\"\n}\n</code></pre> <ol> <li>A mem\u00f3ria \u00e9 definida de forma din\u00e2mica, a primeira tentativa \u00e9 com 2 GB, a segunda com 4 GB, e assim sucessivamente.</li> <li>O tempo de execu\u00e7\u00e3o da tarefa \u00e9 configurado dinamicamente tamb\u00e9m, a primeira tentativa \u00e9 com 1 hora, a segunda com 2 horas, e assim sucessivamente.</li> <li>Se a tarefa retorna um status de sa\u00edda igual a <code>140</code> a estrat\u00e9gia de erro ser\u00e1 <code>retry</code>, caso contr\u00e1rio, a execu\u00e7\u00e3o ser\u00e1 encerrada.</li> <li>O processo ser\u00e1 re-executado at\u00e9 tr\u00eas vezes.</li> </ol>"},{"location":"pt/archive/basic_training/executors/","title":"Executors","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/archive/basic_training/executors/#cenarios-de-implantacao","title":"Cen\u00e1rios de implanta\u00e7\u00e3o","text":"<p>Aplica\u00e7\u00f5es gen\u00f4micas do mundo real podem gerar milhares de tarefas sendo executadas. Nesse cen\u00e1rio, um escalonador de lote (batch scheduler) \u00e9 comumente usado para implantar um fluxo de trabalho em um cluster de computa\u00e7\u00e3o, permitindo a execu\u00e7\u00e3o de muitos trabalhos em paralelo em muitos n\u00f3s de computa\u00e7\u00e3o.</p> <p>O Nextflow possui suporte embutido para os escalonadores de lote mais usados, como o Univa Grid Engine, o SLURM e o IBM LSF. Verifique a documenta\u00e7\u00e3o do Nextflow para obter a lista completa dos ambientes de computa\u00e7\u00e3o.</p>"},{"location":"pt/archive/basic_training/executors/#implantacao-em-cluster","title":"Implanta\u00e7\u00e3o em cluster","text":"<p>Um recurso importante do Nextflow \u00e9 a capacidade de desacoplar a implementa\u00e7\u00e3o do fluxo de trabalho da plataforma de execu\u00e7\u00e3o de fato. A implementa\u00e7\u00e3o de uma camada de abstra\u00e7\u00e3o permite a implanta\u00e7\u00e3o do fluxo de trabalho resultante em qualquer plataforma de execu\u00e7\u00e3o suportada pelo framework.</p> <p></p> <p>Para executar seu fluxo de trabalho com um escalonador de lote, modifique o arquivo <code>nextflow.config</code> especificando o executor de destino e os recursos de computa\u00e7\u00e3o necess\u00e1rios, se necess\u00e1rio. Por exemplo:</p> <pre><code>process.executor = 'slurm'\n</code></pre>"},{"location":"pt/archive/basic_training/executors/#gerenciando-recursos-do-cluster","title":"Gerenciando recursos do cluster","text":"<p>Ao usar um escalonador de lote, geralmente \u00e9 necess\u00e1rio especificar o n\u00famero de recursos (ou seja, CPU, mem\u00f3ria, tempo de execu\u00e7\u00e3o etc.) necess\u00e1rios para cada tarefa.</p> <p>Isso pode ser feito utilizando as seguintes diretivas de processo:</p> queue a fila a ser utilizada no cluster para computa\u00e7\u00e3o cpus o n\u00famero de cpus a serem alocadas para execu\u00e7\u00e3o da tarefa memory a quantidade de mem\u00f3ria a ser alocada para execu\u00e7\u00e3o da tarefa time a quantidade de tempo m\u00e1xima a ser alocada para execu\u00e7\u00e3o da tarefa disk a quantidade de espa\u00e7o de armazenamento necess\u00e1ria para a execu\u00e7\u00e3o da tarefa"},{"location":"pt/archive/basic_training/executors/#recursos-do-fluxo-de-trabalho-de-modo-amplo","title":"Recursos do fluxo de trabalho de modo amplo","text":"<p>Use o escopo <code>process</code> para definir os requisitos de recursos para todos os processos em suas aplica\u00e7\u00f5es de fluxo de trabalho. Por exemplo:</p> <pre><code>process {\n    executor = 'slurm'\n    queue = 'curta'\n    memory = '10 GB'\n    time = '30 min'\n    cpus = 4\n}\n</code></pre>"},{"location":"pt/archive/basic_training/executors/#submeta-o-nextflow-como-uma-tarefa","title":"Submeta o Nextflow como uma tarefa","text":"<p>Embora o comando principal do Nextflow possa ser iniciado no n\u00f3 de login/head de um cluster, esteja ciente de que o n\u00f3 deve ser configurado para comandos que s\u00e3o executados por um longo per\u00edodo de tempo, mesmo que os recursos computacionais usados sejam insignificantes. Outra op\u00e7\u00e3o \u00e9 enviar o processo principal do Nextflow como uma tarefa no cluster.</p> <p>Note</p> <p>Isso requer a configura\u00e7\u00e3o do seu cluster para permitir que as tarefas sejam iniciadas a partir dos n\u00f3s de trabalho, pois o Nextflow enviar\u00e1 novas tarefas e as gerenciar\u00e1 a partir daqui.</p> <p>Por exemplo, se seu cluster usa Slurm como escalonador de tarefas, voc\u00ea pode criar um arquivo semelhante ao abaixo:</p> launch_nf.sh<pre><code>#!/bin/bash\n#SBATCH --partition TRABALHO\n#SBATCH --mem 5G\n#SBATCH -c 1\n#SBATCH -t 12:00:00\n\nFLUXO_DE_TRABALHO=$1\nCONFIG=$2\n\n# Use um ambiente conda onde voc\u00ea instalou o Nextflow\n# (pode n\u00e3o ser necess\u00e1rio se voc\u00ea o tiver instalado de uma maneira diferente)\nconda activate nextflow\n\nnextflow -C ${CONFIG} run ${FLUXO_DE_TRABALHO}\n</code></pre> <p>E, em seguida, submeta-o com:</p> <pre><code>sbatch launch_nf.sh /home/meu_usuario/caminho/meu_fluxo_de_trabalho.nf /home/meu_usuario/caminho/meu_arquivo_configuracao.conf\n</code></pre> <p>Voc\u00ea pode encontrar mais detalhes sobre o exemplo acima aqui. Voc\u00ea tamb\u00e9m poder\u00e1 encontrar mais dicas de como executar o Nextflow em HPC nos seguintes posts de blog:</p> <ul> <li>5 Nextflow Tips for HPC Users</li> <li>Five more tips for Nextflow user on HPC</li> </ul>"},{"location":"pt/archive/basic_training/executors/#configure-processos-por-nome","title":"Configure processos por nome","text":"<p>Em aplica\u00e7\u00f5es do mundo real, diferentes tarefas precisam de diferentes quantidades de recursos de computa\u00e7\u00e3o. \u00c9 poss\u00edvel definir os recursos para uma tarefa espec\u00edfica usando o seletor <code>withName:</code> seguido do nome do processo:</p> <pre><code>process {\n    executor = 'slurm'\n    queue = 'curta'\n    memory = '10 GB'\n    time = '30 min'\n    cpus = 4\n\n    withName: FOO {\n        cpus = 2\n        memory = '20 GB'\n        queue = 'curta'\n    }\n\n    withName: BAR {\n        cpus = 4\n        memory = '32 GB'\n        queue = 'longa'\n    }\n}\n</code></pre> <p>Exercise</p> <p>Execute o script RNA-Seq (<code>script7.nf</code>) visto na se\u00e7\u00e3o de RNAseq, mas especifique dentro do arquivo <code>nextflow.config</code> que o processo de quantifica\u00e7\u00e3o (<code>QUANTIFICATION</code>) requer 2 CPUs e 5 GB de mem\u00f3ria.</p> Solution <pre><code>process {\n    withName: QUANTIFICATION {\n        cpus = 2\n        memory = '5 GB'\n    }\n}\n</code></pre>"},{"location":"pt/archive/basic_training/executors/#configure-processos-por-rotulos","title":"Configure processos por r\u00f3tulos","text":"<p>Quando uma aplica\u00e7\u00e3o de fluxo de trabalho \u00e9 composta por muitos processos, pode ser dif\u00edcil listar todos os nomes de processos e escolher recursos para cada um deles no arquivo de configura\u00e7\u00e3o.</p> <p>Uma melhor estrat\u00e9gia consiste em anotar os processos com uma diretiva de r\u00f3tulo (<code>label</code>). Em seguida, especifique os recursos no arquivo de configura\u00e7\u00e3o usados para todos os processos com o mesmo r\u00f3tulo.</p> <p>O script do fluxo de trabalho:</p> <pre><code>process TAREFA1 {\n    label 'longo'\n\n    script:\n    \"\"\"\n    primeiro_comando --aqui\n    \"\"\"\n}\n\nprocess TAREFA2 {\n    label 'curto'\n\n    script:\n    \"\"\"\n    segundo_comando --aqui\n    \"\"\"\n}\n</code></pre> <p>O arquivo de configura\u00e7\u00e3o:</p> <pre><code>process {\n    executor = 'slurm'\n\n    withLabel: 'curto' {\n        cpus = 4\n        memory = '20 GB'\n        queue = 'alfa'\n    }\n\n    withLabel: 'longo' {\n        cpus = 8\n        memory = '32 GB'\n        queue = 'omega'\n    }\n}\n</code></pre>"},{"location":"pt/archive/basic_training/executors/#configure-varios-conteineres","title":"Configure v\u00e1rios cont\u00eaineres","text":"<p>Os cont\u00eaineres podem ser definidos para cada processo em seu fluxo de trabalho. Voc\u00ea pode definir seus cont\u00eaineres em um arquivo de configura\u00e7\u00e3o conforme mostrado abaixo:</p> <pre><code>process {\n    withName: FOO {\n        container = 'uma/imagem:x'\n    }\n    withName: BAR {\n        container = 'outra/imagem:y'\n    }\n}\n\ndocker.enabled = true\n</code></pre> <p>Tip</p> <p>Devo usar um \u00fanico cont\u00eainer pesado ou muitos cont\u00eaineres leves? Ambas as abordagens t\u00eam pr\u00f3s e contras. Um \u00fanico cont\u00eainer \u00e9 mais simples de construir e manter, por\u00e9m ao usar muitas ferramentas a imagem pode ficar muito grande e as ferramentas podem criar conflitos umas com as outras. O uso de um cont\u00eainer para cada processo pode resultar em muitas imagens diferentes para criar e manter, especialmente quando os processos em seu fluxo de trabalho usam ferramentas diferentes para cada tarefa.</p> <p>Leia mais sobre seletores de processo de configura\u00e7\u00e3o neste link.</p>"},{"location":"pt/archive/basic_training/executors/#perfis-de-configuracao","title":"Perfis de configura\u00e7\u00e3o","text":"<p>Os arquivos de configura\u00e7\u00e3o podem conter a defini\u00e7\u00e3o de um ou mais perfis. Um perfil \u00e9 um conjunto de atributos de configura\u00e7\u00e3o que podem ser ativados/escolhidos ao lan\u00e7ar a execu\u00e7\u00e3o de um fluxo de trabalho usando a op\u00e7\u00e3o de linha de comando <code>-profile</code>.</p> <p>Os perfis de configura\u00e7\u00e3o s\u00e3o definidos usando o escopo especial <code>profiles</code> que agrupa os atributos que pertencem ao mesmo perfil usando um prefixo comum. Por exemplo:</p> <pre><code>profiles {\n    standard {\n        params.genoma = '/local/caminho/ref.fasta'\n        process.executor = 'local'\n    }\n\n    cluster {\n        params.genoma = '/data/stared/ref.fasta'\n        process.executor = 'sge'\n        process.queue = 'longa'\n        process.memory = '10 GB'\n        process.conda = '/um/caminho/ambiente.yml'\n    }\n\n    nuvem {\n        params.genoma = '/data/stared/ref.fasta'\n        process.executor = 'awsbatch'\n        process.container = 'cbcrg/imagex'\n        docker.enabled = true\n    }\n\n}\n</code></pre> <p>Essa configura\u00e7\u00e3o define tr\u00eas perfis diferentes: <code>standard</code>, <code>cluster</code> e <code>nuvem</code> que definem diferentes estrat\u00e9gias de configura\u00e7\u00e3o de processo dependendo da plataforma de tempo de execu\u00e7\u00e3o de destino. Por conven\u00e7\u00e3o, o perfil <code>standard</code> \u00e9 usado implicitamente quando nenhum outro perfil \u00e9 especificado pelo usu\u00e1rio.</p> <p>Para ativar um perfil espec\u00edfico, use a op\u00e7\u00e3o <code>-profile</code> seguida do nome do perfil:</p> <pre><code>nextflow run &lt;seu script&gt; -profile cluster\n</code></pre> <p>Tip</p> <p>Dois ou mais perfis de configura\u00e7\u00e3o podem ser especificados separando os nomes dos perfis com uma v\u00edrgula:</p> <pre><code>nextflow run &lt;seu script&gt; -profile standard,nuvem\n</code></pre>"},{"location":"pt/archive/basic_training/executors/#implantacao-na-nuvem","title":"Implanta\u00e7\u00e3o na nuvem","text":"<p>AWS Batch \u00e9 um servi\u00e7o de computa\u00e7\u00e3o gerenciada que permite a execu\u00e7\u00e3o de cargas de trabalho em cont\u00eaineres na infraestrutura de nuvem da Amazon.</p> <p>O Nextflow fornece suporte embutido para o AWS Batch, que permite uma implanta\u00e7\u00e3o simples de um fluxo de trabalho do Nextflow na nuvem, descarregando as execu\u00e7\u00f5es de processo como trabalhos em lote.</p> <p>Uma vez que o ambiente Batch esteja configurado, especifique os tipos de inst\u00e2ncia a serem usadas e o n\u00famero m\u00e1ximo de CPUs a serem alocadas. Voc\u00ea precisa criar um arquivo de configura\u00e7\u00e3o do Nextflow como o mostrado abaixo:</p> <p>Clique no \u00edcone  no c\u00f3digo para ver explica\u00e7\u00f5es.</p> <pre><code>process.executor = 'awsbatch' // (1)!\nprocess.queue = 'nextflow-ci' // (2)!\nprocess.container = 'nextflow/rnaseq-nf:latest' // (3)!\nworkDir = 's3://nextflow-ci/work/' // (4)!\naws.region = 'eu-west-1' // (5)!\naws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws' // (6)!\n</code></pre> <ol> <li>Defina o AWS Batch como o executor para executar os processos no fluxo de trabalho</li> <li>O nome da fila de computa\u00e7\u00e3o definida no ambiente Batch</li> <li>A imagem do cont\u00eainer do Docker a ser usada para executar cada trabalho</li> <li>O diret\u00f3rio de trabalho do fluxo de trabalho deve ser um bucket AWS S3</li> <li>A regi\u00e3o da AWS a ser usada</li> <li>O caminho da ferramenta de linha de comando da AWS necess\u00e1ria para fazer download/upload de arquivos de/para o cont\u00eainer</li> </ol> <p>Tip</p> <p>A pr\u00e1tica recomendada \u00e9 manter essa configura\u00e7\u00e3o como um perfil separado no arquivo de configura\u00e7\u00e3o do fluxo de trabalho. Isso permite a execu\u00e7\u00e3o com um comando simples.</p> <pre><code>nextflow run script7.nf -profile amazon\n</code></pre> <p>Os detalhes completos sobre a implanta\u00e7\u00e3o no AWS Batch est\u00e3o dispon\u00edveis nesse link.</p>"},{"location":"pt/archive/basic_training/executors/#montagens-de-volume","title":"Montagens de volume","text":"<p>Elastic Block Storage (EBS) (ou outras formas de armazenamento suportadas) podem ser montados no cont\u00eainer da tarefa usando a seguinte configura\u00e7\u00e3o:</p> <pre><code>aws {\n    batch {\n        volumes = '/algum/caminho'\n    }\n}\n</code></pre> <p>V\u00e1rios volumes podem ser especificados usando caminhos separados por v\u00edrgulas. A sintaxe usual de montagem de volume do Docker pode ser usada para definir volumes complexos para os quais o caminho do cont\u00eainer \u00e9 diferente do caminho do hospedeiro ou para especificar uma op\u00e7\u00e3o somente leitura:</p> <pre><code>aws {\n    region = 'eu-west-1'\n    batch {\n        volumes = ['/tmp', '/caminho/no/hospedeiro:/mnt/caminho:ro']\n    }\n}\n</code></pre> <p>Tip</p> <p>Esta \u00e9 uma configura\u00e7\u00e3o global que deve ser especificada em um arquivo de configura\u00e7\u00e3o do Nextflow e ser\u00e1 aplicada a todas as execu\u00e7\u00f5es do processo.</p> <p>Warning</p> <p>O Nextflow espera que os caminhos estejam dispon\u00edveis. Ele n\u00e3o lida com o fornecimento de volumes EBS ou outro tipo de armazenamento.</p>"},{"location":"pt/archive/basic_training/executors/#definicao-de-tarefa-personalizada","title":"Defini\u00e7\u00e3o de tarefa personalizada","text":"<p>O Nextflow cria automaticamente as defini\u00e7\u00f5es de trabalho do Batch necess\u00e1rias para executar seus processos de fluxo de trabalho. Portanto, n\u00e3o \u00e9 necess\u00e1rio defini-las antes de executar seu fluxo de trabalho.</p> <p>No entanto, voc\u00ea ainda pode precisar especificar uma defini\u00e7\u00e3o de tarefa personalizada para fornecer controle refinado das defini\u00e7\u00f5es de configura\u00e7\u00e3o de uma tarefa espec\u00edfica (por exemplo, para definir caminhos de montagem personalizados ou outras configura\u00e7\u00f5es especiais de uma tarefa no Batch).</p> <p>Para usar sua pr\u00f3pria defini\u00e7\u00e3o de tarefa em um fluxo de trabalho do Nextflow, use-a no lugar do nome da imagem do cont\u00eainer, prefixando-a com a string <code>job-definition://</code>. Por exemplo:</p> <pre><code>process {\n    container = 'job-definition://o-nome-da-sua-definicao-de-tarefa'\n}\n</code></pre>"},{"location":"pt/archive/basic_training/executors/#imagem-personalizada","title":"Imagem personalizada","text":"<p>Como o Nextflow exige que a ferramenta de linha de comando da AWS esteja acess\u00edvel no ambiente de computa\u00e7\u00e3o, uma solu\u00e7\u00e3o comum consiste em criar uma Amazon Machine Image (AMI) personalizada e instal\u00e1-la de maneira independente (por exemplo, usando o gerenciador de pacotes Conda).</p> <p>Warning</p> <p>Ao criar sua AMI personalizada para o AWS Batch, certifique-se de usar a Amazon ECS-Optimized Amazon Linux AMI como imagem base.</p> <p>O trecho de c\u00f3digo a seguir mostra como instalar a ferramenta de linha de comando da AWS com o Miniconda:</p> <pre><code>sudo yum install -y bzip2 wget\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -f -p $HOME/miniconda\n$HOME/miniconda/bin/conda install -c conda-forge -y awscli\nrm Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Note</p> <p>A ferramenta <code>aws</code> ser\u00e1 colocada em um diret\u00f3rio chamado <code>bin</code> na pasta principal de instala\u00e7\u00e3o. As ferramentas n\u00e3o funcionar\u00e3o corretamente se voc\u00ea modificar essa estrutura de diret\u00f3rios ap\u00f3s a instala\u00e7\u00e3o.</p> <p>Por fim, especifique o caminho completo para a <code>aws</code> no arquivo de configura\u00e7\u00e3o do Nextflow, conforme mostrado abaixo:</p> <pre><code>aws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws'\n</code></pre>"},{"location":"pt/archive/basic_training/executors/#modelo-de-lancamento","title":"Modelo de lan\u00e7amento","text":"<p>Uma abordagem alternativa \u00e9 criar uma AMI personalizada usando um modelo de lan\u00e7amento que instala a ferramenta de linha de comando da AWS durante o lan\u00e7amento da inst\u00e2ncia por meio de dados de usu\u00e1rio personalizados.</p> <p>No painel do EC2, crie um modelo de lan\u00e7amento especificando o campo de dados do usu\u00e1rio:</p> <pre><code>MIME-Version: 1.0\nContent-Type: multipart/mixed; boundary=\"//\"\n\n--//\nContent-Type: text/x-shellscript; charset=\"us-ascii\"\n\n##!/bin/sh\n### install required deps\nset -x\nexport PATH=/usr/local/bin:$PATH\nyum install -y jq python27-pip sed wget bzip2\npip install -U boto3\n\n### install awscli\nUSER=/home/ec2-user\nwget -q https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -f -p $USER/miniconda\n$USER/miniconda/bin/conda install -c conda-forge -y awscli\nrm Miniconda3-latest-Linux-x86_64.sh\nchown -R ec2-user:ec2-user $USER/miniconda\n\n--//--\n</code></pre> <p>Em seguida, crie um novo ambiente de computa\u00e7\u00e3o no painel Batch e especifique o modelo de lan\u00e7amento rec\u00e9m-criado no campo correspondente.</p>"},{"location":"pt/archive/basic_training/executors/#implantacao-hibrida","title":"Implanta\u00e7\u00e3o h\u00edbrida","text":"<p>O Nextflow permite o uso de v\u00e1rios executores na mesma aplica\u00e7\u00e3o do fluxo de trabalho. Esse recurso permite a implanta\u00e7\u00e3o de cargas de trabalho h\u00edbridas nas quais algumas tarefas s\u00e3o executados no computador local ou cluster de computa\u00e7\u00e3o local e algumas tarefas s\u00e3o transferidas para o servi\u00e7o AWS Batch.</p> <p>Para ativar esse recurso, use um ou mais seletores de processo em seu arquivo de configura\u00e7\u00e3o do Nextflow.</p> <p>Por exemplo, aplique a configura\u00e7\u00e3o do AWS Batch apenas a um subconjunto de processos em seu fluxo de trabalho. Voc\u00ea pode tentar o seguinte:</p> <pre><code>process {\n    executor = 'slurm' // (1)!\n    queue = 'curta' // (2)!\n\n    withLabel: tarefaGrande {  // (3)!\n        executor = 'awsbatch' // (4)!\n        queue = 'minha-fila-do-batch' // (5)!\n        container = 'minha/imagem:tag' // (6)!\n    }\n}\n\naws {\n    region = 'eu-west-1' // (7)!\n}\n</code></pre> <ol> <li>Defina o <code>slurm</code> como o executor padr\u00e3o</li> <li>Defina a fila para o cluster SLURM</li> <li>Configura\u00e7\u00e3o de processo(s) com o r\u00f3tulo <code>tarefaGrande</code></li> <li>Defina <code>awsbatch</code> como o executor para o(s) processo(s) com o r\u00f3tulo <code>tarefaGrande</code></li> <li>Defina a fila para o(s) processo(s) com o r\u00f3tulo <code>tarefaGrande</code></li> <li>Defina a imagem do cont\u00eainer para implantar para o(s) processo(s) com o r\u00f3tulo <code>tarefaGrande</code></li> <li>Defina a regi\u00e3o para execu\u00e7\u00e3o do Batch</li> </ol>"},{"location":"pt/archive/basic_training/groovy/","title":"Introdu\u00e7\u00e3o ao Groovy","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/archive/basic_training/groovy/#estruturas-basicas-e-expressoes-do-groovy","title":"Estruturas b\u00e1sicas e express\u00f5es do Groovy","text":"<p>Nextflow \u00e9 uma linguagem espec\u00edfica de dom\u00ednio (DSL) implementada sobre a linguagem de programa\u00e7\u00e3o Groovy, que por sua vez \u00e9 um superconjunto da linguagem de programa\u00e7\u00e3o Java. Isso significa que o Nextflow pode executar qualquer c\u00f3digo Groovy ou Java.</p> <p>Aqui est\u00e3o algumas sintaxes Groovy importantes que s\u00e3o comumente usadas no Nextflow.</p>"},{"location":"pt/archive/basic_training/groovy/#imprimindo-valores","title":"Imprimindo valores","text":"<p>Imprimir algo \u00e9 t\u00e3o f\u00e1cil quanto usar um dos m\u00e9todos <code>print</code> ou <code>println</code>.</p> <pre><code>println(\"Ol\u00e1, mundo!\")\n</code></pre> <p>A \u00fanica diferen\u00e7a entre os dois \u00e9 que o m\u00e9todo <code>println</code> anexa implicitamente um caractere de nova linha \u00e0 string impressa.</p> <p>Tip</p> <p>Par\u00eanteses para invoca\u00e7\u00f5es de fun\u00e7\u00e3o s\u00e3o opcionais. Portanto, a seguinte sintaxe tamb\u00e9m \u00e9 v\u00e1lida:</p> <pre><code>println \"Ol\u00e1, mundo!\"\n</code></pre>"},{"location":"pt/archive/basic_training/groovy/#comentarios","title":"Coment\u00e1rios","text":"<p>Os coment\u00e1rios usam a mesma sintaxe das linguagens de programa\u00e7\u00e3o da fam\u00edlia C:</p> <pre><code>// comente uma \u00fanica linha\n\n/*\n    um coment\u00e1rio abrangendo\n    v\u00e1rias linhas\n*/\n</code></pre>"},{"location":"pt/archive/basic_training/groovy/#variaveis","title":"Vari\u00e1veis","text":"<p>Para definir uma vari\u00e1vel, basta atribuir um valor a ela:</p> <pre><code>x = 1\nprintln x\n\nx = new java.util.Date()\nprintln x\n\nx = -3.1499392\nprintln x\n\nx = false\nprintln x\n\nx = \"Oi\"\nprintln x\n</code></pre> <p>As vari\u00e1veis locais s\u00e3o definidas usando a palavra-chave <code>def</code>:</p> <pre><code>def x = 'foo'\n</code></pre> <p>O <code>def</code> deve ser sempre usado ao definir vari\u00e1veis locais para uma fun\u00e7\u00e3o ou clausura.</p>"},{"location":"pt/archive/basic_training/groovy/#listas","title":"Listas","text":"<p>Um objeto List pode ser definido colocando os itens da lista entre colchetes:</p> <pre><code>lista = [10, 20, 30, 40]\n</code></pre> <p>Voc\u00ea pode acessar um determinado item na lista com a nota\u00e7\u00e3o de colchetes (\u00edndices come\u00e7am em <code>0</code>) ou usando o m\u00e9todo <code>get</code>:</p> <pre><code>println lista[0]\nprintln lista.get(0)\n</code></pre> <p>Para obter o comprimento de uma lista, voc\u00ea pode usar o m\u00e9todo <code>size</code>:</p> <pre><code>println lista.size()\n</code></pre> <p>Usamos a palavra-chave <code>assert</code> para testar se uma condi\u00e7\u00e3o \u00e9 verdadeira (semelhante a uma fun\u00e7\u00e3o <code>if</code>). Aqui, o Groovy n\u00e3o imprimir\u00e1 nada se estiver correto, caso contr\u00e1rio, gerar\u00e1 uma mensagem AssertionError.</p> <pre><code>assert lista[0] == 10\n</code></pre> <p>Note</p> <p>Esta afirma\u00e7\u00e3o deve estar correta, tente alter\u00e1-la para uma incorreta.</p> <p>As listas tamb\u00e9m podem ser indexadas com \u00edndices negativos e intervalos invertidos.</p> <pre><code>lista = [0, 1, 2]\nassert lista[-1] == 2\nassert lista[-1..0] == lista.reverse()\n</code></pre> <p>Info</p> <p>Na afirma\u00e7\u00e3o da \u00faltima linha, estamos referenciando a lista inicial e convertendo-a com um intervalo \"abreviado\" (<code>..</code>), para executar do -1\u00ba elemento (2), o \u00faltimo, ao 0\u00ba elemento (0), o primeiro.</p> <p>Objetos List implementam todos os m\u00e9todos fornecidos pela interface java.util.List, mais os m\u00e9todos de extens\u00e3o fornecidos pelo Groovy.</p> <pre><code>assert [1, 2, 3] &lt;&lt; 1 == [1, 2, 3, 1]\nassert [1, 2, 3] + [1] == [1, 2, 3, 1]\nassert [1, 2, 3, 1] - [1] == [2, 3]\nassert [1, 2, 3] * 2 == [1, 2, 3, 1, 2, 3]\nassert [1, [2, 3]].flatten() == [1, 2, 3]\nassert [1, 2, 3].reverse() == [3, 2, 1]\nassert [1, 2, 3].collect { it + 3 } == [4, 5, 6]\nassert [1, 2, 3, 1].unique().size() == 3\nassert [1, 2, 3, 1].count(1) == 2\nassert [1, 2, 3, 4].min() == 1\nassert [1, 2, 3, 4].max() == 4\nassert [1, 2, 3, 4].sum() == 10\nassert [4, 2, 1, 3].sort() == [1, 2, 3, 4]\nassert [4, 2, 1, 3].find { it % 2 == 0 } == 4\nassert [4, 2, 1, 3].findAll { it % 2 == 0 } == [4, 2]\n</code></pre>"},{"location":"pt/archive/basic_training/groovy/#mapas","title":"Mapas","text":"<p>Os mapas s\u00e3o como listas que possuem uma chave arbitr\u00e1ria em vez de um n\u00famero inteiro. Portanto, a sintaxe \u00e9 bem parecida.</p> <pre><code>mapa = [a: 0, b: 1, c: 2]\n</code></pre> <p>Os mapas podem ser acessados em uma sintaxe convencional de colchetes ou como se a chave fosse uma propriedade do mapa.</p> <p>Clique no \u00edcone  para ver explica\u00e7\u00f5es no c\u00f3digo.</p> <pre><code>assert mapa['a'] == 0 // (1)!\nassert mapa.b == 1 // (2)!\nassert mapa.get('c') == 2 // (3)!\n</code></pre> <ol> <li>Usando colchetes.</li> <li>Usando a nota\u00e7\u00e3o de ponto.</li> <li>Usando o m\u00e9todo <code>get</code>.</li> </ol> <p>Para adicionar dados ou modificar um mapa, a sintaxe \u00e9 semelhante \u00e0 adi\u00e7\u00e3o de valores a uma lista:</p> <pre><code>mapa['a'] = 'x' // (1)!\nmapa.b = 'y' // (2)!\nmapa.put('c', 'z') // (3)!\nassert mapa == [a: 'x', b: 'y', c: 'z']\n</code></pre> <ol> <li>Usando colchetes.</li> <li>Usando a nota\u00e7\u00e3o de ponto.</li> <li>Usando o m\u00e9todo put.</li> </ol> <p>Objetos Map implementam todos os m\u00e9todos fornecidos pela interface java.util.Map, mais os m\u00e9todos de extens\u00e3o fornecidos pelo Groovy.</p>"},{"location":"pt/archive/basic_training/groovy/#interpolacao-de-strings","title":"Interpola\u00e7\u00e3o de Strings","text":"<p>Strings podem ser definidas colocando-as entre aspas simples ('') ou duplas (\"\").</p> <pre><code>tipoderaposa = 'r\u00e1pida'\ncordaraposa = ['m', 'a', 'r', 'r', 'o', 'm']\nprintln \"A $tipoderaposa raposa ${cordaraposa.join()}\"\n\nx = 'Ol\u00e1'\nprintln '$x + $y'\n</code></pre> Output<pre><code>A r\u00e1pida raposa marrom\n$x + $y\n</code></pre> <p>Info</p> <p>Observe o uso diferente das sintaxes <code>$</code> e <code>${..}</code> para interpolar express\u00f5es de valor em uma string. A vari\u00e1vel <code>$x</code> n\u00e3o foi expandida, pois estava entre aspas simples.</p> <p>Por fim, strings tamb\u00e9m podem ser definidas usando o caractere <code>/</code> como delimitador. Elas s\u00e3o conhecidas como strings com barras e s\u00e3o \u00fateis para definir express\u00f5es regulares e padr\u00f5es, pois n\u00e3o h\u00e1 necessidade de escapar as barras invertidas. Assim como as strings de aspas duplas, elas permitem interpolar vari\u00e1veis prefixadas com um caractere <code>$</code>.</p> <p>Tente o seguinte para ver a diferen\u00e7a:</p> <pre><code>x = /tic\\tac\\toe/\ny = 'tic\\tac\\toe'\n\nprintln x\nprintln y\n</code></pre> Output<pre><code>tic\\tac\\toe\ntic    ac    oe\n</code></pre>"},{"location":"pt/archive/basic_training/groovy/#strings-de-varias-linhas","title":"Strings de v\u00e1rias linhas","text":"<p>Um bloco de texto que abrange v\u00e1rias linhas pode ser definido delimitando-o com aspas simples ou duplas triplas:</p> <pre><code>texto = \"\"\"\n    E a\u00ed, James.\n    Como voc\u00ea est\u00e1 hoje?\n    \"\"\"\nprintln texto\n</code></pre> <p>Por fim, strings de v\u00e1rias linhas tamb\u00e9m podem ser definidas com strings com barras. Por exemplo:</p> <pre><code>texto = /\n    Esta \u00e9 uma string abrangendo\n    v\u00e1rias linhas com barras!\n    Super legal, n\u00e9?!\n    /\nprintln texto\n</code></pre> <p>Info</p> <p>Como antes, strings de v\u00e1rias linhas dentro de aspas duplas e caracteres de barra suportam interpola\u00e7\u00e3o de vari\u00e1vel, enquanto strings de v\u00e1rias linhas com aspas simples n\u00e3o.</p>"},{"location":"pt/archive/basic_training/groovy/#declaracoes-condicionais-com-if","title":"Declara\u00e7\u00f5es condicionais com if","text":"<p>A instru\u00e7\u00e3o <code>if</code> usa a mesma sintaxe comum em outras linguagens de programa\u00e7\u00e3o, como Java, C, JavaScript, etc.</p> <pre><code>if (&lt; express\u00e3o booleana &gt;) {\n    // ramo verdadeiro\n}\nelse {\n    // ramo falso\n}\n</code></pre> <p>O ramo <code>else</code> \u00e9 opcional. Al\u00e9m disso, as chaves s\u00e3o opcionais quando a ramifica\u00e7\u00e3o define apenas uma \u00fanica instru\u00e7\u00e3o.</p> <pre><code>x = 11\nif (x &gt; 10)\n    println 'Ol\u00e1'\n</code></pre> Output<pre><code>Ol\u00e1\n</code></pre> <p>Tip</p> <p><code>null</code>, strings vazias e cole\u00e7\u00f5es (mapas e listas) vazias s\u00e3o avaliadas como <code>false</code>.</p> <p>Portanto, uma declara\u00e7\u00e3o como:</p> <pre><code>lista = [1, 2, 3]\nif (lista != null &amp;&amp; lista.size() &gt; 0) {\n    println lista\n}\nelse {\n    println 'A lista est\u00e1 vazia'\n}\n</code></pre> <p>Pode ser escrita como:</p> <pre><code>lista = [1, 2, 3]\nif (lista)\n    println lista\nelse\n    println 'A lista est\u00e1 vazia'\n</code></pre> <p>Veja o Groovy-Truth para mais detalhes.</p> <p>Tip</p> <p>Em alguns casos, pode ser \u00fatil substituir a instru\u00e7\u00e3o <code>if</code> por uma express\u00e3o tern\u00e1ria (tamb\u00e9m conhecida como express\u00e3o condicional). Por exemplo:</p> <pre><code>println lista ? lista : 'A lista est\u00e1 vazia'\n</code></pre> <p>A declara\u00e7\u00e3o anterior pode ser ainda mais simplificada usando o operador Elvis, como mostrado abaixo:</p> <pre><code>println lista ?: 'A lista est\u00e1 vazia'\n</code></pre>"},{"location":"pt/archive/basic_training/groovy/#declaracoes-de-loop-com-for","title":"Declara\u00e7\u00f5es de loop com for","text":"<p>A sintaxe cl\u00e1ssica do loop <code>for</code> \u00e9 suportada como mostrado aqui:</p> <pre><code>for (int i = 0; i &lt; 3; i++) {\n    println(\"Ol\u00e1 mundo $i\")\n}\n</code></pre> <p>A itera\u00e7\u00e3o sobre objetos de lista tamb\u00e9m \u00e9 poss\u00edvel usando a sintaxe abaixo:</p> <pre><code>list = ['a', 'b', 'c']\n\nfor (String elem : lista) {\n    println elem\n}\n</code></pre>"},{"location":"pt/archive/basic_training/groovy/#funcoes","title":"Fun\u00e7\u00f5es","text":"<p>\u00c9 poss\u00edvel definir uma fun\u00e7\u00e3o personalizada em um script, conforme mostrado aqui:</p> <pre><code>def fib(int n) {\n    return n &lt; 2 ? 1 : fib(n - 1) + fib(n - 2)\n}\n\nassert fib(10)==89\n</code></pre> <p>Uma fun\u00e7\u00e3o pode receber v\u00e1rios argumentos, separando-os com uma v\u00edrgula. A palavra-chave <code>return</code> pode ser omitida e a fun\u00e7\u00e3o retorna implicitamente o valor da \u00faltima express\u00e3o avaliada. Al\u00e9m disso, tipos expl\u00edcitos podem ser omitidos, embora n\u00e3o sejam recomendados:</p> <pre><code>def fact(n) {\n    n &gt; 1 ? n * fact(n - 1) : 1\n}\n\nassert fact(5) == 120\n</code></pre>"},{"location":"pt/archive/basic_training/groovy/#clausuras","title":"Clausuras","text":"<p>Clausuras s\u00e3o o canivete su\u00ed\u00e7o da programa\u00e7\u00e3o com Nextflow/Groovy. Resumindo, uma clausura \u00e9 um bloco de c\u00f3digo que pode ser passado como um argumento para uma fun\u00e7\u00e3o. Clausuras tamb\u00e9m podem ser usadas para definir uma fun\u00e7\u00e3o an\u00f4nima.</p> <p>Mais formalmente, uma clausura permite a defini\u00e7\u00e3o de fun\u00e7\u00f5es como objetos de primeira classe.</p> <pre><code>quadrado = { it * it }\n</code></pre> <p>As chaves ao redor da express\u00e3o <code>it * it</code> informam ao interpretador de scripts para tratar essa express\u00e3o como c\u00f3digo. O identificador <code>it</code> \u00e9 uma vari\u00e1vel impl\u00edcita que representa o valor que \u00e9 passado para a fun\u00e7\u00e3o quando ela \u00e9 invocada.</p> <p>Depois de compilado, o objeto de fun\u00e7\u00e3o \u00e9 atribu\u00eddo \u00e0 vari\u00e1vel <code>quadrado</code> como qualquer outra atribui\u00e7\u00e3o de vari\u00e1vel mostrada anteriormente. Para invocar a execu\u00e7\u00e3o da clausura, use o m\u00e9todo especial <code>call</code> ou simplesmente use os par\u00eanteses para especificar o(s) par\u00e2metro(s) da clausura. Por exemplo:</p> <pre><code>assert quadrado.call(5) == 25\nassert quadrado(9) == 81\n</code></pre> <p>Da forma como foi mostrado, isso pode n\u00e3o parecer interessante, mas agora podemos passar a fun\u00e7\u00e3o <code>quadrado</code> como um argumento para outras fun\u00e7\u00f5es ou m\u00e9todos. Algumas fun\u00e7\u00f5es embutidas aceitam uma fun\u00e7\u00e3o como esta como um argumento. Um exemplo \u00e9 o m\u00e9todo <code>collect</code> em listas:</p> <pre><code>x = [1, 2, 3, 4].collect(quadrado)\nprintln x\n</code></pre> Output<pre><code>[1, 4, 9, 16]\n</code></pre> <p>Por padr\u00e3o, as clausuras recebem um \u00fanico par\u00e2metro chamado <code>it</code>. Para dar a ele um nome diferente, use a sintaxe <code>-&gt;</code>. Por exemplo:</p> <pre><code>quadrado = { num -&gt; num * num }\n</code></pre> <p>Tamb\u00e9m \u00e9 poss\u00edvel definir clausuras com v\u00e1rios par\u00e2metros com nomes personalizados.</p> <p>Por exemplo, quando o m\u00e9todo <code>each()</code> \u00e9 aplicado a um mapa, ele pode receber uma clausura com dois argumentos, para os quais passa o par chave-valor para cada entrada no objeto <code>Map</code>. Por exemplo:</p> <pre><code>imprimirMapa = { a, b -&gt; println \"$a com o valor $b\" }\nvalores = [\"Yue\": \"Wu\", \"Mark\": \"Williams\", \"Sudha\": \"Kumari\"]\nvalores.each(imprimirMapa)\n</code></pre> Output<pre><code>Yue com o valor Wu\nMark com o valor Williams\nSudha com o valor Kumari\n</code></pre> <p>Uma clausura tem duas outras caracter\u00edsticas importantes.</p> <p>Primeiro, ela pode acessar e modificar vari\u00e1veis no escopo em que est\u00e1 definida.</p> <p>Em segundo lugar, uma clausura pode ser definida de maneira an\u00f4nima, o que significa que n\u00e3o recebe um nome e \u00e9 definida no local em que precisa ser usada.</p> <p>Para um exemplo mostrando esses dois recursos, consulte o seguinte trecho de c\u00f3digo:</p> <pre><code>resultado = 0 // (1)!\nvalores = [\"China\": 1, \"India\": 2, \"USA\": 3] // (2)!\nvalores.keySet().each { resultado += valores[it] } // (3)!\nprintln resultado\n</code></pre> <ol> <li>Define uma vari\u00e1vel global.</li> <li>Define um objeto de mapa.</li> <li>Chama o m\u00e9todo <code>each</code> passando o objeto de clausura que modifica a vari\u00e1vel <code>resultado</code>.</li> </ol> <p>Saiba mais sobre clausuras na documenta\u00e7\u00e3o do Groovy.</p>"},{"location":"pt/archive/basic_training/groovy/#mais-recursos","title":"Mais recursos","text":"<p>A documenta\u00e7\u00e3o completa da linguagem Groovy est\u00e1 dispon\u00edvel nesse link.</p> <p>Um \u00f3timo recurso para dominar a sintaxe do Apache Groovy \u00e9 o livro: Groovy in Action.</p>"},{"location":"pt/archive/basic_training/intro/","title":"Intro","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/archive/basic_training/intro/#introducao","title":"Introdu\u00e7\u00e3o","text":""},{"location":"pt/archive/basic_training/intro/#conceitos-basicos","title":"Conceitos b\u00e1sicos","text":"<p>O Nextflow \u00e9 tanto um motor de orquestra\u00e7\u00e3o de fluxo de trabalho quanto uma linguagem de dom\u00ednio espec\u00edfico (Domain-Specific Language - DSL) que facilita a escrita de fluxos de trabalho computacionais que fazem uso intensivo de dados.</p> <p>Ele foi projetado com base na ideia de que a plataforma Linux \u00e9 a l\u00edngua franca da ci\u00eancia de dados. O Linux fornece muitas ferramentas de linha de comando que, ainda que simples, s\u00e3o poderosas ferramentas de script que, quando encadeadas, facilitam manipula\u00e7\u00f5es complexas de dados.</p> <p>O Nextflow estende essa abordagem, adicionando a capacidade de definir intera\u00e7\u00f5es complexas entre programas e um ambiente de computa\u00e7\u00e3o paralela de alto n\u00edvel, baseado no modelo de programa\u00e7\u00e3o Dataflow. Os principais recursos do Nextflow s\u00e3o:</p> <ul> <li>Portabilidade e reprodutibilidade de fluxos de trabalho</li> <li>Escalabilidade na paraleliza\u00e7\u00e3o e na implanta\u00e7\u00e3o</li> <li>Integra\u00e7\u00e3o de ferramentas j\u00e1 existentes, sistemas e padr\u00f5es da ind\u00fastria</li> </ul>"},{"location":"pt/archive/basic_training/intro/#processos-e-canais","title":"Processos e Canais","text":"<p>Na pr\u00e1tica, um fluxo de trabalho Nextflow \u00e9 feito juntando diferentes processos. Cada processo pode ser escrito em qualquer linguagem de script que possa ser executada pela plataforma Linux (Bash, Perl, Ruby, Python, etc.).</p> <p>Os processos s\u00e3o executados de forma independente e isolados uns dos outros, ou seja, n\u00e3o compartilham um estado (grav\u00e1vel) comum. A \u00fanica maneira de eles se comunicarem \u00e9 por meio de filas ass\u00edncronas, chamadas de <code>canais</code>, onde o primeiro elemento a entrar, \u00e9 o primeiro a sair (FIFO - First-in-First-out).</p> <p>Qualquer processo pode definir um ou mais <code>canais</code> como uma <code>entrada</code> e <code>sa\u00edda</code>. A intera\u00e7\u00e3o entre esses processos e, em \u00faltima an\u00e1lise, o pr\u00f3prio fluxo de execu\u00e7\u00e3o do fluxo de trabalho, \u00e9 definido implicitamente por essas declara\u00e7\u00f5es de <code>entrada</code> e <code>sa\u00edda</code>.</p>"},{"location":"pt/archive/basic_training/intro/#abstracao-de-execucao","title":"Abstra\u00e7\u00e3o de execu\u00e7\u00e3o","text":"<p>Enquanto um processo define qual comando ou <code>script</code> deve ser executado, o executor determina como esse <code>script</code> \u00e9 executado na plataforma alvo.</p> <p>Se n\u00e3o for especificado de outra forma, os processos s\u00e3o executados no computador local. O executor local \u00e9 muito \u00fatil para fins de desenvolvimento e teste de fluxos de trabalho, no entanto, para fluxos de trabalho computacionais do mundo real, uma plataforma de computa\u00e7\u00e3o de alto desempenho (High-Performance Computing - HPC) ou de computa\u00e7\u00e3o em nuvem geralmente \u00e9 necess\u00e1ria.</p> <p>Em outras palavras, o Nextflow fornece uma abstra\u00e7\u00e3o entre a l\u00f3gica funcional do fluxo de trabalho e o sistema de execu\u00e7\u00e3o subjacente (ou sistema de tempo de execu\u00e7\u00e3o). Assim, \u00e9 poss\u00edvel escrever um fluxo de trabalho que seja executado perfeitamente em seu computador, em um cluster ou na nuvem, sem ser modificado. Voc\u00ea simplesmente define a plataforma de execu\u00e7\u00e3o alvo no arquivo de configura\u00e7\u00e3o.</p> <p></p>"},{"location":"pt/archive/basic_training/intro/#linguagem-de-script","title":"Linguagem de script","text":"<p>O Nextflow implementa uma DSL declarativa que simplifica a escrita de fluxos de trabalho complexos de an\u00e1lise de dados como uma extens\u00e3o de uma linguagem de programa\u00e7\u00e3o de uso geral.</p> <p>Essa abordagem torna o Nextflow flex\u00edvel \u2014 ele fornece os benef\u00edcios de uma DSL concisa para lidar com casos de uso recorrentes com facilidade e a flexibilidade e o poder de uma linguagem de programa\u00e7\u00e3o de prop\u00f3sito geral para lidar com casos extremos no mesmo ambiente de computa\u00e7\u00e3o. Isso seria dif\u00edcil de implementar usando uma abordagem puramente declarativa.</p> <p>Em termos pr\u00e1ticos, a linguagem de script Nextflow \u00e9 uma extens\u00e3o da linguagem de programa\u00e7\u00e3o Groovy a qual, por sua vez, \u00e9 um superconjunto da linguagem de programa\u00e7\u00e3o Java. Groovy pode ser pensado como \"Python para Java\", pois simplifica a escrita do c\u00f3digo e \u00e9 mais acess\u00edvel.</p>"},{"location":"pt/archive/basic_training/intro/#seu-primeiro-script","title":"Seu primeiro script","text":"<p>Aqui voc\u00ea executar\u00e1 seu primeiro script Nextflow (<code>hello.nf</code>), que veremos linha por linha.</p> <p>Neste exemplo ilustrativo, o script recebe no primeiro processo uma string de entrada (um par\u00e2metro chamado <code>params.greeting</code>) e a divide em blocos de seis caracteres. O segundo processo converte os caracteres em mai\u00fasculas. O resultado \u00e9 ent\u00e3o finalmente exibido na tela.</p>"},{"location":"pt/archive/basic_training/intro/#codigo-em-nextflow","title":"C\u00f3digo em Nextflow","text":"<p>Info</p> <p>Clique no \u00edcone  no c\u00f3digo para ver explica\u00e7\u00f5es.</p> nf-training/hello.nf<pre><code>#!/usr/bin/env nextflow\n// (1)!\n\nparams.greeting = 'Hello world!' // (2)!\ngreeting_ch = Channel.of(params.greeting) // (3)!\n\nprocess SPLITLETTERS { // (4)!\n    input: // (5)!\n    val x // (6)!\n\n    output: // (7)!\n    path 'chunk_*' // (8)!\n\n    script: // (9)!\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n} // (10)!\n\nprocess CONVERTTOUPPER { // (11)!\n    input: // (12)!\n    path y // (13)!\n\n    output: // (14)!\n    stdout // (15)!\n\n    script: // (16)!\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]'\n    \"\"\"\n} // (17)!\n\nworkflow { // (18)!\n    letters_ch = SPLITLETTERS(greeting_ch) // (19)!\n    results_ch = CONVERTTOUPPER(letters_ch.flatten()) // (20)!\n    results_ch.view { it } // (21)!\n} // (22)!\n</code></pre> <ol> <li>O c\u00f3digo come\u00e7a com um shebang, que declara o Nextflow como o interpretador.</li> <li>Declara um par\u00e2metro <code>greeting</code> que \u00e9 inicializado com o valor 'Hello world!'.</li> <li>Inicializa um canal chamado <code>greeting_ch</code>, que cont\u00e9m o valor de <code>params.greeting</code>. Os canais s\u00e3o o tipo de entrada para processos no Nextflow.</li> <li>Inicia o primeiro bloco do processo, definido como <code>SPLITLETTERS</code>.</li> <li>Declara\u00e7\u00e3o de entrada para o processo <code>SPLITLETTERS</code>. As entradas podem ser valores (<code>val</code>), arquivos ou caminhos (<code>path</code>), ou ainda outros qualificadores (veja aqui).</li> <li>Diz ao processo para esperar um valor de entrada (<code>val</code>), que atribu\u00edmos \u00e0 vari\u00e1vel 'x'.</li> <li>Declara\u00e7\u00e3o de sa\u00edda para o processo <code>SPLITLETTERS</code>.</li> <li>Diz ao processo para esperar um ou mais arquivos de sa\u00edda (<code>path</code>), com um nome de arquivo come\u00e7ando com 'chunk_', como sa\u00edda do script. O processo envia a sa\u00edda como um canal.</li> <li>Tr\u00eas aspas duplas iniciam e terminam o bloco de c\u00f3digo para executar este processo. Dentro est\u00e1 o c\u00f3digo a ser executado \u2014 imprimindo o valor de <code>entrada</code> x (chamado usando o prefixo do s\u00edmbolo de d\u00f3lar [$]), dividindo a string em peda\u00e7os com um comprimento de 6 caracteres (\"Hello \" e \"world!\") e salvando cada um para um arquivo (chunk_aa e chunk_ab).</li> <li>Fim do primeiro bloco de processo.</li> <li>Inicia o segundo bloco de processo, definido como <code>CONVERTTOUPPER</code>.</li> <li>Declara\u00e7\u00e3o de entrada para o processo <code>CONVERTTOUPPER</code>.</li> <li>Diz ao processo para esperar um ou mais arquivos de <code>entrada</code> (<code>path</code>; ou seja, chunk_aa e chunk_ab), que atribu\u00edmos \u00e0 vari\u00e1vel 'y'.</li> <li>Declara\u00e7\u00e3o de sa\u00edda para o processo <code>CONVERTTOUPPER</code>.</li> <li>Diz ao processo para esperar a sa\u00edda padr\u00e3o (stdout) como sa\u00edda e envia essa sa\u00edda como um canal.</li> <li>Tr\u00eas aspas duplas iniciam e terminam o bloco de c\u00f3digo para executar este processo. Dentro do bloco, h\u00e1 um script para ler arquivos (cat) usando a vari\u00e1vel de entrada '$y' e, em seguida, um pipe (|) para a convers\u00e3o em mai\u00fasculas, imprimindo na sa\u00edda padr\u00e3o.</li> <li>Fim do segundo bloco de processo.</li> <li>In\u00edcio do bloco de fluxo de trabalho (<code>workflow</code>) onde cada processo pode ser chamado.</li> <li>Execute o processo <code>SPLITLETTERS</code> no <code>greeting_ch</code> (tamb\u00e9m conhecido como canal de sauda\u00e7\u00e3o) e armazene a sa\u00edda no canal <code>letters_ch</code>.</li> <li>Execute o processo <code>CONVERTTOUPPER</code> no canal de letras <code>letters_ch</code>, que \u00e9 achatado usando o operador <code>.flatten()</code>. Isso transforma o canal de entrada de forma que cada item seja um elemento separado. Armazenamos a sa\u00edda no canal <code>results_ch</code>.</li> <li>A sa\u00edda final (no canal <code>results_ch</code>) \u00e9 impressa na tela usando o operador <code>view</code> (aplicado ao nome do canal).</li> <li>Fim do bloco do fluxo de trabalho (<code>workflow</code>).</li> </ol> <p>O uso do operador <code>.flatten()</code> aqui \u00e9 para dividir os dois arquivos em dois itens separados para serem colocados no pr\u00f3ximo processo (caso contr\u00e1rio, eles seriam tratados como um \u00fanico elemento).</p>"},{"location":"pt/archive/basic_training/intro/#hora-de-praticar","title":"Hora de praticar","text":"<p>Agora copie o exemplo acima em seu editor de texto favorito e salve-o em um arquivo chamado <code>hello.nf</code>.</p> <p>Warning</p> <p>Para o tutorial do Gitpod, verifique se voc\u00ea est\u00e1 na pasta chamada <code>nf-training</code></p> <p>Execute o script digitando o seguinte comando em seu terminal:</p> <pre><code>nextflow run hello.nf\n</code></pre> <p>A sa\u00edda ser\u00e1 semelhante ao texto mostrado abaixo:</p> <pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `hello.nf` [cheeky_keller] DSL2 - revision: 197a0e289a\nexecutor &gt;  local (3)\n[31/52c31e] process &gt; SPLITLETTERS (1)   [100%] 1 of 1 \u2714\n[37/b9332f] process &gt; CONVERTTOUPPER (2) [100%] 2 of 2 \u2714\nHELLO\nWORLD!\n</code></pre> <p>A sa\u00edda padr\u00e3o mostra (linha por linha):</p> <ol> <li>A vers\u00e3o do Nextflow que foi executada.</li> <li>Os nomes do script e da vers\u00e3o.</li> <li>O executor usado (no caso acima: local).</li> <li>O primeiro processo \u00e9 executado uma vez, o que significa que houve 1 tarefa. A linha come\u00e7a com um valor hexadecimal exclusivo (consulte a dica abaixo) e termina com a porcentagem e outras informa\u00e7\u00f5es de conclus\u00e3o da tarefa.</li> <li>O segundo processo \u00e9 executado duas vezes (uma vez para chunk_aa e outra para chunk_ab), o que significa duas tarefas.</li> <li>A string de resultado de stdout \u00e9 impressa na tela.</li> </ol> <p>Info</p> <p>Os n\u00fameros hexadecimais, como <code>31/52c31e</code>, identificam de forma \u00fanica a execu\u00e7\u00e3o do processo. Esses n\u00fameros tamb\u00e9m s\u00e3o o prefixo dos diret\u00f3rios onde cada tarefa \u00e9 executado. Voc\u00ea pode inspecionar os arquivos produzidos mudando para o diret\u00f3rio <code>$PWD/work</code> e usando esses n\u00fameros para encontrar o caminho de execu\u00e7\u00e3o espec\u00edfico da tarefa.</p> <p>Tip</p> <p>O segundo processo \u00e9 executado duas vezes, em dois diret\u00f3rios de trabalho diferentes para cada arquivo de entrada. A sa\u00edda de log ANSI do Nextflow \u00e9 atualizada dinamicamente conforme o fluxo de trabalho \u00e9 executado; no exemplo anterior, o diret\u00f3rio de trabalho <code>[37/b9332f]</code> \u00e9 o segundo dos dois diret\u00f3rios que foram processados (sobrescrevendo o log com o primeiro). Para imprimir para a tela todos os caminhos relevantes, desative a sa\u00edda de log ANSI usando o sinalizador <code>-ansi-log</code> (por exemplo, <code>nextflow run hello.nf -ansi-log false</code>).</p> <p>Vale ressaltar que o processo <code>CONVERTTOUPPER</code> \u00e9 executado em paralelo, portanto n\u00e3o h\u00e1 garantia de que a inst\u00e2ncia que processa a primeira divis\u00e3o (o bloco Hello) ser\u00e1 executada antes daquela que processa a segundo divis\u00e3o (o bloco world!).</p> <p>Assim, pode ser que seu resultado final seja impresso em uma ordem diferente:</p> <pre><code>WORLD!\nHELLO\n</code></pre>"},{"location":"pt/archive/basic_training/intro/#modifique-e-retome","title":"Modifique e retome","text":"<p>O Nextflow acompanha todos os processos executados em seu fluxo de trabalho. Se voc\u00ea modificar algumas partes do seu script, apenas os processos alterados ser\u00e3o executados novamente. A execu\u00e7\u00e3o dos processos que n\u00e3o foram alterados ser\u00e1 ignorada e o resultado armazenado em cache ser\u00e1 usado em seu lugar.</p> <p>Isso permite testar ou modificar parte do fluxo de trabalho sem precisar execut\u00e1-lo novamente do zero.</p> <p>Para este tutorial, modifique o processo <code>CONVERTTOUPPER</code> do exemplo anterior, substituindo o script do processo pela string <code>rev $y</code>, para que o processo fique assim:</p> <pre><code>process CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    rev $y\n    \"\"\"\n}\n</code></pre> <p>Em seguida, salve o arquivo com o mesmo nome e execute-o adicionando a op\u00e7\u00e3o <code>-resume</code> \u00e0 linha de comando:</p> <pre><code>$ nextflow run hello.nf -resume\n\nN E X T F L O W  ~  version 23.04.1\nLaunching `hello.nf` [zen_colden] DSL2 - revision: 0676c711e8\nexecutor &gt;  local (2)\n[31/52c31e] process &gt; SPLITLETTERS (1)   [100%] 1 of 1, cached: 1 \u2714\n[0f/8175a7] process &gt; CONVERTTOUPPER (1) [100%] 2 of 2 \u2714\n!dlrow\n olleH\n</code></pre> <p>Voc\u00ea ver\u00e1 que a execu\u00e7\u00e3o do processo <code>SPLITLETTERS</code> \u00e9 ignorada (o ID da tarefa \u00e9 o mesmo da primeira sa\u00edda) \u2014 seus resultados s\u00e3o recuperados do cache. O segundo processo \u00e9 executado conforme o esperado, imprimindo as strings invertidas.</p> <p>Info</p> <p>Os resultados do fluxo de trabalho s\u00e3o armazenados em cache por padr\u00e3o no diret\u00f3rio <code>$PWD/work</code>. Dependendo do seu script, esta pasta pode ocupar muito espa\u00e7o em disco. Se tiver certeza de que n\u00e3o precisar\u00e1 retomar a execu\u00e7\u00e3o do fluxo de trabalho, limpe esta pasta periodicamente.</p>"},{"location":"pt/archive/basic_training/intro/#parametros-do-fluxo-de-trabalho","title":"Par\u00e2metros do fluxo de trabalho","text":"<p>Os par\u00e2metros de fluxo de trabalho s\u00e3o declarados simplesmente adicionando o prefixo <code>params</code> a um nome de vari\u00e1vel, separando-os por um caractere de ponto. Seu valor pode ser especificado na linha de comando prefixando o nome do par\u00e2metro com um tra\u00e7o duplo, ou seja, <code>--nomeParametro</code>.</p> <p>Agora, vamos tentar executar o exemplo anterior especificando um par\u00e2metro de string de entrada diferente, conforme mostrado abaixo:</p> <pre><code>nextflow run hello.nf --greeting 'Bonjour le monde!'\n</code></pre> <p>A string especificada na linha de comando substituir\u00e1 o valor padr\u00e3o do par\u00e2metro. A sa\u00edda ficar\u00e1 assim:</p> <pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `hello.nf` [goofy_kare] DSL2 - revision: 0676c711e8\nexecutor &gt;  local (4)\n[8b/7c7d13] process &gt; SPLITLETTERS (1)   [100%] 1 of 1 \u2714\n[58/3b2df0] process &gt; CONVERTTOUPPER (3) [100%] 3 of 3 \u2714\nuojnoB\nm el r\n!edno\n</code></pre>"},{"location":"pt/archive/basic_training/intro/#em-formato-de-dag","title":"Em formato de DAG","text":"<p>Para entender melhor como o Nextflow est\u00e1 lidando com os dados neste fluxo de trabalho, abaixo est\u00e1 uma figura tipo DAG para visualizar todas as entradas (<code>input</code>), sa\u00eddas (<code>output</code>), canais (<code>channel</code>) e processos (<code>process</code>):</p> <p></p>"},{"location":"pt/archive/basic_training/modules/","title":"Modules","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/archive/basic_training/modules/#modularizacao","title":"Modulariza\u00e7\u00e3o","text":"<p>A defini\u00e7\u00e3o de bibliotecas modulares simplifica a escrita de fluxos de trabalho complexos de an\u00e1lise de dados, al\u00e9m tornar o reuso de processos mais f\u00e1cil.</p> <p>Ao usar o exemplo <code>hello.nf</code> da se\u00e7\u00e3o de introdu\u00e7\u00e3o, n\u00f3s converteremos os processos do fluxo de trabalho em m\u00f3dulos e, em seguida, executaremos estes processos dentro do escopo <code>workflow</code> de diferentes formas.</p>"},{"location":"pt/archive/basic_training/modules/#modulos","title":"M\u00f3dulos","text":"<p>A DSL2 do Nextflow permite a defini\u00e7\u00e3o de scripts de m\u00f3dulos aut\u00f4nomos que podem ser inclu\u00eddos e compartilhados em v\u00e1rios fluxos de trabalho. Cada m\u00f3dulo pode conter sua pr\u00f3pria defini\u00e7\u00e3o de <code>process</code> ou <code>workflow</code>.</p>"},{"location":"pt/archive/basic_training/modules/#importando-modulos","title":"Importando m\u00f3dulos","text":"<p>Os componentes definidos no script do m\u00f3dulo podem ser importados para outros scripts do Nextflow usando a instru\u00e7\u00e3o <code>include</code>. Isso permite que voc\u00ea armazene esses componentes em arquivos separados para que possam ser reutilizados em v\u00e1rios fluxos de trabalho.</p> <p>Usando o exemplo <code>hello.nf</code>, podemos fazer isso:</p> <ul> <li>Criando um arquivo chamado <code>modules.nf</code> no mesmo diret\u00f3rio do <code>hello.nf</code>.</li> <li>Copiando e colando as duas defini\u00e7\u00f5es de processo para <code>SPLITLETTERS</code> e <code>CONVERTTOUPPER</code> em <code>modules.nf</code>.</li> <li>Removendo as defini\u00e7\u00f5es <code>process</code> no script <code>hello.nf</code>.</li> <li>Importando os processos de <code>modules.nf</code> dentro do script <code>hello.nf</code> em qualquer lugar acima da defini\u00e7\u00e3o de <code>workflow</code>:</li> </ul> <pre><code>include { SPLITLETTERS } from './modules.nf'\ninclude { CONVERTTOUPPER } from './modules.nf'\n</code></pre> <p>Note</p> <p>Em geral, voc\u00ea deve usar caminhos relativos para definir a localiza\u00e7\u00e3o dos scripts do m\u00f3dulo usando o prefixo <code>./</code>.</p> <p>Exercise</p> <p>Crie um arquivo <code>modules.nf</code> com os processos previamente definidos no script <code>hello.nf</code>. Em seguida, remova esses processos de <code>hello.nf</code> e adicione as defini\u00e7\u00f5es <code>include</code> mostradas acima.</p> Solution <p>O script <code>hello.nf</code> deve ser similar a este:</p> <pre><code>#!/usr/bin/env nextflow\n\nparams.greeting  = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\n\ninclude { SPLITLETTERS   } from './modules.nf'\ninclude { CONVERTTOUPPER } from './modules.nf'\n\nworkflow {\n    letters_ch = SPLITLETTERS(greeting_ch)\n    results_ch = CONVERTTOUPPER(letters_ch.flatten())\n    results_ch.view { it }\n}\n</code></pre> <p>Voc\u00ea deve ter o seguinte c\u00f3digo em <code>./modules.nf</code>:</p> <pre><code>process SPLITLETTERS {\n    input:\n    val x\n\n    output:\n    path 'chunk_*'\n\n    script:\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]'\n    \"\"\"\n}\n</code></pre> <p>Agora n\u00f3s modularizamos os processos, o que faz com que o c\u00f3digo seja reutiliz\u00e1vel.</p>"},{"location":"pt/archive/basic_training/modules/#importacoes-multiplas","title":"Importa\u00e7\u00f5es m\u00faltiplas","text":"<p>Se um script de m\u00f3dulo Nextflow contiver v\u00e1rias defini\u00e7\u00f5es de <code>process</code>, elas tamb\u00e9m podem ser importadas usando uma \u00fanica instru\u00e7\u00e3o <code>include</code>, conforme mostrado no exemplo abaixo:</p> <pre><code>include { SPLITLETTERS; CONVERTTOUPPER } from './modules.nf'\n</code></pre>"},{"location":"pt/archive/basic_training/modules/#apelidos-dos-modulos","title":"Apelidos dos m\u00f3dulos","text":"<p>Ao incluir um componente de um m\u00f3dulo, \u00e9 poss\u00edvel especificar um apelido para os processos usando a declara\u00e7\u00e3o <code>as</code>. Isso permite a inclus\u00e3o e a invoca\u00e7\u00e3o do mesmo componente v\u00e1rias vezes usando diferentes apelidos:</p> <pre><code>#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\n\ninclude { SPLITLETTERS as SPLITLETTERS_one } from './modules.nf'\ninclude { SPLITLETTERS as SPLITLETTERS_two } from './modules.nf'\n\ninclude { CONVERTTOUPPER as CONVERTTOUPPER_one } from './modules.nf'\ninclude { CONVERTTOUPPER as CONVERTTOUPPER_two } from './modules.nf'\n\nworkflow {\n    letters_ch1 = SPLITLETTERS_one(greeting_ch)\n    results_ch1 = CONVERTTOUPPER_one(letters_ch1.flatten())\n    results_ch1.view { it }\n\n    letters_ch2 = SPLITLETTERS_two(greeting_ch)\n    results_ch2 = CONVERTTOUPPER_two(letters_ch2.flatten())\n    results_ch2.view { it }\n}\n</code></pre> <p>Exercise</p> <p>Salve o trecho anterior como <code>hello.2.nf</code>, e tente adivinhar qual sa\u00edda ser\u00e1 mostrada na tela.</p> Solution <p>A sa\u00edda de <code>hello.2.nf</code> deve ser semelhante a essa:</p> Output<pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `hello.2.nf` [crazy_shirley] DSL2 - revision: 99f6b6e40e\nexecutor &gt;  local (6)\n[2b/ec0395] process &gt; SPLITLETTERS_one (1)   [100%] 1 of 1 \u2714\n[d7/be3b77] process &gt; CONVERTTOUPPER_one (1) [100%] 2 of 2 \u2714\n[04/9ffc05] process &gt; SPLITLETTERS_two (1)   [100%] 1 of 1 \u2714\n[d9/91b029] process &gt; CONVERTTOUPPER_two (2) [100%] 2 of 2 \u2714\nWORLD!\nHELLO\nHELLO\nWORLD!\n</code></pre> <p>Tip</p> <p>Voc\u00ea pode armazenar cada processo em arquivos separados em subpastas separadas ou combinados em um arquivo grande (ambos s\u00e3o v\u00e1lidos). Voc\u00ea pode encontrar exemplos disso em reposit\u00f3rios p\u00fablicos, como no tutorial de RNA-Seq da Seqera ou em fluxos de trabalho do nf-core, como o nf-core/rnaseq.</p>"},{"location":"pt/archive/basic_training/modules/#definicao-de-saida","title":"Defini\u00e7\u00e3o de sa\u00edda","text":"<p>O Nextflow permite o uso de defini\u00e7\u00f5es de sa\u00edda alternativas em fluxos de trabalho para simplificar seu c\u00f3digo.</p> <p>No exemplo b\u00e1sico anterior (<code>hello.nf</code>), definimos os nomes dos canais para especificar a entrada para o pr\u00f3ximo processo:</p> <pre><code>workflow  {\n    greeting_ch = Channel.of(params.greeting)\n    letters_ch = SPLITLETTERS(greeting_ch)\n    results_ch = CONVERTTOUPPER(letters_ch.flatten())\n    results_ch.view { it }\n}\n</code></pre> <p>Note</p> <p>N\u00f3s movemos o <code>greeting_ch</code> para o escopo <code>workflow</code> para este exerc\u00edcio.</p> <p>Tamb\u00e9m podemos definir explicitamente a sa\u00edda de um canal para outro usando o atributo <code>.out</code>, removendo completamente as defini\u00e7\u00f5es de canal:</p> <pre><code>workflow  {\n    greeting_ch = Channel.of(params.greeting)\n    SPLITLETTERS(greeting_ch)\n    CONVERTTOUPPER(SPLITLETTERS.out.flatten())\n    CONVERTTOUPPER.out.view()\n}\n</code></pre> <p>Se um processo define dois ou mais canais de sa\u00edda, cada canal pode ser acessado indexando o atributo <code>.out</code>, por exemplo, <code>.out[0]</code>, <code>.out[1]</code>, etc. Em nosso exemplo, temos apenas a sa\u00edda <code>[0]'th</code>:</p> <pre><code>workflow  {\n    greeting_ch = Channel.of(params.greeting)\n    SPLITLETTERS(greeting_ch)\n    CONVERTTOUPPER(SPLITLETTERS.out.flatten())\n    CONVERTTOUPPER.out[0].view()\n}\n</code></pre> <p>Alternativamente, a defini\u00e7\u00e3o de <code>output</code> do processo permite o uso da instru\u00e7\u00e3o <code>emit</code> para definir um identificador nomeado que pode ser usado para referenciar o canal no escopo externo.</p> <p>Por exemplo, tente adicionar a instru\u00e7\u00e3o <code>emit</code> no processo <code>CONVERTTOUPPER</code> em seu arquivo <code>modules.nf</code>:</p> modules.nf<pre><code>process SPLITLETTERS {\n    input:\n    val x\n\n    output:\n    path 'chunk_*'\n\n    script:\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout emit: upper\n\n    script:\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]'\n    \"\"\"\n}\n</code></pre> <p>Em seguida, altere o escopo <code>workflow</code> em <code>hello.nf</code> para chamar essa sa\u00edda nomeada espec\u00edfica (observe o <code>.upper</code> adicionado):</p> hello.nf<pre><code>workflow {\n    greeting_ch = Channel.of(params.greeting)\n    SPLITLETTERS(greeting_ch)\n    CONVERTTOUPPER(SPLITLETTERS.out.flatten())\n    CONVERTTOUPPER.out.upper.view { it }\n}\n</code></pre>"},{"location":"pt/archive/basic_training/modules/#usando-saidas-canalizadas","title":"Usando sa\u00eddas canalizadas","text":"<p>Outra maneira de lidar com as sa\u00eddas no escopo <code>workflow</code> \u00e9 usar pipes <code>|</code>.</p> <p>Exercise</p> <p>Tente alterar o script do fluxo de trabalho para o trecho abaixo:</p> <pre><code>workflow {\n    Channel.of(params.greeting) | SPLITLETTERS | flatten | CONVERTTOUPPER | view\n}\n</code></pre> <p>Aqui usamos um pipe que passa a sa\u00edda de um processo como um canal para o pr\u00f3ximo processo sem a necessidade de aplicar <code>.out</code> ao nome do processo.</p>"},{"location":"pt/archive/basic_training/modules/#definicao-do-escopo-workflow","title":"Defini\u00e7\u00e3o do escopo workflow","text":"<p>O escopo <code>workflow</code> permite a defini\u00e7\u00e3o de componentes que definem a invoca\u00e7\u00e3o de um ou mais processos ou operadores:</p> <pre><code>#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\n\ninclude { SPLITLETTERS } from './modules.nf'\ninclude { CONVERTTOUPPER } from './modules.nf'\n\n\nworkflow meu_fluxo_de_trabalho {\n    greeting_ch = Channel.of(params.greeting)\n    SPLITLETTERS(greeting_ch)\n    CONVERTTOUPPER(SPLITLETTERS.out.flatten())\n    CONVERTTOUPPER.out.upper.view { it }\n}\n\nworkflow {\n    meu_fluxo_de_trabalho()\n}\n</code></pre> <p>Por exemplo, o trecho acima define um <code>workflow</code> chamado <code>meu_fluxo_de_trabalho</code>, que pode ser chamado por meio de outra defini\u00e7\u00e3o de <code>workflow</code>.</p> <p>Note</p> <p>Certifique-se de que seu arquivo <code>modules.nf</code> \u00e9 o que cont\u00e9m o <code>emit</code> no processo <code>CONVERTTOUPPER</code>.</p> <p>Warning</p> <p>Um componente de um fluxo de trabalho pode acessar qualquer vari\u00e1vel ou par\u00e2metro definido no escopo externo. No exemplo em execu\u00e7\u00e3o, tamb\u00e9m podemos acessar <code>params.greeting</code> diretamente na defini\u00e7\u00e3o de <code>workflow</code>.</p>"},{"location":"pt/archive/basic_training/modules/#entradas-no-escopo-workflow","title":"Entradas no escopo workflow","text":"<p>Um componente <code>workflow</code> pode declarar um ou mais canais de entrada usando a instru\u00e7\u00e3o <code>take</code>. Por exemplo:</p> <pre><code>#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\n\ninclude { SPLITLETTERS } from './modules.nf'\ninclude { CONVERTTOUPPER } from './modules.nf'\n\nworkflow meu_fluxo_de_trabalho {\n    take:\n    greeting\n\n    main:\n    SPLITLETTERS(greeting)\n    CONVERTTOUPPER(SPLITLETTERS.out.flatten())\n    CONVERTTOUPPER.out.upper.view { it }\n}\n</code></pre> <p>Note</p> <p>Quando a instru\u00e7\u00e3o <code>take</code> \u00e9 usada, a defini\u00e7\u00e3o <code>workflow</code> precisa ser declarada dentro do bloco <code>main</code>.</p> <p>A entrada para o <code>workflow</code> pode ent\u00e3o ser especificada como um argumento:</p> <pre><code>workflow {\n    meu_fluxo_de_trabalho(Channel.of(params.greeting))\n}\n</code></pre>"},{"location":"pt/archive/basic_training/modules/#saidas-no-escopo-workflow","title":"Sa\u00eddas no escopo workflow","text":"<p>Um bloco <code>workflow</code> pode declarar um ou mais canais de sa\u00edda usando a instru\u00e7\u00e3o <code>emit</code>. Por exemplo:</p> <pre><code>workflow meu_fluxo_de_trabalho {\n    take:\n    greeting\n\n    main:\n    SPLITLETTERS(greeting)\n    CONVERTTOUPPER(SPLITLETTERS.out.flatten())\n\n    emit:\n    CONVERTTOUPPER.out.upper\n}\n\nworkflow {\n    meu_fluxo_de_trabalho(Channel.of(params.greeting))\n    meu_fluxo_de_trabalho.out.view()\n}\n</code></pre> <p>Como resultado, podemos usar a nota\u00e7\u00e3o <code>meu_fluxo_de_trabalho.out</code> para acessar as sa\u00eddas de <code>meu_fluxo_de_trabalho</code> na chamada <code>workflow</code>.</p> <p>Tamb\u00e9m podemos declarar sa\u00eddas nomeadas dentro do bloco <code>emit</code>.</p> <pre><code>workflow meu_fluxo_de_trabalho {\n    take:\n    greeting\n\n    main:\n    SPLITLETTERS(greeting)\n    CONVERTTOUPPER(SPLITLETTERS.out.flatten())\n\n    emit:\n    meus_dados = CONVERTTOUPPER.out.upper\n}\n\nworkflow {\n    meu_fluxo_de_trabalho(Channel.of(params.greeting))\n    meu_fluxo_de_trabalho.out.meus_dados.view()\n}\n</code></pre> <p>O resultado do trecho de c\u00f3digo acima pode ser acessado usando <code>meu_fluxo_de_trabalho.out.meus_dados</code>.</p>"},{"location":"pt/archive/basic_training/modules/#chamando-escopos-workflows-nomeados","title":"Chamando escopos workflows nomeados","text":"<p>Dentro de um script <code>main.nf</code> (chamado <code>hello.nf</code> em nosso exemplo), tamb\u00e9m podemos ter v\u00e1rios fluxos de trabalho. Nesse caso, podemos chamar um fluxo de trabalho espec\u00edfico ao executar o c\u00f3digo. Para isso, usamos a chamada de ponto de entrada <code>-entry &lt;nome_do_flux_de_trabalho&gt;</code>.</p> <p>O trecho a seguir tem dois fluxos de trabalho nomeados (<code>meu_fluxo_de_trabalho_um</code> e <code>meu_fluxo_de_trabalho_dois</code>):</p> <pre><code>#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\n\ninclude { SPLITLETTERS as SPLITLETTERS_one } from './modules.nf'\ninclude { SPLITLETTERS as SPLITLETTERS_two } from './modules.nf'\n\ninclude { CONVERTTOUPPER as CONVERTTOUPPER_one } from './modules.nf'\ninclude { CONVERTTOUPPER as CONVERTTOUPPER_two } from './modules.nf'\n\n\nworkflow meu_fluxo_de_trabalho_um {\n    letras_canal1 = SPLITLETTERS_one(params.greeting)\n    resultados_canal1 = CONVERTTOUPPER_one(letters_ch1.flatten())\n    resultados_canal1.view { it }\n}\n\nworkflow meu_fluxo_de_trabalho_dois {\n    letras_canal2 = SPLITLETTERS_two(params.greeting)\n    resultados_canal2 = CONVERTTOUPPER_two(letters_ch2.flatten())\n    resultados_canal2.view { it }\n}\n\nworkflow {\n    meu_fluxo_de_trabalho_um(Channel.of(params.greeting))\n    meu_fluxo_de_trabalho_dois(Channel.of(params.greeting))\n}\n</code></pre> <p>Voc\u00ea pode escolher qual fluxo de trabalho \u00e9 executado usando o sinalizador <code>entry</code>:</p> <pre><code>nextflow run hello.2.nf -entry meu_fluxo_de_trabalho_um\n</code></pre>"},{"location":"pt/archive/basic_training/modules/#escopos-de-parametros","title":"Escopos de par\u00e2metros","text":"<p>Um script de m\u00f3dulo pode definir um ou mais par\u00e2metros ou fun\u00e7\u00f5es personalizadas usando a mesma sintaxe de qualquer outro script Nextflow. Usando os exemplos m\u00ednimos abaixo:</p> Script do m\u00f3dulo (<code>./modules.nf</code>)<pre><code>params.foo = 'Hello'\nparams.bar = 'world!'\n\ndef DIGAOLA() {\n    println \"$params.foo $params.bar\"\n}\n</code></pre> Script principal (<code>./hello.nf</code>)<pre><code>#!/usr/bin/env nextflow\n\nparams.foo = 'Hola'\nparams.bar = 'mundo!'\n\ninclude { DIGAOLA } from './modules.nf'\n\nworkflow {\n    DIGAOLA()\n}\n</code></pre> <p>A execu\u00e7\u00e3o de <code>hello.nf</code> deve imprimir:</p> <pre><code>Hola mundo!\n</code></pre> <p>Como destacado acima, o script imprimir\u00e1 <code>Hola mundo!</code> em vez de <code>Hello world!</code> porque os par\u00e2metros herdados do contexto de inclus\u00e3o s\u00e3o substitu\u00eddos pelas defini\u00e7\u00f5es no arquivo de script onde est\u00e3o sendo inclu\u00eddos.</p> <p>Info</p> <p>Para evitar que sejam ignorados, os par\u00e2metros do fluxo de trabalho devem ser definidos no in\u00edcio do script antes de qualquer declara\u00e7\u00e3o de inclus\u00e3o.</p> <p>A op\u00e7\u00e3o <code>addParams</code> pode ser usada para estender os par\u00e2metros do m\u00f3dulo sem afetar o escopo externo. Por exemplo:</p> <pre><code>#!/usr/bin/env nextflow\n\nparams.foo = 'Hola'\nparams.bar = 'mundo!'\n\ninclude { DIGAOLA } from './modules.nf' addParams(foo: 'Ol\u00e1')\n\nworkflow {\n    DIGAOLA()\n}\n</code></pre> <p>A execu\u00e7\u00e3o do script principal acima deve imprimir:</p> <pre><code>Ol\u00e1 mundo!\n</code></pre>"},{"location":"pt/archive/basic_training/modules/#notas-de-migracao-dsl2","title":"Notas de migra\u00e7\u00e3o DSL2","text":"<p>Para visualizar um resumo das altera\u00e7\u00f5es introduzidas quando o Nextflow migrou da DSL1 para a DSL2, consulte as notas de migra\u00e7\u00e3o da DSL2 na documenta\u00e7\u00e3o oficial do Nextflow.</p>"},{"location":"pt/archive/basic_training/operators/","title":"Operators","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/archive/basic_training/operators/#operadores","title":"Operadores","text":"<p>Operadores s\u00e3o m\u00e9todos para conectar canais, transformar valores emitidos por canais ou executar regras pr\u00f3prias em canais.</p> <p>Existem sete grupos de operadores descritos em detalhe na Documenta\u00e7\u00e3o do Nextflow, estes s\u00e3o:</p> <ol> <li>Operadores de filtragem</li> <li>Operadores de transforma\u00e7\u00e3o</li> <li>Operadores de divis\u00e3o</li> <li>Operadores de combina\u00e7\u00e3o</li> <li>Operadores de bifurca\u00e7\u00e3o</li> <li>Operadores matem\u00e1ticos</li> <li>Outros operadores</li> </ol>"},{"location":"pt/archive/basic_training/operators/#exemplo-basico","title":"Exemplo b\u00e1sico","text":"<p>Clique no \u00edcone  para ver explica\u00e7\u00f5es do c\u00f3digo.</p> <pre><code>nums = Channel.of(1, 2, 3, 4) // (1)!\nquadrados = nums.map { it -&gt; it * it } // (2)!\nquadrados.view() // (3)!\n</code></pre> <ol> <li>Cria um canal de fila que emite quatro valores</li> <li>Cria um novo canal, transformando cada n\u00famero ao quadrado</li> <li>Imprime o conte\u00fado do canal</li> </ol> <p>Para implementar funcionalidades espec\u00edficas operadores tamb\u00e9m podem ser encadeados. Ent\u00e3o, o c\u00f3digo anterior tamb\u00e9m pode ser escrito assim:</p> <pre><code>Channel\n    .of(1, 2, 3, 4)\n    .map { it -&gt; it * it }\n    .view()\n</code></pre>"},{"location":"pt/archive/basic_training/operators/#operadores-basicos","title":"Operadores b\u00e1sicos","text":"<p>Agora iremos explorar alguns dos operadores mais comuns.</p>"},{"location":"pt/archive/basic_training/operators/#view","title":"<code>view()</code>","text":"<p>O operador <code>view</code> imprime os itens emitidos por um canal para o terminal, acrescentando um caractere de quebra de linha ap\u00f3s cada item. Por exemplo:</p> <pre><code>Channel\n    .of('foo', 'bar', 'baz')\n    .view()\n</code></pre> Output<pre><code>foo\nbar\nbaz\n</code></pre> <p>Voc\u00ea tamb\u00e9m pode especificar uma clausura para personalizar como os itens s\u00e3o impressos. Por exemplo:</p> <pre><code>Channel\n    .of('foo', 'bar', 'baz')\n    .view { \"- $it\" }\n</code></pre> Output<pre><code>- foo\n- bar\n- baz\n</code></pre>"},{"location":"pt/archive/basic_training/operators/#map","title":"<code>map()</code>","text":"<p>O operador <code>map</code> aplica uma fun\u00e7\u00e3o de sua escolha em cada item emitido por um canal e retorna os items obtidos como um novo canal. A fun\u00e7\u00e3o aplicada \u00e9 chamada de fun\u00e7\u00e3o de mapeamento e \u00e9 expressa com uma clausura, como demonstrado no exemplo abaixo:</p> <pre><code>Channel\n    .of('ol\u00e1', 'mundo')\n    .map { it -&gt; it.reverse() }\n    .view()\n</code></pre> <p>Um <code>map</code> pode associar uma tupla gen\u00e9rica a cada elemento e pode conter qualquer tipo de dado.</p> <pre><code>Channel\n    .of('ol\u00e1', 'mundo')\n    .map { palavra -&gt; [palavra, palavra.size()] }\n    .view { palavra, comprimento -&gt; \"$palavra cont\u00e9m $comprimento letras\" }\n</code></pre> <p>Exercise</p> <p>Use <code>fromPath</code> para criar um canal emitindo os arquivos fastq que correspondam \u00e0 express\u00e3o <code>data/ggal/*.fq</code>, ent\u00e3o use <code>map</code> para retornar um par contendo o nome e o caminho para o arquivo, e, por fim, use <code>view</code> para imprimir o canal resultante.</p> Solution <pre><code>Channel\n    .fromPath('data/ggal/*.fq')\n    .map { arquivo -&gt; [arquivo.name, arquivo] }\n    .view { nome, arquivo -&gt; \"&gt; $nome : $arquivo\" }\n</code></pre>"},{"location":"pt/archive/basic_training/operators/#mix","title":"<code>mix()</code>","text":"<p>O operador <code>mix</code> combina os itens emitidos por dois (ou mais) canais em um \u00fanico canal.</p> <pre><code>meu_canal_1 = Channel.of(1, 2, 3)\nmeu_canal_2 = Channel.of('a', 'b')\nmeu_canal_3 = Channel.of('z')\n\nmeu_canal_1\n    .mix(meu_canal_2, meu_canal_3)\n    .view()\n</code></pre> Output<pre><code>1\n2\na\n3\nb\nz\n</code></pre> <p>Warning</p> <p>Os itens no canal resultante possuem a mesma ordem dos seus respectivos canais originais. No entanto, n\u00e3o h\u00e1 garantia que o elemento do segundo canal \u00e9 acrescentado ao final dos elementos do primeiro canal. Como se pode observar acima, o elemento <code>a</code> foi impresso antes de <code>3</code>.</p>"},{"location":"pt/archive/basic_training/operators/#flatten","title":"<code>flatten()</code>","text":"<p>O operador <code>flatten</code> transforma um canal de maneira que cada tupla \u00e9 achatada, isto \u00e9, cada entrada \u00e9 emitida como um \u00fanico elemento pelo canal resultante.</p> <pre><code>foo = [1, 2, 3]\nbar = [4, 5, 6]\n\nChannel\n    .of(foo, bar)\n    .flatten()\n    .view()\n</code></pre> Output<pre><code>1\n2\n3\n4\n5\n6\n</code></pre>"},{"location":"pt/archive/basic_training/operators/#collect","title":"<code>collect()</code>","text":"<p>O operador <code>collect</code> coleta todos os itens emitidos por um canal em uma lista e retorna o objeto como uma \u00fanica emiss\u00e3o.</p> <pre><code>Channel\n    .of(1, 2, 3, 4)\n    .collect()\n    .view()\n</code></pre> <p>Isto imprime o valor:</p> Output<pre><code>[1, 2, 3, 4]\n</code></pre> <p>Info</p> <p>O resultado do operador <code>collect</code> \u00e9 um canal de valor.</p>"},{"location":"pt/archive/basic_training/operators/#grouptuple","title":"<code>groupTuple()</code>","text":"<p>O operador <code>groupTuple</code> coleta as tuplas (ou listas) de valores emitidos pelo canal de entrada, agrupando os elementos que possuem a mesma chave. Por fim, ele emite uma nova tupla para cada chave distinta.</p> <p>Por exemplo:</p> <pre><code>Channel\n    .of([1, 'A'], [1, 'B'], [2, 'C'], [3, 'B'], [1, 'C'], [2, 'A'], [3, 'D'])\n    .groupTuple()\n    .view()\n</code></pre> Output<pre><code>[1, [A, B, C]]\n[2, [C, A]]\n[3, [B, D]]\n</code></pre> <p>Esse operador \u00e9 \u00fatil para processar um grupo, juntando elementos que possuem uma propriedade ou uma chave em comum.</p> <p>Exercise</p> <p>Use <code>fromPath</code> para criar um canal emitindo todos os arquivos no diret\u00f3rio <code>data/meta/</code>, ent\u00e3o use <code>map</code> para associar o prefixo <code>baseName</code> a cada arquivo. Por fim, agrupe todo os arquivos que possuem o mesmo prefixo.</p> Solution <pre><code>Channel\n    .fromPath('data/meta/*')\n    .map { arquivo -&gt; tuple(arquivo.baseName, arquivo) }\n    .groupTuple()\n    .view { baseName, arquivo -&gt; \"&gt; $baseName : $arquivo\" }\n</code></pre>"},{"location":"pt/archive/basic_training/operators/#join","title":"<code>join()</code>","text":"<p>O operador <code>join</code> cria um canal que combina os itens emitidos por dois canais que possuam uma chave em comum. Por padr\u00e3o, a chave \u00e9 definida como o primeiro elemento em cada item emitido.</p> <pre><code>esquerda = Channel.of(['X', 1], ['Y', 2], ['Z', 3], ['P', 7])\ndireita = Channel.of(['Z', 6], ['Y', 5], ['X', 4])\nesquerda.join(direita).view()\n</code></pre> Output<pre><code>[Z, 3, 6]\n[Y, 2, 5]\n[X, 1, 4]\n</code></pre> <p>Note</p> <p>Perceba como P est\u00e1 ausente no resultado final.</p>"},{"location":"pt/archive/basic_training/operators/#branch","title":"<code>branch()</code>","text":"<p>O operador <code>branch</code> permite que voc\u00ea envie os itens emitidos por um canal de entrada para um ou mais canais de sa\u00edda.</p> <p>O crit\u00e9rio de sele\u00e7\u00e3o de cada canal de sa\u00edda \u00e9 definido especificando uma clausura que forne\u00e7a uma ou mais express\u00f5es booleanas, cada uma das quais \u00e9 identificada por um r\u00f3tulo \u00fanico. Para a primeira express\u00e3o verdadeira, o item \u00e9 ligado a um canal nomeado com o r\u00f3tulo. Por exemplo:</p> <pre><code>Channel\n    .of(1, 2, 3, 40, 50)\n    .branch {\n        pequeno: it &lt; 10\n        grande: it &gt; 10\n    }\n    .set { resultado }\n\nresultado.pequeno.view { \"$it \u00e9 pequeno\" }\nresultado.grande.view { \"$it \u00e9 grande\" }\n</code></pre> <p>Info</p> <p>O operador <code>branch</code> retorna um objeto multi-canal (isto \u00e9, uma vari\u00e1vel que possui mais de um canal).</p> <p>Note</p> <p>No exemplo acima, o que aconteceria com um valor igual a 10? Para lidar com isso, voc\u00ea pode usar <code>&gt;=</code>.</p>"},{"location":"pt/archive/basic_training/operators/#outros-recursos","title":"Outros recursos","text":"<p>Veja a documenta\u00e7\u00e3o de operadores no site oficial do Nextflow.</p>"},{"location":"pt/archive/basic_training/orientation/","title":"Orienta\u00e7\u00e3o","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/archive/basic_training/orientation/#orientacao","title":"Orienta\u00e7\u00e3o","text":"<p>O ambiente do Gitpod contem dados de teste que ser\u00e3o utilizados nesse treinamento.</p> <p>Note</p> <p>V\u00e1 para este link se voc\u00ea ainda n\u00e3o configurou seu ambiente no Gitpod.</p>"},{"location":"pt/archive/basic_training/orientation/#comecando","title":"Come\u00e7ando","text":"<p>Voc\u00ea ir\u00e1 completar esse m\u00f3dulo na pasta <code>nf-training/</code>.</p> <p>Nessa pasta voc\u00ea ir\u00e1 encontrar v\u00e1rios arquivos de dados (<code>ggal</code>, <code>index</code>, <code>meta</code>...) e tamb\u00e9m alguns scripts e arquivos de configura\u00e7\u00e3o.</p> <pre><code>.\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 ggal\n\u2502   \u2502   \u2514\u2500\u2500 &lt;data files&gt;\n\u2502   \u251c\u2500\u2500 index\n\u2502   \u2502   \u2514\u2500\u2500 &lt;data files&gt;\n\u2502   \u251c\u2500\u2500 meta\n\u2502   \u2502   \u2514\u2500\u2500 &lt;data files&gt;\n\u2502   \u251c\u2500\u2500 prots\n\u2502   \u2502   \u2514\u2500\u2500 &lt;data files&gt;\n\u2502   \u251c\u2500\u2500 reads\n\u2502   \u2502   \u2514\u2500\u2500 &lt;data files&gt;\n\u2502   \u2514\u2500\u2500 test\n\u2502       \u2514\u2500\u2500 &lt;data files&gt;\n\u251c\u2500\u2500 env.yml\n\u251c\u2500\u2500 hello.nf\n\u251c\u2500\u2500 hello_py.nf\n\u251c\u2500\u2500 modules.hello.nf\n\u251c\u2500\u2500 nextflow.config\n\u251c\u2500\u2500 script1.nf\n\u251c\u2500\u2500 script2.nf\n\u251c\u2500\u2500 script3.nf\n\u251c\u2500\u2500 script4.nf\n\u251c\u2500\u2500 script5.nf\n\u251c\u2500\u2500 script6.nf\n\u251c\u2500\u2500 script7.nf\n\u2514\u2500\u2500 snippet.nf\n</code></pre> <p>Cada arquivo ser\u00e1 utilizado nesse treinamento.</p>"},{"location":"pt/archive/basic_training/orientation/#escolhendo-uma-versao-do-nextflow","title":"Escolhendo uma vers\u00e3o do Nextflow","text":"<p>Por padr\u00e3o, o Nextflow ir\u00e1 trazer a \u00faltima vers\u00e3o est\u00e1vel para seu ambiente.</p> <p>No entanto, Nextflow vive em uma evolu\u00e7\u00e3o constante na medida que melhorias s\u00e3o implementadas.</p> <p>As \u00faltimas vers\u00f5es podem ser conferidas no GitHub, aqui.</p> <p>Se voc\u00ea deseja utilizar uma vers\u00e3o espec\u00edfica do Nextflow, voc\u00ea pode configurar a vari\u00e1vel <code>NXF_VER</code> como mostrado abaixo:</p> <pre><code>export NXF_VER=23.10.1\n</code></pre> <p>Exercise</p> <p>Abra o ambiente de treinamento no Gitpod e use o seguinte comando para ir at\u00e9 a pasta <code>nf-customize</code>. Visualize os arquivos nessa pasta utilizando o comando <code>tree</code>:</p> <pre><code>cd /workspaces/training/nf-training\ntree .\n</code></pre>"},{"location":"pt/archive/basic_training/orientation/#variaveis-do-ambiente","title":"Vari\u00e1veis do ambiente","text":"<p>Por padr\u00e3o, o Nextflow ir\u00e1 trazer a \u00faltima vers\u00e3o est\u00e1vel para seu ambiente.</p> <p>No entanto, Nextflow vive em uma evolu\u00e7\u00e3o constante na medida que melhorias s\u00e3o implementadas.</p> <p>As \u00faltimas vers\u00f5es podem ser conferidas no GitHub, aqui.</p> <p>Se voc\u00ea deseja utilizar uma vers\u00e3o espec\u00edfica do Nextflow, voc\u00ea pode configurar a vari\u00e1vel <code>NXF_VER</code> como mostrado abaixo:</p> <pre><code>export NXF_VER=23.10.1\n</code></pre> <p>Note</p> <p>Esse material requer uma vers\u00e3o igual ou posterior a <code>NXF_VER=23.10.1</code>.</p> <p>Se voc\u00ea exportou a vari\u00e1vel <code>NXF_VER</code> como acima, execute <code>nextflow -version</code> novamente para confirmar que suas mudan\u00e7as foram aplicadas.</p>"},{"location":"pt/archive/basic_training/processes/","title":"Processes","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/archive/basic_training/processes/#processos","title":"Processos","text":"<p>No Nextflow, um processo (<code>process</code>) \u00e9 a primitiva de computa\u00e7\u00e3o b\u00e1sica para executar fun\u00e7\u00f5es estrangeiras (ou seja, scripts personalizados ou ferramentas).</p> <p>A defini\u00e7\u00e3o do processo come\u00e7a com a palavra-chave <code>process</code>, seguida pelo nome do processo e, finalmente, o corpo do processo delimitado por chaves.</p> <p>O nome do processo (<code>process</code>) \u00e9 comumente escrito em letras mai\u00fasculas por conven\u00e7\u00e3o.</p> <p>Um processo b\u00e1sico, usando apenas o bloco de defini\u00e7\u00e3o <code>script</code>, se parece com o seguinte:</p> <pre><code>process DIGAOLA {\n    script:\n    \"\"\"\n    echo 'Ol\u00e1 mundo!'\n    \"\"\"\n}\n</code></pre> <p>Em exemplos mais complexos, o corpo do processo pode conter at\u00e9 cinco blocos de defini\u00e7\u00e3o:</p> <ol> <li>Diretivas s\u00e3o declara\u00e7\u00f5es iniciais que definem configura\u00e7\u00f5es opcionais</li> <li>Input (Bloco de entrada) define o(s) canal(is) de entrada esperado(s)</li> <li>Output (Bloco de sa\u00edda) define o(s) canal(is) de sa\u00edda esperado(s)</li> <li>When \u00e9 uma declara\u00e7\u00e3o de cl\u00e1usula opcional para permitir processos condicionais</li> <li>Script \u00e9 uma string que define o comando a ser executado pela tarefa do processo</li> </ol> <p>A sintaxe completa do processo \u00e9 definida da seguinte forma:</p> <p>Clique no \u00edcone  no c\u00f3digo para ver explica\u00e7\u00f5es.</p> <pre><code>process &lt; nome &gt; {\n    [ diretivas ] // (1)!\n\n    input: // (2)!\n    &lt; entradas do processo &gt;\n\n    output: // (3)!\n    &lt; sa\u00eddas do processo &gt;\n\n    when: // (4)!\n    &lt; condi\u00e7\u00e3o &gt;\n\n    [script|shell|exec]: // (5)!\n    \"\"\"\n    &lt; script do usu\u00e1rio a ser executado &gt;\n    \"\"\"\n}\n</code></pre> <ol> <li>Zero, uma ou mais diretivas de processo</li> <li>Zero, uma ou mais entradas para o processo</li> <li>Zero, uma ou mais sa\u00eddas para o processo</li> <li>Uma condicional booleana opcional para acionar a execu\u00e7\u00e3o do processo</li> <li>O comando a ser executado</li> </ol>"},{"location":"pt/archive/basic_training/processes/#script","title":"Script","text":"<p>O bloco <code>script</code> \u00e9 uma string que define o comando a ser executado pelo processo.</p> <p>Um processo pode executar apenas um bloco <code>script</code>. Deve ser a \u00faltima instru\u00e7\u00e3o quando o processo cont\u00e9m declara\u00e7\u00f5es de entrada e sa\u00edda.</p> <p>O bloco <code>script</code> pode ser uma string de uma ou v\u00e1rias linhas. A de v\u00e1rias linhas simplifica a escrita de scripts n\u00e3o triviais compostos por v\u00e1rios comandos abrangendo v\u00e1rias linhas. Por exemplo:</p> <pre><code>process EXEMPLO {\n    script:\n    \"\"\"\n    echo 'Ol\u00e1 mundo!\\nHola mundo!\\nCiao mondo!\\nHallo Welt!' &gt; arquivo\n    cat arquivo | head -n 1 | head -c 5 &gt; pedaco_1.txt\n    gzip -c pedaco_1.txt &gt; pedacos.gz\n    \"\"\"\n}\n\nworkflow {\n    EXEMPLO()\n}\n</code></pre> <p>Por padr\u00e3o, o comando do processo \u00e9 interpretado como um script Bash. No entanto, qualquer outra linguagem de script pode ser usada simplesmente iniciando o script com a declara\u00e7\u00e3o Shebang adequada. Por exemplo:</p> <pre><code>process CODIGOPYTHON {\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    x = 'Ol\u00e1'\n    y = 'mundo!'\n    print (\"%s - %s\" % (x, y))\n    \"\"\"\n}\n\nworkflow {\n    CODIGOPYTHON()\n}\n</code></pre> <p>Tip</p> <p>V\u00e1rias linguagens de programa\u00e7\u00e3o podem ser usadas no mesmo script de fluxo de trabalho. No entanto, para grandes blocos de c\u00f3digo, \u00e9 melhor salv\u00e1-los em arquivos separados e invoc\u00e1-los a partir do script do processo. Pode-se armazenar os scripts espec\u00edficos na pasta <code>./bin/</code>.</p>"},{"location":"pt/archive/basic_training/processes/#parametros-do-script","title":"Par\u00e2metros do script","text":"<p>Par\u00e2metros de script (<code>params</code>) podem ser definidos dinamicamente usando valores vari\u00e1veis. Por exemplo:</p> <pre><code>params.dado = 'Mundo'\n\nprocess FOO {\n    script:\n    \"\"\"\n    echo Ol\u00e1 $params.dado\n    \"\"\"\n}\n\nworkflow {\n    FOO()\n}\n</code></pre> <p>Info</p> <p>Um script de processo pode conter qualquer formato de string suportado pela linguagem de programa\u00e7\u00e3o Groovy. Isso nos permite usar a interpola\u00e7\u00e3o de strings como no script acima ou strings multilinha. Consulte Interpola\u00e7\u00e3o de string para obter mais informa\u00e7\u00f5es.</p> <p>Warning</p> <p>Como o Nextflow usa a mesma sintaxe Bash para substitui\u00e7\u00f5es de vari\u00e1veis em strings, as vari\u00e1veis de ambiente Bash precisam ser escapadas usando o caractere <code>\\</code>. A vers\u00e3o escapada ser\u00e1 resolvida posteriormente, retornando o diret\u00f3rio da tarefa (por exemplo, work/7f/f285b80022d9f61e82cd7f90436aa4/), enquanto <code>$PWD</code> mostraria o diret\u00f3rio onde voc\u00ea est\u00e1 executando o Nextflow.</p> <pre><code>process FOO {\n    script:\n    \"\"\"\n    echo \"O diret\u00f3rio atual \u00e9 \\$PWD\"\n    \"\"\"\n}\n\nworkflow {\n    FOO()\n}\n</code></pre> <p>Pode ser complicado escrever um script que usa muitas vari\u00e1veis Bash. Uma alternativa poss\u00edvel \u00e9 usar uma string de script delimitada por aspas simples</p> <pre><code>process BAR {\n    script:\n    '''\n    echo \"The current directory is $PWD\"\n    '''\n}\n\nworkflow {\n    BAR()\n}\n</code></pre> <p>No entanto, isso bloqueia o uso de vari\u00e1veis Nextflow no script de comando.</p> <p>Outra alternativa \u00e9 usar uma instru\u00e7\u00e3o <code>shell</code> em vez de <code>script</code> e usar uma sintaxe diferente para vari\u00e1veis do Nextflow, por exemplo, <code>!{..}</code>. Isso permite o uso das vari\u00e1veis Nextflow e Bash no mesmo script.</p> <pre><code>params.dado = 'le monde'\n\nprocess BAZ {\n    shell:\n    '''\n    X='Bonjour'\n    echo $X !{params.dado}\n    '''\n}\n\nworkflow {\n    BAZ()\n}\n</code></pre>"},{"location":"pt/archive/basic_training/processes/#scripts-condicionais","title":"Scripts condicionais","text":"<p>O script do processo tamb\u00e9m pode ser definido de maneira completamente din\u00e2mica usando uma instru\u00e7\u00e3o <code>if</code> ou qualquer outra express\u00e3o para avaliar um valor de string. Por exemplo:</p> <pre><code>params.compressor = 'gzip'\nparams.arquivo_a_comprimir = \"$projectDir/data/ggal/transcriptome.fa\"\n\nprocess FOO {\n    input:\n    path arquivo\n\n    script:\n    if (params.compressor == 'gzip')\n        \"\"\"\n        gzip -c $arquivo &gt; ${arquivo}.gz\n        \"\"\"\n    else if (params.compressor == 'bzip2')\n        \"\"\"\n        bzip2 -c $arquivo &gt; ${arquivo}.bz2\n        \"\"\"\n    else\n        throw new IllegalArgumentException(\"Compressor $params.compressor desconhecido\")\n}\n\nworkflow {\n    FOO(params.arquivo_a_comprimir)\n}\n</code></pre>"},{"location":"pt/archive/basic_training/processes/#canais-de-entradas","title":"Canais de entradas","text":"<p>As inst\u00e2ncias dos processos (tarefas) Nextflow s\u00e3o isoladas umas das outras, mas podem se comunicar entre si enviando valores por meio de canais.</p> <p>As entradas determinam implicitamente as depend\u00eancias e a execu\u00e7\u00e3o paralela do processo. A execu\u00e7\u00e3o do processo \u00e9 disparada cada vez que dados novos est\u00e3o prontos para serem consumidos do canal de entrada:</p> <p>O bloco <code>input</code> define os nomes e qualificadores das vari\u00e1veis que se referem aos elementos do canal direcionados ao processo. Voc\u00ea s\u00f3 pode definir um bloco <code>input</code> por vez e deve conter uma ou mais declara\u00e7\u00f5es de entrada.</p> <p>O bloco <code>input</code> segue a sintaxe mostrada abaixo:</p> <pre><code>input:\n&lt;qualificador da vari\u00e1vel de entrada&gt; &lt;nome da vari\u00e1vel de entrada&gt;\n</code></pre>"},{"location":"pt/archive/basic_training/processes/#valores-de-entrada","title":"Valores de entrada","text":"<p>O qualificador <code>val</code> permite receber dados de qualquer tipo como entrada. Ele pode ser acessado no script do processo usando o nome de entrada especificado, conforme mostrado no exemplo a seguir:</p> <pre><code>num = Channel.of(1, 2, 3)\n\nprocess EXEMPLOBASICO {\n    debug true\n\n    input:\n    val x\n\n    script:\n    \"\"\"\n    echo tarefa $x do processo\n    \"\"\"\n}\n\nworkflow {\n    minha_execucacao = EXEMPLOBASICO(num)\n}\n</code></pre> <p>No exemplo acima, o processo \u00e9 executado tr\u00eas vezes, cada vez que um valor \u00e9 recebido do canal <code>num</code> e usado para processar o script. Assim, resulta em uma sa\u00edda semelhante \u00e0 mostrada abaixo:</p> <pre><code>tarefa 3 do processo\ntarefa 1 do processo\ntarefa 2 do processo\n</code></pre> <p>Warning</p> <p>O canal garante que os itens sejam entregues na mesma ordem em que foram enviados - mas - como o processo \u00e9 executado de forma paralela, n\u00e3o h\u00e1 garantia de que sejam processados na mesma ordem em que foram recebidos.</p>"},{"location":"pt/archive/basic_training/processes/#arquivo-e-caminhos-de-entrada","title":"Arquivo e caminhos de entrada","text":"<p>O qualificador <code>path</code> permite a manipula\u00e7\u00e3o de arquivos no contexto de execu\u00e7\u00e3o do processo. Isso significa que o Nextflow ir\u00e1 mover os arquivos necess\u00e1rios para o diret\u00f3rio de execu\u00e7\u00e3o do processo e estes poder\u00e3o ser acessados no script usando o nome especificado na declara\u00e7\u00e3o de entrada.</p> <pre><code>leituras = Channel.fromPath('data/ggal/*.fq')\n\nprocess FOO {\n    debug true\n\n    input:\n    path 'amostra.fastq'\n\n    script:\n    \"\"\"\n    ls amostra.fastq\n    \"\"\"\n}\n\nworkflow {\n    resultado = FOO(leituras)\n}\n</code></pre> <p>O nome do arquivo de entrada tamb\u00e9m pode ser definido usando uma refer\u00eancia de vari\u00e1vel conforme mostrado abaixo:</p> <pre><code>leituras = Channel.fromPath('data/ggal/*.fq')\n\nprocess FOO {\n    debug true\n\n    input:\n    path amostra\n\n    script:\n    \"\"\"\n    ls $amostra\n    \"\"\"\n}\n\nworkflow {\n    resultado = FOO(leituras)\n}\n</code></pre> <p>A mesma sintaxe tamb\u00e9m \u00e9 capaz de lidar com mais de um arquivo de entrada na mesma execu\u00e7\u00e3o e requer apenas a altera\u00e7\u00e3o da composi\u00e7\u00e3o do canal.</p> <pre><code>leituras = Channel.fromPath('data/ggal/*.fq')\n\nprocess FOO {\n    debug true\n\n    input:\n    path amostra\n\n    script:\n    \"\"\"\n    ls -lh $amostra\n    \"\"\"\n}\n\nworkflow {\n    FOO(leituras.collect())\n}\n</code></pre> <p>Warning</p> <p>No passado, o qualificador <code>file</code> era usado para arquivos, mas o qualificador <code>path</code> deve ser preferido ao <code>file</code> para lidar com arquivos de entrada de processo ao usar o Nextflow 19.10.0 ou posterior. Quando um processo declara um arquivo de entrada, os elementos de canal correspondentes devem ser objetos file criados com a fun\u00e7\u00e3o auxiliar de arquivo das f\u00e1bricas de canal espec\u00edficas de arquivo (por exemplo, <code>Channel.fromPath</code> ou <code>Channel.fromFilePairs</code>).</p> <p>Exercise</p> <p>Escreva um script que crie um canal contendo todos as leituras correspondentes ao padr\u00e3o <code>data/ggal/*_1.fq</code> seguido por um processo que os concatene em um \u00fanico arquivo e imprima as primeiras 10 linhas.</p> Solution <pre><code>params.leituras = \"$projectDir/data/ggal/*_1.fq\"\n\nChannel\n    .fromPath(params.leituras)\n    .set { canal_leituras }\n\nprocess CONCATENE {\n    tag \"Concatene todos os arquivos\"\n\n    input:\n    path '*'\n\n    output:\n    path 'top_10_linhas'\n\n    script:\n    \"\"\"\n    cat * &gt; concatenado.txt\n    head -n 10 concatenado.txt &gt; top_10_linhas\n    \"\"\"\n}\n\nworkflow {\n    canal_concatenado = CONCATENE(canal_leituras.collect())\n    canal_concatenado.view()\n}\n</code></pre>"},{"location":"pt/archive/basic_training/processes/#combinando-canais-de-entrada","title":"Combinando canais de entrada","text":"<p>Uma caracter\u00edstica fundamental dos processos \u00e9 a capacidade de lidar com entradas de v\u00e1rios canais. No entanto, \u00e9 importante entender como o conte\u00fado do canal e sua sem\u00e2ntica afetam a execu\u00e7\u00e3o de um processo.</p> <p>Considere o seguinte exemplo:</p> <pre><code>canal1 = Channel.of(1, 2, 3)\ncanal2 = Channel.of('a', 'b', 'c')\n\nprocess FOO {\n    debug true\n\n    input:\n    val x\n    val y\n\n    script:\n    \"\"\"\n    echo $x e $y\n    \"\"\"\n}\n\nworkflow {\n    FOO(canal1, canal2)\n}\n</code></pre> <p>Ambos os canais emitem tr\u00eas valores, portanto o processo \u00e9 executado tr\u00eas vezes, cada vez com um par diferente:</p> <ul> <li><code>(1, a)</code></li> <li><code>(2, b)</code></li> <li><code>(3, c)</code></li> </ul> <p>O que est\u00e1 acontecendo \u00e9 que o processo espera at\u00e9 que haja uma configura\u00e7\u00e3o de entrada completa, ou seja, recebe um valor de entrada de todos os canais declarados como entrada.</p> <p>Quando essa condi\u00e7\u00e3o \u00e9 verificada, ele consome os valores de entrada provenientes dos respectivos canais, gera uma execu\u00e7\u00e3o de tarefa e repete a mesma l\u00f3gica at\u00e9 que um ou mais canais n\u00e3o tenham mais conte\u00fado.</p> <p>Isso significa que os valores do canal s\u00e3o consumidos serialmente um ap\u00f3s o outro e o primeiro canal vazio faz com que a execu\u00e7\u00e3o do processo pare, mesmo que existam outros valores em outros canais.</p> <p>Ent\u00e3o, o que acontece quando os canais n\u00e3o t\u00eam a mesma cardinalidade (isto \u00e9, eles emitem um n\u00famero diferente de elementos)?</p> <p>Por exemplo:</p> <pre><code>entrada1 = Channel.of(1, 2)\nentrada2 = Channel.of('a', 'b', 'c', 'd')\n\nprocess FOO {\n    debug true\n\n    input:\n    val x\n    val y\n\n    script:\n    \"\"\"\n    echo $x e $y\n    \"\"\"\n}\n\nworkflow {\n    FOO(entrada1, entrada2)\n}\n</code></pre> <p>No exemplo acima, o processo s\u00f3 \u00e9 executado duas vezes porque o processo para quando um canal n\u00e3o tem mais dados para serem processados.</p> <p>No entanto, o que acontece se voc\u00ea substituir o valor <code>x</code> por um canal de valor?</p> <p>Compare o exemplo anterior com o seguinte:</p> <pre><code>entrada1 = Channel.value(1)\nentrada2 = Channel.of('a', 'b', 'c')\n\nprocess BAR {\n    debug true\n\n    input:\n    val x\n    val y\n\n    script:\n    \"\"\"\n    echo $x e $y\n    \"\"\"\n}\n\nworkflow {\n    BAR(entrada1, entrada2)\n}\n</code></pre> Script output<pre><code>1 e b\n1 e a\n1 e c\n</code></pre> <p>Isso ocorre porque os canais de valor podem ser consumidos v\u00e1rias vezes e n\u00e3o afetam o t\u00e9rmino do processo.</p> <p>Exercise</p> <p>Escreva um processo que \u00e9 executado para cada arquivo de leitura correspondente ao padr\u00e3o <code>data/ggal/*_1.fq</code> e use o mesmo <code>data/ggal/transcriptome.fa</code> em cada execu\u00e7\u00e3o.</p> Solution <pre><code>params.leituras = \"$projectDir/data/ggal/*_1.fq\"\nparams.arquivo_transcriptoma = \"$projectDir/data/ggal/transcriptome.fa\"\n\nChannel\n    .fromPath(params.leituras)\n    .set { canal_leituras }\n\nprocess COMANDO {\n    tag \"Execute_comando\"\n\n    input:\n    path leituras\n    path transcriptoma\n\n    output:\n    path resultado\n\n    script:\n    \"\"\"\n    echo seu_comando $leituras $transcriptoma &gt; resultado\n    \"\"\"\n}\n\nworkflow {\n    canal_concatenado = COMANDO(canal_leituras, params.arquivo_transcriptoma)\n    canal_concatenado.view()\n}\n</code></pre>"},{"location":"pt/archive/basic_training/processes/#repetidores-de-entradas","title":"Repetidores de entradas","text":"<p>O qualificador <code>each</code> permite que voc\u00ea repita a execu\u00e7\u00e3o de um processo para cada item em uma cole\u00e7\u00e3o toda vez que novos dados s\u00e3o recebidos. Por exemplo:</p> <pre><code>sequencias = Channel.fromPath('data/prots/*.tfa')\nmetodos = ['regular', 'espresso', 'psicoffee']\n\nprocess ALINHESEQUENCIAS {\n    debug true\n\n    input:\n    path sequencia\n    each modo\n\n    script:\n    \"\"\"\n    echo t_coffee -in $sequencia -mode $modo\n    \"\"\"\n}\n\nworkflow {\n    ALINHESEQUENCIAS(sequencias, metodos)\n}\n</code></pre> <p>No exemplo acima, toda vez que um arquivo de sequ\u00eancias \u00e9 recebido como entrada pelo processo, ele executa tr\u00eas tarefas, cada uma executando um m\u00e9todo de alinhamento diferente definido como uma vari\u00e1vel <code>modo</code>. Isso \u00e9 \u00fatil quando voc\u00ea precisa repetir a mesma tarefa para um determinado conjunto de par\u00e2metros.</p> <p>Exercise</p> <p>Estenda o exemplo anterior para que uma tarefa seja executada para cada arquivo de leitura correspondente ao padr\u00e3o <code>data/ggal/*_1.fq</code> e repita a mesma tarefa com <code>salmon</code> e <code>kallisto</code>.</p> Solution <pre><code>params.leituras = \"$projectDir/data/ggal/*_1.fq\"\nparams.arquivo_transcriptoma = \"$projectDir/data/ggal/transcriptome.fa\"\nmetodos= ['salmon', 'kallisto']\n\nChannel\n    .fromPath(params.leituras)\n    .set { canal_leituras }\n\nprocess COMANDO {\n    tag \"Execute_comando\"\n\n    input:\n    path leituras\n    path transcriptoma\n    each modo\n\n    output:\n    path resultado\n\n    script:\n    \"\"\"\n    echo $modo $leituras $transcriptoma &gt; resultado\n    \"\"\"\n}\n\nworkflow {\n    canal_concatenado = COMANDO(canal_leituras, params.arquivo_transcriptoma, metodos)\n    canal_concatenado.view { \"Para executar : ${it.text}\" }\n}\n</code></pre>"},{"location":"pt/archive/basic_training/processes/#canais-de-saida","title":"Canais de sa\u00edda","text":"<p>O bloco <code>output</code> define os canais usados pelo processo para enviar os resultados produzidos.</p> <p>Apenas um bloco de sa\u00edda, que pode conter uma ou mais declara\u00e7\u00f5es de sa\u00edda, pode ser definido. O bloco de sa\u00edda segue a sintaxe mostrada abaixo:</p> <pre><code>output:\n&lt;qualificador da vari\u00e1vel de sa\u00edda&gt; &lt;nome da vari\u00e1vel de sa\u00edda&gt;, emit: &lt;nome do canal de sa\u00edda&gt;\n</code></pre>"},{"location":"pt/archive/basic_training/processes/#valores-de-saida","title":"Valores de sa\u00edda","text":"<p>O qualificador <code>val</code> especifica um valor definido no contexto do script. Os valores s\u00e3o frequentemente definidos nos blocos de input e/ou output, conforme mostrado no exemplo a seguir:</p> <pre><code>metodos = ['prot', 'dna', 'rna']\n\nprocess FOO {\n    input:\n    val x\n\n    output:\n    val x\n\n    script:\n    \"\"\"\n    echo $x &gt; arquivo\n    \"\"\"\n}\n\nworkflow {\n    canal_de_recebimento = FOO(Channel.of(metodos))\n    canal_de_recebimento.view { \"Recebido: $it\" }\n}\n</code></pre>"},{"location":"pt/archive/basic_training/processes/#caminhos-e-arquivos-de-saida","title":"Caminhos e arquivos de sa\u00edda","text":"<p>O qualificador <code>path</code> especifica um ou mais arquivos produzidos pelo processo no canal especificado como uma sa\u00edda.</p> <pre><code>process NUMALEATORIO {\n    output:\n    path 'resultado.txt'\n\n    script:\n    \"\"\"\n    echo \\$RANDOM &gt; resultado.txt\n    \"\"\"\n}\n\nworkflow {\n    canal_de_recebimento = NUMALEATORIO()\n    canal_de_recebimento.view { \"Recebido: \" + it.text }\n}\n</code></pre> <p>No exemplo acima, o processo <code>NUMALEATORIO</code> cria um arquivo chamado <code>resultado.txt</code> contendo um n\u00famero aleat\u00f3rio.</p> <p>Como um par\u00e2metro de arquivo usando o mesmo nome \u00e9 declarado no bloco de sa\u00edda, o arquivo \u00e9 enviado pelo canal <code>canal_de_recebimento</code> quando a tarefa \u00e9 conclu\u00edda. Um processo posterior declarando o mesmo canal como input ser\u00e1 capaz de receb\u00ea-lo.</p>"},{"location":"pt/archive/basic_training/processes/#multiplos-arquivos-de-saida","title":"M\u00faltiplos arquivos de sa\u00edda","text":"<p>Quando um nome de arquivo de sa\u00edda cont\u00e9m um caractere curinga (<code>*</code> ou <code>?</code>), ele \u00e9 interpretado como um glob de correspond\u00eancia para um caminho. Isso nos permite capturar v\u00e1rios arquivos em um objeto de lista e exibi-los como uma \u00fanica emiss\u00e3o. Por exemplo:</p> <pre><code>process SEPARARLETRAS {\n    output:\n    path 'pedaco_*'\n\n    script:\n    \"\"\"\n    printf 'Hola' | split -b 1 - pedaco_\n    \"\"\"\n}\n\nworkflow {\n    letters = SEPARARLETRAS()\n    letters\n        .flatMap()\n        .view { \"Arquivo: ${it.name} =&gt; ${it.text}\" }\n}\n</code></pre> <p>Imprime o seguinte:</p> <pre><code>Arquivo: pedaco_aa =&gt; H\nArquivo: pedaco_ab =&gt; o\nArquivo: pedaco_ac =&gt; l\nArquivo: pedaco_ad =&gt; a\n</code></pre> <p>Algumas advert\u00eancias sobre o comportamento de padr\u00f5es de glob:</p> <ul> <li>Os arquivos de entrada n\u00e3o est\u00e3o inclu\u00eddos na lista de poss\u00edveis correspond\u00eancias</li> <li>O padr\u00e3o glob corresponde tanto a arquivos quanto caminhos de diret\u00f3rio</li> <li>Quando um padr\u00e3o de duas estrelas <code>**</code> \u00e9 usado para acessar os diret\u00f3rios, apenas os caminhos de arquivo s\u00e3o correspondidos, ou seja, os diret\u00f3rios n\u00e3o s\u00e3o inclu\u00eddos na lista de resultados.</li> </ul> <p>Exercise</p> <p>Remova o operador <code>flatMap</code> e veja a mudan\u00e7a de sa\u00edda. A documenta\u00e7\u00e3o para o operador <code>flatMap</code> est\u00e1 dispon\u00edvel nesse link.</p> Solution <pre><code>File: [pedaco_aa, pedaco_ab, pedaco_ac, pedaco_ad] =&gt; [H, o, l, a]\n</code></pre>"},{"location":"pt/archive/basic_training/processes/#nomes-dinamicos-de-arquivos-de-saida","title":"Nomes din\u00e2micos de arquivos de sa\u00edda","text":"<p>Quando um nome de arquivo de sa\u00edda precisa ser expresso dinamicamente, \u00e9 poss\u00edvel defini-lo usando uma string din\u00e2mica que faz refer\u00eancia a valores definidos no bloco de declara\u00e7\u00e3o de entrada ou no contexto global do script. Por exemplo:</p> <pre><code>especies = ['gato', 'cachorro', 'pregui\u00e7a']\nsequencias = ['AGATAG', 'ATGCTCT', 'ATCCCAA']\n\nChannel\n    .fromList(especies)\n    .set { canal_especies }\n\nprocess ALINHAR {\n    input:\n    val x\n    val sequencia\n\n    output:\n    path \"${x}.aln\"\n\n    script:\n    \"\"\"\n    echo alinhar -in $sequencia &gt; ${x}.aln\n    \"\"\"\n}\n\nworkflow {\n    genomas = ALINHAR(canal_especies, sequencias)\n    genomas.view()\n}\n</code></pre> <p>No exemplo acima, cada vez que o processo \u00e9 executado, \u00e9 gerado um arquivo de alinhamento cujo nome depende do valor da entrada <code>x</code>.</p>"},{"location":"pt/archive/basic_training/processes/#entradas-e-saidas-compostas","title":"Entradas e sa\u00eddas compostas","text":"<p>At\u00e9 agora, vimos como declarar v\u00e1rios canais de entrada e sa\u00edda que podem lidar com um valor por vez. No entanto, o Nextflow tamb\u00e9m pode lidar com tuplas de valores.</p> <p>As declara\u00e7\u00f5es de entrada e sa\u00edda para tuplas devem ser declaradas com um qualificador <code>tuple</code> seguido pela defini\u00e7\u00e3o de cada elemento na tupla.</p> <pre><code>canal_leituras = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')\n\nprocess FOO {\n    input:\n    tuple val(id_amostra), path(arquivos_amostra)\n\n    output:\n    tuple val(id_amostra), path('sample.bam')\n\n    script:\n    \"\"\"\n    echo seu_comando_aqui --leituras $arquivos_amostra &gt; sample.bam\n    \"\"\"\n}\n\nworkflow {\n    canal_bam = FOO(canal_leituras)\n    canal_bam.view()\n}\n</code></pre> <p>Info</p> <p>Nas vers\u00f5es anteriores do Nextflow <code>tuple</code> era chamado <code>set</code>, mas era usado da mesma forma com a mesma sem\u00e2ntica.</p> <p>Exercise</p> <p>Modifique o script do exerc\u00edcio anterior para que o arquivo bam seja nomeado com o <code>id_amostra</code> fornecido.</p> Solution <pre><code>canal_leituras = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')\n\nprocess FOO {\n    input:\n    tuple val(id_amostra), path(arquivos_amostra)\n\n    output:\n    tuple val(id_amostra), path(\"${id_amostra}.bam\")\n\n    script:\n    \"\"\"\n    echo seu_comando_aqui --leituras $arquivos_amostra &gt; ${id_amostra}.bam\n    \"\"\"\n}\n\nworkflow {\n    canal_bam = FOO(canal_leituras)\n    canal_bam.view()\n}\n</code></pre>"},{"location":"pt/archive/basic_training/processes/#when","title":"When","text":"<p>A declara\u00e7\u00e3o <code>when</code> permite que voc\u00ea defina uma condi\u00e7\u00e3o que deve ser verificada para executar o processo. Pode ser qualquer express\u00e3o que avalie um valor booleano.</p> <p>\u00c9 \u00fatil habilitar/desabilitar a execu\u00e7\u00e3o do processo dependendo do estado de v\u00e1rias entradas e par\u00e2metros. Por exemplo:</p> <pre><code>params.tipo_banco = 'nr'\nparams.prot = 'data/prots/*.tfa'\nproteinas = Channel.fromPath(params.prot)\n\nprocess ENCONTRAR {\n    debug true\n\n    input:\n    path fasta\n    val tipo\n\n    when:\n    fasta.name =~ /^BB11.*/ &amp;&amp; tipo == 'nr'\n\n    script:\n    \"\"\"\n    echo blastp -checar $fasta -tipo_banco nr\n    \"\"\"\n}\n\nworkflow {\n    resultado = ENCONTRAR(proteinas, params.tipo_banco)\n}\n</code></pre>"},{"location":"pt/archive/basic_training/processes/#diretivas","title":"Diretivas","text":"<p>As declara\u00e7\u00f5es de diretiva permitem a defini\u00e7\u00e3o de configura\u00e7\u00f5es opcionais que afetam a execu\u00e7\u00e3o do processo atual sem afetar a sem\u00e2ntica da pr\u00f3pria tarefa.</p> <p>Elas devem ser inseridas no topo do corpo do processo, antes de quaisquer outros blocos de declara\u00e7\u00e3o (ou seja, <code>input</code>, <code>output</code>, etc.).</p> <p>Diretivas s\u00e3o comumente usadas para definir a quantidade de recursos computacionais a serem usados ou outras meta diretivas que permitem a defini\u00e7\u00e3o de configura\u00e7\u00e3o extra de informa\u00e7\u00f5es de log. Por exemplo:</p> <pre><code>process FOO {\n    cpus 2\n    memory 1.GB\n    container 'nome/imagem'\n\n    script:\n    \"\"\"\n    echo seu_comando --isso --aquilo\n    \"\"\"\n}\n</code></pre> <p> A lista completa de diretivas est\u00e1 dispon\u00edvel aqui.</p> Nome Descri\u00e7\u00e3o <code>cpus</code> Permite definir o n\u00famero de CPUs (l\u00f3gicas) necess\u00e1rias para a tarefa do processo. <code>time</code> Permite definir por quanto tempo a tarefa pode ser executado (por exemplo, tempo 1h: 1 hora, 1s 1 segundo, 1m 1 minuto, 1d 1 dia). <code>memory</code> Permite definir quanta mem\u00f3ria a tarefa pode usar (por exemplo, 2 GB \u00e9 2 GB). Tamb\u00e9m pode usar B, KB, MB, GB e TB. <code>disk</code> Permite definir a quantidade de armazenamento em disco local que a tarefa pode usar. <code>tag</code> Permite associar cada execu\u00e7\u00e3o de processo a um r\u00f3tulo personalizado para facilitar sua identifica\u00e7\u00e3o no arquivo de log ou no relat\u00f3rio de execu\u00e7\u00e3o do rastreamento."},{"location":"pt/archive/basic_training/processes/#organizando-as-saidas","title":"Organizando as sa\u00eddas","text":""},{"location":"pt/archive/basic_training/processes/#a-diretiva-publishdir","title":"A diretiva PublishDir","text":"<p>Dado que cada tarefa est\u00e1 sendo executada em uma pasta <code>work/</code> tempor\u00e1ria separada (por exemplo, <code>work/f1/850698\u2026</code>; <code>work/g3/239712\u2026</code>; etc.), podemos salvar informa\u00e7\u00f5es importantes, n\u00e3o intermedi\u00e1rias, e/ou arquivos finais em uma pasta de resultados.</p> <p>Tip</p> <p>Lembre-se de excluir a pasta de trabalho (<code>work</code>) de vez em quando para limpar seus arquivos intermedi\u00e1rios e impedir que eles encham seu computador!</p> <p>Para armazenar nossos arquivos de resultado do fluxo de trabalho, precisamos marc\u00e1-los explicitamente usando a diretiva publishDir no processo que est\u00e1 criando os arquivos. Por exemplo:</p> <pre><code>params.diretorio_saida = 'meus-resultados'\nparams.prot = 'data/prots/*.tfa'\nproteinas = Channel.fromPath(params.prot)\n\n\nprocess BLASTSEQ {\n    publishDir \"$params.diretorio_saida/arquivos_bam\", mode: 'copy'\n\n    input:\n    path fasta\n\n    output:\n    path ('*.txt')\n\n    script:\n    \"\"\"\n    echo blastp $fasta &gt; ${fasta}_resultado.txt\n    \"\"\"\n}\n\nworkflow {\n    canal_blast = BLASTSEQ(proteinas)\n    canal_blast.view()\n}\n</code></pre> <p>O exemplo acima copiar\u00e1 todos os arquivos de script blast criados pelo processo <code>BLASTSEQ</code> no caminho do diret\u00f3rio <code>meus-resultados</code>.</p> <p>Tip</p> <p>O diret\u00f3rio de publica\u00e7\u00e3o pode ser local ou remoto. Por exemplo, os arquivos de sa\u00edda podem ser armazenados usando um bucket AWS S3 usando o prefixo <code>s3://</code> no caminho de destino.</p>"},{"location":"pt/archive/basic_training/processes/#gerenciando-semantica-de-subdiretorios","title":"Gerenciando sem\u00e2ntica de subdiret\u00f3rios","text":"<p>Voc\u00ea pode usar mais de um <code>publishDir</code> para manter sa\u00eddas diferentes em diret\u00f3rios separados. Por exemplo:</p> <pre><code>params.leituras = 'data/reads/*_{1,2}.fq.gz'\nparams.diretorio_saida = 'meus-resultados'\n\ncanal_amostras = Channel.fromFilePairs(params.leituras, flat: true)\n\nprocess FOO {\n    publishDir \"$params.diretorio_saida/$id_amostra/\", pattern: '*.fq'\n    publishDir \"$params.diretorio_saida/$id_amostra/contagens\", pattern: \"*_contagens.txt\"\n    publishDir \"$params.diretorio_saida/$id_amostra/panoramas\", pattern: '*_panorama.txt'\n\n    input:\n    tuple val(id_amostra), path('amostra1.fq.gz'), path('amostra2.fq.gz')\n\n    output:\n    path \"*\"\n\n    script:\n    \"\"\"\n    zcat amostra1.fq.gz &gt; amostra1.fq\n    zcat amostra2.fq.gz &gt; amostra2.fq\n\n    awk '{s++}END{print s/4}' amostra1.fq &gt; amostra1_contagens.txt\n    awk '{s++}END{print s/4}' amostra2.fq &gt; amostra2_contagens.txt\n\n    head -n 50 amostra1.fq &gt; amostra1_panorama.txt\n    head -n 50 amostra2.fq &gt; amostra2_panorama.txt\n    \"\"\"\n}\n\nworkflow {\n    canal_saida = FOO(canal_amostras)\n}\n</code></pre> <p>O exemplo acima criar\u00e1 uma estrutura de sa\u00edda no diret\u00f3rio <code>meus-resultados</code>, que cont\u00e9m um subdiret\u00f3rio separado para cada ID de amostra fornecido, cada um contendo as pastas <code>contagens</code> e <code>panoramas</code>.</p>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/","title":"Rnaseq pipeline","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#fluxo-de-trabalho-simples-de-rna-seq","title":"Fluxo de trabalho simples de RNA-Seq","text":"<p>Para demonstrar um cen\u00e1rio biom\u00e9dico da vida real, n\u00f3s iremos implementar uma prova de conceito de fluxo de trabalho RNA-Seq que:</p> <ol> <li>Cria arquivo de \u00edndice de transcriptoma</li> <li>Realiza controles de qualidade</li> <li>Realiza quantifica\u00e7\u00e3o</li> <li>Cria um relat\u00f3rio MultiQC</li> </ol> <p>Isso ser\u00e1 feito usando uma s\u00e9rie de sete scripts, cada um se baseando no script anterior, para criar um fluxo de trabalho completo. Voc\u00ea poder\u00e1 encontr\u00e1-los no diret\u00f3rio do tutorial (<code>script1.nf</code> - <code>script7.nf</code>). Esses scripts far\u00e3o uso de ferramentas de terceiros que s\u00e3o conhecidas por bioinformatas, mas que podem ser novas para voc\u00ea, ent\u00e3o vamos apresent\u00e1-las brevemente abaixo.</p> <ol> <li>Salmon \u00e9 uma ferramenta para quantificar mol\u00e9culas conhecidas como transcritos por meio de um tipo de dados chamado dados de RNA-seq.</li> <li>FastQC \u00e9 uma ferramenta para executar o controle de qualidade para dados de sequenciamento de alta vaz\u00e3o. Voc\u00ea pode pensar nisso como uma forma de avaliar a qualidade de seus dados.</li> <li>MultiQC pesquisa um determinado diret\u00f3rio por logs de an\u00e1lises e compila um relat\u00f3rio HTML. \u00c9 uma ferramenta de uso geral, perfeita para resumir a sa\u00edda de v\u00e1rias ferramentas de bioinform\u00e1tica.</li> </ol> <p>Embora essas ferramentas possam n\u00e3o ser as que voc\u00ea usar\u00e1 em seu pipeline, elas podem ser substitu\u00eddas por qualquer ferramenta comum de sua \u00e1rea. Esse \u00e9 o poder do Nextflow!</p>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#defina-os-parametros-do-fluxo-de-trabalho","title":"Defina os par\u00e2metros do fluxo de trabalho","text":"<p>Par\u00e2metros s\u00e3o entradas e op\u00e7\u00f5es que podem ser modificadas quando um fluxo de trabalho \u00e9 executado.</p> <p>O script <code>script1.nf</code> define os par\u00e2metros de entrada do fluxo de trabalho.</p> <pre><code>params.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\n\nprintln \"reads: $params.reads\"\n</code></pre> <p>Execute-o usando o comando a seguir:</p> <pre><code>nextflow run script1.nf\n</code></pre> <p>Tente especificar um par\u00e2metro de entrada diferente no seu comando de execu\u00e7\u00e3o, por exemplo:</p> <pre><code>nextflow run script1.nf --reads '/workspaces/training/nf-training/data/ggal/lung_{1,2}.fq'\n</code></pre>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#exercicios","title":"Exerc\u00edcios","text":"<p>Exercise</p> <p>Modifique o <code>script1.nf</code> ao adicionar um quarto par\u00e2metro chamado <code>outdir</code> e defina-o como um caminho padr\u00e3o que ser\u00e1 usado como o diret\u00f3rio de sa\u00edda do fluxo de trabalho.</p> Solution <pre><code>params.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\nparams.outdir = \"results\"\n</code></pre> <p>Exercise</p> <p>Modifique o <code>script1.nf</code> para imprimir todos os par\u00e2metros do fluxo de trabalho usando um \u00fanico comando<code>log.info</code> como uma declara\u00e7\u00e3o de string multilinha.</p> <p> Veja um exemplo aqui.</p> Solution <p>Adicione o c\u00f3digo abaixo para seu arquivo de script:</p> <pre><code>log.info \"\"\"\\\n    R N A S E Q - N F   P I P E L I N E\n    ===================================\n    transcriptome: ${params.transcriptome_file}\n    reads        : ${params.reads}\n    outdir       : ${params.outdir}\n    \"\"\"\n    .stripIndent(true)\n</code></pre>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#resumo","title":"Resumo","text":"<p>Nesta etapa voc\u00ea aprendeu:</p> <ol> <li>Como definir par\u00e2metros em seu script de fluxo de trabalho</li> <li>Como atribuir par\u00e2metros usando a linha de comando</li> <li>O uso de <code>$var</code> e <code>${var}</code> como espa\u00e7o reservado para vari\u00e1veis</li> <li>Como usar strings multilinhas</li> <li>Como usar <code>log.info</code> para imprimir informa\u00e7\u00f5es e salv\u00e1-las no arquivo de execu\u00e7\u00e3o de log</li> </ol>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#crie-um-arquivo-de-indice-de-transcriptoma","title":"Crie um arquivo de \u00edndice de transcriptoma","text":"<p>O Nextflow permite a execu\u00e7\u00e3o de qualquer comando ou script usando uma defini\u00e7\u00e3o de processo (<code>process</code>).</p> <p>Um processo \u00e9 definido por tr\u00eas principais declara\u00e7\u00f5es: as entradas (<code>input</code>), sa\u00eddas (<code>output</code>) e comandos de <code>script</code> do processo.</p> <p>Para adicionar uma etapa de processamento de \u00edndice do transcriptoma (<code>INDEX</code>), tente adicionar os blocos de c\u00f3digo a seguir no seu <code>script1.nf</code>. Como alternativa, esses blocos de c\u00f3digo j\u00e1 foram adicionados ao <code>script2.nf</code>.</p> <pre><code>/*\n * define o processo INDEX que cria um \u00edndice bin\u00e1rio\n * dado um arquivo de transcriptoma\n */\nprocess INDEX {\n    input:\n    path transcriptome\n\n    output:\n    path 'salmon_index'\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_index\n    \"\"\"\n}\n</code></pre> <p>Al\u00e9m disso, adicione um escopo <code>workflow</code> contendo uma defini\u00e7\u00e3o de canal de entrada e o processo de \u00edndice:</p> <pre><code>workflow {\n    index_ch = INDEX(params.transcriptome_file)\n}\n</code></pre> <p>Aqui, o par\u00e2metro <code>params.transcriptome_file</code> \u00e9 usado como entrada para o processo <code>INDEX</code>. O processo <code>INDEX</code> (usando a ferramenta <code>salmon</code>) cria um arquivo chamado <code>salmon_index</code>, que \u00e9 um arquivo de \u00edndice de transcriptoma que \u00e9 passado como sa\u00edda ao canal <code>index_ch</code>.</p> <p>Info</p> <p>A declara\u00e7\u00e3o de entrada define a vari\u00e1vel <code>transcriptome</code> com o qualificador <code>path</code> que \u00e9 usada no <code>script</code> como uma refer\u00eancia (usando o s\u00edmbolo de cifr\u00e3o) na linha de comando do Salmon.</p> <p>Warning</p> <p>Os requisitos de recursos, como CPUs e limites de mem\u00f3ria, podem mudar com diferentes execu\u00e7\u00f5es do fluxo de trabalho e plataformas. O Nextflow pode usar <code>$task.cpus</code> como uma vari\u00e1vel para o n\u00famero de CPUs. Veja a documenta\u00e7\u00e3o de diretivas de processo para mais detalhes.</p> <p>Execute-o usando o comando:</p> <pre><code>nextflow run script2.nf\n</code></pre> <p>A execu\u00e7\u00e3o ir\u00e1 falhar porque o <code>salmon</code> n\u00e3o est\u00e1 instalado em seu ambiente.</p> <p>Adicione a op\u00e7\u00e3o de linha de comando <code>-with-docker</code> para iniciar a execu\u00e7\u00e3o atrav\u00e9s do cont\u00eainer Docker, como mostrado abaixo:</p> <pre><code>nextflow run script2.nf -with-docker\n</code></pre> <p>Dessa vez a execu\u00e7\u00e3o vai funcionar porque usa o cont\u00eainer Docker <code>nextflow/rnaseq-nf</code> que \u00e9 definido no arquivo <code>nextflow.config</code> do seu diret\u00f3rio atual. Se voc\u00ea est\u00e1 executando esse script localmente, voc\u00ea precisar\u00e1 baixar o Docker em seu computador, fazer login e ativar o Docker, e permitir que o script baixe o cont\u00eainer contendo os scripts de execu\u00e7\u00e3o. Voc\u00ea pode aprender mais sobre o Docker na documenta\u00e7\u00e3o oficial do Nextflow aqui.</p> <p>Para evitar adicionar <code>-with-docker</code> cada vez que voc\u00ea executar o script, adicione a linha a seguir ao arquivo <code>nextflow.config</code>:</p> <pre><code>docker.enabled = true\n</code></pre>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#exercicios_1","title":"Exerc\u00edcios","text":"<p>Exercise</p> <p>Ative a execu\u00e7\u00e3o do Docker por padr\u00e3o adicionando a configura\u00e7\u00e3o acima no arquivo <code>nextflow.config</code>.</p> <p>Exercise</p> <p>Imprima a sa\u00edda do canal <code>index_ch</code> usando o operador view.</p> Solution <p>Adicione o c\u00f3digo a seguir ao final do bloco <code>workflow</code> em seu arquivo de script</p> <pre><code>index_ch.view()\n</code></pre> <p>Exercise</p> <p>Se voc\u00ea tiver mais CPUs dispon\u00edveis, tente alterar seu script para solicitar mais recursos para este processo. Por exemplo, consulte os documentos de diretivas. <code>$task.cpus</code> j\u00e1 est\u00e1 especificado no script, portanto definir o n\u00famero de CPUs como uma diretiva informar\u00e1 ao Nextflow para executar este processo levando isso em considera\u00e7\u00e3o.</p> Solution <p>Adicione <code>cpus 2</code> no top do processo de \u00edndice:</p> <pre><code>process INDEX {\n    cpus 2\n\n    input:\n    ...\n</code></pre> <p>Em seguida verifique se funcionou observando o script executado no diret\u00f3rio de trabalho. Procure pelo hexadecimal (por exemplo, <code>work/7f/f285b80022d9f61e82cd7f90436aa4/</code>), depois use <code>cat</code> no arquivo <code>.command.sh</code>.</p> <p>Bonus Exercise</p> <p>Use o comando <code>tree work</code> para observar como o Nextflow organiza o diret\u00f3rio de trabalho do processo. Leia mais aqui se voc\u00ea precisa baixar o <code>tree</code>.</p> Solution <p>Voc\u00ea deve ver algo parecido com a sa\u00edda abaixo:</p> <pre><code>work\n\u251c\u2500\u2500 17\n\u2502   \u2514\u2500\u2500 263d3517b457de4525513ae5e34ea8\n\u2502       \u251c\u2500\u2500 index\n\u2502       \u2502   \u251c\u2500\u2500 complete_ref_lens.bin\n\u2502       \u2502   \u251c\u2500\u2500 ctable.bin\n\u2502       \u2502   \u251c\u2500\u2500 ctg_offsets.bin\n\u2502       \u2502   \u251c\u2500\u2500 duplicate_clusters.tsv\n\u2502       \u2502   \u251c\u2500\u2500 eqtable.bin\n\u2502       \u2502   \u251c\u2500\u2500 info.json\n\u2502       \u2502   \u251c\u2500\u2500 mphf.bin\n\u2502       \u2502   \u251c\u2500\u2500 pos.bin\n\u2502       \u2502   \u251c\u2500\u2500 pre_indexing.log\n\u2502       \u2502   \u251c\u2500\u2500 rank.bin\n\u2502       \u2502   \u251c\u2500\u2500 refAccumLengths.bin\n\u2502       \u2502   \u251c\u2500\u2500 ref_indexing.log\n\u2502       \u2502   \u251c\u2500\u2500 reflengths.bin\n\u2502       \u2502   \u251c\u2500\u2500 refseq.bin\n\u2502       \u2502   \u251c\u2500\u2500 seq.bin\n\u2502       \u2502   \u2514\u2500\u2500 versionInfo.json\n\u2502       \u2514\u2500\u2500 transcriptome.fa -&gt; /workspaces/training/data/ggal/transcriptome.fa\n\u251c\u2500\u2500 7f\n</code></pre>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#resumo_1","title":"Resumo","text":"<p>Nesta etapa voc\u00ea aprendeu:</p> <ol> <li>Como definir um processo executando um comando personalizado</li> <li>Como as entradas do processo s\u00e3o declaradas</li> <li>Como as sa\u00eddas do processo s\u00e3o declaradas</li> <li>Como imprimir o conte\u00fado de um canal</li> <li>Como acessar o n\u00famero de CPUs dispon\u00edveis</li> </ol>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#colete-arquivos-de-leitura-por-pares","title":"Colete arquivos de leitura por pares","text":"<p>Essa etapa mostra como combinar arquivos de leitura em pares, para que eles possam ser mapeados pelo Salmon.</p> <p>Edite o script <code>script3.nf</code> adicionando a seguinte declara\u00e7\u00e3o como a \u00faltima linha do arquivo:</p> <pre><code>read_pairs_ch.view()\n</code></pre> <p>Salve-o e execute-o com o comando a seguir:</p> <pre><code>nextflow run script3.nf\n</code></pre> <p>Isso ir\u00e1 imprimir algo semelhante a isso:</p> <pre><code>[gut, [/.../data/ggal/gut_1.fq, /.../data/ggal/gut_2.fq]]\n</code></pre> <p>O exemplo acima mostra como o canal <code>read_pairs_ch</code> emite tuplas compostas de dois elementos, onde o primeiro \u00e9 o prefixo do par de leitura e o segundo \u00e9 uma lista que representa os arquivos de fato.</p> <p>Tente novamente especificando arquivos de leituras diferentes usando um padr\u00e3o glob:</p> <pre><code>nextflow run script3.nf --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>Warning</p> <p>Caminhos de arquivo que incluem um ou mais caracteres especiais, como <code>*</code>, <code>?</code> etc., DEVEM ser colocados entre aspas simples para evitar que o Bash expanda o glob.</p>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#exercicios_2","title":"Exerc\u00edcios","text":"<p>Exercise</p> <p>Use o operador set no lugar da atribui\u00e7\u00e3o <code>=</code> para definir o canal <code>read_pairs_ch</code>.</p> Solution <pre><code>Channel\n    .fromFilePairs(params.reads)\n    .set { read_pairs_ch }\n</code></pre> <p>Exercise</p> <p>Use a op\u00e7\u00e3o <code>checkIfExists</code> para a f\u00e1brica de canal fromFilePairs para checar se o caminho especificado cont\u00e9m os pares de arquivos.</p> Solution <pre><code>Channel\n    .fromFilePairs(params.reads, checkIfExists: true)\n    .set { read_pairs_ch }\n</code></pre>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#resumo_2","title":"Resumo","text":"<p>Nessa etapa voc\u00ea aprendeu:</p> <ol> <li>Como usar <code>fromFilePairs</code> para lidar com pares de arquivos de leituras</li> <li>Como usar a op\u00e7\u00e3o <code>checkIfExists</code> para checar a exist\u00eancia de arquivos de entrada</li> <li>Como usar o operador <code>set</code> para definir uma uma nova vari\u00e1vel de canal</li> </ol> <p>Info</p> <p>A declara\u00e7\u00e3o de um canal pode ser feita antes do escopo <code>workflow</code> ou dentro dele. Desde que a declara\u00e7\u00e3o esteja acima do processo que requer o canal espec\u00edfico.</p>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#realize-a-quantificacao-da-expressao","title":"Realize a quantifica\u00e7\u00e3o da express\u00e3o","text":"<p>O script <code>script4.nf</code> adiciona um processo de quantifica\u00e7\u00e3o de express\u00e3o g\u00eanica (<code>QUANTIFICATION</code>) e uma chamada para esse processo dentro do escopo <code>workflow</code>. A quantifica\u00e7\u00e3o requer o arquivo de \u00edndice de transcriptoma e os arquivos fastq do par de leitura de RNA-Seq.</p> <p>No escopo <code>workflow</code>, observe como o canal <code>index_ch</code> \u00e9 designado como sa\u00edda do processo <code>INDEX</code>.</p> <p>A seguir, note que o primeiro canal de entrada para o processo de <code>QUANTIFICATION</code> \u00e9 o <code>index_ch</code> declarado previamente, que cont\u00e9m o caminho para o arquivo <code>salmon_index</code>.</p> <p>Al\u00e9m disso, observe que o segundo canal de entrada para o processo <code>QUANTIFICATION</code> \u00e9 o <code>read_pair_ch</code> que acabamos de criar. Este sendo uma <code>tupla</code> composta de dois elementos (um valor: <code>sample_id</code> e a lista de caminhos para os arquivos de leituras fastq: <code>reads</code>) para corresponder \u00e0 estrutura dos itens emitidos pela f\u00e1brica de canais <code>fromFilePairs</code>.</p> <p>Execute-o usando o comando a seguir:</p> <pre><code>nextflow run script4.nf -resume\n</code></pre> <p>Voc\u00ea ir\u00e1 ver a execu\u00e7\u00e3o do processo <code>QUANTIFICATION</code>.</p> <p>Ao usar a op\u00e7\u00e3o <code>-resume</code>, qualquer etapa que j\u00e1 foi processada \u00e9 ignorada.</p> <p>Tente executar o mesmo script novamente com mais arquivos de leituras, como mostrado abaixo:</p> <pre><code>nextflow run script4.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>Voc\u00ea ir\u00e1 perceber que o processo <code>QUANTIFICATION</code> \u00e9 executado m\u00faltiplas vezes.</p> <p>O Nextflow paraleliza a execu\u00e7\u00e3o de seu fluxo de trabalho simplesmente fornecendo v\u00e1rios conjuntos de dados de entrada para seu script.</p> <p>Tip</p> <p>Pode ser \u00fatil aplicar configura\u00e7\u00f5es opcionais a um processo espec\u00edfico usando diretivas especificando-as no corpo do processo.</p>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#exercicios_3","title":"Exerc\u00edcios","text":"<p>Exercise</p> <p>Adicione uma diretiva de tag, ao processo <code>QUANTIFICATION</code> para fornecer um log de execu\u00e7\u00e3o mais leg\u00edvel.</p> Solution <p>Adicione o c\u00f3digo a seguir antes da declara\u00e7\u00e3o de entrada:</p> <pre><code>tag \"Salmon on $sample_id\"\n</code></pre> <p>Exercise</p> <p>Adicione a diretiva publishDir para o processo <code>QUANTIFICATION</code> para armazenar os resultados do processo em um diret\u00f3rio de sua escolha.</p> Solution <p>Adicione o c\u00f3digo a seguir antes da declara\u00e7\u00e3o de entrada no processo de <code>QUANTIFICATION</code>:</p> <pre><code>publishDir params.outdir, mode: 'copy'\n</code></pre>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#resumo_3","title":"Resumo","text":"<p>Nessa etapa voc\u00ea aprendeu:</p> <ol> <li>Como conectar dois processos juntos usando declara\u00e7\u00f5es de canal</li> <li>Como retomar a execu\u00e7\u00e3o do script e pular etapas em cache</li> <li>Como usar a diretiva <code>tag</code> para fornecer uma sa\u00edda de execu\u00e7\u00e3o mais leg\u00edvel</li> <li>Como usar a diretiva <code>publishDir</code> para armazenar os resultados do processo em um caminho da sua escolha</li> </ol>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#controle-de-qualidade","title":"Controle de qualidade","text":"<p>A seguir, n\u00f3s implementamos uma etapa de controle de qualidade <code>FASTQC</code> para seus arquivos de leituras de entrada (usando a etiqueta <code>fastqc</code>). As entradas s\u00e3o as mesmas que os pares de arquivos de leituras na etapa <code>QUANTIFICATION</code>.</p> <p>Voc\u00ea pode execut\u00e1-lo usando o comando a seguir:</p> <pre><code>nextflow run script5.nf -resume\n</code></pre> <p>O Nextflow DSL2 sabe como dividir <code>reads_pair_ch</code> em dois canais id\u00eanticos, j\u00e1 que eles s\u00e3o requiridos duas vezes como entrada para os processos <code>FASTQC</code> e <code>QUANTIFICATION</code>.</p>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#relatorio-multiqc","title":"Relat\u00f3rio MultiQC","text":"<p>Essa etapa coleta as sa\u00eddas dos processos <code>QUANTIFICATION</code> e <code>FASTQC</code> para criar um relat\u00f3rio final usando a ferramenta MultiQC.</p> <p>Execute o pr\u00f3ximo script com o comando a seguir:</p> <pre><code>nextflow run script6.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>Isso cria um relat\u00f3rio final na pasta <code>results</code> no diret\u00f3rio de trabalho atual.</p> <p>Neste script, observe o uso dos operadores mix e collect de forma encadeada para reunir as sa\u00eddas dos processos <code>QUANTIFICATION</code> e <code>FASTQC</code> como uma \u00fanica entrada. Operadores podem ser usados para combinar e transformar canais.</p> <pre><code>MULTIQC(quant_ch.mix(fastqc_ch).collect())\n</code></pre> <p>Queremos que apenas uma tarefa do MultiQC seja executada para produzir um relat\u00f3rio. Portanto, usamos o operador de canal <code>mix</code> para combinar os dois canais, seguido pelo operador <code>collect</code> para retornar os conte\u00fados completos do canal como um \u00fanico elemento.</p>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#resumo_4","title":"Resumo","text":"<p>Nessa etapa voc\u00ea aprendeu:</p> <ol> <li>Como coletar v\u00e1rias sa\u00eddas para uma \u00fanica entrada com o operador <code>collect</code></li> <li>Como combinar com <code>mix</code> dois canais em um \u00fanico canal</li> <li>Como encadear dois ou mais operadores juntos</li> </ol>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#lide-com-evento-de-conclusao","title":"Lide com evento de conclus\u00e3o","text":"<p>Essa etapa mostra como executar uma a\u00e7\u00e3o quando o fluxo de trabalho completa a execu\u00e7\u00e3o.</p> <p>Observe que processos do Nextflow definem a execu\u00e7\u00e3o de tarefas ass\u00edncronas, ou seja, elas n\u00e3o s\u00e3o executadas uma ap\u00f3s a outra como se elas fossem escritas no script do fluxo de trabalho em uma linguagem de programa\u00e7\u00e3o imperativa comum.</p> <p>O script usa o manipulador de evento <code>workflow.onComplete</code> para imprimir uma mensagem de confirma\u00e7\u00e3o quando o script for conclu\u00eddo.</p> <p>Tente execut\u00e1-lo usando o comando a seguir:</p> <pre><code>nextflow run script7.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#notificacoes-por-email","title":"Notifica\u00e7\u00f5es por email","text":"<p>Envie uma notifica\u00e7\u00e3o por email quando a execu\u00e7\u00e3o do fluxo de trabalho for conclu\u00edda usando a op\u00e7\u00e3o de linha de comando <code>-N &lt;endere\u00e7o de email&gt;</code>.</p> <p>Nota: isso requer a configura\u00e7\u00e3o de um servidor SMTP no arquivo de configura\u00e7\u00e3o do Nextflow. Abaixo h\u00e1 um exemplo de um arquivo <code>nextflow.config</code> mostrando as configura\u00e7\u00f5es que voc\u00ea teria que configurar:</p> <pre><code>mail {\n    from = 'info@nextflow.io'\n    smtp.host = 'email-smtp.eu-west-1.amazonaws.com'\n    smtp.port = 587\n    smtp.user = \"xxxxx\"\n    smtp.password = \"yyyyy\"\n    smtp.auth = true\n    smtp.starttls.enable = true\n    smtp.starttls.required = true\n}\n</code></pre> <p>Veja a documenta\u00e7\u00e3o de email para mais detalhes.</p>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#scripts-personalizados","title":"Scripts personalizados","text":"<p>Os fluxos de trabalho do mundo real usam muitos scripts de usu\u00e1rio personalizados (BASH, R, Python, etc.). O Nextflow permite que voc\u00ea use e gerencie consistentemente esses scripts. Simplesmente os coloque em um diret\u00f3rio chamado <code>bin</code> na raiz do projeto do fluxo de trabalho. Eles ser\u00e3o automaticamente adicionados para o <code>PATH</code> da execu\u00e7\u00e3o do fluxo de trabalho.</p> <p>Por exemplo, crie um arquivo chamado de <code>fastqc.sh</code> com o conte\u00fado a seguir:</p> <pre><code>#!/bin/bash\nset -e\nset -u\n\nsample_id=${1}\nreads=${2}\n\nmkdir fastqc_${sample_id}_logs\nfastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n</code></pre> <p>Salve-o, d\u00ea permiss\u00e3o de execu\u00e7\u00e3o e mova-o para o diret\u00f3rio <code>bin</code> conforme mostrado abaixo:</p> <pre><code>chmod +x fastqc.sh\nmkdir -p bin\nmv fastqc.sh bin\n</code></pre> <p>Ent\u00e3o, abra o arquivo <code>script7.nf</code> e substitua o script do processo <code>FASTQC</code> com o c\u00f3digo a seguir:</p> <pre><code>script:\n\"\"\"\nfastqc.sh \"$sample_id\" \"$reads\"\n\"\"\"\n</code></pre> <p>Execute-o como anteriormente:</p> <pre><code>nextflow run script7.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#resumo_5","title":"Resumo","text":"<p>Nessa etapa voc\u00ea aprendeu:</p> <ol> <li>Como escrever ou usar scripts personalizados existentes em seu fluxo de trabalho do Nextflow.</li> <li>Como evitar o uso de caminhos absolutos tendo seus scripts na pasta <code>bin/</code>.</li> </ol>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#metricas-e-relatorios","title":"M\u00e9tricas e relat\u00f3rios","text":"<p>O Nextflow pode produzir v\u00e1rios relat\u00f3rios e gr\u00e1ficos fornecendo v\u00e1rias m\u00e9tricas de tempo de execu\u00e7\u00e3o e informa\u00e7\u00f5es de execu\u00e7\u00e3o.</p> <p>Execute o fluxo de trabalho rnaseq-nf introduzido anteriormente, conforme mostrado abaixo:</p> <pre><code>nextflow run rnaseq-nf -with-docker -with-report -with-trace -with-timeline -with-dag dag.png\n</code></pre> <p>A op\u00e7\u00e3o <code>-with-docker</code> inicia cada tarefa da execu\u00e7\u00e3o como um comando de execu\u00e7\u00e3o de cont\u00eainer do Docker.</p> <p>A op\u00e7\u00e3o <code>-with-report</code> permite a cria\u00e7\u00e3o do relat\u00f3rio de execu\u00e7\u00e3o do fluxo de trabalho. Abra o arquivo <code>report.html</code> com um navegador para ver o relat\u00f3rio criado com o comando acima.</p> <p>A op\u00e7\u00e3o <code>-with-trace</code> permite a cria\u00e7\u00e3o de um arquivo separado por tabula\u00e7\u00f5es (TSV) contendo informa\u00e7\u00f5es de tempo de execu\u00e7\u00e3o para cada tarefa executada. Verifique o <code>trace.txt</code> para um exemplo.</p> <p>A op\u00e7\u00e3o <code>-with-timeline</code> permite a cria\u00e7\u00e3o do relat\u00f3rio da linha do tempo do fluxo de trabalho mostrando como os processos foram executados ao longo do tempo. Isso pode ser \u00fatil para identificar as tarefas e gargalos que consomem mais tempo. Veja um exemplo neste link.</p> <p>Por fim, a op\u00e7\u00e3o <code>-with-dag</code> permite a renderiza\u00e7\u00e3o da representa\u00e7\u00e3o de grafo ac\u00edclico direcionado da execu\u00e7\u00e3o do fluxo de trabalho. Nota: Este recurso requer a instala\u00e7\u00e3o do Graphviz em seu computador. Veja aqui para mais detalhes. Ent\u00e3o tente executar:</p> <pre><code>open dag.png\n</code></pre> <p>Warning</p> <p>As m\u00e9tricas de tempo de execu\u00e7\u00e3o podem estar incompletas para execu\u00e7\u00f5es com tarefas com curto tempo de execu\u00e7\u00e3o, como no caso deste tutorial.</p> <p>Info</p> <p>Voc\u00ea visualiza os arquivos HTML clicando com o bot\u00e3o direito do mouse no nome do arquivo na barra lateral esquerda e escolhendo o item de menu Show Preview.</p>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#execute-um-projeto-do-github","title":"Execute um projeto do GitHub","text":"<p>O Nextflow permite a execu\u00e7\u00e3o de um projeto de fluxo de trabalho diretamente de um reposit\u00f3rio do GitHub (ou servi\u00e7os semelhantes, por exemplo, BitBucket e GitLab).</p> <p>Isso simplifica o compartilhamento e implanta\u00e7\u00e3o de projetos complexos e o rastreamento de mudan\u00e7as de uma maneira consistente.</p> <p>O reposit\u00f3rio do GitHub a seguir hospeda uma vers\u00e3o completa do fluxo de trabalho apresentado neste tutorial: https://github.com/nextflow-io/rnaseq-nf</p> <p>Voc\u00ea pode execut\u00e1-lo especificando o nome do projeto e com isso iniciar a execu\u00e7\u00e3o de cada tarefa como um comando de execu\u00e7\u00e3o de cont\u00eainer do Docker:</p> <pre><code>nextflow run nextflow-io/rnaseq-nf -with-docker\n</code></pre> <p>Ele baixa automaticamente o cont\u00eainer e o armazena na pasta <code>$HOME/.nextflow</code>.</p> <p>Use o comando <code>info</code> para mostrar as informa\u00e7\u00f5es do projeto:</p> <pre><code>nextflow info nextflow-io/rnaseq-nf\n</code></pre> <p>O Nextflow permite a execu\u00e7\u00e3o de uma revis\u00e3o espec\u00edfica do seu projeto usando a op\u00e7\u00e3o de linha de comando <code>-r</code>. Por exemplo:</p> <pre><code>nextflow run nextflow-io/rnaseq-nf -r v2.1 -with-docker\n</code></pre> <p>As revis\u00f5es s\u00e3o definidas usando etiquetas do Git ou ramos definidos no reposit\u00f3rio do projeto.</p> <p>As etiquetas permitem o controle preciso das altera\u00e7\u00f5es nos arquivos e depend\u00eancias do projeto ao longo do tempo.</p>"},{"location":"pt/archive/basic_training/rnaseq_pipeline/#mais-recursos","title":"Mais recursos","text":"<ul> <li>Documenta\u00e7\u00e3o do Nextflow - A p\u00e1gina inicial dos documentos do Nextflow.</li> <li>Nextflow patterns - Uma cole\u00e7\u00e3o de padr\u00f5es de implementa\u00e7\u00e3o do Nextflow.</li> <li>CalliNGS-NF - Um fluxo de trabalho de chamada de variante implementando as melhores pr\u00e1ticas recomendadas do GATK.</li> <li>nf-core - Uma cole\u00e7\u00e3o comunit\u00e1ria de fluxos de trabalho gen\u00f4micos prontos para produ\u00e7\u00e3o.</li> </ul>"},{"location":"pt/archive/basic_training/seqera_platform/","title":"Seqera Platform","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/archive/basic_training/seqera_platform/#comece-a-usar-a-seqera-platform","title":"Comece a usar a Seqera Platform","text":""},{"location":"pt/archive/basic_training/seqera_platform/#conceitos-basicos","title":"Conceitos B\u00e1sicos","text":"<p>A Seqera Platform, conhecida anteriormente como Nextflow Tower, \u00e9 o posto de comando centralizado para gerenciamento de dados e fluxos de trabalho. Ele traz monitoramento, gerenciamento de logs e observabilidade para fluxos de trabalho distribu\u00eddos e simplifica a implanta\u00e7\u00e3o de fluxos de trabalho em qualquer nuvem, cluster ou laptop. Na terminologia da Seqera Platform, um fluxo de trabalho \u00e9 o que temos trabalhado at\u00e9 agora, e os pipelines s\u00e3o fluxos de trabalho pr\u00e9-configurados que podem ser usados por todos os usu\u00e1rios em um espa\u00e7o de trabalho. Ele \u00e9 composto por um reposit\u00f3rio de fluxo de trabalho, par\u00e2metros de inicializa\u00e7\u00e3o e um ambiente de computa\u00e7\u00e3o. Vamos nos ater a essas defini\u00e7\u00f5es nesta se\u00e7\u00e3o.</p> <p>Os principais recursos da Seqera Platform incluem:</p> <ul> <li>O lan\u00e7amento de pipelines pr\u00e9-configurados com facilidade.</li> <li>Integra\u00e7\u00e3o program\u00e1tica para atender \u00e0s necessidades de uma organiza\u00e7\u00e3o.</li> <li>Disponibiliza\u00e7\u00e3o pipelines em \u00e1reas de trabalho compartilhadas.</li> <li>Gerenciamento da infraestrutura necess\u00e1ria para executar an\u00e1lise de dados em escala.</li> </ul> <p>Tip</p> <p>Registre-se para experimentar a Seqera Platform gratuitamente ou solicitar uma demonstra\u00e7\u00e3o para implanta\u00e7\u00f5es em seu pr\u00f3prio ambiente local ou na nuvem.</p>"},{"location":"pt/archive/basic_training/seqera_platform/#como-usar","title":"Como usar","text":"<p>Voc\u00ea pode usar a Seqera Platform por meio da op\u00e7\u00e3o <code>-with-tower</code> ao usar o comando <code>nextflow run</code>, por meio da interface gr\u00e1fica online ou da API.</p>"},{"location":"pt/archive/basic_training/seqera_platform/#com-o-comando-nextflow-run","title":"Com o comando <code>nextflow run</code>","text":"<p>Crie uma conta e fa\u00e7a login na Seqera Platform.</p> <p>1. Crie um novo token</p> <p>Voc\u00ea pode acessar seus tokens no menu suspenso Settings:</p> <p></p> <p>2. D\u00ea um nome para seu token</p> <p></p> <p>3. Salve seu token com seguran\u00e7a</p> <p>Copie e guarde seu novo token em um local seguro.</p> <p></p> <p>4. Exporte seu token</p> <p>Uma vez que seu token foi criado, abra um terminal e digite:</p> <pre><code>export TOWER_ACCESS_TOKEN=eyxxxxxxxxxxxxxxxQ1ZTE=\n</code></pre> <p>Onde <code>eyxxxxxxxxxxxxxxxQ1ZTE=</code> \u00e9 o token que voc\u00ea acabou de criar.</p> <p>Note</p> <p>Verifique seu <code>nextflow -version</code>. Os tokens de portador requerem ao menos a vers\u00e3o 20.10.0 do Nextflow ou vers\u00f5es posteriores e isso pode ser configurado com o segundo comando mostrado acima. Voc\u00ea pode alterar para uma outra vers\u00e3o, se necess\u00e1rio.</p> <p>Para enviar um pipeline para uma \u00e1rea de trabalho (workspace) usando a ferramenta de linha de comando do Nextflow, adicione o ID da \u00e1rea de trabalho em seu ambiente. Por exemplo:</p> <pre><code>export TOWER_WORKSPACE_ID=000000000000000\n</code></pre> <p>O ID da \u00e1rea de trabalho pode ser encontrado na p\u00e1gina de vis\u00e3o geral das \u00e1reas de trabalho (workspaces) da organiza\u00e7\u00e3o.</p> <p>5. Execute o Nextflow com a Seqera Platform</p> <p>Execute normalmente seus fluxos de trabalho do Nextflow com a adi\u00e7\u00e3o do comando <code>-with-tower</code>:</p> <pre><code>nextflow run hello.nf -with-tower\n</code></pre> <p>Voc\u00ea ver\u00e1 e poder\u00e1 monitorar suas tarefas do Nextflow na Seqera Platform.</p> <p>Para configurar e executar tarefas do Nextflow em ambientes na nuvem, visite a se\u00e7\u00e3o de ambientes de computa\u00e7\u00e3o (compute environments).</p> <p>Exercise</p> <p>Execute o script de RNA-Seq <code>script7.nf</code> usando o sinalizador <code>-with-tower</code>, depois de concluir corretamente as configura\u00e7\u00f5es de token descritas acima.</p> Tip <p>V\u00e1 para https://tower.nf/, fa\u00e7a o login, em seguida, clique na guia de execu\u00e7\u00f5es (Run) e selecione a execu\u00e7\u00e3o que voc\u00ea acabou de enviar. Se voc\u00ea n\u00e3o conseguir encontr\u00e1-la, verifique novamente se seu token foi adicionado corretamente ao ambiente.</p>"},{"location":"pt/archive/basic_training/seqera_platform/#com-uma-interface-grafica-online","title":"Com uma interface gr\u00e1fica online","text":"<p>Para executar usando a interface gr\u00e1fica (GUI), existem tr\u00eas etapas principais:</p> <ol> <li>Crie uma conta e fa\u00e7a login na Seqera Platform, dispon\u00edvel gratuitamente, em tower.nf.</li> <li>Crie e configure um novo ambiente de computa\u00e7\u00e3o (compute environment).</li> <li>Comece a lan\u00e7ar pipelines.</li> </ol>"},{"location":"pt/archive/basic_training/seqera_platform/#configurando-seu-ambiente-de-computacao","title":"Configurando seu ambiente de computa\u00e7\u00e3o","text":"<p>A Seqera Platform usa o conceito de Ambientes de Computa\u00e7\u00e3o (compute environments) para definir a plataforma de execu\u00e7\u00e3o onde um fluxo de trabalho ser\u00e1 executado.</p> <p>Ele suporta o lan\u00e7amento de fluxos de trabalho em um n\u00famero crescente de infraestruturas de nuvem e on-prem (infraestrutura dedicada).</p> <p></p> <p>Cada ambiente de computa\u00e7\u00e3o deve ser pr\u00e9-configurado para permitir que a Seqera Platform envie tarefas. Voc\u00ea pode ler mais sobre como configurar cada ambiente usando os links abaixo.</p> <p>Os guias a seguir descrevem como configurar cada um desses ambientes de computa\u00e7\u00e3o.</p> <ul> <li>AWS Batch</li> <li>Azure Batch</li> <li>Google Cloud</li> <li>Google Batch</li> <li>Google Life Sciences</li> <li>IBM LSF</li> <li>Slurm</li> <li>Grid Engine</li> <li>Altair PBS Pro</li> <li>Amazon Kubernetes (EKS)</li> <li>Google Kubernetes (GKE)</li> <li>Hosted Kubernetes</li> </ul>"},{"location":"pt/archive/basic_training/seqera_platform/#selecionando-um-ambiente-de-computacao-padrao","title":"Selecionando um ambiente de computa\u00e7\u00e3o padr\u00e3o","text":"<p>Se voc\u00ea tiver mais de um Ambiente de computa\u00e7\u00e3o, poder\u00e1 selecionar qual deles ser\u00e1 usado por padr\u00e3o ao lan\u00e7ar um pipeline.</p> <ol> <li>Navegue at\u00e9 os seus ambientes de computa\u00e7\u00e3o.</li> <li>Escolha seu ambiente padr\u00e3o selecionando o bot\u00e3o Make primary.</li> </ol> <p>Parab\u00e9ns!</p> <p>Agora voc\u00ea est\u00e1 pronto para lan\u00e7ar fluxos de trabalho com seu ambiente de computa\u00e7\u00e3o principal.</p>"},{"location":"pt/archive/basic_training/seqera_platform/#launchpad","title":"Launchpad","text":"<p>O Launchpad torna f\u00e1cil para qualquer usu\u00e1rio da \u00e1rea de trabalho lan\u00e7ar um pipeline pr\u00e9-configurado.</p> <p></p> <p>Um pipeline \u00e9 um reposit\u00f3rio que cont\u00e9m um fluxo de trabalho do Nextflow, um ambiente de computa\u00e7\u00e3o e par\u00e2metros de fluxo de trabalho.</p>"},{"location":"pt/archive/basic_training/seqera_platform/#formulario-de-parametros-de-pipeline","title":"Formul\u00e1rio de Par\u00e2metros de Pipeline","text":"<p>O Launchpad detecta automaticamente a presen\u00e7a de um <code>nextflow_schema.json</code> na raiz do reposit\u00f3rio e cria dinamicamente um formul\u00e1rio onde os usu\u00e1rios podem facilmente atualizar os par\u00e2metros.</p> <p>Info</p> <p>A exibi\u00e7\u00e3o de formul\u00e1rios de par\u00e2metro aparecer\u00e1 se o pipeline tiver um arquivo de esquema do Nextflow para os par\u00e2metros. Consulte o Guia do esquema do Nextflow para saber mais sobre os casos de uso do arquivo de esquema e como cri\u00e1-los.</p> <p>Isso torna trivial para usu\u00e1rios sem experi\u00eancia em Nextflow inserir seus par\u00e2metros de fluxo de trabalho e lan\u00e7\u00e1-lo.</p> <p></p>"},{"location":"pt/archive/basic_training/seqera_platform/#adicionando-um-novo-pipeline","title":"Adicionando um novo pipeline","text":"<p>A adi\u00e7\u00e3o de um pipeline ao launchpad da \u00e1rea de trabalho \u00e9 detalhada na \u00edntegra na documenta\u00e7\u00e3o da Seqera Platform.</p> <p>Em resumo, essas s\u00e3o as etapas que voc\u00ea precisa seguir para configurar um pipeline.</p> <ol> <li>Selecione o bot\u00e3o Launchpad na barra de navega\u00e7\u00e3o. Isso abrir\u00e1 o Formul\u00e1rio de inicializa\u00e7\u00e3o.</li> <li>Selecione um ambiente de computa\u00e7\u00e3o.</li> <li>Insira o reposit\u00f3rio do fluxo de trabalho que voc\u00ea deseja iniciar. e.g. https://github.com/nf-core/rnaseq.git</li> <li>Selecione um n\u00famero de revis\u00e3o para o fluxo de trabalho. O branching padr\u00e3o do Git (main/master) ou <code>manifest.defaultBranch</code> na configura\u00e7\u00e3o do Nextflow ser\u00e1 usada por padr\u00e3o.</li> <li>Defina o local do diret\u00f3rio de trabalho (<code>workDir</code>) do Nextflow. O local associado ao ambiente de computa\u00e7\u00e3o ser\u00e1 selecionado por padr\u00e3o.</li> <li>Digite o(s) nome(s) de cada um dos perfis de configura\u00e7\u00e3o do Nextflow seguido da tecla <code>enter</code>. Veja mais na documenta\u00e7\u00e3o oficial sobre a configura\u00e7\u00e3o de perfis.</li> <li>Insira quaisquer par\u00e2metros do fluxo de trabalho no formato YAML ou JSON. Exemplo com YAML:</li> </ol> <pre><code>leituras: \"s3://nf-bucket/exome-data/ERR013140_{1,2}.fastq.bz2\"\npares_de_leituras: true\n</code></pre> <ol> <li>Selecione Launch para iniciar a execu\u00e7\u00e3o do pipeline.</li> </ol> <p>Info</p> <p>Os fluxos de trabalho do Nextflow s\u00e3o simplesmente reposit\u00f3rios Git e podem ser alterados para qualquer plataforma de hospedagem Git p\u00fablica ou privada. Consulte Integra\u00e7\u00e3o com o Git nos documentos da Seqera Platform e Compartilhamento de Pipelines na documenta\u00e7\u00e3o do Nextflow para obter mais detalhes.</p> <p>Note</p> <p>As credenciais associadas ao ambiente de computa\u00e7\u00e3o devem ser capazes de acessar o diret\u00f3rio de trabalho.</p> <p>Info</p> <p>Na configura\u00e7\u00e3o, o caminho completo para um bucket deve ser especificado com aspas simples em torno de strings e sem aspas em booleanas ou n\u00fameros.</p> <p>Tip</p> <p>Para criar seu pr\u00f3prio esquema de Nextflow personalizado para seu fluxo de trabalho, veja os exemplos dos fluxos de trabalho do <code>nf-core</code> que adotaram essa abordagem. Por exemplo, o eager e o rnaseq.</p> <p>Para op\u00e7\u00f5es de configura\u00e7\u00f5es avan\u00e7adas, confira essa p\u00e1gina.</p> <p>Tamb\u00e9m h\u00e1 suporte da comunidade dispon\u00edvel se voc\u00ea tiver problemas, junte-se ao Slack do Nextflow seguindo este link.</p>"},{"location":"pt/archive/basic_training/seqera_platform/#api","title":"API","text":"<p>Para saber mais sobre como usar a API da Seqera Platform, visite a se\u00e7\u00e3o da API na documenta\u00e7\u00e3o.</p>"},{"location":"pt/archive/basic_training/seqera_platform/#areas-de-trabalho-e-organizacoes","title":"\u00c1reas de trabalho e Organiza\u00e7\u00f5es","text":"<p>A Seqera Platform simplifica o desenvolvimento e a execu\u00e7\u00e3o de pipelines, fornecendo uma interface centralizada para usu\u00e1rios e organiza\u00e7\u00f5es.</p> <p>Cada usu\u00e1rio tem uma \u00e1rea de trabalho exclusiva onde pode interagir e gerenciar todos os recursos, como fluxos de trabalho, ambientes de computa\u00e7\u00e3o e credenciais. Detalhes disso podem ser encontrados aqui.</p> <p>As organiza\u00e7\u00f5es podem ter v\u00e1rios espa\u00e7os de trabalho com acesso personalizado para membros e colaboradores espec\u00edficos da organiza\u00e7\u00e3o.</p>"},{"location":"pt/archive/basic_training/seqera_platform/#recursos-de-organizacao","title":"Recursos de organiza\u00e7\u00e3o","text":"<p>Voc\u00ea pode criar sua pr\u00f3pria organiza\u00e7\u00e3o e \u00e1rea de trabalho de participante seguindo a documenta\u00e7\u00e3o aqui.</p> <p>A Seqera Platform permite a cria\u00e7\u00e3o de v\u00e1rias organiza\u00e7\u00f5es, cada uma das quais pode conter v\u00e1rias \u00e1reas de trabalho com usu\u00e1rios e recursos compartilhados. Isso permite que qualquer organiza\u00e7\u00e3o personalize e organize o uso de recursos enquanto mant\u00e9m uma camada de controle de acesso para usu\u00e1rios associados a uma \u00e1rea de trabalho.</p>"},{"location":"pt/archive/basic_training/seqera_platform/#usuarios-da-organizacao","title":"Usu\u00e1rios da organiza\u00e7\u00e3o","text":"<p>Qualquer usu\u00e1rio pode ser adicionado ou removido de uma determinada organiza\u00e7\u00e3o ou \u00e1rea de trabalho e pode receber um papel de acesso espec\u00edfico dentro dessa \u00e1rea de trabalho.</p> <p>O recurso Equipes fornece uma maneira para as organiza\u00e7\u00f5es agruparem v\u00e1rios usu\u00e1rios e participantes em equipes. Por exemplo, <code>desenvolvedores-fluxos de trabalho</code> ou <code>analistas</code>, e aplicar controle de acesso a todos os usu\u00e1rios dentro desta equipe coletivamente.</p> <p>Para mais informa\u00e7\u00f5es, consulte a se\u00e7\u00e3o de Gerenciamento de Usu\u00e1rio.</p>"},{"location":"pt/archive/basic_training/seqera_platform/#configurando-uma-nova-organizacao","title":"Configurando uma nova organiza\u00e7\u00e3o","text":"<p>As organiza\u00e7\u00f5es s\u00e3o a estrutura de n\u00edvel mais alto e cont\u00eam \u00e1reas de trabalho, membros, equipes e colaboradores.</p> <p>Para criar uma nova Organiza\u00e7\u00e3o:</p> <ol> <li>Clique no menu suspenso ao lado do seu nome e selecione New organization para abrir a caixa de di\u00e1logo de cria\u00e7\u00e3o.</li> <li> <p>Na caixa de di\u00e1logo, preencha os campos de acordo com sua organiza\u00e7\u00e3o. Os campos Name e Full name s\u00e3o obrigat\u00f3rios.</p> <p>Note</p> <p>Um nome v\u00e1lido para a organiza\u00e7\u00e3o deve seguir um padr\u00e3o espec\u00edfico. Consulte a interface de usu\u00e1rio para obter mais instru\u00e7\u00f5es.</p> </li> <li> <p>O restante dos campos, como Description, Location, Website URL e logo URL, s\u00e3o opcionais.</p> </li> <li> <p>Depois que os detalhes forem preenchidos, voc\u00ea poder\u00e1 acessar a organiza\u00e7\u00e3o rec\u00e9m-criada usando a p\u00e1gina da organiza\u00e7\u00e3o, que lista todas as suas organiza\u00e7\u00f5es.</p> <p>Note</p> <p>\u00c9 poss\u00edvel alterar os valores dos campos opcionais usando a op\u00e7\u00e3o Edit na p\u00e1gina da organiza\u00e7\u00e3o ou usando a guia Settings na p\u00e1gina da organiza\u00e7\u00e3o, desde que voc\u00ea seja o Propriet\u00e1rio (Owner) da organiza\u00e7\u00e3o.</p> <p>Tip</p> <p>Uma lista de todos os Membros, Equipes e Colaboradores inclu\u00eddos pode ser encontrada na p\u00e1gina da organiza\u00e7\u00e3o.</p> </li> </ol>"},{"location":"pt/archive/basic_training/setup/","title":"Setup","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"pt/archive/basic_training/setup/#configuracao-do-ambiente","title":"Configura\u00e7\u00e3o do ambiente","text":"<p>Existem duas principais maneiras de come\u00e7ar este treinamento da Comunidade do Nextflow.</p> <p>A primeira \u00e9 instalar os requisitos localmente, o que \u00e9 melhor se voc\u00ea j\u00e1 estiver familiarizado com Git e Docker ou trabalhando offline.</p> <p>A segunda \u00e9 usar o Gitpod, o que \u00e9 melhor para iniciantes, pois esta plataforma cont\u00e9m todos os programas e dados necess\u00e1rios. Basta clicar no link abaixo e fazer login usando sua conta do GitHub para iniciar o tutorial:</p> <p></p>"},{"location":"pt/archive/basic_training/setup/#instalacao-local","title":"Instala\u00e7\u00e3o local","text":"<p>O Nextflow pode ser usado em qualquer sistema compat\u00edvel com POSIX (Linux, macOS, Windows Subsystem for Linux, etc.).</p>"},{"location":"pt/archive/basic_training/setup/#requisitos","title":"Requisitos","text":"<ul> <li>Bash</li> <li>Java 11 (ou uma vers\u00e3o posterior, at\u00e9 a 18)</li> <li>Git</li> <li>Docker</li> </ul>"},{"location":"pt/archive/basic_training/setup/#requisitos-opcionais-para-este-tutorial","title":"Requisitos opcionais para este tutorial","text":"<ul> <li>Singularity 2.5.x (ou uma vers\u00e3o posterior)</li> <li>Conda 4.5 (ou uma vers\u00e3o posterior)</li> <li>Graphviz</li> <li>AWS CLI</li> <li>Um ambiente de computa\u00e7\u00e3o do AWS Batch configurado</li> </ul>"},{"location":"pt/archive/basic_training/setup/#baixe-o-nextflow","title":"Baixe o Nextflow","text":"<p>Digite este comando no seu terminal:</p> <pre><code>wget -qO- https://get.nextflow.io | bash\n</code></pre> <p>Ou, se preferir o <code>curl</code>:</p> <pre><code>curl -s https://get.nextflow.io | bash\n</code></pre> <p>Em seguida, certifique-se que o bin\u00e1rio baixado \u00e9 execut\u00e1vel:</p> <pre><code>chmod +x nextflow\n</code></pre> <p>E coloque o execut\u00e1vel <code>nextflow</code> em seu <code>$PATH</code> (por exemplo, <code>/usr/local/bin</code> ou <code>/bin/</code>)</p>"},{"location":"pt/archive/basic_training/setup/#docker","title":"Docker","text":"<p>Certifique-se de ter o Docker Desktop em execu\u00e7\u00e3o em sua m\u00e1quina. Baixe o Docker aqui.</p>"},{"location":"pt/archive/basic_training/setup/#material-de-treinamento","title":"Material de treinamento","text":"<p>Voc\u00ea pode ver o material de treinamento aqui: https://training.nextflow.io/</p> <p>Para baixar o material use o comando:</p> <pre><code>git clone https://github.com/nextflow-io/training.git\n</code></pre> <p>Em seguida, <code>cd</code> no diret\u00f3rio <code>nf-training</code>.</p>"},{"location":"pt/archive/basic_training/setup/#verificando-sua-instalacao","title":"Verificando sua instala\u00e7\u00e3o","text":"<p>Verifique se o <code>nextflow</code> foi instalado corretamente executando o seguinte comando:</p> <pre><code>nextflow info\n</code></pre> <p>Isso deve mostrar a vers\u00e3o atual, sistema operacional e sistema de tempo de execu\u00e7\u00e3o.</p>"},{"location":"pt/archive/basic_training/setup/#gitpod","title":"Gitpod","text":"<p>Um ambiente de desenvolvimento Nextflow pr\u00e9-configurado est\u00e1 dispon\u00edvel no Gitpod.</p>"},{"location":"pt/archive/basic_training/setup/#requisitos_1","title":"Requisitos","text":"<ul> <li>Uma conta no GitHub</li> <li>Navegador Web (Google Chrome, Firefox)</li> <li>Conex\u00e3o com a Internet</li> </ul>"},{"location":"pt/archive/basic_training/setup/#gitpod-quick-start","title":"Gitpod quick start","text":"<p>Para executar o Gitpod:</p> <ul> <li>Clique na URL a seguir: https://gitpod.io/#https://github.com/nextflow-io/training</li> <li>Essa URL \u00e9 o link do reposit\u00f3rio do treinamento no GitHub, prefixado com <code>https://gitpod.io/#</code></li> <li>Fa\u00e7a login na sua conta do GitHub (e permita a autoriza\u00e7\u00e3o).</li> </ul> <p>Depois de fazer login, o Gitpod deve carregar (pule a pr\u00e9-compila\u00e7\u00e3o, se solicitado).</p>"},{"location":"pt/archive/basic_training/setup/#explore-a-interface-de-desenvolvimento-do-gitpod","title":"Explore a interface de desenvolvimento do Gitpod","text":"<p>Agora voc\u00ea deve ver algo semelhante a imagem a seguir:</p> <p></p> <ul> <li>A barra lateral permite que voc\u00ea personalize seu ambiente Gitpod e execute tarefas b\u00e1sicas (copiar, colar, abrir arquivos, pesquisar, git, etc.). Clique no bot\u00e3o Explorer para ver quais arquivos est\u00e3o neste reposit\u00f3rio.</li> <li>O terminal permite que voc\u00ea execute todos os programas mencionados no reposit\u00f3rio. Por exemplo, tanto <code>nextflow</code> quanto <code>docker</code> est\u00e3o instalados e podem ser executados.</li> <li>A janela principal permite visualizar e editar arquivos. Clique em um arquivo no explorer para abri-lo na janela principal. Voc\u00ea tamb\u00e9m deve ver o material de treinamento em uma das janelas (https://training.nextflow.io/).</li> </ul> <p>Para testar se o ambiente est\u00e1 funcionando corretamente, digite o seguinte comando no terminal:</p> <pre><code>nextflow info\n</code></pre> <p>Isso deve mostrar a vers\u00e3o do Nextflow e as informa\u00e7\u00f5es do sistema de tempo de execu\u00e7\u00e3o:</p> <pre><code>Version: 22.10.4 build 5836\nCreated: 09-12-2022 09:58 UTC\nSystem: Linux 5.15.0-47-generic\nRuntime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 17.0.3-internal+0-adhoc..src\nEncoding: UTF-8 (UTF-8)\n</code></pre>"},{"location":"pt/archive/basic_training/setup/#recursos-do-gitpod","title":"Recursos do Gitpod","text":"<ul> <li>O Gitpod oferece 500 cr\u00e9ditos gratuitos por m\u00eas, o que equivale a 50 horas de tempo de execu\u00e7\u00e3o no ambiente de trabalho padr\u00e3o (at\u00e9 4 n\u00facleos, 8 GB de RAM e 30 GB de armazenamento).</li> <li>H\u00e1 tamb\u00e9m uma op\u00e7\u00e3o de ambiente de trabalho que oferece at\u00e9 8 n\u00facleos, 16 GB de RAM e 50 GB de armazenamento. No entanto, o ambiente de trabalho grande usar\u00e1 seus cr\u00e9ditos gratuitos mais rapidamente e voc\u00ea ter\u00e1 menos horas de acesso a esse ambiente.</li> <li>Sua sess\u00e3o no Gitpod expirar\u00e1 ap\u00f3s 30 minutos de inatividade e suas altera\u00e7\u00f5es ser\u00e3o salvas por at\u00e9 2 semanas (consulte a pr\u00f3xima se\u00e7\u00e3o para reabrir uma sess\u00e3o expirada).</li> </ul> <p>Acesse gitpod.io para mais detalhes.</p>"},{"location":"pt/archive/basic_training/setup/#reabrindo-uma-sessao-do-gitpod","title":"Reabrindo uma sess\u00e3o do Gitpod","text":"<p>Voc\u00ea pode reabrir um ambiente em https://gitpod.io/workspaces. Encontre seu ambiente anterior na lista, selecione as retic\u00eancias (\u00edcone de tr\u00eas pontos) e selecione Abrir.</p> <p>Se voc\u00ea salvou a URL do seu ambiente Gitpod anterior, basta abri-lo em seu navegador.</p> <p>Como alternativa, voc\u00ea pode iniciar um novo ambiente de trabalho seguindo a seguinte URL do Gitpod: https://gitpod.io/#https://github.com/nextflow-io/training</p> <p>Se voc\u00ea perdeu seu ambiente, pode encontrar os principais scripts usados neste tutorial no diret\u00f3rio <code>nf-training</code>.</p>"},{"location":"pt/archive/basic_training/setup/#salvando-arquivos-do-gitpod-em-sua-maquina-local","title":"Salvando arquivos do Gitpod em sua m\u00e1quina local","text":"<p>Para salvar qualquer arquivo a partir do Explorar na barra lateral, clique com o bot\u00e3o direito do mouse no arquivo e selecione Download.</p>"},{"location":"pt/archive/basic_training/setup/#material-do-treinamento","title":"Material do treinamento","text":"<p>O material do treinamento pode ser acessado atrav\u00e9s do seu navegador a partir de https://training.nextflow.io/</p>"},{"location":"pt/archive/basic_training/setup/#selecionando-a-versao-do-nextflow","title":"Selecionando a vers\u00e3o do Nextflow","text":"<p>Por padr\u00e3o, o Nextflow baixar\u00e1 a vers\u00e3o est\u00e1vel mais recente para o seu ambiente.</p> <p>No entanto, o Nextflow est\u00e1 em constante evolu\u00e7\u00e3o \u00e0 medida que fazemos melhorias e corrigimos bugs.</p> <p>Os \u00faltimos lan\u00e7amentos podem ser vistos no GitHub aqui.</p> <p>Se voc\u00ea deseja usar uma vers\u00e3o espec\u00edfica do Nextflow, pode definir a vari\u00e1vel <code>NXF_VER</code> conforme mostrado abaixo:</p> <pre><code>export NXF_VER=23.10.0\n</code></pre> <p>Note</p> <p>Este treinamento requer <code>NXF_VER=23.10.0</code>, ou posterior. Esta vers\u00e3o usar\u00e1 a DSL2 como padr\u00e3o.</p> <p>Execute <code>nextflow -version</code> novamente para confirmar que a altera\u00e7\u00e3o entrou em vigor.</p>"},{"location":"pt/envsetup/","title":"Configura\u00e7\u00e3o do ambiente","text":"<p>Os cursos de treinamento oferecidos no portal de treinamento da comunidade de Nextflow s\u00e3o otimizados para uso dentro do nosso ambiente Gitpod.</p> <p>O Gitpod oferece uma m\u00e1quina virtual com tudo j\u00e1 configurado para seu uso, acess\u00e1vel atrav\u00e9s do seu navegador ou integrados com seu editor de texto (por exemplo, VSCode).</p> <p>Se voc\u00ea j\u00e1 possui uma conta no Gitpod, clique no bot\u00e3o abaixo, caso contr\u00e1rio, continue nesse m\u00f3dulo para configurar sua conta.</p> <p>Vamos come\u00e7ar!</p> <p></p>"},{"location":"pt/envsetup/01_setup/","title":"Gitpod","text":"<p>Gitpod \u00e9 um ambiente de desenvolvimento em nuvem feito para equipes desenvolverem software de forma segura e eficiente. Ele pode aprimorar sua experi\u00eancia de desenvolvedor ao possibilitar que programe em um ambiente de desenvolvimento em nuvem.</p>"},{"location":"pt/envsetup/01_setup/#criando-uma-conta-no-gitpod","title":"Criando uma conta no Gitpod","text":"<p>Voc\u00ea pode criar uma conta gratuita no Gitpod usando sua conta pr\u00e9-existente no GitLab, GitHub ou Bitbucket.</p> <p>Voc\u00ea pode criar uma conta usando a p\u00e1gina de login do Gitpod.</p> <p></p> <p>Recomendamos que conecte sua conta do LinkedIn para receber um tempo adicional de uso de 50 horas.</p> <p></p> <p>Depois de selecionar seu editor e tema preferidos e conferir os detalhes do seu perfil, clique em continuar e sua conta ser\u00e1 criada e j\u00e1 estar\u00e1 pronta para uso.</p> <p>Note</p> <p>Recomendamos que utilize o editor de texto VS Code.</p>"},{"location":"pt/envsetup/01_setup/#executando-o-gitpod","title":"Executando o Gitpod","text":"<p>Clique na URL a seguir para executar o Gitpod: https://gitpod.io/#https://github.com/nextflow-io/training</p> <p>Essa URL \u00e9 o reposit\u00f3rio de treinamento do Nextflow prefixado com <code>https://gitpod.io/#</code>.</p> <p>Voc\u00ea tamb\u00e9m pode acessar o material clicando no bot\u00e3o abaixo.</p> <p></p> <p>Se voc\u00ea j\u00e1 estiver logado, seu ambiente no Gitpod come\u00e7ar\u00e1 a carregar.</p>"},{"location":"pt/envsetup/01_setup/#explore-sua-ide-no-gitpod","title":"Explore sua IDE no Gitpod","text":"<p>Ap\u00f3s o carregamento concluir, voc\u00ea poder\u00e1 ver algo semelhante a isso:</p> <p></p> <ul> <li>A barra lateral permite que voc\u00ea customize seu ambiente Gitpod e realize tarefas b\u00e1sicas (copiar, colar, abrir arquivos, buscar, git, etc.). Voc\u00ea pode clicar no explorador para ver que arquivos est\u00e3o presentes em seu reposit\u00f3rio.</li> <li>O terminal permite que voc\u00ea execute todos os programas no reposit\u00f3rio. Por exemplo, <code>nextflow</code> e <code>docker</code> est\u00e3o instalados e podem ser executados.</li> <li>O explorador de arquivos permite que voc\u00ea visualize e edite arquivos. Clicar em um arquivo no explorador ir\u00e1 abri-lo na janela principal.</li> <li>O navegador permite que voc\u00ea visualize o material de treinamento (https://training.nextflow.io/). Caso o feche acidentalmente, voc\u00ea pode iniciar o navegador novamente executando o seguinte comando no terminal: <code>gp preview https://training.nextflow.io</code>.</li> </ul>"},{"location":"pt/envsetup/01_setup/#recursos-do-gitpod","title":"Recursos do Gitpod","text":"<p>O Gitpod fornece 500 cr\u00e9ditos gratuitos por m\u00eas, o que \u00e9 equivalente a 50 horas de uso gratuito do ambiente de execu\u00e7\u00e3o usando a \u00e1rea de trabalho padr\u00e3o (at\u00e9 4 n\u00facleos, 8 GB de RAM e 30 GB de armazenamento).</p> <p>Tamb\u00e9m h\u00e1 a op\u00e7\u00e3o de uma \u00e1rea de trabalho maior, que fornece at\u00e9 8 n\u00facleos, 16 GB de RAM e 50 GB de armazenamento. No entanto, essa \u00e1rea de trabalho maior ir\u00e1 utilizar seus recursos mais rapidamente e voc\u00ea ter\u00e1 menos horas de acesso a ela.</p> <p>O ambiente Gitpod ir\u00e1 pausar ap\u00f3s 30 minutos de inatividade e salvar\u00e1 suas mudan\u00e7as por at\u00e9 2 semanas.</p> <p>Mais informa\u00e7\u00e3o sobre o Gitpod est\u00e1 dispon\u00edvel em gitpod.io.</p>"},{"location":"pt/envsetup/01_setup/#reiniciando-uma-sessao-no-gitpod","title":"Reiniciando uma sess\u00e3o no Gitpod","text":"<p>Voc\u00ea pode reiniciar um ambiente na p\u00e1gina https://gitpod.io/workspaces. Ambientes anteriores ser\u00e3o listados nessa p\u00e1gina. Basta selecionar a elipse (os tr\u00eas pontos) e ent\u00e3o eselcionar <code>Open</code> para reiniciar um ambiente anterior.</p> <p>Se voc\u00ea salvou a URL de um ambiente anterior do Gitpod, para reinici\u00e1-lo basta abrir a URL em seu navegador.</p> <p>Voc\u00ea tamb\u00e9m pode apenas iniciar um novo ambiente de treinamento na URL a seguir: https://gitpod.io/#https://github.com/nextflow-io/training</p>"},{"location":"pt/envsetup/01_setup/#salvando-arquivos-do-gitpod-na-sua-maquina-local","title":"Salvando arquivos do Gitpod na sua m\u00e1quina local","text":"<p>Para salvar qualquer arquivo do painel do explorador, clique no arquivo com o bot\u00e3o direito do mouse e selcione <code>Download</code>.</p>"},{"location":"pt/envsetup/02_local/","title":"Instala\u00e7\u00e3o local","text":"<p>Se voc\u00ea n\u00e3o conseguiu acessar o Gitpod, uma alternativa \u00e9 instalar tudo localmente.</p> <p>Alguns dos requisitos podem ser diferentes, a depender da sua m\u00e1quina local.</p>"},{"location":"pt/envsetup/02_local/#requisitos","title":"Requisitos","text":"<p>Nextflow pode ser utilizado em qualquer sistema compat\u00edvel com o POSIX (Linux, macOS, Subsistema Linux para Windows, etc.).</p> <p>Requisitos</p> <ul> <li>Bash</li> <li>Java 11 (ou posterior, at\u00e9 o 21)</li> <li>Git</li> <li>Docker</li> </ul> <p>Requisitos opcionais</p> <ul> <li>Singularity 2.5.x (ou posterior)</li> <li>Conda 4.5 (ou posterior)</li> <li>Graphviz</li> <li>AWS CLI</li> <li>Um ambiente computacional configurado no AWS Batch</li> </ul>"},{"location":"pt/envsetup/02_local/#baixando-nextflow","title":"Baixando Nextflow","text":"<p>Execute esse comando em seu terminal:</p> <pre><code>wget -qO- https://get.nextflow.io | bash\n</code></pre> <p>Voc\u00ea tamb\u00e9m pode usar o comando <code>curl</code>:</p> <pre><code>curl -s https://get.nextflow.io | bash\n</code></pre> <p>Em seguida, garanta que o bin\u00e1rio baixado \u00e9 execut\u00e1vel:</p> <pre><code>chmod +x nextflow\n</code></pre> <p>Por fim, garanta que o execut\u00e1vel do <code>nextflow</code> est\u00e1 na sua <code>$PATH</code>. O execut\u00e1vel pode estar presente em <code>/usr/local/bin</code>, <code>/bin/</code>, etc.</p>"},{"location":"pt/envsetup/02_local/#docker","title":"Docker","text":"<p>Garanta que o Docker Desktop est\u00e1 rodando em sua m\u00e1quina. Voc\u00ea pode baixar o Docker aqui.</p>"},{"location":"pt/envsetup/02_local/#material-de-treinamento","title":"Material de treinamento","text":"<p>Voc\u00ea pode ver o material de treinamento aqui.</p> <p>Para baixar o material, execute esse comando:</p> <pre><code>git clone https://github.com/nextflow-io/training.git\n</code></pre> <p>Ent\u00e3o use <code>cd</code> para entrar no diret\u00f3rio <code>nf-training</code>.</p>"},{"location":"pt/envsetup/02_local/#verificando-sua-instalacao","title":"Verificando sua instala\u00e7\u00e3o","text":"<p>Verifique que voc\u00ea instalou <code>nextflow</code> corretamente executando o seguinte comando:</p> <pre><code>nextflow info\n</code></pre> <p>Esse comando deve imprimir a vers\u00e3o, o sistema e o ambiente de execu\u00e7\u00e3o atuais.</p> <p>Exercise</p> <p>Para testar que seu ambiente est\u00e1 funcionando corretamente, execute o seguinte comando:</p> <pre><code>nextflow info\n</code></pre> <p>Esse comando deve trazer informa\u00e7\u00e3o sobre a vers\u00e3o do Nextflow e sobre seu ambiente de execu\u00e7\u00e3o:</p> <pre><code>Version: 23.10.1 build 5891\nCreated: 12-01-2024 22:01 UTC\nSystem: Linux 6.1.75-060175-generic\nRuntime: Groovy 3.0.19 on OpenJDK 64-Bit Server VM 11.0.1-internal+0-adhoc..src\nEncoding: UTF-8 (UTF-8)\n</code></pre>"},{"location":"pt/hello_nextflow/","title":"Hello Nextflow","text":"<p>Ol\u00e1! Agora voc\u00ea est\u00e1 no caminho para escrever fluxos de trabalho cient\u00edficos que s\u00e3o reproduz\u00edveis e escal\u00e1veis \u200b\u200busando o Nextflow.</p> <p>O surgimento do big data tornou cada vez mais necess\u00e1rio ser capaz de analisar e executar experimentos em grandes conjuntos de dados de forma port\u00e1vel e reproduz\u00edvel. Paraleliza\u00e7\u00e3o e computa\u00e7\u00e3o distribu\u00edda s\u00e3o as melhores maneiras de lidar com esse desafio, mas as ferramentas comumente dispon\u00edveis para cientistas computacionais geralmente n\u00e3o t\u00eam um bom suporte para essas t\u00e9cnicas ou fornecem um modelo que se encaixa mal com as necessidades dos cientistas computacionais. O Nextflow foi criado especialmente para lidar com esses desafios.</p> <p>Durante este treinamento, voc\u00ea ser\u00e1 apresentado ao Nextflow em uma s\u00e9rie de workshops pr\u00e1ticos complementares.</p> <p>Vamos come\u00e7ar!</p> <p></p>   Siga os v\u00eddeos   <p>A forma\u00e7\u00e3o Hello Nextflow tem um v\u00eddeo para cada cap\u00edtulo, incorporado no topo de cada p\u00e1gina.</p> <p>Pode tamb\u00e9m encontrar a playlist completa no canal de YouTube do Nextflow.</p>"},{"location":"pt/hello_nextflow/#objetivos-de-aprendizagem","title":"Objetivos de aprendizagem","text":"<p>Neste workshop, voc\u00ea aprender\u00e1 conceitos fundamentais para a constru\u00e7\u00e3o de pipelines.</p> <p>Ao final deste workshop voc\u00ea ser\u00e1 capaz de:</p> <ul> <li>Descrever e utilizar os principais componentes do Nextflow o suficiente para criar um fluxo de trabalho simples de v\u00e1rias etapas</li> <li>Descrever conceitos como operadores e f\u00e1bricas de canal</li> <li>Lan\u00e7ar um fluxo de trabalho localmente com Nextflow</li> <li>Encontrar e interpretar sa\u00eddas (resultados) e arquivos de log gerados pelo Nextflow</li> <li>Solucionar problemas b\u00e1sicos</li> </ul>"},{"location":"pt/hello_nextflow/#publico-e-pre-requisitos","title":"P\u00fablico e pr\u00e9-requisitos","text":"<p>Este \u00e9 um workshop para aqueles que s\u00e3o completamente novos no Nextflow. Alguma familiaridade b\u00e1sica com a linha de comando e formatos de arquivo comuns \u00e9 assumida.</p> <p>Pr\u00e9-requisitos</p> <ul> <li>Uma conta GitHub</li> <li>Experi\u00eancia com linha de comando</li> </ul>"},{"location":"pt/hello_nextflow/00_orientation/","title":"Orienta\u00e7\u00e3o","text":"<p> Veja a playlist completa no canal de YouTube do Nextflow.</p> <p> A transcri\u00e7\u00e3o desse v\u00eddeo est\u00e1 dispon\u00edvel aqui.</p> <p>O ambiente GitHub Codespaces cont\u00e9m todo o software, c\u00f3digo e dados necess\u00e1rios para este curso. Voc\u00ea n\u00e3o precisa instalar nada por conta pr\u00f3pria. No entanto, \u00e9 necess\u00e1ria uma conta (gratuita) para logar - e recomendamos que voc\u00ea reserve alguns minutos para se familiarizar com a interface.</p> <p>Caso ainda n\u00e3o tenha feito isso, siga este link antes de prosseguir.</p>"},{"location":"pt/hello_nextflow/00_orientation/#materiais-fornecidos","title":"Materiais fornecidos","text":"<p>Ao longo deste curso, trabalharemos no diret\u00f3rio <code>hello-nextflow/</code>, que \u00e9 carregado por padr\u00e3o quando voc\u00ea abre o ambiente de trabalho do Gitpod. Este diret\u00f3rio cont\u00e9m todos os arquivos de c\u00f3digo, dados de teste e arquivos auxiliares que voc\u00ea precisar\u00e1.</p> <p>Sinta-se \u00e0 vontade para explorar o conte\u00fado deste diret\u00f3rio; a maneira mais f\u00e1cil de fazer isso \u00e9 usando o explorador de arquivos no lado esquerdo do ambiente de trabalho do Gitpod. Alternativamente, voc\u00ea pode usar o comando <code>tree</code>. Ao longo do curso, usamos a sa\u00edda do <code>tree</code> para representar a estrutura e o conte\u00fado do diret\u00f3rio de forma leg\u00edvel - \u00e0s vezes com pequenas modifica\u00e7\u00f5es para maior clareza.</p> <p>Aqui, geramos um \u00edndice at\u00e9 o segundo n\u00edvel:</p> <pre><code>tree . -L 2\n</code></pre> <p>Se voc\u00ea executar isso dentro do diret\u00f3rio <code>hello-nextflow</code>, ver\u00e1 a seguinte sa\u00edda:</p> Directory contents<pre><code>.\n\u251c\u2500\u2500 greetings.csv\n\u251c\u2500\u2500 hello-channels.nf\n\u251c\u2500\u2500 hello-config.nf\n\u251c\u2500\u2500 hello-containers.nf\n\u251c\u2500\u2500 hello-modules.nf\n\u251c\u2500\u2500 hello-workflow.nf\n\u251c\u2500\u2500 hello-world.nf\n\u251c\u2500\u2500 nextflow.config\n\u251c\u2500\u2500 solutions\n\u2502   \u251c\u2500\u2500 1-hello-world\n\u2502   \u251c\u2500\u2500 2-hello-channels\n\u2502   \u251c\u2500\u2500 3-hello-workflow\n\u2502   \u251c\u2500\u2500 4-hello-modules\n\u2502   \u251c\u2500\u2500 5-hello-containers\n\u2502   \u2514\u2500\u2500 6-hello-config\n\u2514\u2500\u2500 test-params.json\n\n7 directories, 9 files\n</code></pre> <p>Nota</p> <p>N\u00e3o se preocupe se isso parecer muita informa\u00e7\u00e3o at\u00e9 o momento. N\u00f3s passaremos pelas partes relevantes ao longo das etapas do curso. Isso \u00e9 apenas para te dar uma vis\u00e3o geral.</p> <p>Aqui est\u00e1 um resumo do que voc\u00ea deveria saber para come\u00e7ar:</p> <ul> <li> <p>Os arquivos <code>.nf</code> s\u00e3o scripts de fluxo de trabalho nomeados com base na parte do curso em que s\u00e3o utilizados.</p> </li> <li> <p>O arquivo <code>nextflow.config</code> \u00e9 um arquivo de configura\u00e7\u00e3o que define propriedades m\u00ednimas do ambiente. Voc\u00ea pode ignor\u00e1-lo por enquanto.</p> </li> <li> <p>The file <code>greetings.csv</code> contains input data we'll use in most of the course. It is described in Part 1, when we introduce it for the first time.</p> </li> <li> <p>The file <code>test-params.json</code> is a file we'll use in Part 6. You can ignore it for now.</p> </li> <li> <p>O diret\u00f3rio <code>solutions</code> cont\u00e9m os scripts de fluxo de trabalho completos resultantes de cada etapa do curso. Eles servem como refer\u00eancia para verificar seu trabalho e solucionar quaisquer problemas. As informa\u00e7\u00f5es de nome e o n\u00famero presentes no nome do arquivo correspondem \u00e0 etapa da parte relevante do curso. Por exemplo, o arquivo <code>hello-world-4.nf</code> \u00e9 o resultado esperado ao completar as etapas 1 a 4 da Parte 1: Hello World.</p> </li> </ul> <p>Dica</p> <p>Se, por algum motivo, voc\u00ea sair deste diret\u00f3rio, sempre poder\u00e1 executar este comando para retornar:</p> <pre><code>cd /workspaces/training/hello-nextflow\n</code></pre> <p>Agora, para iniciar o curso, clique na seta no canto inferior direito desta p\u00e1gina.</p>"},{"location":"pt/hello_nextflow/01_hello_world/","title":"Part 1: Hello World","text":"<p> Veja a playlist completa no canal de YouTube do Nextflow.</p> <p> A transcri\u00e7\u00e3o desse v\u00eddeo est\u00e1 dispon\u00edvel aqui.</p> <p>Um exemplo \"Hello World!\" \u00e9 um modelo minimalista projetado para demonstrar a sintaxe b\u00e1sica e a estrutura de uma linguagem de programa\u00e7\u00e3o ou framework de software. Esse exemplo normalmente consiste em exibir a frase \"Hello, World!\" no dispositivo de sa\u00edda, como o console ou terminal, ou grav\u00e1-la em um arquivo.</p> <p>Nesta primeira parte do curso de treinamento Hello Nextflow, introduzimos o t\u00f3pico com um exemplo Hello World muito simples e independente de dom\u00ednio, que ser\u00e1 gradualmente expandido para demonstrar o uso dos componentes e da l\u00f3gica fundamental do Nextflow.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#0-aquecimento-execute-hello-world-diretamente","title":"0. Aquecimento: Execute Hello World diretamente","text":"<p>Vamos demonstrar isso com um comando simples executado diretamente no terminal, para entender seu funcionamento antes de encapsul\u00e1-lo no Nextflow.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#01-faca-o-terminal-dizer-hello","title":"0.1. Fa\u00e7a o terminal dizer hello","text":"<pre><code>echo 'Hello World!'\n</code></pre>"},{"location":"pt/hello_nextflow/01_hello_world/#02-agora-faca-o-escrever-a-saida-em-um-arquivo","title":"0.2. Agora fa\u00e7a-o escrever a sa\u00edda em um arquivo","text":"<pre><code>echo 'Hello World!' &gt; output.txt\n</code></pre>"},{"location":"pt/hello_nextflow/01_hello_world/#03-verifique-se-o-arquivo-de-saida-esta-presente-usando-o-comando-ls","title":"0.3. Verifique se o arquivo de sa\u00edda est\u00e1 presente usando o comando <code>ls</code>","text":"<pre><code>ls\n</code></pre>"},{"location":"pt/hello_nextflow/01_hello_world/#04-exiba-o-conteudo-do-arquivo","title":"0.4. Exiba o conte\u00fado do arquivo","text":"<pre><code>cat output.txt\n</code></pre> <p>Dica</p> <p>No ambiente Gitpod, voc\u00ea tamb\u00e9m pode encontrar o arquivo de sa\u00edda no explorador de arquivos e visualizar seu conte\u00fado clicando nele. Alternativamente, pode usar o comando <code>code</code> para abrir o arquivo para visualiza\u00e7\u00e3o.</p> <pre><code>code output.txt\n</code></pre>"},{"location":"pt/hello_nextflow/01_hello_world/#conclusao","title":"Conclus\u00e3o","text":"<p>Agora voc\u00ea sabe como executar um comando simples no terminal que exibe um texto e, opcionalmente, como gravar essa sa\u00edda em um arquivo.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#o-que-vem-a-seguir","title":"O que vem a seguir?","text":"<p>Descubra como isso seria estruturado em um fluxo de trabalho do Nextflow.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#1-experimente-o-script-inicial-do-workflow-hello-world","title":"1. Experimente o script inicial do workflow Hello World","text":"<p>Conforme mencionado na introdu\u00e7\u00e3o, fornecemos um script de workflow funcional, embora minimalista, chamado hello-world.nf. Ele realiza a mesma tarefa de antes (escrever \"Hello World!\"), mas utilizando o Nextflow.</p> <p>Para come\u00e7ar, primeiro abriremos o script do workflow para entender sua estrutura, e s\u00f3 depois o executaremos (antes de tentar qualquer modifica\u00e7\u00e3o) para verificar se ele se comporta conforme esperado.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#11-decifrando-a-estrutura-do-codigo","title":"1.1. Decifrando a estrutura do c\u00f3digo","text":"<p>Let's open the <code>hello-world.nf</code> script in the editor pane.</p> <p>Nota</p> <p>O arquivo est\u00e1 no diret\u00f3rio hello-nextflow, que deve ser seu diret\u00f3rio de trabalho atual. Voc\u00ea pode abrir o arquivo clicando no mesmo no explorador de arquivo ou digitar <code>ls</code> no terminal e Cmd+Click (MacOS) ou Ctrl+Click (PC).</p> hello-world.nf<pre><code>#!/usr/bin/env nextflow\n\n/*\n * Use echo para exibir 'Hello World!'\n */\nprocess sayHello {\n\n    output:\n        stdout\n\n    script:\n    \"\"\"\n    echo 'Hello World!'\n    \"\"\"\n}\n\nworkflow {\n\n    // emita uma sauda\u00e7\u00e3o\n    sayHello()\n}\n</code></pre> <p>Como voc\u00ea pode ver, um script Nextflow envolve dois tipos de componentes principais: um ou mais processos e o fluxo de trabalho propriamente dito. Cada processo descreve a(s) opera\u00e7\u00e3o(\u00f5es) que a etapa correspondente no pipeline deve realizar, enquanto o fluxo de trabalho descreve a l\u00f3gica do fluxo de dados que conecta as v\u00e1rias etapas. Vamos dar uma olhada mais de perto no bloco process primeiro e, depois, no bloco workflow.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#111-a-definicao-process","title":"1.1.1. A defini\u00e7\u00e3o <code>process</code>","text":"<p>O primeiro bloco de c\u00f3digo descreve um processo. A defini\u00e7\u00e3o do processo come\u00e7a com a palavra-chave <code>process</code>, seguida do nome do processo e, finalmente, do corpo do processo delimitado por chaves. O corpo do processo deve conter um bloco de script que especifica o comando a ser executado, que pode ser qualquer coisa que voc\u00ea executaria em um terminal de linha de comando.</p> <p>Aqui temos um processo chamado <code>sayHello</code> que grava sua sa\u00edda em <code>stdout</code>.</p> hello-world.nf<pre><code>/*\n * Use echo para imprimir \"Hello World!\" na sa\u00edda padr\u00e3o\n */\nprocesso sayHello {\n    output:\n        stdout\n    script:\n    \"\"\"\n    echo 'Hello World!'\n    \"\"\"\n}\n</code></pre> <p>Essa \u00e9 uma defini\u00e7\u00e3o m\u00ednima de processo que cont\u00e9m apenas uma defini\u00e7\u00e3o de sa\u00edda e o pr\u00f3prio script. Em um pipeline do mundo real, um processo geralmente cont\u00e9m blocos adicionais, como diretivas, entradas e cl\u00e1usulas condicionais, que apresentaremos mais adiante neste curso de treinamento.</p> <p>Nota</p> <p>A defini\u00e7\u00e3o de sa\u00edda n\u00e3o determina qual sa\u00edda ser\u00e1 criada. Ela simplesmente declara qual \u00e9 a sa\u00edda esperada, de modo que o Nextflow possa procur\u00e1-la quando a execu\u00e7\u00e3o estiver conclu\u00edda. Isso \u00e9 necess\u00e1rio para verificar se o comando foi executado com \u00eaxito e para passar a sa\u00edda para processos posteriores, se necess\u00e1rio.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#112-a-definicao-workflow","title":"1.1.2. A defini\u00e7\u00e3o <code>workflow</code>","text":"<p>O segundo bloco de c\u00f3digo descreve o fluxo de trabalho em si. A defini\u00e7\u00e3o do fluxo de trabalho come\u00e7a com a palavra-chave <code>workflow</code>, seguida de um nome opcional e, em seguida, o corpo do fluxo de trabalho delimitado por chaves. Aqui temos um fluxo de trabalho que consiste em uma chamada para o processo <code>sayHello</code>.</p> hello-world.nf<pre><code>workflow {\n    // emite uma sauda\u00e7\u00e3o\n    sayHello()\n}\n</code></pre> <p>Essa \u00e9 uma defini\u00e7\u00e3o m\u00ednima de fluxo de trabalho. Em um pipeline do mundo real, o fluxo de trabalho normalmente cont\u00e9m v\u00e1rias chamadas para processos conectados por canais. Voc\u00ea aprender\u00e1 como adicionar mais processos e conect\u00e1-los por canais daqui a pouco.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#12-executando-o-fluxo-de-trabalho","title":"1.2. Executando o fluxo de trabalho","text":"<p>Olhar para o c\u00f3digo n\u00e3o \u00e9 t\u00e3o divertido quanto execut\u00e1-lo, portanto, vamos testar isso na pr\u00e1tica.</p> <pre><code>nextflow run hello-world.nf\n</code></pre> <p>A sa\u00edda do console deve ser semelhante a esta:</p> Output<pre><code> N E X T F L O W   ~  version 24.02.0-edge\n\n \u2503 Launching `hello-world.nf` [reverent_carson] DSL2 - revision: 463b611a35\n\nexecutor &gt;  local (1)\n[1c/7d08e6] sayHello [100%] 1 of 1 \u2714\n</code></pre> <p>Parab\u00e9ns, voc\u00ea acabou de executar seu primeiro fluxo de trabalho Nextflow!</p> <p>O resultado mais importante aqui \u00e9 a \u00faltima linha (linha 6), que informa que o processo <code>sayHello</code> foi executado com sucesso uma vez.</p> <p>Ok, isso \u00e9 \u00f3timo, mas onde podemos encontrar o resultado? A defini\u00e7\u00e3o do processo <code>sayHello</code> dizia que a sa\u00edda seria enviada para a sa\u00edda padr\u00e3o, mas nada foi impresso no console, n\u00e3o \u00e9 mesmo?</p>"},{"location":"pt/hello_nextflow/01_hello_world/#13-localizando-a-saida-e-os-logs-no-diretorio-work","title":"1.3. Localizando a sa\u00edda e os logs no diret\u00f3rio <code>work</code>","text":"<p>Quando voc\u00ea executa o Nextflow pela primeira vez em um determinado diret\u00f3rio, ele cria um diret\u00f3rio chamado <code>work</code> no qual gravar\u00e1 todos os arquivos (e links simb\u00f3licos) gerados durante a execu\u00e7\u00e3o.</p> <p>D\u00ea uma olhada l\u00e1 dentro; voc\u00ea encontrar\u00e1 um subdiret\u00f3rio nomeado com um hash (para torn\u00e1-lo exclusivo; discutiremos o motivo daqui a pouco), aninhado em dois n\u00edveis de profundidade e contendo alguns arquivos de log.</p> <p>Dica</p> <p>Se voc\u00ea procurar o conte\u00fado do subdiret\u00f3rio de tarefas no explorador de arquivos VSCode do Gitpod, ver\u00e1 todos esses arquivos imediatamente. No entanto, esses arquivos est\u00e3o configurados para serem ocultados no terminal. Portanto, se quiser usar o <code>ls</code> ou o <code>tree</code> para visualiz\u00e1-los, ser\u00e1 necess\u00e1rio definir a op\u00e7\u00e3o relevante para exibir arquivos ocultados.</p> <pre><code>```bash\ntree -a work\n```\n\nVoc\u00ea dever\u00e1 ver algo parecido com isto, embora os nomes exatos dos subdiret\u00f3rios sejam diferentes em seu sistema.\n\n```console title=\"Directory contents\"\nwork\n\u2514\u2500\u2500 1c\n    \u2514\u2500\u2500 7d08e685a7aa7060b9c21667924824\n        \u251c\u2500\u2500 .command.begin\n        \u251c\u2500\u2500 .command.err\n        \u251c\u2500\u2500 .command.log\n        \u251c\u2500\u2500 .command.out\n        \u251c\u2500\u2500 .command.run\n        \u251c\u2500\u2500 .command.sh\n        \u2514\u2500\u2500 .exitcode\n```\n</code></pre> <p>Voc\u00ea deve ter notado que os nomes dos subdiret\u00f3rios apareceram (de forma truncada) na sa\u00edda da execu\u00e7\u00e3o do fluxo de trabalho, na linha que diz:</p> Output<pre><code>[1c/7d08e6] sayHello [100%] 1 of 1 \u2714\n</code></pre> <p>Isso informa qual \u00e9 o caminho do subdiret\u00f3rio para essa chamada de processo espec\u00edfica (\u00e0s vezes chamada de tarefa).</p> <p>Nota</p> <p>O Nextflow cria um subdiret\u00f3rio exclusivo e separado para cada chamada de processo. Ele prepara os arquivos de entrada relevantes, o script e outros arquivos auxiliares l\u00e1, e grava todos os arquivos de sa\u00edda e registros l\u00e1 tamb\u00e9m.</p> <p>Se olharmos dentro do subdiret\u00f3rio, encontraremos os seguintes arquivos de log:</p> <ul> <li><code>.command.begin</code>: Metadados relacionados ao in\u00edcio da execu\u00e7\u00e3o da tarefa do processo.</li> <li><code>.command.err</code>: Mensagens de erro (stderr) emitidas pela tarefa de processo.</li> <li><code>.command.log</code>: Sa\u00edda de registro completa emitida pela tarefa de processo.</li> <li><code>.command.out</code>: Sa\u00edda regular (stdout) pela tarefa de processo.</li> <li><code>.command.sh</code>: O comando que foi executado pela chamada da tarefa de processo</li> <li><code>.exitcode</code>: O c\u00f3digo de sa\u00edda resultante do comando.</li> </ul> <p>Nesse caso, voc\u00ea pode procurar sua sa\u00edda no arquivo <code>.command.out</code>, pois \u00e9 nele que a sa\u00edda stdout \u00e9 capturada. Se voc\u00ea abri-lo, encontrar\u00e1 a sauda\u00e7\u00e3o <code>Hello World!</code>, que era o resultado esperado de nosso fluxo de trabalho minimalista.</p> <p>Tamb\u00e9m vale a pena dar uma olhada no arquivo <code>.command.sh</code>, que informa qual comando o Nextflow realmente executou. Nesse caso, \u00e9 muito simples, mas mais adiante no curso voc\u00ea ver\u00e1 comandos que envolvem alguma interpola\u00e7\u00e3o de vari\u00e1veis. Quando estiver lidando com isso, voc\u00ea precisar\u00e1 ser capaz de verificar exatamente o que foi executado, especialmente ao solucionar um problema.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#conclusao_1","title":"Conclus\u00e3o","text":"<p>Voc\u00ea agora sabe como decifrar um script simples do Nextflow, execut\u00e1-lo e encontrar a sa\u00edda e os logs no diret\u00f3rio de trabalho.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#o-que-vem-a-seguir_1","title":"O que vem a seguir?","text":"<p>Saiba como fazer com que o script produza um arquivo nomeado.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#2-enviando-a-saida-para-um-arquivo","title":"2. Enviando a sa\u00edda para um arquivo","text":"<p>Em vez de imprimir \"Hello World!\" na sa\u00edda padr\u00e3o, seria melhor salvar essa sa\u00edda em um arquivo espec\u00edfico, exatamente como fizemos ao executar no terminal anteriormente. \u00c9 assim que a maioria das ferramentas que voc\u00ea executar\u00e1 como parte dos pipelines do mundo real normalmente se comporta; veremos exemplos disso mais tarde. Para obter esse resultado, o script e os blocos de defini\u00e7\u00e3o de sa\u00edda precisam ser atualizados.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#21-alterando-o-comando-process-para-gerar-um-arquivo-nomeado","title":"2.1. Alterando o comando <code>process</code> para gerar um arquivo nomeado","text":"<p>Essa \u00e9 a mesma altera\u00e7\u00e3o que fizemos quando executamos o comando diretamente no terminal anteriormente.</p> <p>Antes:</p> hello-world.nf<pre><code>'''\necho 'Hello World!'\n'''\n</code></pre> <p>Depois:</p> hello-world.nf<pre><code>'''\necho 'Hello World!' &gt; output.txt\n'''\n</code></pre>"},{"location":"pt/hello_nextflow/01_hello_world/#22-alterando-a-declaracao-de-saida-no-processo-sayhello","title":"2.2. Alterando a declara\u00e7\u00e3o de sa\u00edda no processo <code>sayHello</code>","text":"<p>Precisamos informar ao Nextflow que agora ele deve procurar um arquivo espec\u00edfico a ser produzido pela execu\u00e7\u00e3o do processo.</p> <p>Antes:</p> hello-world.nf<pre><code>output:\n    stdout\n</code></pre> <p>Depois:</p> hello-world.nf<pre><code>output:\n    path 'output.txt'\n</code></pre> <p>Nota</p> <p>As entradas e sa\u00eddas nos blocos de processo normalmente exigem um qualificador e um nome de vari\u00e1vel:</p> <pre><code>```\n&lt;qualificador de entrada/sa\u00edda&gt; &lt;nome da entrada/sa\u00edda&gt;\n```\n\nO qualificador define o tipo de dados a serem recebidos.\nEssas informa\u00e7\u00f5es s\u00e3o usadas pelo Nextflow para aplicar as regras sem\u00e2nticas associadas a cada qualificador e trat\u00e1-las adequadamente.\n\nOs qualificadores comuns incluem `val` e `path`.\n\nNo exemplo acima, `stdout` \u00e9 uma exce\u00e7\u00e3o, pois n\u00e3o est\u00e1 associado a um nome.\n</code></pre>"},{"location":"pt/hello_nextflow/01_hello_world/#23-executando-o-fluxo-de-trabalho-novamente","title":"2.3. Executando o fluxo de trabalho novamente","text":"<pre><code>nextflow run hello-world.nf\n</code></pre> <p>A sa\u00edda do log deve ser muito semelhante \u00e0 primeira vez que voc\u00ea executou o fluxo de trabalho:</p> Output<pre><code> N E X T F L O W   ~  version 24.02.0-edge\n\n \u2503 Launching `hello-world.nf` [cranky_sinoussi] DSL2 - revision: 30b437bb96\n\nexecutor &gt;  local (1)\n[7a/6bd54c] sayHello [100%] 1 of 1 \u2714\n</code></pre> <p>Como fez anteriormente, localize o diret\u00f3rio <code>work</code> no explorador de arquivos. L\u00e1, localize o arquivo de sa\u00edda <code>output.txt</code>, clique nele para abri-lo e verifique se ele cont\u00e9m a sauda\u00e7\u00e3o conforme esperado.</p> <p>Aviso</p> <p>Este exemplo \u00e9 fr\u00e1gil porque codificamos o nome do arquivo de sa\u00edda em dois lugares diferentes (nos blocos <code>script</code> e <code>output</code>). Se alterarmos um, mas n\u00e3o o outro, o script ser\u00e1 interrompido. Mais tarde, voc\u00ea aprender\u00e1 a usar vari\u00e1veis para evitar esse problema.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#24-adicionando-uma-diretiva-publishdir-ao-processo","title":"2.4. Adicionando uma diretiva <code>publishDir</code> ao processo","text":"<p>Voc\u00ea deve ter notado que a sa\u00edda est\u00e1 enterrada em um diret\u00f3rio de trabalho com v\u00e1rias camadas de profundidade. O Nextflow est\u00e1 no controle desse diret\u00f3rio e n\u00e3o devemos interagir com ele. Para tornar o arquivo de sa\u00edda mais acess\u00edvel, podemos utilizar a diretiva <code>publishDir</code>.</p> <p>Ao especificar essa diretiva, estamos dizendo ao Nextflow para copiar automaticamente o arquivo de sa\u00edda para um diret\u00f3rio de sa\u00edda designado. Isso nos permite deixar o diret\u00f3rio de trabalho sozinho e, ao mesmo tempo, ter acesso f\u00e1cil ao arquivo de sa\u00edda desejado.</p> <p>Antes:</p> hello-world.nf<pre><code>process sayHello {\n\n    output:\n        path 'output.txt'\n</code></pre> <p>Depois:</p> hello-world.nf<pre><code>process sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    output:\n        path 'output.txt'\n</code></pre> <p>Nota</p> <p>H\u00e1 uma nova op\u00e7\u00e3o de sintaxe que possibilita declarar e publicar sa\u00eddas no n\u00edvel do fluxo de trabalho, documentada [aqui] (https://www.nextflow.io/docs/latest/workflow.html#publishing-outputs), o que torna redundante o uso do <code>publishDir</code> no n\u00edvel do processo quando o pipeline estiver totalmente operacional. No entanto, o <code>publishDir</code> ainda \u00e9 muito \u00fatil durante o desenvolvimento do pipeline; \u00e9 por isso que o inclu\u00edmos nesta s\u00e9rie de treinamento. Isso tamb\u00e9m garantir\u00e1 que voc\u00ea possa ler e entender o grande n\u00famero de pipelines que j\u00e1 foram escritos com o <code>publishDir</code>. Voc\u00ea aprender\u00e1 a usar a sintaxe de sa\u00eddas em n\u00edvel de fluxo de trabalho mais adiante nesta s\u00e9rie de treinamento.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#25-executando-o-fluxo-de-trabalho-novamente","title":"2.5. Executando o fluxo de trabalho novamente","text":"<pre><code>nextflow run hello-world.nf\n</code></pre> <p>A sa\u00edda do log deve come\u00e7ar a parecer muito familiar:</p> Output<pre><code> N E X T F L O W   ~  version 24.02.0-edge\n\n \u2503 Launching `hello-world.nf` [mighty_lovelace] DSL2 - revision: 6654bc1327\n\nexecutor &gt;  local (1)\n[10/15498d] sayHello [100%] 1 of 1 \u2714\n</code></pre> <p>Desta vez, o Nextflow ter\u00e1 criado um novo diret\u00f3rio chamado <code>results/</code>. Nesse diret\u00f3rio, est\u00e1 o nosso arquivo <code>output.txt</code>. Se voc\u00ea verificar o conte\u00fado, ele dever\u00e1 corresponder \u00e0 sa\u00edda em nosso diret\u00f3rio work/task. \u00c9 assim que movemos os arquivos de resultados para fora dos diret\u00f3rios de trabalho.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#conclusao_2","title":"Conclus\u00e3o","text":"<p>Voc\u00ea agora j\u00e1 sabe como enviar resultados para um arquivo nomeado espec\u00edfico e usar a diretiva <code>publishDir</code> para mover arquivos para fora do diret\u00f3rio de trabalho do Nextflow.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#o-que-vem-a-seguir_2","title":"O que vem a seguir?","text":"<p>Saiba como fazer com que o Nextflow retome a execu\u00e7\u00e3o de um pipeline usando resultados em cache de uma execu\u00e7\u00e3o anterior para ignorar quaisquer etapas que j\u00e1 tenham sido conclu\u00eddas com \u00eaxito.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#3-usando-o-recurso-resume-do-nextflow","title":"3. Usando o recurso <code>resume</code> do Nextflow","text":"<p>O Nextflow tem uma op\u00e7\u00e3o chamada <code>resume</code>, que permite que voc\u00ea execute novamente um pipeline que j\u00e1 tenha sido iniciado anteriormente. Quando iniciado com <code>-resume</code>, qualquer processo que j\u00e1 tenha sido executado exatamente com o mesmo c\u00f3digo, configura\u00e7\u00f5es e entradas ser\u00e1 ignorado. Usar esse modo significa que o Nextflow s\u00f3 executar\u00e1 processos novos, que tenham sido modificados ou que estejam recebendo novas configura\u00e7\u00f5es ou entradas.</p> <p>H\u00e1 duas vantagens principais em fazer isso:</p> <ul> <li> <p>Se voc\u00ea estiver no meio do desenvolvimento do pipeline, poder\u00e1 iterar mais rapidamente, pois s\u00f3 precisar\u00e1 executar efetivamente o(s) processo(s) em que estiver trabalhando ativamente para testar as altera\u00e7\u00f5es.</p> </li> <li> <p>Se estiver executando um pipeline em produ\u00e7\u00e3o e algo der errado, em muitos casos, voc\u00ea poder\u00e1 corrigir o problema e reiniciar o pipeline, e ele voltar\u00e1 a ser executado a partir do ponto de falha, o que pode economizar muito tempo e computa\u00e7\u00e3o.</p> </li> </ul>"},{"location":"pt/hello_nextflow/01_hello_world/#31-executando-o-fluxo-de-trabalho-novamente-com-resume","title":"3.1. Executando o fluxo de trabalho novamente com <code>-resume</code>","text":"<pre><code>nextflow run hello-world.nf -resume\n</code></pre> <p>A sa\u00edda do console deve ser semelhante.</p> Output<pre><code> N E X T F L O W   ~  version 24.02.0-edge\n\n \u2503 Launching `hello-world.nf` [thirsty_gautier] DSL2 - revision: 6654bc1327\n\n[10/15498d] sayHello [100%] 1 of 1, cached: 1 \u2714\n</code></pre> <p>Observe o bit adicional <code>cached:</code> na linha de status do processo, o que significa que o Nextflow reconheceu que j\u00e1 havia feito esse trabalho e simplesmente reutilizou o resultado da \u00faltima execu\u00e7\u00e3o.</p> <p>Nota</p> <p>Quando voc\u00ea executa novamente um pipeline com <code>resume</code>, o Nextflow n\u00e3o sobrescreve nenhum arquivo gravado em um diret\u00f3rio publishDir por qualquer chamada de processo que tenha sido executada com sucesso anteriormente.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#conclusao_3","title":"Conclus\u00e3o","text":"<p>Agora voc\u00ea sabe como reiniciar um pipeline sem repetir etapas que j\u00e1 foram executadas de maneira id\u00eantica.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#o-que-vem-a-seguir_3","title":"O que vem a seguir?","text":"<p>Saiba como adicionar entradas vari\u00e1veis.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#4-adicionando-entradas-variaveis-usando-um-canal","title":"4. Adicionando entradas vari\u00e1veis usando um canal","text":"<p>At\u00e9 agora, estamos emitindo uma sauda\u00e7\u00e3o codificada no comando <code>process</code>. Agora, vamos adicionar alguma flexibilidade usando uma vari\u00e1vel de entrada, para que possamos alterar facilmente a sauda\u00e7\u00e3o.</p> <p>Para isso, precisamos fazer uma s\u00e9rie de altera\u00e7\u00f5es inter-relacionadas:</p> <ol> <li>Informar o processo sobre as entradas de vari\u00e1veis esperadas usando o bloco <code>input:</code></li> <li>Editar o processo para usar a entrada</li> <li>Criar um canal para passar a entrada para o processo (falaremos mais sobre isso em um minuto)</li> <li>Adicionar o canal como entrada \u00e0 chamada do processo</li> </ol>"},{"location":"pt/hello_nextflow/01_hello_world/#41-adicionando-uma-definicao-de-input-ao-bloco-do-processo","title":"4.1. Adicionando uma defini\u00e7\u00e3o de <code>input</code> ao bloco do processo","text":"<p>Primeiro, precisamos adaptar a defini\u00e7\u00e3o do processo para aceitar uma entrada.</p> <p>Antes:</p> hello-world.nf<pre><code>process sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    output:\n        path \"output.txt\"\n</code></pre> <p>Depois:</p> hello-world.nf<pre><code>process sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        val greeting\n\n    output:\n        path \"output.txt\"\n</code></pre>"},{"location":"pt/hello_nextflow/01_hello_world/#42-editando-o-comando-do-processo-para-usar-a-variavel-de-entrada","title":"4.2. Editando o comando do processo para usar a vari\u00e1vel de entrada","text":"<p>Agora, trocamos o valor original codificado para a vari\u00e1vel de entrada.</p> <p>Antes:</p> hello-world.nf<pre><code>\"\"\"\necho 'Hello World!' &gt; output.txt\n\"\"\"\n</code></pre> <p>Depois:</p> hello-world.nf<pre><code>\"\"\"\necho '$greeting' &gt; output.txt\n\"\"\"\n</code></pre>"},{"location":"pt/hello_nextflow/01_hello_world/#43-criando-um-canal-de-entrada","title":"4.3. Criando um canal de entrada","text":"<p>Agora que o nosso processo espera uma entrada, precisamos configurar essa entrada no corpo do fluxo de trabalho.</p> <p>\u00c9 aqui que entram os canais: o Nextflow usa canais para alimentar as entradas dos processos e transportar dados entre os processos que est\u00e3o conectados entre si. H\u00e1 v\u00e1rias maneiras de fazer isso, mas, por enquanto, usaremos apenas o canal mais simples poss\u00edvel, contendo um \u00fanico valor.</p> <p>Vamos criar o canal usando a f\u00e1brica <code>Channel.of()</code>, que configura um canal de valor simples, e fornecer a ele uma string codificada para ser usada como sauda\u00e7\u00e3o, declarando <code>greeting_ch = Channel.of('Hello world!')</code>.</p> <p>Antes:</p> hello-world.nf<pre><code>workflow {\n\n    // emite uma sauda\u00e7\u00e3o\n    sayHello()\n}\n</code></pre> <p>Depois:</p> hello-world.nf<pre><code>workflow {\n\n    // cria um canal para os inputs\n    greeting_ch = Channel.of('Hello world!')\n\n    // emite uma sauda\u00e7\u00e3o\n    sayHello()\n}\n</code></pre>"},{"location":"pt/hello_nextflow/01_hello_world/#44-adicionando-o-canal-como-entrada-a-chamada-de-processo","title":"4.4. Adicionando o canal como entrada \u00e0 chamada de processo","text":"<p>Agora precisamos realmente conectar nosso canal rec\u00e9m-criado \u00e0 chamada de processo <code>sayHello()</code>.</p> <p>Antes:</p> hello-world.nf<pre><code>// emit a greeting\nsayHello()\n</code></pre> <p>Depois:</p> hello-world.nf<pre><code>// emit a greeting\nsayHello(greeting_ch)\n</code></pre>"},{"location":"pt/hello_nextflow/01_hello_world/#45-executando-o-comando-do-fluxo-de-trabalho-novamente","title":"4.5. Executando o comando do fluxo de trabalho novamente","text":"<p>Vamos execut\u00e1-lo!</p> <pre><code>nextflow run hello-world.nf\n</code></pre> <p>Se voc\u00ea fez todas as quatro edi\u00e7\u00f5es corretamente, dever\u00e1 obter outra execu\u00e7\u00e3o bem-sucedida:</p> Output<pre><code> N E X T F L O W   ~  version 24.02.0-edge\n\n \u2503 Launching `hello-world.nf` [prickly_avogadro] DSL2 - revision: b58b6ab94b\n\nexecutor &gt;  local (1)\n[1f/50efd5] sayHello (1) [100%] 1 of 1 \u2714\n</code></pre> <p>Sinta-se \u00e0 vontade para verificar o diret\u00f3rio de resultados para se certificar de que o resultado ainda \u00e9 o mesmo de antes; at\u00e9 agora, estamos apenas ajustando progressivamente o encanamento interno para aumentar a flexibilidade do nosso fluxo de trabalho e, ao mesmo tempo, obter o mesmo resultado final.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#conclusao_4","title":"Conclus\u00e3o","text":"<p>Voc\u00ea agora sabe como usar um canal simples para fornecer uma entrada a um processo.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#o-que-vem-a-seguir_4","title":"O que vem a seguir?","text":"<p>Saiba como passar entradas da linha de comando.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#5-usando-parametros-da-cli-para-entradas","title":"5. Usando par\u00e2metros da CLI para entradas","text":"<p>Queremos poder especificar a entrada da linha de comando, pois essa \u00e9 a parte que quase sempre ser\u00e1 diferente nas execu\u00e7\u00f5es subsequentes do fluxo de trabalho.</p> <p>Boas not\u00edcias: O Nextflow tem um sistema de par\u00e2metros de fluxo de trabalho integrado chamado <code>params</code>, que facilita a declara\u00e7\u00e3o e o uso de par\u00e2metros da CLI.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#51-editando-a-declaracao-do-canal-de-entrada-para-usar-um-parametro","title":"5.1. Editando a declara\u00e7\u00e3o do canal de entrada para usar um par\u00e2metro","text":"<p>Aqui, trocamos a string codificada por <code>params.greeting</code> na linha de cria\u00e7\u00e3o do canal.</p> <p>Antes:</p> hello-world.nf<pre><code>// criar um canal para os inputs\ngreeting_ch = Channel.of('Hello world!')\n</code></pre> <p>Depois:</p> hello-world.nf<pre><code>// criar um canal para os inputs\ngreeting_ch = Channel.of(params.greeting)\n</code></pre> <p>Isso cria automaticamente um par\u00e2metro chamado <code>greeting</code> que voc\u00ea pode usar para fornecer um valor na linha de comando.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#52-executar-o-fluxo-de-trabalho-novamente-com-o-parametro-greeting","title":"5.2. Executar o fluxo de trabalho novamente com o par\u00e2metro <code>--greeting</code>","text":"<p>Para fornecer um valor para esse par\u00e2metro, basta adicionar <code>--greeting &lt;value&gt;</code> \u00e0 sua linha de comando.</p> <pre><code>nextflow run hello-world.nf --greeting 'Bonjour le monde!'\n</code></pre> <p>A execu\u00e7\u00e3o desse comando j\u00e1 deve lhe parecer extremamente familiar.</p> Output<pre><code> N E X T F L O W   ~  version 24.02.0-edge\n\n \u2503 Launching `hello-world.nf` [cheesy_engelbart] DSL2 - revision: b58b6ab94b\n\nexecutor &gt;  local (1)\n[1c/9b6dc9] sayHello (1) [100%] 1 of 1 \u2714\n</code></pre> <p>N\u00e3o se esque\u00e7a de abrir o arquivo de sa\u00edda para verificar se agora voc\u00ea tem a nova vers\u00e3o da sauda\u00e7\u00e3o. Pronto!</p> <p>Dica</p> <p>\u00c9 \u00fatil distinguir os par\u00e2metros em n\u00edvel de Nextflow dos par\u00e2metros em n\u00edvel de pipeline. Para par\u00e2metros que se aplicam a um pipeline, usamos um h\u00edfen duplo (<code>--</code>), enquanto usamos um \u00fanico h\u00edfen (<code>-</code>) para par\u00e2metros que modificam uma configura\u00e7\u00e3o espec\u00edfica do Nextflow, por exemplo, o recurso <code>-resume</code> que usamos anteriormente.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#53-definindo-um-valor-padrao-para-um-parametro-de-linha-de-comando","title":"5.3. Definindo um valor padr\u00e3o para um par\u00e2metro de linha de comando","text":"<p>Em muitos casos, faz sentido fornecer um valor padr\u00e3o para um determinado par\u00e2metro para que voc\u00ea n\u00e3o precise especific\u00e1-lo em cada execu\u00e7\u00e3o. Vamos inicializar o par\u00e2metro <code>greeting</code> com um valor padr\u00e3o, adicionando a declara\u00e7\u00e3o do par\u00e2metro na parte superior do script (com um bloco de coment\u00e1rios como b\u00f4nus gratuito).</p> hello-world.nf<pre><code>/*\n * Par\u00e2metros do Pipeline\n */\nparams.greeting = \"Ol\u00e1 mundo!\"\n</code></pre>"},{"location":"pt/hello_nextflow/01_hello_world/#54-executando-o-fluxo-de-trabalho-novamente-sem-especificar-o-parametro","title":"5.4. Executando o fluxo de trabalho novamente sem especificar o par\u00e2metro","text":"<p>Agora que voc\u00ea tem um valor padr\u00e3o definido, pode executar o fluxo de trabalho novamente sem precisar especificar um valor na linha de comando.</p> <pre><code>nextflow run hello-world.nf\n</code></pre> <p>A sa\u00edda deve ser a mesma.</p> Output<pre><code> N E X T F L O W   ~  version 24.02.0-edge\n\n \u2503 Launching `hello-world.nf` [wise_waddington] DSL2 - revision: 988fc779cf\n\nexecutor &gt;  local (1)\n[c0/8b8332] sayHello (1) [100%] 1 of 1 \u2714\n</code></pre> <p>Verifique a sa\u00edda no diret\u00f3rio de resultados e... Tcharam! Funciona! O Nextflow usou o valor padr\u00e3o para nomear a sa\u00edda. Mas espere a\u00ed, o que acontece agora se fornecermos o par\u00e2metro na linha de comando?</p>"},{"location":"pt/hello_nextflow/01_hello_world/#55-executando-o-fluxo-de-trabalho-novamente-com-o-parametro-greeting-na-linha-de-comando-usando-uma-saudacao-diferente","title":"5.5. Executando o fluxo de trabalho novamente com o par\u00e2metro <code>--greeting</code> na linha de comando usando uma sauda\u00e7\u00e3o diferente","text":"<pre><code>nextflow run hello-world.nf --greeting 'Hola Mundo!'\n</code></pre> <p>O Nextflow n\u00e3o est\u00e1 reclamando, o que \u00e9 um bom sinal:</p> Output<pre><code> N E X T F L O W   ~  version 24.02.0-edge\n\n \u2503 Launching `hello-world.nf` [prickly_miescher] DSL2 - revision: 988fc779cf\n\nexecutor &gt;  local (1)\n[56/f88a56] sayHello (1) [100%] 1 of 1 \u2714\n</code></pre> <p>Verifique o diret\u00f3rio de resultados e veja o conte\u00fado de <code>output.txt</code>. Tcharam novamente! O valor do par\u00e2metro que passamos na linha de comando substituiu o valor que demos \u00e0 vari\u00e1vel no script. De fato, os par\u00e2metros podem ser definidos de v\u00e1rias maneiras diferentes; se o mesmo par\u00e2metro for definido em v\u00e1rios locais, seu valor ser\u00e1 determinado com base na ordem de preced\u00eancia descrita aqui.</p> <p>Dica</p> <p>Voc\u00ea pode colocar a declara\u00e7\u00e3o do par\u00e2metro dentro do bloco de fluxo de trabalho, se preferir. Seja qual for a sua escolha, tente agrupar coisas semelhantes no mesmo lugar para n\u00e3o acabar com declara\u00e7\u00f5es espalhadas por toda parte.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#conclusao_5","title":"Conclus\u00e3o","text":"<p>At\u00e9 agora voc\u00ea j\u00e1 sabe como configurar uma vari\u00e1vel de entrada para um processo e fornecer um valor na linha de comando.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#o-que-vem-a-seguir_5","title":"O que vem a seguir?","text":"<p>Saiba como adicionar um segundo processo e encade\u00e1-los.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#6-adicionando-uma-segunda-etapa-ao-fluxo-de-trabalho","title":"6. Adicionando uma segunda etapa ao fluxo de trabalho","text":"<p>A maioria dos fluxos de trabalho do mundo real envolve mais de uma etapa. Aqui apresentamos um segundo processo que converte o texto em mai\u00fasculas (all-caps), usando o cl\u00e1ssico UNIX one-liner:</p> <pre><code>tr '[a-z]' '[A-Z]'\n</code></pre> <p>Primeiro, vamos executar o comando sozinho no terminal para verificar se ele funciona conforme o esperado, sem que nenhum c\u00f3digo de fluxo de trabalho atrapalhe a clareza, assim como fizemos no in\u00edcio com <code>echo 'Hello World'</code>. Em seguida, escreveremos um processo que faz a mesma coisa e, finalmente, conectaremos os dois processos para que a sa\u00edda do primeiro sirva de entrada para o segundo.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#61-executando-o-comando-no-terminal-por-si-so","title":"6.1. Executando o comando no terminal por si s\u00f3","text":"<pre><code>echo 'Hello World' | tr '[a-z]' '[A-Z]'\n</code></pre> <p>A sa\u00edda \u00e9 simplesmente a vers\u00e3o em mai\u00fasculas da string de texto:</p> Output<pre><code>HELLO WORLD\n</code></pre> <p>Nota</p> <p>Esse \u00e9 um one-liner de substitui\u00e7\u00e3o de texto muito ing\u00eanuo que n\u00e3o leva em conta as letras acentuadas, portanto, por exemplo, 'ol\u00e1' se tornar\u00e1 'OL\u00e0'. Isso \u00e9 esperado.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#62-fazendo-com-que-o-comando-receba-um-arquivo-como-entrada-e-grave-a-saida-em-um-arquivo","title":"6.2. Fazendo com que o comando receba um arquivo como entrada e grave a sa\u00edda em um arquivo","text":"<p>Como anteriormente, queremos enviar os resultados para um arquivo dedicado, que nomeamos prefixando o nome do arquivo original com <code>UPPER-</code>.</p> <pre><code>cat output.txt | tr '[a-z]' '[A-Z]' &gt; UPPER-output.txt\n</code></pre> <p>Agora, a sa\u00edda do <code>HELLO WORLD</code> est\u00e1 no novo arquivo de sa\u00edda, <code>UPPER-output.txt</code>.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#63-envolvendo-o-comando-em-uma-nova-definicao-de-processo-do-nextflow","title":"6.3. Envolvendo o comando em uma nova defini\u00e7\u00e3o de processo do Nextflow","text":"<p>Podemos modelar nosso novo processo com base no primeiro, j\u00e1 que queremos usar todos os mesmos componentes.</p> hello-world.nf<pre><code>/*\n * Use um utilit\u00e1rio de substitui\u00e7\u00e3o de texto para converter a sauda\u00e7\u00e3o em mai\u00fasculas\n */\nprocess convertToUpper {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        path input_file\n\n    output:\n        path \"UPPER-${input_file}\"\n\n    script:\n    \"\"\"\n    cat '$input_file' | tr '[a-z]' '[A-Z]' &gt; UPPER-${input_file}\n    \"\"\"\n}\n</code></pre> <p>Como um pequeno b\u00f4nus, aqui compomos o segundo nome de arquivo de sa\u00edda com base no primeiro.</p> <p>Dica</p> <p>\u00c9 muito importante lembrar: \u00e9 necess\u00e1rio usar aspas duplas ao redor da express\u00e3o do nome do arquivo de sa\u00edda (N\u00c3O aspas simples) ou haver\u00e1 falha.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#64-adicionando-uma-chamada-ao-novo-processo-no-corpo-do-fluxo-de-trabalho","title":"6.4. Adicionando uma chamada ao novo processo no corpo do fluxo de trabalho","text":"<p>N\u00e3o se esque\u00e7a de que precisamos dizer ao Nextflow para realmente chamar o processo que acabamos de criar! Para fazer isso, n\u00f3s o adicionamos ao corpo do fluxo de trabalho.</p> hello-world.nf<pre><code>workflow {\n    // criar um canal para entradas\n    greeting_ch = Channel.of(params.greeting)\n    // emite uma sauda\u00e7\u00e3o\n    sayHello(greeting_ch)\n    // converter a sauda\u00e7\u00e3o em mai\u00fasculas\n    convertToUpper()\n}\n</code></pre> <p>Parece bom! Mas ainda precisamos conectar a chamada do processo <code>convertToUpper</code> para ser executada na sa\u00edda do <code>sayHello</code>.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#65-passando-a-saida-do-primeiro-processo-para-o-segundo-processo","title":"6.5. Passando a sa\u00edda do primeiro processo para o segundo processo","text":"<p>A sa\u00edda do processo <code>sayHello</code> \u00e9 automaticamente empacotada como um canal chamado <code>sayHello.out</code>, portanto, tudo o que precisamos fazer \u00e9 pass\u00e1-la como entrada para o processo <code>convertToUpper</code>.</p> hello-world.nf<pre><code>// converter a sauda\u00e7\u00e3o em letras mai\u00fasculas\nconvertToUpper(sayHello.out)\n</code></pre> <p>Para um caso simples como esse, isso \u00e9 tudo o que precisamos fazer para conectar dois processos!</p>"},{"location":"pt/hello_nextflow/01_hello_world/#66-executar-o-mesmo-comando-de-fluxo-de-trabalho-anterior","title":"6.6. Executar o mesmo comando de fluxo de trabalho anterior","text":"<p>Vamos nos certificar de que isso funcione:</p> <pre><code>nextflow run hello-world.nf --greeting 'Hello World!'\n</code></pre> <p>Que emocionante! Agora h\u00e1 uma linha extra na sa\u00edda de registro, que corresponde ao novo processo que acabamos de adicionar:</p> Output<pre><code> N E X T F L O W   ~  version 24.02.0-edge\n\n \u2503 Launching `hello-world.nf` [magical_brenner] DSL2 - revision: 0e18f34798\n\nexecutor &gt;  local (2)\n[57/3836c0] sayHello (1)       [100%] 1 of 1 \u2714\n[ee/bb3cc8] convertToUpper (1) [100%] 1 of 1 \u2714\n</code></pre> <p>Voc\u00ea notar\u00e1 que, desta vez, o fluxo de trabalho produziu dois novos subdiret\u00f3rios de trabalho, um por chamada de processo. Verifique o diret\u00f3rio de trabalho da chamada para o segundo processo, onde voc\u00ea deve encontrar dois arquivos de sa\u00edda diferentes listados. Se olhar com aten\u00e7\u00e3o, voc\u00ea notar\u00e1 que um deles (a sa\u00edda do primeiro processo) tem um pequeno \u00edcone de seta \u00e0 direita, o que significa que \u00e9 um link simb\u00f3lico. Ele aponta para o local onde esse arquivo est\u00e1 no diret\u00f3rio de trabalho do primeiro processo. Por padr\u00e3o, o Nextflow usa links simb\u00f3licos para encenar arquivos de entrada sempre que poss\u00edvel, para evitar fazer c\u00f3pias duplicadas.</p> <p>Nota</p> <p>Tudo o que fizemos foi conectar a sa\u00edda do <code>sayHello</code> \u00e0 entrada do <code>convertToUpper</code> e os dois processos puderam ser executados em s\u00e9rie. O Nextflow fez o trabalho pesado de manipular os arquivos de entrada e sa\u00edda e pass\u00e1-los entre os dois comandos para n\u00f3s. Esse \u00e9 o poder dos canais no Nextflow, fazendo o trabalho pesado de conectar as etapas do nosso pipeline. Al\u00e9m disso, o Nextflow determinar\u00e1 automaticamente qual chamada precisa ser executada primeiro com base em como elas est\u00e3o conectadas, de modo que a ordem em que est\u00e3o escritas no corpo do fluxo de trabalho n\u00e3o importa. No entanto, recomendamos que voc\u00ea seja gentil com seus colaboradores e com seu futuro eu, e tente escrev\u00ea-las em uma ordem l\u00f3gica!</p>"},{"location":"pt/hello_nextflow/01_hello_world/#conclusao_6","title":"Conclus\u00e3o","text":"<p>Voc\u00ea j\u00e1 sabe como adicionar uma segunda etapa que usa a sa\u00edda da primeira etapa como entrada.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#o-que-vem-a-seguir_6","title":"O que vem a seguir?","text":"<p>Saiba como fazer com que o fluxo de trabalho seja executado em um lote de valores de entrada.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#7-modificando-o-fluxo-de-trabalho-para-ser-executado-em-um-lote-de-valores-de-entrada","title":"7. Modificando o fluxo de trabalho para ser executado em um lote de valores de entrada","text":"<p>Os fluxos de trabalho normalmente s\u00e3o executados em lotes de entradas que devem ser processados em massa, portanto, queremos atualizar o fluxo de trabalho para aceitar v\u00e1rios valores de entrada.</p> <p>Convenientemente, a f\u00e1brica <code>Channel.of()</code> que estamos usando aceita de bom grado mais de um valor, portanto, n\u00e3o precisamos modific\u00e1-la; basta carregar mais valores no canal.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#71-carregando-varias-saudacoes-no-canal-de-entrada","title":"7.1. Carregando v\u00e1rias sauda\u00e7\u00f5es no canal de entrada","text":"<p>Para manter as coisas simples, voltamos a codificar as sauda\u00e7\u00f5es na f\u00e1brica do canal em vez de usar um par\u00e2metro para a entrada, mas melhoraremos isso em breve. Antes:</p> hello-world.nf<pre><code>// criar um canal para entradas\ngreeting_ch = Channel.of(params.greeting)\n</code></pre> <p>Depois:</p> hello-world.nf<pre><code>// criar um canal para entradas\ngreeting_ch = Channel.of('Hello','Bonjour','Hol\u00e0')\n</code></pre> <p>A documenta\u00e7\u00e3o nos diz que isso deve funcionar. Ser\u00e1 que realmente pode ser t\u00e3o simples?</p>"},{"location":"pt/hello_nextflow/01_hello_world/#72-executando-o-comando-e-vendo-a-saida-do-registro","title":"7.2. Executando o comando e vendo a sa\u00edda do registro","text":"<p>Vamos tentar.</p> <pre><code>nextflow run hello-world.nf\n</code></pre> <p>Bem, ele parece estar funcionando bem.</p> Output<pre><code> N E X T F L O W   ~  version 24.02.0-edge\n\n \u2503 Launching `hello-world.nf` [lonely_pare] DSL2 - revision: b9f1d96905\n\nexecutor &gt;  local (6)\n[3d/1fe62c] sayHello (2)       [100%] 3 of 3 \u2714\n[86/695813] convertToUpper (3) [100%] 3 of 3 \u2714\n</code></pre> <p>No entanto... Isso parece indicar que foram feitas \u201c3 de 3\u201d chamadas para cada processo, o que \u00e9 animador, mas isso nos d\u00e1 apenas um caminho de subdiret\u00f3rio para cada um. O que est\u00e1 acontecendo?</p> <p>Por padr\u00e3o, o sistema de registro ANSI grava o registro de v\u00e1rias chamadas para o mesmo processo na mesma linha. Felizmente, podemos desativar esse comportamento.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#73-execute-o-comando-novamente-com-a-opcao-ansi-log-false","title":"7.3. Execute o comando novamente com a op\u00e7\u00e3o <code>-ansi-log false</code>","text":"<p>Para expandir o registro em log para exibir uma linha por chamada de processo, basta adicionar <code>-ansi-log false</code> ao comando.</p> <pre><code>nextflow run hello-world.nf -ansi-log false\n</code></pre> <p>Desta vez, vemos todos os seis subdiret\u00f3rios de trabalho listados na sa\u00edda:</p> Output<pre><code>N E X T F L O W  ~  version 24.02.0-edge\nLaunching `hello-world.nf` [big_woese] DSL2 - revision: 53f20aeb70\n[62/d81e63] Submitted process &gt; sayHello (1)\n[19/507af3] Submitted process &gt; sayHello (2)\n[8a/3126e6] Submitted process &gt; sayHello (3)\n[12/48a5c6] Submitted process &gt; convertToUpper (1)\n[73/e6e746] Submitted process &gt; convertToUpper (2)\n[c5/4fedda] Submitted process &gt; convertToUpper (3)\n</code></pre> <p>Isso \u00e9 muito melhor, pelo menos para esse n\u00famero de processos. No caso de um fluxo de trabalho complexo ou de um grande n\u00famero de entradas, ter a lista completa de sa\u00edda para o terminal pode ser um pouco cansativo.</p> <p>Dito isso, temos outro problema. Se voc\u00ea olhar o diret\u00f3rio <code>results</code>, h\u00e1 apenas dois arquivos: <code>output.txt</code> e <code>UPPER-output.txt</code>!</p> <p>```console title=\"Conte\u00fado do diret\u00f3rio\u201d resultados \u251c\u2500\u2500 output.txt \u2514\u2500\u2500 UPPER-output.txt <pre><code>O que est\u00e1 acontecendo com isso? N\u00e3o dever\u00edamos estar esperando dois arquivos por sauda\u00e7\u00e3o de entrada, ou seja, seis arquivos no total?\nVoc\u00ea deve se lembrar que codificamos o nome do arquivo de sa\u00edda para o primeiro processo.\nIsso foi bom desde que houvesse apenas uma \u00fanica chamada feita por processo, mas quando come\u00e7amos a processar v\u00e1rios valores de entrada e a publicar as sa\u00eddas no mesmo diret\u00f3rio de resultados, isso se torna um problema.\nPara um determinado processo, cada chamada produz uma sa\u00edda com o mesmo nome de arquivo, portanto, o Nextflow simplesmente substitui o arquivo de sa\u00edda anterior sempre que um novo \u00e9 produzido.\n\n### 7.4. Certifique-se de que os nomes dos arquivos de sa\u00edda sejam exclusivos\n\nComo publicaremos todos os resultados no mesmo diret\u00f3rio de resultados, precisamos garantir que eles tenham nomes exclusivos.\nEspecificamente, precisamos modificar o primeiro processo para gerar um nome de arquivo dinamicamente, de modo que os nomes dos arquivos finais sejam exclusivos.\nEnt\u00e3o, como tornar os nomes dos arquivos exclusivos? Uma maneira comum de fazer isso \u00e9 usar alguma parte exclusiva de metadados como parte do nome do arquivo.\nAqui, por conveni\u00eancia, usaremos apenas a sauda\u00e7\u00e3o em si.\n\n_Antes:_\n\n```groovy title=\"hello-world.nf\" linenums=\"11\"\nprocess sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        val greeting\n\n    output:\n        path \"output.txt\"\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; \"output.txt\"\n    \"\"\"\n}\n</code></pre></p> <p>Depois:</p> hello-world.nf<pre><code>process sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        val greeting\n\n    output:\n        path \"${greeting}-output.txt\"\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; '$greeting-output.txt'\n    \"\"\"\n}\n</code></pre> <p>Isso deve produzir um nome de arquivo de sa\u00edda exclusivo para cada chamada de cada processo.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#75-execute-o-fluxo-de-trabalho-e-veja-o-diretorio-de-resultados","title":"7.5. Execute o fluxo de trabalho e veja o diret\u00f3rio de resultados","text":"<p>Vamos execut\u00e1-lo e verificar se ele funciona.</p> <pre><code>nextflow run hello-world.nf\n</code></pre> <p>Ao voltar para a visualiza\u00e7\u00e3o de resumo, a sa\u00edda tem a seguinte apar\u00eancia novamente:</p> Output<pre><code> N E X T F L O W   ~  version 24.02.0-edge\n\n \u2503 Launching `hello-world.nf` [jovial_mccarthy] DSL2 - revision: 53f20aeb70\n\nexecutor &gt;  local (6)\n[03/f007f2] sayHello (1)       [100%] 3 of 3 \u2714\n[e5/dd2890] convertToUpper (3) [100%] 3 of 3 \u2714\n</code></pre> <p>Mas o mais importante \u00e9 que agora temos seis novos arquivos, al\u00e9m dos dois que j\u00e1 t\u00ednhamos no diret\u00f3rio <code>results</code>:</p> Directory contents<pre><code>results\n\u251c\u2500\u2500 Bonjour-output.txt\n\u251c\u2500\u2500 Hello-output.txt\n\u251c\u2500\u2500 Hol\u00e0-output.txt\n\u251c\u2500\u2500 output.txt\n\u251c\u2500\u2500 UPPER-Bonjour-output.txt\n\u251c\u2500\u2500 UPPER-Hello-output.txt\n\u251c\u2500\u2500 UPPER-Hol\u00e0-output.txt\n\u2514\u2500\u2500 UPPER-output.txt\n</code></pre> <p>Sucesso! Agora podemos adicionar quantas sauda\u00e7\u00f5es quisermos sem nos preocuparmos com o fato de os arquivos de sa\u00edda serem sobrescritos.</p> <p>Nota</p> <p>Na pr\u00e1tica, nomear arquivos com base nos pr\u00f3prios dados de entrada \u00e9 quase sempre impratic\u00e1vel. A melhor maneira de gerar nomes de arquivos din\u00e2micos \u00e9 usar uma planilha de amostras que contenha metadados relevantes (como IDs de amostra exclusivos) e criar uma estrutura de dados chamada \u201cmapa\u201d, que passamos para os processos e da qual podemos obter um identificador apropriado para gerar os nomes de arquivos. Mostraremos a voc\u00ea como fazer isso mais adiante neste curso de treinamento.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#conclusao_7","title":"Conclus\u00e3o","text":"<p>Voc\u00ea j\u00e1 sabe como alimentar um lote de v\u00e1rios elementos de entrada por meio de um canal.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#o-que-vem-a-seguir_7","title":"O que vem a seguir?","text":"<p>Saiba como fazer com que o fluxo de trabalho use um arquivo como sua fonte de valores de entrada.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#8-modificando-o-fluxo-de-trabalho-para-usar-um-arquivo-como-fonte-de-valores-de-entrada","title":"8. Modificando o fluxo de trabalho para usar um arquivo como fonte de valores de entrada","text":"<p>Muitas vezes, quando queremos executar um lote de v\u00e1rios elementos de entrada, os valores de entrada podem estar contidos em um arquivo.</p> <p>Como exemplo, fornecemos a voc\u00ea um arquivo CSV chamado <code>greetings.csv</code> no diret\u00f3rio <code>data/</code>, contendo v\u00e1rias sauda\u00e7\u00f5es separadas por v\u00edrgulas.</p> greetings.csv<pre><code>Hello,Bonjour,Hol\u00e0\n</code></pre> <p>Portanto, s\u00f3 precisamos modificar nosso fluxo de trabalho para ler os valores de um arquivo como esse.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#81-configurando-um-parametro-da-cli-com-um-valor-padrao-apontando-para-um-arquivo-de-entrada","title":"8.1. Configurando um par\u00e2metro da CLI com um valor padr\u00e3o apontando para um arquivo de entrada","text":"<p>Primeiro, vamos usar o argumento <code>params</code> para configurar um novo par\u00e2metro chamado <code>input_file</code>, substituindo o par\u00e2metro <code>greeting</code>, agora in\u00fatil, com um valor padr\u00e3o que aponta para o arquivo <code>greetings.csv</code>.</p> <p>Antes:</p> hello-world.nf<pre><code>/*\n * Par\u00e2metros do pipeline\n */\nparams.greeting = \"Bonjour le monde!\"\n</code></pre> <p>Depois:</p> hello-world.nf<pre><code>/*\n * Par\u00e2metros do pipeline\n */\nparams.input_file = \"data/greetings.csv\"\n</code></pre>"},{"location":"pt/hello_nextflow/01_hello_world/#82-atualizando-a-declaracao-do-canal-para-lidar-com-o-arquivo-de-entrada","title":"8.2. Atualizando a declara\u00e7\u00e3o do canal para lidar com o arquivo de entrada","text":"<p>Neste ponto, apresentamos uma nova f\u00e1brica de canais, <code>Channel.fromPath()</code>, que tem algumas funcionalidades integradas para lidar com caminhos de arquivos.</p> <p>Vamos us\u00e1-la em vez da f\u00e1brica <code>Channel.of()</code> que usamos anteriormente; a sintaxe b\u00e1sica \u00e9 a seguinte:</p> channel construction syntax<pre><code>Channel.fromPath(input_file)\n</code></pre> <p>Agora, vamos implantar um novo conceito, um \u201coperador\u201d para transformar esse arquivo CSV em conte\u00fado de canal. Voc\u00ea aprender\u00e1 mais sobre operadores mais tarde, mas, por enquanto, basta entend\u00ea-los como formas de transformar canais de v\u00e1rias maneiras.</p> <p>Como nosso objetivo \u00e9 ler o conte\u00fado de um arquivo <code>.csv</code>, adicionaremos o operador <code>.splitCsv()</code> para fazer com que o Nextflow analise o conte\u00fado do arquivo adequadamente, bem como o operador <code>.flatten()</code> para transformar o elemento de matriz produzido por <code>.splitCsv()</code> em um canal de elementos individuais. Portanto, a instru\u00e7\u00e3o de constru\u00e7\u00e3o do canal passa a ser:</p> channel construction syntax<pre><code>Channel.fromPath(input_file)\n       .splitCsv()\n       .flatten()\n</code></pre> <p>E aqui est\u00e1 ele no contexto do corpo do fluxo de trabalho:</p> <p>Antes:</p> hello-world.nf<pre><code>// criar um canal para entradas\ngreeting_ch = Channel.of('Hello','Bonjour','Hol\u00e0')\n</code></pre> <p>Depois:</p> hello-world.nf<pre><code>// criar um canal para entradas de um arquivo CSV\ngreeting_ch = Channel.fromPath(params.input_file)\n                     .splitCsv()\n                     .flatten()\n</code></pre>"},{"location":"pt/hello_nextflow/01_hello_world/#83-executando-o-fluxo-de-trabalho-uma-ultima-vez","title":"8.3. Executando o fluxo de trabalho (uma \u00faltima vez!)","text":"<pre><code>nextflow run hello-world.nf\n</code></pre> <p>Mais uma vez, vemos que cada processo \u00e9 executado tr\u00eas vezes:</p> Output<pre><code> N E X T F L O W   ~  version 24.02.0-edge\n\n \u2503 Launching `hello-world.nf` [angry_spence] DSL2 - revision: d171cc0193\n\nexecutor &gt;  local (6)\n[0e/ceb175] sayHello (2)       [100%] 3 of 3 \u2714\n[01/046714] convertToUpper (3) [100%] 3 of 3 \u2714\n</code></pre> <p>Observando os resultados, vemos que cada sauda\u00e7\u00e3o foi extra\u00edda e processada corretamente pelo fluxo de trabalho. Obtivemos o mesmo resultado da etapa anterior, mas agora temos muito mais flexibilidade para adicionar mais elementos ao canal de sauda\u00e7\u00f5es que queremos processar.</p> <p>Dica</p> <p>Durante o desenvolvimento do pipeline, voc\u00ea pode inspecionar o conte\u00fado de qualquer canal adicionando o operador <code>.view()</code> ao nome do canal. Por exemplo, se voc\u00ea adicionar <code>greeting_ch.view()</code> em qualquer lugar do corpo do fluxo de trabalho, ao executar o script, o Nextflow imprimir\u00e1 o conte\u00fado do canal na sa\u00edda padr\u00e3o. Voc\u00ea tamb\u00e9m pode usar isso para inspecionar o efeito dos operadores.</p> <pre><code>Por exemplo, a sa\u00edda de `Channel.fromPath(params.input_file).splitCsv().view()` ter\u00e1 a seguinte apar\u00eancia:\n\n```console title=\"Output\"\n[Hello, Bonjour, Hol\u00e0]\n```\n\nEnquanto a sa\u00edda de `Channel.fromPath(params.input_file).splitCsv().flatten().view()` ter\u00e1 a seguinte apar\u00eancia:\n\n```console title=\"Output\u201d\nOl\u00e1\nBom dia\nOl\u00e1\n```\n</code></pre>"},{"location":"pt/hello_nextflow/01_hello_world/#conclusao_8","title":"Conclus\u00e3o","text":"<p>Voc\u00ea agora sabe como fornecer os valores de entrada para o fluxo de trabalho por meio de um arquivo. De modo mais geral, voc\u00ea aprendeu a usar os componentes essenciais do Nextflow e tem uma compreens\u00e3o b\u00e1sica da l\u00f3gica de como criar um fluxo de trabalho e gerenciar entradas e sa\u00eddas.</p>"},{"location":"pt/hello_nextflow/01_hello_world/#o-que-vem-a-seguir_8","title":"O que vem a seguir?","text":"<p>Comemore seu sucesso e fa\u00e7a uma pausa! N\u00e3o se preocupe se os tipos de canais e operadores parecerem muito dif\u00edceis de lidar na primeira vez que voc\u00ea os encontrar. Voc\u00ea ter\u00e1 mais oportunidades de praticar o uso desses componentes em v\u00e1rias configura\u00e7\u00f5es ao longo deste curso de treinamento. Quando estiver pronto, v\u00e1 para a Parte 2 para aprender sobre outro conceito importante: o provisionamento do software necess\u00e1rio para cada processo.</p>"},{"location":"es/","title":"Capacitaci\u00f3n en Nextflow","text":"<p>\u00a1Bienvenido al portal de capacitaci\u00f3n de la comunidad Nextflow!</p> <p>Estos materiales de capacitaci\u00f3n son de c\u00f3digo abierto y de uso gratuito para cualquier persona. Est\u00e1n alojados en un repositorio de GitHub junto con scripts de ejemplo y configuraci\u00f3n de desarrollo.</p> <p>Si bien puedes aprender con estos materiales en cualquier momento, probablemente los aproveches al m\u00e1ximo al participar en una sesi\u00f3n de entrenamiento. La comunidad de nf-core organiza regularmente eventos online gratuitos; consulta la p\u00e1gina de eventos de nf-core para obtener m\u00e1s informaci\u00f3n.</p> <p></p> <p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"es/#talleres-disponibles","title":"Talleres disponibles","text":"<p>Tenemos varios talleres disponibles en este sitio web. Encuentra el adecuado para ti:</p> <p>Taller de Entrenamiento B\u00e1sico de Nextflow</p> <p> Este es el principal material de capacitaci\u00f3n de Nextflow utilizado en la mayor\u00eda de los eventos de capacitaci\u00f3n de Nextflow y nf-core.</p> <p>Entrenamiento b\u00e1sico para todo lo relacionado con Nextflow. Perfecto para cualquiera que busque familiarizarse con el uso de Nextflow para ejecutar an\u00e1lisis y construir pipelines.</p> <p>Comienza ahora el entrenamiento de Nextflow </p> <p>Entrenamiento pr\u00e1ctico</p> <p> Este material es bastante corto y pr\u00e1ctico, excelente si desea practicar sus habilidades de Nextflow.</p> <p>Un tutorial para \"aprender haciendo\" con menos enfoque en la teor\u00eda, gui\u00e1ndote hacia ejercicios que aumentan en complejidad.</p> <p>Comienza el entrenamiento pr\u00e1ctico </p>"},{"location":"es/#recursos","title":"Recursos","text":"<p>Referencia r\u00e1pida a algunos enlaces \u00fatiles:</p> Referencias Comunidad Documentaci\u00f3n de Nextflow Slack de Nextflow P\u00e1gina principal de Nextflow nf-core Seqera Seqera Community <p>\u00bfNo est\u00e1s seguro por d\u00f3nde comenzar? Consulta la p\u00e1gina de ayuda.</p>"},{"location":"es/#creditos-y-contribuciones","title":"Cr\u00e9ditos y contribuciones","text":"<p>Todo el material de capacitaci\u00f3n fue escrito originalmente por Seqera pero se ha hecho de c\u00f3digo abierto (CC BY-NC-ND) para la comunidad.</p> <p>Damos la bienvenida a las correcciones y mejoras de la comunidad. Cada p\u00e1gina tiene un \u00edcono  en la parte superior derecha de la p\u00e1gina, que te llevar\u00e1 a GitHub, donde puedes editar el material a trav\u00e9s de un Pull Request.</p> <p></p> <p></p>"},{"location":"es/archive/basic_training/","title":"Index","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"es/archive/basic_training/#bienvenido","title":"Bienvenido","text":"<p>Nos complace acompa\u00f1arlo en el camino para escribir flujos de trabajo cient\u00edficos reproducibles y escalables usando Nextflow. Esta gu\u00eda complementa la documentaci\u00f3n completa de Nextflow; si alguna vez tienes alguna duda, dir\u00edjete a los documentos que se encuentran en el siguiente enlace.</p>"},{"location":"es/archive/basic_training/#objetivos","title":"Objetivos","text":"<p>Al final de este curso deber\u00eda:</p> <ol> <li>Ser competente en la escritura de scripts de Nextflow</li> <li>Conocer los conceptos b\u00e1sicos de Nextflow de Canales, Procesos y Operadores</li> <li>Comprender los flujos de trabajo en contenedores</li> <li>Comprender las diferentes plataformas de ejecuci\u00f3n compatibles con Nextflow</li> <li>Conocer la comunidad y el ecosistema de Nextflow</li> </ol>"},{"location":"es/archive/basic_training/#sigue-los-videos-de-entrenamiento","title":"Sigue los videos de entrenamiento","text":"<p>Realizamos un evento de capacitaci\u00f3n en l\u00ednea gratuito para este curso aproximadamente cada seis meses. Los videos se transmiten a YouTube y las preguntas se manejan en la comunidad nf-core de Slack. Puedes ver la grabaci\u00f3n del \u00faltimo entrenamiento (marzo de 2024) en la lista de reproducci\u00f3n de YouTube aqu\u00ed debajo:</p> <p>Si el ingl\u00e9s no es su idioma preferido, puede resultarle \u00fatil seguir la capacitaci\u00f3n del evento de marzo de 2023, que realizamos en m\u00faltiples idiomas. Tenga en cuenta que algunas partes del material de capacitaci\u00f3n pueden haberse actualizado desde que se registr\u00f3.</p> <ul> <li> En Ingl\u00e9s</li> <li> En Hind\u00fa</li> <li> En Espa\u00f1ol</li> <li> En Portugues</li> <li> En Franc\u00e9s</li> </ul>"},{"location":"es/archive/basic_training/#descripcion-general","title":"Descripci\u00f3n general","text":"<p>Para que comience con Nextflow lo m\u00e1s r\u00e1pido posible, seguiremos los siguientes pasos:</p> <ol> <li>Configurar un entorno de desarrollo para ejecutar Nextflow</li> <li>Explorar los conceptos de Nextflow utilizando algunos flujos de trabajo b\u00e1sicos, incluido un an\u00e1lisis de RNA-Seq en varios pasos</li> <li>Crear y use contenedores Docker para encapsular todas las dependencias del flujo de trabajo</li> <li>Profundizar en la sintaxis central de Nextflow, incluidos canales, procesos y operadores</li> <li>Probar escenarios de implementaci\u00f3n en la nube y explore las capacidades de Nextflow Tower</li> </ol> <p>Esto te dar\u00e1 una amplia comprensi\u00f3n de Nextflow para comenzar a escribir tus propios flujos de trabajo. \u00a1Esperamos que disfrutes del curso! Este es un documento en constante evoluci\u00f3n: los comentarios siempre son bienvenidos.</p>"},{"location":"fr/","title":"Formation Nextflow","text":"<p>Bienvenue sur le portail de formation de la communaut\u00e9 Nextflow !</p> <p>Nous proposons plusieurs formations distinctes sur ce site. Faites d\u00e9filer vers le bas pour trouver celle qui vous convient!</p> <p>Les formations list\u00e9es ci-dessous sont con\u00e7ues comme des ressources en libre-service ; vous pouvez les suivre \u00e0 votre rythme (voir la section \"Configuration de l\u2019environnement\" pour les d\u00e9tails pratiques). Vous pouvez toutefois en tirer encore plus profit en participant \u00e0 une session de formation en groupe.</p> <ul> <li>Des \u00e9v\u00e9nements en ligne gratuits sont r\u00e9guli\u00e8rement organis\u00e9s par la communaut\u00e9 nf-core. Consultez la page des \u00e9v\u00e9nements nf-core pour en savoir plus.</li> <li>Seqera (l'entreprise qui d\u00e9veloppe Nextflow) propose divers \u00e9v\u00e9nements de formation. Consultez la page \u00c9v\u00e9nements Seqera pour trouver les \"Seqera Sessions\" et les \"Nextflow Summit\".</li> <li>Notre \u00e9quipe communautaire organise \u00e9galement r\u00e9guli\u00e8rement des formations via des organisations partenaires ; les annonces et inscriptions sont g\u00e9n\u00e9ralement g\u00e9r\u00e9es par ces derni\u00e8res.</li> </ul> <p>Quand vous \u00eates pr\u00eat\u00b7e \u00e0 commencer, cliquez sur le bouton \"Open in GitHub Codespaces\" (pr\u00e9sent sur cette page ou sur la page d\u2019accueil de la formation choisie) pour ouvrir un environnement de formation en ligne (n\u00e9cessite un compte GitHub gratuit).</p> <p></p>"},{"location":"fr/#configuration-de-lenvironnement-de-formation","title":"Configuration de l\u2019environnement de formation","text":"<p>Environment Setup</p> <p> Configurez votre environnement pour la premi\u00e8re fois.</p> <p>Instructions pour configurer votre environnement afin de suivre les formations (tous les cours). Introduction \u00e0 GitHub Codespaces et alternatives pour une installation locale.</p> <p>Commencer la formation sur la configuration de l\u2019environnement </p>"},{"location":"fr/#nextflow-pour-les-debutantes","title":"Nextflow pour les d\u00e9butant\u00b7es","text":"<p>Ces cours de base, ind\u00e9pendants du domaine, s\u2019adressent \u00e0 celles et ceux qui d\u00e9couvrent totalement Nextflow. Chaque cours est constitu\u00e9 de modules con\u00e7us pour d\u00e9velopper les comp\u00e9tences progressivement.</p> <p>Hello Nextflow</p> <p> Apprenez \u00e0 d\u00e9velopper des pipelines avec Nextflow.</p> <p> Mat\u00e9riel vid\u00e9o disponible.</p> <p>Ce cours s\u2019adresse aux d\u00e9butant\u00b7es qui souhaitent apprendre \u00e0 d\u00e9velopper leurs propres pipelines. Il couvre les composants essentiels du langage Nextflow afin de permettre la cr\u00e9ation de pipelines simples mais fonctionnels. Il inclut \u00e9galement des \u00e9l\u00e9ments cl\u00e9s de conception, d\u00e9veloppement et configuration.</p> <p>Commencer la formation Hello Nextflow </p> <p>Hello nf-core</p> <p> Apprenez \u00e0 d\u00e9velopper des pipelines compatibles nf-core.</p> <p>Ce cours s\u2019adresse aux d\u00e9butant\u00b7es souhaitant apprendre \u00e0 utiliser et d\u00e9velopper des pipelines conformes \u00e0 nf-core. Il pr\u00e9sente la structure des pipelines nf-core et les bonnes pratiques de d\u00e9veloppement.</p> <p>Commencer la formation Hello nf-core </p> <p>\u00c0 venir : \"Nextflow Run\" \u2014 Apprenez \u00e0 ex\u00e9cuter des pipelines Nextflow (ex\u00e9cution uniquement, sans d\u00e9veloppement)</p>"},{"location":"fr/#nextflow-pour-la-science","title":"Nextflow pour la science","text":"<p>Ces cours montrent comment appliquer les concepts du cours \"Hello Nextflow\" \u00e0 des cas d\u2019usage scientifiques sp\u00e9cifiques.</p> <p>Nextflow for Genomics</p> <p> Apprenez \u00e0 d\u00e9velopper un pipeline de g\u00e9nomique avec Nextflow.</p> <p>Ce cours s\u2019adresse aux chercheur\u00b7euses souhaitant cr\u00e9er leurs propres pipelines de g\u00e9nomique. Il utilise un exemple d\u2019appel de variants pour illustrer la cr\u00e9ation d\u2019un pipeline simple mais fonctionnel.</p> <p>Commencer la formation pour la g\u00e9nomique </p> <p>Nextflow for RNAseq</p> <p> Apprenez \u00e0 d\u00e9velopper un pipeline RNAseq avec Nextflow.</p> <p>Ce cours s\u2019adresse aux chercheur\u00b7euses souhaitant cr\u00e9er des pipelines RNAseq. Il utilise un cas d\u2019analyse de bulk RNAseq pour d\u00e9montrer le d\u00e9veloppement d\u2019un pipeline fonctionnel.</p> <p>Commencer la formation RNAseq </p> <p>Faites-nous savoir quels autres domaines ou cas d\u2019usage vous aimeriez voir couverts en postant sur la section Formation du forum de la communaut\u00e9.</p>"},{"location":"fr/#formation-approfondie-nextflow","title":"Formation approfondie Nextflow","text":"<p>Ces cours montrent comment utiliser les fonctionnalit\u00e9s de Nextflow de mani\u00e8re plus d\u00e9taill\u00e9e ou \u00e0 un niveau plus avanc\u00e9. Chaque cours est compos\u00e9 d\u2019un ou plusieurs modules con\u00e7us pour aider les apprenant\u00b7es \u00e0 perfectionner leurs comp\u00e9tences sur les sujets correspondants.</p> <p>Qu\u00eates secondaires</p> <p> Modules de formation sur divers sujets d\u2019int\u00e9r\u00eat.</p> <p>Ce cours s\u2019adresse aux d\u00e9veloppeur\u00b7euses Nextflow souhaitant \u00e9largir leur champ de comp\u00e9tences et/ou approfondir leurs connaissances. Bien que les modules soient pr\u00e9sent\u00e9s de mani\u00e8re lin\u00e9aire, les apprenant\u00b7es peuvent les suivre dans n\u2019importe quel ordre. Toute d\u00e9pendance \u00e0 des composants ou comp\u00e9tences d\u00e9passant le cadre du cours \"Hello Nextflow\" est indiqu\u00e9e dans l\u2019introduction du module concern\u00e9.</p> <p>Commencer la formation Qu\u00eates secondaires </p> <p>Formation Fondamentaux</p> <p> Mat\u00e9riel de formation complet pour explorer l\u2019ensemble des fonctionnalit\u00e9s de Nextflow.</p> <p>Le cours sur les fondamentaux couvre tous les aspects de Nextflow. Il est con\u00e7u comme une ressource de r\u00e9f\u00e9rence pour toute personne souhaitant construire des workflows complexes avec Nextflow.</p> <p>Commencer la formation Fondamentaux </p> <p>Formation Avanc\u00e9e</p> <p> Mat\u00e9riel de formation avanc\u00e9 pour ma\u00eetriser Nextflow.</p> <p>Ce cours approfondit les fonctionnalit\u00e9s les plus avanc\u00e9es du langage et de l\u2019environnement d\u2019ex\u00e9cution Nextflow, et explique comment les utiliser pour cr\u00e9er des workflows efficaces, \u00e9volutifs et adapt\u00e9s aux traitements intensifs de donn\u00e9es.</p> <p>Commencer la formation Avanc\u00e9e </p>"},{"location":"fr/#autres-experimental","title":"Autres / Exp\u00e9rimental","text":"<p>Ce sont des formations qui ne sont plus activement dispens\u00e9es ou maintenues et que nous pourrions r\u00e9utiliser ailleurs ou supprimer dans un avenir proche. Les supports correspondants ne sont pas disponibles dans l\u2019environnement de formation. Vous pouvez toutefois les retrouver dans le d\u00e9p\u00f4t GitHub et les t\u00e9l\u00e9charger pour une utilisation locale.</p> <ul> <li> <p>nf-customize \u2014 Configuration des pipelines nf-core (docs / code)</p> </li> <li> <p>troubleshoot \u2014 Exercices de d\u00e9pannage (docs / code)</p> </li> <li> <p>hands-on (rnaseq) \u2014 D\u00e9veloppement d\u2019un pipeline pour l\u2019analyse bulk RNAseq (obsol\u00e8te) (docs / code)</p> </li> </ul>"},{"location":"fr/#resources","title":"Resources","text":"<p>R\u00e9f\u00e9rence rapide de quelques liens utiles:</p> R\u00e9f\u00e9rence \u00a0Communaut\u00e9 Nextflow Docs Nextflow Slack Nextflow Homepage nf-core Seqera Seqera Community Forum <p>Vous ne savez pas o\u00f9 aller ? Consultez la page Obtenir de l'aide.</p>"},{"location":"fr/#credits-et-contributions","title":"Cr\u00e9dits et contributions","text":"<p>Ce mat\u00e9riel de formation est d\u00e9velopp\u00e9 et maintenu par Seqera et publi\u00e9 sous une licence open source (CC BY-NC-ND) au b\u00e9n\u00e9fice de la communaut\u00e9. Vous \u00eates libre de r\u00e9utiliser ces ressources conform\u00e9ment aux conditions de la licence. Si vous \u00eates formateur\u00b7trice et que vous organisez vos propres sessions, nous serions ravis d\u2019avoir votre retour sur votre exp\u00e9rience et de savoir ce que nous pourrions faire pour vous faciliter la t\u00e2che.</p> <p>Nous accueillons avec plaisir les corrections et am\u00e9liorations propos\u00e9es par la communaut\u00e9. Chaque page comporte une ic\u00f4ne  en haut \u00e0 droite qui vous redirige vers GitHub, o\u00f9 vous pouvez proposer des modifications du contenu via une pull request.</p> <p></p> <p></p>"},{"location":"fr/help/","title":"Obtenir de l'aide","text":""},{"location":"fr/help/#documentation-nextflow","title":"Documentation Nextflow","text":"<p>Nextflow dispose d'une excellente documentation, qui constitue votre premi\u00e8re ressource d\u00e8s que vous rencontrez quelque chose que vous ne comprenez pas ou si vous souhaitez en savoir plus. Vous pouvez parcourir les diff\u00e9rents sujets ou rechercher des termes sp\u00e9cifiques.</p>"},{"location":"fr/help/#forum-communautaire","title":"Forum communautaire","text":"<p>Si vous \u00e9prouvez des difficult\u00e9s, n\u2019h\u00e9sitez pas \u00e0 demander de l\u2019aide. Notre formidable communaut\u00e9 est l\u2019un des grands atouts de Nextflow !</p> <p>Le forum communautaire de Seqera est un excellent endroit pour poser des questions sur Nextflow. C\u2019est aussi une tr\u00e8s bonne ressource pour trouver des r\u00e9ponses \u00e0 des questions qui ont d\u00e9j\u00e0 \u00e9t\u00e9 pos\u00e9es !</p> <p>Si vous ne trouvez pas de solution \u00e0 votre probl\u00e8me, connectez-vous simplement et cliquez sur le bouton \u00ab Nouveau sujet \u00bb pour publier votre question dans la cat\u00e9gorie Ask for Help. N\u2019h\u00e9sitez pas \u00e0 ajouter des tags que vous jugez pertinents, car ils peuvent aider notre \u00e9quipe et vos pairs \u00e0 rep\u00e9rer et \u00e0 r\u00e9pondre plus facilement \u00e0 votre question.</p>"},{"location":"fr/help/#support-professionnel","title":"Support professionnel","text":"<p>Nextflow est un logiciel libre et open source d\u00e9velopp\u00e9 par Seqera, une entreprise dont le si\u00e8ge est en Espagne et qui poss\u00e8de des bureaux secondaires au Royaume-Uni et aux \u00c9tats-Unis.</p> <p>Seqera propose des services de support professionnel pour Nextflow et les produits associ\u00e9s, y compris des sessions de formation sur mesure. Si cela vous int\u00e9resse, n\u2019h\u00e9sitez pas \u00e0 nous contacter.</p>"},{"location":"fr/archive/basic_training/","title":"Index","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"fr/archive/basic_training/#bienvenue","title":"Bienvenue","text":"<p>Nous sommes heureux de vous accompagner sur le chemin de r\u00e9daction de workflows scientifiques reproductibles et \u00e9volutifs en utilisant Nextflow. Ce guide compl\u00e8te toute la documentation de Nextflow - si vous avez des doutes, consultez la documentation situ\u00e9e ici.</p>"},{"location":"fr/archive/basic_training/#objectifs","title":"Objectifs","text":"<p>\u00c0 la fin de ce cours, vous devriez:</p> <ol> <li>Ma\u00eetriser la redaction de workflows Nextflow</li> <li>Conna\u00eetre les concepts de base de Nextflow : canaux, processus et op\u00e9rateurs.</li> <li>Avoir une compr\u00e9hension des workflows conteneuris\u00e9s</li> <li>Comprendre les diff\u00e9rentes plateformes d'ex\u00e9cution support\u00e9es par Nextflow</li> <li>Conna\u00eetre la communaut\u00e9 et l'\u00e9cosyst\u00e8me Nextflow</li> </ol>"},{"location":"fr/archive/basic_training/#suivez-les-videos-de-formation","title":"Suivez les vid\u00e9os de formation","text":"<p>Nous organisons un \u00e9v\u00e9nement de formation en ligne gratuit pour ce cours environ tous les six mois. Les vid\u00e9os sont diffus\u00e9es sur YouTube et les questions sont trait\u00e9es dans la communaut\u00e9 nf-core Slack. Vous pouvez regarder l'enregistrement de la formation la plus r\u00e9cente (mars 2024) dans la playlist YouTube ci-dessous\u00a0:</p> <p>Si l'anglais n'est pas votre langue pr\u00e9f\u00e9r\u00e9e, vous trouverez peut-\u00eatre utile de suivre la formation de l'\u00e9v\u00e9nement de mars 2023, que nous avons organis\u00e9 plusieurs langues. Veuillez noter que certaines parties du mat\u00e9riel de formation peuvent avoir \u00e9t\u00e9 mises \u00e0 jour depuis leur enregistrement.</p> <ul> <li> En Anglais</li> <li> En Hindi</li> <li> En Espagnol</li> <li> En Portugais</li> <li> En Fran\u00e7ais</li> </ul>"},{"location":"fr/archive/basic_training/#apercu","title":"Aper\u00e7u","text":"<p>Pour que vous puissiez commencer \u00e0 utiliser Nextflow le plus rapidement possible, nous allons suivre les \u00e9tapes suivantes :</p> <ol> <li>Mettre en place un environnement de d\u00e9veloppement pour ex\u00e9cuter Nextflow</li> <li>Explorer les concepts de Nextflow en utilisant quelques workflows de base, y compris une analyse RNA-Seq en plusieurs \u00e9tapes.</li> <li>Construire et utiliser des conteneurs Docker pour encapsuler toutes les d\u00e9pendances des workflows.</li> <li>Approfondir la syntaxe de base de Nextflow, y compris les canaux, les processus et les op\u00e9rateurs.</li> <li>Couvrir les sc\u00e9narios de d\u00e9ploiement en cluster et en cloud et explorer les capacit\u00e9s de Nextflow Tower.</li> </ol> <p>Cela vous donnera une large compr\u00e9hension de Nextflow, pour commencer \u00e0 \u00e9crire vos propres workflows. Nous esp\u00e9rons que vous appr\u00e9cierez ce cours ! Il s'agit d'un document en constante \u00e9volution - les commentaires sont toujours les bienvenus.</p>"},{"location":"fr/archive/basic_training/cache_and_resume/","title":"Cache and resume","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"fr/archive/basic_training/cache_and_resume/#cache-dexecution-et-reprise","title":"Cache d'ex\u00e9cution et reprise","text":"<p>Le m\u00e9canisme de mise en cache de Nextflow fonctionne en attribuant un identifiant unique \u00e0 chaque t\u00e2che qui est utilis\u00e9 pour cr\u00e9er un r\u00e9pertoire d'ex\u00e9cution distinct o\u00f9 les t\u00e2ches sont ex\u00e9cut\u00e9es et les r\u00e9sultats stock\u00e9s.</p> <p>L'identifiant unique de la t\u00e2che est g\u00e9n\u00e9r\u00e9 sous la forme d'une valeur de hachage de 128 bits compos\u00e9e des valeurs d'entr\u00e9e de la t\u00e2che, des fichiers et de la cha\u00eene de commande.</p> <p>Le workflow du r\u00e9pertoire work est organis\u00e9 comme indiqu\u00e9 ci-dessous :</p> <pre><code>work/\n\u251c\u2500\u2500 12\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 1adacb582d2198cd32db0e6f808bce\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 genome.fa -&gt; /data/../genome.fa\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 index\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 hash.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 header.json\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 indexing.log\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 quasi_index.log\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 refInfo.json\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 rsd.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 sa.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 txpInfo.bin\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 versionInfo.json\n\u251c\u2500\u2500 19\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 663679d1d87bfeafacf30c1deaf81b\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 aux_info\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ambig_info.tsv\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 expected_bias.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fld.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta_info.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 observed_bias.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 observed_bias_3p.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 cmd_info.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 libParams\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 flenDist.txt\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 lib_format_counts.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 salmon_quant.log\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 quant.sf\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut_1.fq -&gt; /data/../ggal_gut_1.fq\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut_2.fq -&gt; /data/../ggal_gut_2.fq\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 index -&gt; /data/../asciidocs/day2/work/12/1adacb582d2198cd32db0e6f808bce/index\n</code></pre> <p>Info</p> <p>Vous pouvez cr\u00e9er ces plots en utilisant la fonction <code>tree</code> si vous l'avez install\u00e9e. Sur les syst\u00e8mes d'exploitation bas\u00e9s sur Debian, il suffit de <code>sudo apt install -y tree</code> ou, pour macOS, d'utiliser Homebrew : <code>brew install tree</code></p>"},{"location":"fr/archive/basic_training/cache_and_resume/#comment-fonctionne-la-reprise","title":"Comment fonctionne la reprise","text":"<p>L'option de ligne de commande <code>-resume</code> permet de poursuivre l'ex\u00e9cution d'un workflow \u00e0 partir de la derni\u00e8re \u00e9tape qui s'est achev\u00e9e avec succ\u00e8s :</p> <pre><code>nextflow run &lt;script&gt; -resume\n</code></pre> <p>En pratique, le workflow est ex\u00e9cut\u00e9 depuis le d\u00e9but. Cependant, avant de lancer l'ex\u00e9cution d'un processus, Nextflow utilise l'identifiant unique de la t\u00e2che pour v\u00e9rifier si le r\u00e9pertoire work existe d\u00e9j\u00e0 et s'il contient un \u00e9tat de sortie de commande valide avec les fichiers de sortie attendus.</p> <p>Si cette condition est remplie, l'ex\u00e9cution de la t\u00e2che est saut\u00e9e et les r\u00e9sultats calcul\u00e9s pr\u00e9c\u00e9demment sont utilis\u00e9s comme r\u00e9sultats du processus.</p> <p>La premi\u00e8re t\u00e2che pour laquelle une nouvelle sortie est calcul\u00e9e invalide toutes les ex\u00e9cutions en aval dans le DAG restant.</p>"},{"location":"fr/archive/basic_training/cache_and_resume/#repertoire-work","title":"repertoire Work","text":"<p>Les r\u00e9pertoires de travail des t\u00e2ches sont cr\u00e9\u00e9s par d\u00e9faut dans le dossier <code>work</code> du chemin de lancement. Ce dossier est cens\u00e9 \u00eatre une zone de stockage scratch qui peut \u00eatre nettoy\u00e9e une fois le calcul termin\u00e9.</p> <p>Note</p> <p>Les r\u00e9sultats finaux du workflow sont cens\u00e9s \u00eatre stock\u00e9s dans un emplacement diff\u00e9rent sp\u00e9cifi\u00e9 \u00e0 l'aide d'une ou plusieurs directives publishDir.</p> <p>Warning</p> <p>Veillez \u00e0 supprimer votre r\u00e9pertoire work de temps en temps, sinon votre machine/environnement risque d'\u00eatre rempli de fichiers inutilis\u00e9s.</p> <p>Un emplacement diff\u00e9rent pour le r\u00e9pertoire d'ex\u00e9cution work peut \u00eatre sp\u00e9cifi\u00e9 en utilisant l'option de ligne de commande <code>-w</code>. Par exemple :</p> <pre><code>nextflow run &lt;script&gt; -w /some/scratch/dir\n</code></pre> <p>Warning</p> <p>Si vous supprimez ou d\u00e9placez le workflow du r\u00e9pertoire work, cela emp\u00eachera l'utilisation de la fonction de resume lors des ex\u00e9cutions suivantes.</p> <p>Le code de hachage des fichiers d'entr\u00e9e est calcul\u00e9 en utilisant :</p> <ul> <li>le chemin d'acc\u00e8s complet au fichier</li> <li>la taille du fichier</li> <li>l'horodatage de la derni\u00e8re modification</li> </ul> <p>Par cons\u00e9quent, le simple fait de ** toucher** un fichier invalidera l'ex\u00e9cution de la t\u00e2che correspondante.</p>"},{"location":"fr/archive/basic_training/cache_and_resume/#comment-organiser-des-experiences-in-silico","title":"Comment organiser des exp\u00e9riences in-silico ?","text":"<p>Il est conseill\u00e9 d'organiser chaque exp\u00e9rience dans son propre dossier. Les principaux param\u00e8tres d'entr\u00e9e de l'exp\u00e9rience doivent \u00eatre sp\u00e9cifi\u00e9s en utilisant un fichier de configuration Nextflow. Cela facilite le suivi et la r\u00e9plication d'une exp\u00e9rience dans le temps.</p> <p>Note</p> <p>Dans la m\u00eame exp\u00e9rience, le m\u00eame flux de travail peut \u00eatre ex\u00e9cut\u00e9 plusieurs fois, cependant, le lancement simultan\u00e9 de deux (ou plus) instances Nextflow dans le m\u00eame r\u00e9pertoire doit \u00eatre \u00e9vit\u00e9.</p> <p>La commande <code>nextflow log</code> liste les ex\u00e9cutions effectu\u00e9es dans le dossier courant :</p> <pre><code>$ nextflow log\n\nTIMESTAMP            DURATION  RUN NAME          STATUS  REVISION ID  SESSION ID                            COMMAND\n2019-05-06 12:07:32  1.2s      focused_carson    ERR     a9012339ce   7363b3f0-09ac-495b-a947-28cf430d0b85  nextflow run hello\n2019-05-06 12:08:33  21.1s     mighty_boyd       OK      a9012339ce   7363b3f0-09ac-495b-a947-28cf430d0b85  nextflow run rnaseq-nf -with-docker\n2019-05-06 12:31:15  1.2s      insane_celsius    ERR     b9aefc67b4   4dc656d2-c410-44c8-bc32-7dd0ea87bebf  nextflow run rnaseq-nf\n2019-05-06 12:31:24  17s       stupefied_euclid  OK      b9aefc67b4   4dc656d2-c410-44c8-bc32-7dd0ea87bebf  nextflow run rnaseq-nf -resume -with-docker\n</code></pre> <p>Vous pouvez utiliser l'identifiant de session ou le nom d'ex\u00e9cution pour r\u00e9cup\u00e9rer une ex\u00e9cution sp\u00e9cifique. Par exemple:</p> <pre><code>nextflow run rnaseq-nf -resume mighty_boyd\n</code></pre>"},{"location":"fr/archive/basic_training/cache_and_resume/#provenance-de-lexecution","title":"Provenance de l'ex\u00e9cution","text":"<p>La commande <code>log</code>, lorsqu'elle est fournie avec un nom d'ex\u00e9cution ou un identifiant de session, peut renvoyer de nombreuses informations utiles sur l'ex\u00e9cution d'un workflow qui peuvent \u00eatre utilis\u00e9es pour cr\u00e9er un rapport de provenance.</p> <p>Par d\u00e9faut, il \u00e9num\u00e8re les r\u00e9pertoires de travail utilis\u00e9s pour calculer chaque t\u00e2che. Par exemple :</p> <pre><code>$ nextflow log tiny_fermat\n\n/data/.../work/7b/3753ff13b1fa5348d2d9b6f512153a\n/data/.../work/c1/56a36d8f498c99ac6cba31e85b3e0c\n/data/.../work/f7/659c65ef60582d9713252bcfbcc310\n/data/.../work/82/ba67e3175bd9e6479d4310e5a92f99\n/data/.../work/e5/2816b9d4e7b402bfdd6597c2c2403d\n/data/.../work/3b/3485d00b0115f89e4c202eacf82eba\n</code></pre> <p>L'option <code>-f</code> (fields) peut \u00eatre utilis\u00e9e pour sp\u00e9cifier quelles m\u00e9tadonn\u00e9es doivent \u00eatre imprim\u00e9es par la commande <code>log</code>. Par exemple :</p> <pre><code>$ nextflow log tiny_fermat -f 'process,exit,hash,duration'\n\nindex    0   7b/3753ff  2.0s\nfastqc   0   c1/56a36d  9.3s\nfastqc   0   f7/659c65  9.1s\nquant    0   82/ba67e3  2.7s\nquant    0   e5/2816b9  3.2s\nmultiqc  0   3b/3485d0  6.3s\n</code></pre> <p>La liste compl\u00e8te des domaines disponibles peut \u00eatre consult\u00e9e \u00e0 l'aide de la commande :</p> <pre><code>nextflow log -l\n</code></pre> <p>L'option <code>-F</code> permet de sp\u00e9cifier des crit\u00e8res de filtrage pour n'imprimer qu'un sous-ensemble de t\u00e2ches. Par exemple :</p> <pre><code>$ nextflow log tiny_fermat -F 'process =~ /fastqc/'\n\n/data/.../work/c1/56a36d8f498c99ac6cba31e85b3e0c\n/data/.../work/f7/659c65ef60582d9713252bcfbcc310\n</code></pre> <p>Cela peut \u00eatre utile pour localiser les r\u00e9pertoires de travail d'une t\u00e2che sp\u00e9cifique.</p> <p>Enfin, l'option <code>-t</code> permet de cr\u00e9er un rapport de provenance personnalis\u00e9 de base, en affichant un fichier mod\u00e8le dans le format de votre choix. Par exemple:</p> <pre><code>&lt;div&gt;\n  &lt;h2&gt;${name}&lt;/h2&gt;\n  &lt;div&gt;\n    Script:\n    &lt;pre&gt;${script}&lt;/pre&gt;\n  &lt;/div&gt;\n\n  &lt;ul&gt;\n    &lt;li&gt;Exit: ${exit}&lt;/li&gt;\n    &lt;li&gt;Status: ${status}&lt;/li&gt;\n    &lt;li&gt;Work dir: ${workdir}&lt;/li&gt;\n    &lt;li&gt;Container: ${container}&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n</code></pre> <p>Exercise</p> <p>Sauvegardez l'extrait ci-dessus dans un fichier nomm\u00e9 <code>template.html</code>. Lancez ensuite cette commande (en utilisant l'identifiant correct pour votre ex\u00e9cution, par exemple <code>tiny_fermat</code>) :</p> <pre><code>nextflow log tiny_fermat -t template.html &gt; prov.html\n</code></pre> <p>Enfin, ouvrez le fichier <code>prov.html</code> avec un navigateur.</p>"},{"location":"fr/archive/basic_training/cache_and_resume/#depanner-la-reprise","title":"D\u00e9panner la reprise","text":"<p>La possibilit\u00e9 de reprendre les workflows est une fonctionnalit\u00e9 cl\u00e9 de Nextflow, mais elle ne fonctionne pas toujours comme vous l'attendez. Dans cette section, nous allons passer en revue quelques raisons courantes pour lesquelles Nextflow peut ignorer vos r\u00e9sultats mis en cache.</p> <p>Tip</p> <p>Pour en savoir plus sur le m\u00e9canisme de reprise et sur la mani\u00e8re de r\u00e9soudre les probl\u00e8mes, veuillez consulter les trois articles de blog suivants :</p> <ol> <li>D\u00e9mystifier la reprise de  Nextflow</li> <li>Depanner la reprise Nextflow resume</li> <li>Analyse du comportement des pipelines en mati\u00e8re de mise en cache</li> </ol>"},{"location":"fr/archive/basic_training/cache_and_resume/#fichier-dentree-modifie","title":"Fichier d'entr\u00e9e modifi\u00e9","text":"<p>Assurez-vous qu'il n'y a pas de changement dans le(s) fichier(s) d'entr\u00e9e. N'oubliez pas que le hachage unique de la t\u00e2che est calcul\u00e9 en tenant compte du chemin complet du fichier, de l'horodatage de la derni\u00e8re modification et de la taille du fichier. Si l'une de ces informations a chang\u00e9, le workflow sera r\u00e9ex\u00e9cut\u00e9 m\u00eame si le contenu d'entr\u00e9e est identique.</p>"},{"location":"fr/archive/basic_training/cache_and_resume/#un-processus-modifie-une-entree","title":"Un processus modifie une entr\u00e9e","text":"<p>Un processus ne doit jamais modifier les fichiers d'entr\u00e9e, sinon le <code>resume</code> pour les ex\u00e9cutions futures sera invalid\u00e9 pour la m\u00eame raison que celle expliqu\u00e9e dans le point pr\u00e9c\u00e9dent.</p>"},{"location":"fr/archive/basic_training/cache_and_resume/#attributs-de-fichiers-incoherents","title":"Attributs de fichiers incoh\u00e9rents","text":"<p>Certains syst\u00e8mes de fichiers partag\u00e9s, tels que NFS, peuvent signaler un horodatage incoh\u00e9rent (c'est-\u00e0-dire un horodatage diff\u00e9rent pour le m\u00eame fichier) m\u00eame s'il n'a pas \u00e9t\u00e9 modifi\u00e9. Pour \u00e9viter ce probl\u00e8me, utilisez la strat\u00e9gie de cache indulgente.</p>"},{"location":"fr/archive/basic_training/cache_and_resume/#condition-de-course-dans-une-variable-globale","title":"Condition de course dans une variable globale","text":"<p>Nextflow est con\u00e7u pour simplifier la programmation parall\u00e8le sans se pr\u00e9occuper des conditions de course et de l'acc\u00e8s aux ressources partag\u00e9es. L'un des rares cas o\u00f9 une condition de course peut survenir est l'utilisation d'une variable globale avec deux (ou plus) op\u00e9rateurs. Par exemple:</p> <pre><code>Channel\n    .of(1, 2, 3)\n    .map { it -&gt; X = it; X += 2 }\n    .view { \"ch1 = $it\" }\n\nChannel\n    .of(1, 2, 3)\n    .map { it -&gt; X = it; X *= 2 }\n    .view { \"ch2 = $it\" }\n</code></pre> <p>Le probl\u00e8me dans cet extrait est que la variable <code>X</code> dans la d\u00e9finition de la fermeture est d\u00e9finie dans la port\u00e9e globale. Par cons\u00e9quent, puisque les op\u00e9rateurs sont ex\u00e9cut\u00e9s en parall\u00e8le, la valeur <code>X</code> peut \u00eatre \u00e9cras\u00e9e par l'autre invocation de <code>map</code>.</p> <p>L'impl\u00e9mentation correcte n\u00e9cessite l'utilisation du mot-cl\u00e9 <code>def</code> pour d\u00e9clarer la variable locale.</p> <pre><code>Channel\n    .of(1, 2, 3)\n    .map { it -&gt; def X = it; X += 2 }\n    .println { \"ch1 = $it\" }\n\nChannel\n    .of(1, 2, 3)\n    .map { it -&gt; def X = it; X *= 2 }\n    .println { \"ch2 = $it\" }\n</code></pre>"},{"location":"fr/archive/basic_training/cache_and_resume/#canaux-dentree-non-deterministes","title":"Canaux d'entr\u00e9e non d\u00e9terministes","text":"<p>Si l'ordre des canaux de flux de donn\u00e9es est garanti - les donn\u00e9es sont lues dans l'ordre dans lequel elles sont \u00e9crites dans le canal - il faut savoir qu'il n'y a aucune garantie que les \u00e9l\u00e9ments conservent leur ordre dans le canal de sortie du processus. En effet, un processus peut engendrer plusieurs t\u00e2ches, qui peuvent s'ex\u00e9cuter en parall\u00e8le. Par exemple, l'op\u00e9ration sur le deuxi\u00e8me \u00e9l\u00e9ment peut se terminer plus t\u00f4t que l'op\u00e9ration sur le premier \u00e9l\u00e9ment, ce qui modifie l'ordre du canal de sortie.</p> <p>En pratique, consid\u00e9rons l'extrait suivant :</p> <pre><code>process FOO {\n    input:\n    val x\n\n    output:\n    tuple val(task.index), val(x)\n\n    script:\n    \"\"\"\n    sleep \\$((RANDOM % 3))\n    \"\"\"\n}\n\nworkflow {\n    channel.of('A', 'B', 'C', 'D') | FOO | view\n}\n</code></pre> <p>Tout comme nous l'avons vu au d\u00e9but de ce tutoriel avec HELLO WORLD ou WORLD HELLO, la sortie de l'extrait ci-dessus peut \u00eatre :</p> <pre><code>[3, C]\n[4, D]\n[2, B]\n[1, A]\n</code></pre> <p>... et cet ordre sera probablement diff\u00e9rent \u00e0 chaque fois que le flux de travail sera ex\u00e9cut\u00e9.</p> <p>Imaginons maintenant que nous ayons deux processus de ce type, dont les canaux de sortie servent de canaux d'entr\u00e9e \u00e0 un troisi\u00e8me processus. Les deux canaux seront ind\u00e9pendamment al\u00e9atoires, de sorte que le troisi\u00e8me processus ne doit pas s'attendre \u00e0 ce qu'ils conservent une s\u00e9quence appari\u00e9e. S'il suppose que le premier \u00e9l\u00e9ment du canal de sortie du premier processus est li\u00e9 au premier \u00e9l\u00e9ment du canal de sortie du deuxi\u00e8me processus, il y aura inad\u00e9quation.</p> <p>Une solution courante consiste \u00e0 utiliser ce que l'on appelle commun\u00e9ment une meta map. Un objet groovy contenant des informations sur les \u00e9chantillons est transmis avec les r\u00e9sultats du fichier dans un canal de sortie sous la forme d'un tuple. Cet objet peut ensuite \u00eatre utilis\u00e9 pour associer des \u00e9chantillons provenant de canaux distincts en vue d'une utilisation en aval. Par exemple, au lieu de mettre juste <code>/some/path/myoutput.bam</code> dans un canal, vous pouvez utiliser <code>['SRR123', '/some/path/myoutput.bam']</code> pour vous assurer que les processus ne sont pas en conflit. Regardez l'exemple ci-dessous :</p> <pre><code>// For example purposes only.\n// These would normally be outputs from upstream processes.\nChannel\n    .of(\n        [[id: 'sample_1'], '/path/to/sample_1.bam'],\n        [[id: 'sample_2'], '/path/to/sample_2.bam']\n    )\n    .set { bam }\n\n// NB: sample_2 is now the first element, instead of sample_1\nChannel\n    .of(\n        [[id: 'sample_2'], '/path/to/sample_2.bai'],\n        [[id: 'sample_1'], '/path/to/sample_1.bai']\n    )\n    .set { bai }\n\n// Instead of feeding the downstream process with these two channels separately, we can\n// join them and provide a single channel where the sample meta map is implicitly matched:\nbam\n    .join(bai)\n    | PROCESS_C\n</code></pre> <p>Si les m\u00e9ta-cartes ne sont pas possibles, une alternative est d'utiliser la directive de processus <code>fair</code>. Lorsque cette directive est sp\u00e9cifi\u00e9e, Nextflow garantira que l'ordre des sorties correspondra \u00e0 l'ordre des entr\u00e9es. Il est important de mentionner que l'ordre dans lequel les t\u00e2ches seront termin\u00e9es ne suivra pas n\u00e9cessairement l'ordre dans le canal d'entr\u00e9e, mais Nextflow garantit qu'\u00e0 la fin de celui-ci, le canal de sortie contiendra les \u00e9l\u00e9ments dans l'ordre respectif.</p> <p>Warning</p> <p>En fonction de votre situation, l'utilisation de la directive <code>fair</code> peut entra\u00eener une diminution des performances.</p>"},{"location":"fr/archive/basic_training/debugging/","title":"Debugging","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"fr/archive/basic_training/debugging/#traitement-des-erreurs-et-depannage","title":"Traitement des erreurs et d\u00e9pannage","text":""},{"location":"fr/archive/basic_training/debugging/#debogage-des-erreurs-dexecution","title":"D\u00e9bogage des erreurs d'ex\u00e9cution","text":"<p>Lorsqu'une ex\u00e9cution de processus se termine avec un statut de sortie non nul, Nextflow arr\u00eate l'ex\u00e9cution du workflow et signale la t\u00e2che d\u00e9faillante :</p> <p>Cliquez sur les ic\u00f4nes  dans le code pour obtenir des explications.</p> <pre><code>ERROR ~ Error executing process &gt; 'INDEX'\n\nCaused by: # (1)!\n  Process `INDEX` terminated with an error exit status (127)\n\nCommand executed: # (2)!\n\n  salmon index --threads 1 -t transcriptome.fa -i index\n\nCommand exit status: # (3)!\n  127\n\nCommand output: # (4)!\n  (empty)\n\nCommand error: # (5)!\n  .command.sh: line 2: salmon: command not found\n\nWork dir: # (6)!\n  /Users/pditommaso/work/0b/b59f362980defd7376ee0a75b41f62\n</code></pre> <ol> <li>Une description de la cause de l'erreur</li> <li>La commande ex\u00e9cut\u00e9e</li> <li>L'\u00e9tat de sortie de la commande</li> <li>La sortie standard de la commande, si elle est disponible</li> <li>L'erreur standard de la commande</li> <li>Le r\u00e9pertoire de travail de la commande</li> </ol> <p>Examinez attentivement toutes les donn\u00e9es d'erreur, car elles peuvent fournir des informations pr\u00e9cieuses pour le d\u00e9bogage.</p> <p>Si cela ne suffit pas, <code>cd</code> dans le r\u00e9pertoire de travail de la t\u00e2che. Il contient tous les fichiers n\u00e9cessaires pour reproduire le probl\u00e8me de mani\u00e8re isol\u00e9e.</p> <p>Le r\u00e9pertoire d'ex\u00e9cution de la t\u00e2che contient ces fichiers :</p> <ul> <li><code>.command.sh</code> : Le script de commande.</li> <li><code>.command.run</code> : La commande envelopp\u00e9e utilis\u00e9e pour ex\u00e9cuter la t\u00e2che.</li> <li><code>.command.out</code> : La sortie standard compl\u00e8te de la t\u00e2che.</li> <li><code>.command.err</code> : L'erreur standard de la t\u00e2che compl\u00e8te.</li> <li><code>.command.log</code> : La sortie de l'ex\u00e9cution du wrapper.</li> <li><code>.command.begin</code> : Fichier sentinelle cr\u00e9\u00e9 d\u00e8s que la t\u00e2che est lanc\u00e9e.</li> <li><code>.exitcode</code> : Un fichier contenant le code de sortie de la t\u00e2che.</li> <li>Fichiers d'entr\u00e9e de la t\u00e2che (liens symboliques)</li> <li>Fichiers de sortie de la t\u00e2che</li> </ul> <p>V\u00e9rifiez que le fichier <code>.command.sh</code> contient la commande attendue et que toutes les variables sont correctement r\u00e9solues.</p> <p>V\u00e9rifiez \u00e9galement l'existence des fichiers <code>.exitcode</code> et <code>.command.begin</code>, qui, s'ils sont absents, sugg\u00e8rent que la t\u00e2che n'a jamais \u00e9t\u00e9 ex\u00e9cut\u00e9e par le sous-syst\u00e8me (par exemple, le planificateur batch). Si le fichier <code>.command.begin</code> existe, la t\u00e2che a \u00e9t\u00e9 lanc\u00e9e mais a probablement \u00e9t\u00e9 interrompue brutalement.</p> <p>Vous pouvez reproduire l'\u00e9chec de l'ex\u00e9cution en utilisant la commande <code>bash .command.run</code> pour v\u00e9rifier la cause de l'erreur.</p>"},{"location":"fr/archive/basic_training/debugging/#ignorer-les-erreurs","title":"Ignorer les erreurs","text":"<p>Dans certains cas, une erreur de processus peut \u00eatre attendue et ne doit pas interrompre l'ex\u00e9cution globale du workflow.</p> <p>Pour g\u00e9rer ce cas d'utilisation, d\u00e9finissez le processus <code>errorStrategy</code> \u00e0 <code>ignore</code> :</p> <pre><code>process FOO {\n    errorStrategy 'ignore'\n\n    script:\n    \"\"\"\n    your_command --this --that\n    \"\"\"\n}\n</code></pre> <p>Si vous souhaitez ignorer toute erreur, vous pouvez d\u00e9finir la m\u00eame directive dans le fichier de configuration comme param\u00e8tre par d\u00e9faut :</p> <pre><code>process.errorStrategy = 'ignore'\n</code></pre>"},{"location":"fr/archive/basic_training/debugging/#basculement-automatique-en-cas-derreur","title":"Basculement automatique en cas d'erreur","text":"<p>Dans de rares cas, les erreurs peuvent \u00eatre caus\u00e9es par des conditions transitoires. Dans ce cas, une strat\u00e9gie efficace consiste \u00e0 r\u00e9ex\u00e9cuter la t\u00e2che d\u00e9faillante.</p> <pre><code>process FOO {\n    errorStrategy 'retry'\n\n    script:\n    \"\"\"\n    your_command --this --that\n    \"\"\"\n}\n</code></pre> <p>En utilisant la strat\u00e9gie d'erreur <code>retry</code>, la t\u00e2che est r\u00e9ex\u00e9cut\u00e9e une seconde fois si elle renvoie un statut de sortie non nul avant d'arr\u00eater l'ex\u00e9cution compl\u00e8te du flux de travail.</p> <p>La directive maxRetries peut \u00eatre utilis\u00e9e pour d\u00e9finir le nombre de tentatives de r\u00e9ex\u00e9cution de la t\u00e2che avant de d\u00e9clarer qu'elle a \u00e9chou\u00e9 avec une condition d'erreur.</p>"},{"location":"fr/archive/basic_training/debugging/#reessayer-avec-backoff","title":"R\u00e9essayer avec backoff","text":"<p>Dans certains cas, les ressources d'ex\u00e9cution requises peuvent \u00eatre temporairement indisponibles (par exemple, en cas de congestion du r\u00e9seau). Dans ce cas, la simple r\u00e9ex\u00e9cution de la m\u00eame t\u00e2che entra\u00eenera probablement une erreur identique. Une nouvelle tentative avec un d\u00e9lai exponentiel backoff permet de mieux r\u00e9cup\u00e9rer ces conditions d'erreur.</p> <pre><code>process FOO {\n    errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n    maxRetries 5\n\n    script:\n    '''\n    your_command --here\n    '''\n}\n</code></pre>"},{"location":"fr/archive/basic_training/debugging/#allocation-dynamique-des-ressources","title":"Allocation dynamique des ressources","text":"<p>Il est tr\u00e8s fr\u00e9quent que diff\u00e9rentes instances d'un m\u00eame processus aient des besoins tr\u00e8s diff\u00e9rents en termes de ressources informatiques. Dans de telles situations, le fait de demander, par exemple, une quantit\u00e9 de m\u00e9moire trop faible entra\u00eenera l'\u00e9chec de certaines t\u00e2ches. Au contraire, l'utilisation d'une limite plus \u00e9lev\u00e9e qui correspond \u00e0 toutes les t\u00e2ches de votre ex\u00e9cution pourrait r\u00e9duire de mani\u00e8re significative la priorit\u00e9 d'ex\u00e9cution de votre travail dans un syst\u00e8me d'ordonnancement.</p> <p>Pour g\u00e9rer ce cas d'utilisation, vous pouvez utiliser une strat\u00e9gie d'erreur <code>retry</code> et augmenter les ressources informatiques allou\u00e9es par la t\u00e2che \u00e0 chaque attempt successif.</p> <pre><code>process FOO {\n    cpus 4\n    memory { 2.GB * task.attempt } // (1)!\n    time { 1.hour * task.attempt } // (2)!\n    errorStrategy { task.exitStatus == 140 ? 'retry' : 'terminate' } // (3)!\n    maxRetries 3 // (4)!\n\n    script:\n    \"\"\"\n    your_command --cpus $task.cpus --mem $task.memory\n    \"\"\"\n}\n</code></pre> <ol> <li>La m\u00e9moire est d\u00e9finie de mani\u00e8re dynamique, la premi\u00e8re tentative \u00e9tant de 2 Go, la seconde de 4 Go, etc.</li> <li>La hauteur du temps d'ex\u00e9cution est \u00e9galement d\u00e9fini de mani\u00e8re dynamique, la premi\u00e8re tentative d'ex\u00e9cution est fix\u00e9e \u00e0 1 heure, la seconde \u00e0 2 heures, etc.</li> <li>Si la t\u00e2che renvoie un statut de sortie \u00e9gal \u00e0 <code>140</code>, la strat\u00e9gie d'erreur sera <code>retry</code>, sinon elle mettra fin \u00e0 l'ex\u00e9cution.</li> <li>Elle r\u00e9essayera l'ex\u00e9cution du processus jusqu'\u00e0 trois fois.</li> </ol>"},{"location":"fr/archive/basic_training/executors/","title":"Executors","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"fr/archive/basic_training/executors/#scenarios-de-deploiement","title":"Sc\u00e9narios de d\u00e9ploiement","text":"<p>Les applications g\u00e9nomiques du monde r\u00e9el peuvent engendrer l'ex\u00e9cution de milliers de t\u00e2ches. Dans ce sc\u00e9nario, un planificateur de lots est g\u00e9n\u00e9ralement utilis\u00e9 pour d\u00e9ployer un workflow dans un cluster informatique, permettant l'ex\u00e9cution de nombreuses t\u00e2ches en parall\u00e8le sur de nombreux n\u0153uds informatiques.</p> <p>Nextflow a un support int\u00e9gr\u00e9 pour les plannificateurs batch les plus couramment utilis\u00e9s, tels que Univa Grid Engine, SLURM et IBM LSF. Consultez la documentation de Nextflow pour obtenir la liste compl\u00e8te des prises en charge plates-formes d'ex\u00e9cution.</p>"},{"location":"fr/archive/basic_training/executors/#deploiement-du-cluster","title":"D\u00e9ploiement du cluster","text":"<p>L'une des principales caract\u00e9ristiques de Nextflow est la capacit\u00e9 de d\u00e9coupler la mise en \u0153uvre du workflow de la plate-forme d'ex\u00e9cution proprement dite. La mise en \u0153uvre d'une couche d'abstraction permet de d\u00e9ployer le workflow r\u00e9sultant sur n'importe quelle plate-forme d'ex\u00e9cution prise en charge par le cadre.</p> <p></p> <p>Pour ex\u00e9cuter votre workflow avec un planificateur batch, modifiez le fichier <code>nextflow.config</code> en sp\u00e9cifiant l'ex\u00e9cuteur cible et les ressources informatiques requises si n\u00e9cessaire. Par exemple, le fichier <code>nextflow.config</code> peut \u00eatre modifi\u00e9 :</p> <pre><code>process.executor = 'slurm'\n</code></pre>"},{"location":"fr/archive/basic_training/executors/#gestion-des-ressources-du-cluster","title":"Gestion des ressources du cluster","text":"<p>Lors de l'utilisation d'un planificateur batch, il est souvent n\u00e9cessaire de sp\u00e9cifier le nombre de ressources (cpus, m\u00e9moire, temps d'ex\u00e9cution, etc.) requises par chaque t\u00e2che.</p> <p>Pour ce faire, il convient d'utiliser les directives de processus suivantes :</p> queue la queue de cluster \u00e0 utiliser pour le calcul cpus le nombre de cpus \u00e0 allouer pour l'ex\u00e9cution d'une t\u00e2che memoire la quantit\u00e9 de m\u00e9moire \u00e0 allouer pour l'ex\u00e9cution d'une t\u00e2che temps la quantit\u00e9 maximale de temps \u00e0 allouer pour l'ex\u00e9cution d'une t\u00e2che disque la quantit\u00e9 de m\u00e9moire disk requise pour l'ex\u00e9cution d'une t\u00e2che"},{"location":"fr/archive/basic_training/executors/#ressources-a-lechelle-du-workflow","title":"Ressources \u00e0 l'\u00e9chelle du workflow","text":"<p>Utilisez le champ d'application <code>process</code> pour d\u00e9finir les besoins en ressources de tous les processus de vos applications de workflow. Par exemple :</p> <pre><code>process {\n    executor = 'slurm'\n    queue = 'short'\n    memory = '10 GB'\n    time = '30 min'\n    cpus = 4\n}\n</code></pre>"},{"location":"fr/archive/basic_training/executors/#soumettre-nextflow-en-tant-que-tache","title":"Soumettre Nextflow en tant que tache","text":"<p>Bien que la commande principale de Nextflow puisse \u00eatre lanc\u00e9e sur le n\u0153ud de connexion / t\u00eate d'un cluster, il faut savoir que le n\u0153ud doit \u00eatre configur\u00e9 pour des commandes qui s'ex\u00e9cutent pendant une longue p\u00e9riode, m\u00eame si les ressources informatiques utilis\u00e9es sont n\u00e9gligeables. Une autre option est de soumettre le processus Nextflow principal en tant que tache sur le cluster.</p> <p>Remarque</p> <p>Cela n\u00e9cessite que la configuration de votre cluster permette de lancer des t\u00e2ches \u00e0 partir des n\u0153uds de travail, car Nextflow soumettra de nouvelles t\u00e2ches et les g\u00e9rera \u00e0 partir d'ici.</p> <p>Par exemple, si votre cluster utilise Slurm comme planificateur de t\u00e2ches, vous pouvez cr\u00e9er un fichier similaire \u00e0 celui ci-dessous :</p> launch_nf.sh<pre><code>#!/bin/bash\n#SBATCH --partition WORK\n#SBATCH --mem 5G\n#SBATCH -c 1\n#SBATCH -t 12:00:00\n\nWORKFLOW=$1\nCONFIG=$2\n\n# Utiliser un environnement conda o\u00f9 vous avez install\u00e9 Nextflow\n# (peut ne pas \u00eatre n\u00e9cessaire si vous l'avez install\u00e9 d'une autre mani\u00e8re)\nconda activate nextflow\n\nnextflow -C ${CONFIG} run ${WORKFLOW}\n</code></pre> <p>Puis soumettez-le avec :</p> <pre><code>sbatch launch_nf.sh /home/my_user/path/my_workflow.nf /home/my_user/path/my_config_file.conf\n</code></pre> <p>Vous trouverez plus de d\u00e9tails sur l'exemple ci-dessus ici. Vous trouverez d'autres conseils pour l'ex\u00e9cution de Nextflow sur HPC dans les articles de blog suivants :</p> <ul> <li>5 astuces Nextflow pour les utilisateurs HPC</li> <li>Cinq astuces suppl\u00e9mentaires pour les utilisateurs de Nextflow sur le HPC</li> </ul>"},{"location":"fr/archive/basic_training/executors/#configurer-le-processus-par-nom","title":"Configurer le processus par nom","text":"<p>Dans les applications r\u00e9elles, des t\u00e2ches diff\u00e9rentes n\u00e9cessitent des ressources informatiques diff\u00e9rentes. Il est possible de d\u00e9finir les ressources pour une t\u00e2che sp\u00e9cifique en utilisant la commande <code>withName:</code> suivie du nom du processus :</p> <pre><code>process {\n    executor = 'slurm'\n    queue = 'short'\n    memory = '10 GB'\n    time = '30 min'\n    cpus = 4\n\n    withName: FOO {\n        cpus = 2\n        memory = '20 GB'\n        queue = 'short'\n    }\n\n    withName: BAR {\n        cpus = 4\n        memory = '32 GB'\n        queue = 'long'\n    }\n}\n</code></pre> <p>Exercice</p> <p>Ex\u00e9cuter le script RNA-Seq (<code>script7.nf</code>) de tout \u00e0 l'heure, mais sp\u00e9cifier que le processus <code>QUANTIFICATION</code> n\u00e9cessite 2 CPUs et 5 GB de m\u00e9moire, dans le fichier <code>nextflow.config</code>.</p> Solution <pre><code>process {\n    withName: QUANTIFICATION {\n        cpus = 2\n        memory = '5 GB'\n    }\n}\n</code></pre>"},{"location":"fr/archive/basic_training/executors/#configurer-le-processus-par-etiquettes","title":"Configurer le processus par \u00e9tiquettes","text":"<p>Lorsqu'une application de workflow est compos\u00e9e de nombreux processus, il peut \u00eatre difficile de dresser la liste de tous les noms de processus et de choisir des ressources pour chacun d'entre eux dans le fichier de configuration.</p> <p>Une meilleure strat\u00e9gie consiste \u00e0 annoter les processus avec une directive label. Sp\u00e9cifiez ensuite les ressources dans le fichier de configuration utilis\u00e9 pour tous les processus ayant le m\u00eame label.</p> <p>Le script du workflow :</p> <pre><code>process TASK1 {\n    label 'long'\n\n    script:\n    \"\"\"\n    first_command --here\n    \"\"\"\n}\n\nprocess TASK2 {\n    label 'short'\n\n    script:\n    \"\"\"\n    second_command --here\n    \"\"\"\n}\n</code></pre> <p>Le fichier de configuration :</p> <pre><code>process {\n    executor = 'slurm'\n\n    withLabel: 'short' {\n        cpus = 4\n        memory = '20 GB'\n        queue = 'alpha'\n    }\n\n    withLabel: 'long' {\n        cpus = 8\n        memory = '32 GB'\n        queue = 'omega'\n    }\n}\n</code></pre>"},{"location":"fr/archive/basic_training/executors/#configurer-plusieurs-containers","title":"Configurer plusieurs containers","text":"<p>Les containers peuvent \u00eatre d\u00e9finis pour chaque processus de votre flux de travail. Vous pouvez d\u00e9finir leurs conteneurs dans un fichier de configuration, comme indiqu\u00e9 ci-dessous :</p> <pre><code>process {\n    withName: FOO {\n        container = 'some/image:x'\n    }\n    withName: BAR {\n        container = 'other/image:y'\n    }\n}\n\ndocker.enabled = true\n</code></pre> <p>Astuce</p> <p>Dois-je utiliser un seul container fat ou plusieurs containers slim ? Les deux approches ont des avantages et des inconv\u00e9nients. Un container unique est plus simple \u00e0 construire et \u00e0 maintenir, mais lorsque vous utilisez de nombreux outils, l'image peut devenir tr\u00e8s volumineuse et les outils peuvent cr\u00e9er des conflits entre eux. L'utilisation d'un container pour chaque processus peut donner lieu \u00e0 de nombreuses images diff\u00e9rentes \u00e0 construire et \u00e0 maintenir, en particulier lorsque les processus de votre workflow utilisent des outils diff\u00e9rents pour chaque t\u00e2che.</p> <p>Pour en savoir plus sur les s\u00e9lecteurs de processus de configuration, consultez ce lien.</p>"},{"location":"fr/archive/basic_training/executors/#configuration-des-profils","title":"Configuration des profils","text":"<p>Les fichiers de configuration peuvent contenir la d\u00e9finition d'un ou plusieurs profils. Un profil est un ensemble d'attributs de configuration qui peuvent \u00eatre activ\u00e9s/choisis lors du lancement de l'ex\u00e9cution d'un workflow en utilisant l'option de ligne de commande <code>-profile</code>.</p> <p>Les profils de configuration sont d\u00e9finis en utilisant la port\u00e9e sp\u00e9ciale <code>profiles</code> qui regroupe les attributs appartenant au m\u00eame profil en utilisant un pr\u00e9fixe commun. Par exemple :</p> <pre><code>profiles {\n    standard {\n        params.genome = '/local/path/ref.fasta'\n        process.executor = 'local'\n    }\n\n    cluster {\n        params.genome = '/data/stared/ref.fasta'\n        process.executor = 'sge'\n        process.queue = 'long'\n        process.memory = '10 GB'\n        process.conda = '/some/path/env.yml'\n    }\n\n    cloud {\n        params.genome = '/data/stared/ref.fasta'\n        process.executor = 'awsbatch'\n        process.container = 'cbcrg/imagex'\n        docker.enabled = true\n    }\n\n}\n</code></pre> <p>Cette configuration d\u00e9finit trois profils diff\u00e9rents : <code>standard</code>, <code>cluster</code> et <code>cloud</code> qui d\u00e9finissent diff\u00e9rentes strat\u00e9gies de configuration de processus en fonction de la plateforme d'ex\u00e9cution cible. Par convention, le profil <code>standard</code> est implicitement utilis\u00e9 lorsqu'aucun autre profil n'est sp\u00e9cifi\u00e9 par l'utilisateur.</p> <p>Pour activer un profil sp\u00e9cifique, utilisez l'option <code>-profile</code> suivie du nom du profil :</p> <pre><code>nextflow run &lt;your script&gt; -profile cluster\n</code></pre> <p>Astuce</p> <p>Il est possible de sp\u00e9cifier deux profils de configuration ou plus en s\u00e9parant les noms des profils par une virgule :</p> <pre><code>nextflow run &lt;your script&gt; -profile standard,cloud\n</code></pre>"},{"location":"fr/archive/basic_training/executors/#deploiement-dans-le-cloud","title":"D\u00e9ploiement dans le cloud","text":"<p>AWS Batch est un service informatique g\u00e9r\u00e9 qui permet l'ex\u00e9cution de charges de travail contain\u00e9ris\u00e9es dans l'infrastructure cloud d'Amazon.</p> <p>Nextflow fournit un support int\u00e9gr\u00e9 pour AWS Batch qui permet le d\u00e9ploiement transparent d'un workflow Nextflow dans le cloud, en d\u00e9chargeant les ex\u00e9cutions de processus en tant que t\u00e2ches Batch.</p> <p>Une fois que l'environnement Batch est configur\u00e9, sp\u00e9cifiez les types d'instances \u00e0 utiliser et le nombre maximum de CPU \u00e0 allouer, vous devez cr\u00e9er un fichier de configuration Nextflow comme celui pr\u00e9sent\u00e9 ci-dessous :</p> <p>Cliquez sur les ic\u00f4nes :material-plus-circle : dans le code pour obtenir des explications.</p> <pre><code>process.executor = 'awsbatch' // (1)!\nprocess.queue = 'nextflow-ci' // (2)!\nprocess.container = 'nextflow/rnaseq-nf:latest' // (3)!\nworkDir = 's3://nextflow-ci/work/' // (4)!\naws.region = 'eu-west-1' // (5)!\naws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws' // (6)!\n</code></pre> <ol> <li>D\u00e9finir AWS Batch comme l'ex\u00e9cuteur pour ex\u00e9cuter les processus dans le workflow</li> <li>Le nom de la file d'attente informatique d\u00e9finie dans l'environnement Batch</li> <li>L'image du container Docker \u00e0 utiliser pour ex\u00e9cuter chaque t\u00e2che</li> <li>Le r\u00e9pertoire de travail du workflow doit \u00eatre un bucket AWS S3.</li> <li>La r\u00e9gion AWS \u00e0 utiliser</li> <li>Chemin d'acc\u00e8s \u00e0 l'outil AWS cli n\u00e9cessaire pour t\u00e9l\u00e9charger des fichiers vers/depuis le container.</li> </ol> <p>Astuce</p> <p>La meilleure pratique consiste \u00e0 conserver ce param\u00e8tre en tant que profil distinct dans le fichier de configuration de votre flux de travail. Cela permet de l'ex\u00e9cuter \u00e0 l'aide d'une simple commande.</p> <pre><code>nextflow run script7.nf -profile amazon\n</code></pre> <p>Les d\u00e9tails complets sur le d\u00e9ploiement par lots d'AWS sont disponibles sur ce lien.</p>"},{"location":"fr/archive/basic_training/executors/#montages-des-volumes","title":"Montages des volumes","text":"<p>Les volumes Elastic Block Storage (EBS) (ou tout autre type de stockage pris en charge) peuvent \u00eatre mont\u00e9s dans le Container de t\u00e2ches \u00e0 l'aide de l'extrait de configuration suivant :</p> <pre><code>aws {\n    batch {\n        volumes = '/some/path'\n    }\n}\n</code></pre> <p>Plusieurs volumes peuvent \u00eatre sp\u00e9cifi\u00e9s en utilisant des chemins d'acc\u00e8s s\u00e9par\u00e9s par des virgules. La syntaxe habituelle de montage de volume de Docker peut \u00eatre utilis\u00e9e pour d\u00e9finir des volumes complexes pour lesquels le chemin du conteneur est diff\u00e9rent du chemin de l'h\u00f4te ou pour sp\u00e9cifier une option de lecture seule :</p> <pre><code>aws {\n    region = 'eu-west-1'\n    batch {\n        volumes = ['/tmp', '/host/path:/mnt/path:ro']\n    }\n}\n</code></pre> <p>Astuce</p> <p>Il s'agit d'une configuration globale qui doit \u00eatre sp\u00e9cifi\u00e9e dans un fichier de configuration Nextflow et qui sera appliqu\u00e9e \u00e0 toutes les ex\u00e9cutions de processus.</p> <p>Avertissement</p> <p>Nextflow s'attend \u00e0 ce que les chemins d'acc\u00e8s soient disponibles. Il ne g\u00e8re pas la mise \u00e0 disposition de volumes EBS ou d'un autre type de stockage.</p>"},{"location":"fr/archive/basic_training/executors/#definition-des-taches-personnalisees","title":"D\u00e9finition des t\u00e2ches personnalis\u00e9es","text":"<p>Nextflow cr\u00e9e automatiquement les Batch definitions de taches n\u00e9cessaires \u00e0 l'ex\u00e9cution de vos processus de workflow. Il n'est donc pas n\u00e9cessaire de les d\u00e9finir avant d'ex\u00e9cuter votre workflow.</p> <p>Cependant, vous pouvez toujours avoir besoin de sp\u00e9cifier une d\u00e9finition de travail personnalis\u00e9e pour permettre un contr\u00f4le fin des param\u00e8tres de configuration d'un travail sp\u00e9cifique (par exemple, pour d\u00e9finir des chemins de montage personnalis\u00e9s ou d'autres param\u00e8tres sp\u00e9ciaux d'un batch de taches).</p> <p>Pour utiliser votre propre d\u00e9finition de travail dans un workflow Nextflow, utilisez-la \u00e0 la place du nom de l'image du conteneur, en la pr\u00e9fixant avec la cha\u00eene <code>job-definition://</code>. Par exemple :</p> <pre><code>process {\n    container = 'job-definition://your-job-definition-name'\n}\n</code></pre>"},{"location":"fr/archive/basic_training/executors/#image-personnalisee","title":"Image personnalis\u00e9e","text":"<p>Comme Nextflow exige que l'outil AWS CLI soit accessible dans l'environnement informatique, une solution courante consiste \u00e0 cr\u00e9er une Amazon Machine Image (AMI) personnalis\u00e9e et \u00e0 l'installer de mani\u00e8re autonome (par exemple \u00e0 l'aide du gestionnaire de paquets Conda).</p> <p>Avertissement</p> <p>Lorsque vous cr\u00e9ez votre AMI personnalis\u00e9e pour AWS Batch, assurez-vous d'utiliser l'AMI Amazon ECS-Optimized Amazon Linux comme image de base.</p> <p>L'extrait suivant montre comment installer AWS CLI avec Miniconda :</p> <pre><code>sudo yum install -y bzip2 wget\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -f -p $HOME/miniconda\n$HOME/miniconda/bin/conda install -c conda-forge -y awscli\nrm Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Remarque</p> <p>L'outil <code>aws</code> sera plac\u00e9 dans un r\u00e9pertoire nomm\u00e9 <code>bin</code> dans le dossier d'installation principal. Les outils ne fonctionneront pas correctement si vous modifiez la structure de ce r\u00e9pertoire apr\u00e8s l'installation.</p> <p>Enfin, sp\u00e9cifiez le chemin complet <code>aws</code> dans le fichier de configuration de Nextflow comme indiqu\u00e9 ci-dessous :</p> <pre><code>aws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws'\n</code></pre>"},{"location":"fr/archive/basic_training/executors/#lancer-le-modele","title":"Lancer le mod\u00e8le","text":"<p>Une autre approche consiste \u00e0 cr\u00e9er une AMI personnalis\u00e9e \u00e0 l'aide d'un mod\u00e8le de lancement qui installe l'outil AWS CLI lors du d\u00e9marrage de l'instance via des donn\u00e9es utilisateur personnalis\u00e9es.</p> <p>Dans le tableau de bord EC2, cr\u00e9ez un mod\u00e8le de lancement en sp\u00e9cifiant le champ de donn\u00e9es de l'utilisateur :</p> <pre><code>MIME-Version: 1.0\nContent-Type: multipart/mixed; boundary=\"//\"\n\n--//\nContent-Type: text/x-shellscript; charset=\"us-ascii\"\n\n##!/bin/sh\n### install required deps\nset -x\nexport PATH=/usr/local/bin:$PATH\nyum install -y jq python27-pip sed wget bzip2\npip install -U boto3\n\n### install awscli\nUSER=/home/ec2-user\nwget -q https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -f -p $USER/miniconda\n$USER/miniconda/bin/conda install -c conda-forge -y awscli\nrm Miniconda3-latest-Linux-x86_64.sh\nchown -R ec2-user:ec2-user $USER/miniconda\n\n--//--\n</code></pre> <p>Cr\u00e9ez ensuite un nouvel environnement informatique dans le tableau de bord Batch et indiquez le mod\u00e8le de lancement nouvellement cr\u00e9\u00e9 dans le champ correspondant.</p>"},{"location":"fr/archive/basic_training/executors/#deploiements-hybrides","title":"D\u00e9ploiements hybrides","text":"<p>Nextflow permet l'utilisation de plusieurs ex\u00e9cuteurs dans la m\u00eame application de workflow. Cette fonctionnalit\u00e9 permet de d\u00e9ployer des charges de travail hybrides dans lesquelles certains travaux sont ex\u00e9cut\u00e9s sur l'ordinateur local ou le cluster de calcul local, et d'autres travaux sont d\u00e9charg\u00e9s sur le service AWS Batch.</p> <p>Pour activer cette fonctionnalit\u00e9, utilisez un ou plusieurs selecteur de processes dans votre fichier de configuration Nextflow.</p> <p>Par exemple, appliquez la configuration AWS Batch uniquement \u00e0 un sous-ensemble de processus dans votre flux de travail. Vous pouvez essayer ce qui suit :</p> <pre><code>process {\n    executor = 'slurm' // (1)!\n    queue = 'short' // (2)!\n\n    withLabel: bigTask {  // (3)!\n        executor = 'awsbatch' // (4)!\n        queue = 'my-batch-queue' // (5)!\n        container = 'my/image:tag' // (6)!\n    }\n}\n\naws {\n    region = 'eu-west-1' // (7)!\n}\n</code></pre> <ol> <li>D\u00e9finir <code>slurm</code> comme ex\u00e9cuteur par d\u00e9faut</li> <li>D\u00e9finir la file d'attente pour le cluster SLURM</li> <li>Mise en place de processus avec l'\u00e9tiquette <code>bigTask</code></li> <li>D\u00e9finir <code>awsbatch</code> comme l'ex\u00e9cuteur pour le(s) processus avec le label <code>bigTask</code>.</li> <li>D\u00e9finir la file d'attente pour le(s) processus avec le label <code>bigTask</code>.</li> <li>D\u00e9finir l'image du container \u00e0 d\u00e9ployer pour le(s) processus avec le label <code>bigTask</code>.</li> <li>D\u00e9finir la r\u00e9gion pour l'ex\u00e9cution par batch</li> </ol>"},{"location":"fr/archive/basic_training/intro/","title":"Intro","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"fr/archive/basic_training/intro/#introduction","title":"Introduction","text":""},{"location":"fr/archive/basic_training/intro/#concepts-de-base","title":"Concepts de base","text":"<p>Nextflow est un moteur d'orchestration de workflow et un langage de domaine sp\u00e9cifique (DSL) qui facilite la redaction dun workflow informatiques \u00e0 forte intensit\u00e9 de donn\u00e9es.</p> <p>Il est con\u00e7u autour de l'id\u00e9e que la plateforme Linux est la lingua franca de la science des donn\u00e9es. Linux fournit de nombreux outils de ligne de commande et de script simples mais puissants qui, lorsqu'ils sont combin\u00e9s, facilitent les manipulations de donn\u00e9es complexes.</p> <p>Nextflow \u00e9tend cette approche en ajoutant la possibilit\u00e9 de d\u00e9finir des interactions complexes entre les programmes et un environnement de calcul parall\u00e8le de haut niveau, bas\u00e9 sur le mod\u00e8le de programmation par flux de donn\u00e9es. Les principales caract\u00e9ristiques de Nextflow sont les suivantes:</p> <ul> <li>Portabilit\u00e9 et reproductibilit\u00e9 du workflow</li> <li>la scalabilit\u00e9 de la parall\u00e9lisation et du d\u00e9ploiement</li> <li>Int\u00e9gration des outils, syst\u00e8mes et normes industrielles existants</li> </ul>"},{"location":"fr/archive/basic_training/intro/#processus-et-canaux","title":"Processus et canaux","text":"<p>En pratique, un workflow Nextflow est constitu\u00e9 par l'assemblage de diff\u00e9rents processus. Chaque <code>processus</code> peut \u00eatre \u00e9crit dans n'importe quel langage de script qui peut \u00eatre ex\u00e9cut\u00e9 par la plateforme Linux (Bash, Perl, Ruby, Python, etc.).</p> <p>Les processus sont ex\u00e9cut\u00e9s ind\u00e9pendamment et sont isol\u00e9s les uns des autres, c'est-\u00e0-dire qu'ils ne partagent pas d'\u00e9tat commun (redigeable). Ils ne peuvent communiquer que par l'interm\u00e9diaire de files d'attente asynchrones FIFO (first-in, first-out), appel\u00e9es <code>canaux</code>.</p> <p>Tout <code>processus</code> peut d\u00e9finir un ou plusieurs <code>canaux</code> comme 'input' et 'output'. L'interaction entre ces processus, et finalement le flux d'ex\u00e9cution du workflow lui-m\u00eame, est implicitement d\u00e9finie par ces d\u00e9clarations <code>input</code> et <code>output</code>.</p>"},{"location":"fr/archive/basic_training/intro/#abstraction-dexecution","title":"Abstraction d'ex\u00e9cution","text":"<p>Alors qu'un <code>processus</code> d\u00e9finit quelle commande ou <code>script</code> doit \u00eatre ex\u00e9cut\u00e9, l'ex\u00e9cuteur d\u00e9termine comment ce <code>script</code> est ex\u00e9cut\u00e9 sur la plateforme cible.</p> <p>Sauf indication contraire, les processus sont ex\u00e9cut\u00e9s sur l'ordinateur local. L'ex\u00e9cuteur local est tr\u00e8s utile pour le d\u00e9veloppement et les tests de workflows, mais pour les workflows de calcul r\u00e9els, une plateforme de calcul haute performance (HPC) ou une plateforme cloud est souvent n\u00e9cessaire.</p> <p>En d'autres termes, Nextflow fournit une abstraction entre la logique fonctionnelle du workflow et le syst\u00e8me d'ex\u00e9cution sous-jacent (ou runtime). Ainsi, il est possible d'\u00e9crire un workflow qui s'ex\u00e9cute de mani\u00e8re transparente sur votre ordinateur, un cluster ou le cloud, sans \u00eatre modifi\u00e9. Il vous suffit de d\u00e9finir la plateforme d'ex\u00e9cution cible dans le fichier de configuration.</p> <p></p>"},{"location":"fr/archive/basic_training/intro/#langage-de-script","title":"Langage de script","text":"<p>Nextflow met en \u0153uvre un DSL d\u00e9claratif qui simplifie l'\u00e9criture de workflows d'analyse de donn\u00e9es complexes en tant qu'extension d'un langage de programmation g\u00e9n\u00e9ral.</p> <p>Cette approche rend Nextflow flexible - il offre les avantages d'un DSL concis pour le traitement des cas d'utilisation r\u00e9currents avec facilit\u00e9 et la flexibilit\u00e9 et la puissance d'un langage de programmation polyvalent pour traiter les cas d'angle dans le m\u00eame environnement informatique. Cela serait difficile \u00e0 mettre en \u0153uvre en utilisant une approche purement d\u00e9clarative.</p> <p>En termes pratiques, le script Nextflow est une extension du [langage de programmation Groovy] (https://groovy-lang.org/) qui, \u00e0 son tour, est un super-ensemble du langage de programmation Java. Groovy peut \u00eatre consid\u00e9r\u00e9 comme \"Python pour Java\", en ce sens qu'il simplifie l'\u00e9criture du code et qu'il est plus accessible.</p>"},{"location":"fr/archive/basic_training/intro/#votre-premier-script","title":"Votre premier script","text":"<p>Ici vous allez ex\u00e9cuter votre premier script Nextflow (<code>hello.nf</code>), que nous allons parcourir ligne par ligne.</p> <p>Dans cet exemple, le script prend une cha\u00eene d'entr\u00e9e (un param\u00e8tre appel\u00e9 <code>params.greeting</code>) et la divise en morceaux de six caract\u00e8res dans le premier processus. Le deuxi\u00e8me processus convertit ensuite les caract\u00e8res en majuscules. Le r\u00e9sultat est finalement affich\u00e9 \u00e0 l'\u00e9cran.</p>"},{"location":"fr/archive/basic_training/intro/#code-nextflow","title":"Code Nextflow","text":"<p>Info</p> <p>Cliquez sur les ic\u00f4nes  dans le code pour obtenir des explications.</p> nf-training/hello.nf<pre><code>#!/usr/bin/env nextflow\n// (1)!\nparams.greeting = 'Hello world!' // (2)!\ngreeting_ch = Channel.of(params.greeting) // (3)!\n\nprocess SPLITLETTERS { // (4)!\n    input: // (5)!\n    val x // (6)!\n\n    output: // (7)!\n    path 'chunk_*' // (8)!\n\n    script: // (9)!\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n} // (10)!\n\nprocess CONVERTTOUPPER { // (11)!\n    input: // (12)!\n    path y // (13)!\n\n    output: // (14)!\n    stdout // (15)!\n\n    script: // (16)!\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]'\n    \"\"\"\n} // (17)!\n\nworkflow { // (18)!\n    letters_ch = SPLITLETTERS(greeting_ch) // (19)!\n    results_ch = CONVERTTOUPPER(letters_ch.flatten()) // (20)!\n    results_ch.view { it } // (21)!\n} // (22)!\n</code></pre> <ol> <li>Le code commence par un shebang, qui d\u00e9clare Nextflow comme interpr\u00e8te.</li> <li>D\u00e9clare un param\u00e8tre <code>greeting</code> qui est initialis\u00e9 avec la valeur 'Hello world!'.</li> <li>Initialise un <code>canal</code> nomm\u00e9 <code>greeting_ch</code>, qui contient la valeur de <code>params.greeting</code>. Les canaux sont le type d'entr\u00e9e pour les processus dans Nextflow.</li> <li>Commence le premier bloc de processus, d\u00e9fini comme <code>SPLITLETTERS</code>.</li> <li>D\u00e9claration d'entr\u00e9e pour le processus <code>SPLITLETTERS</code>. Les entr\u00e9es peuvent \u00eatre des valeurs (<code>val</code>), des fichiers ou des chemins (<code>path</code>), ou d'autres qualificatifs (voir ici).</li> <li>Indique au processus d'attendre une valeur d'entr\u00e9e (<code>val</code>), que nous assignons \u00e0 la variable 'x'.</li> <li>D\u00e9claration de sortie pour le processus <code>SPLITLETTERS</code>.</li> <li>Indique au processus d'attendre un ou plusieurs fichiers de sortie (<code>path</code>), avec un nom de fichier commen\u00e7ant par 'chunk_', en tant que sortie du script. Le processus envoie la sortie sous forme de canal.</li> <li>Trois guillemets doubles commencent et terminent le bloc de code pour ex\u00e9cuter ce <code>processus</code>.    Le code \u00e0 ex\u00e9cuter se trouve \u00e0 l'int\u00e9rieur : il s'agit d'imprimer la valeur d<code>input</code> x (appel\u00e9e \u00e0 l'aide du pr\u00e9fixe dollar [$]), de diviser la cha\u00eene en morceaux de 6 caract\u00e8res (\"Hello\" et \"world !\") et de sauvegarder chacun d'eux dans un fichier (chunk_aa et chunk_ab).</li> <li>Fin du premier bloc de processus.</li> <li>Commence le deuxi\u00e8me bloc de processus, d\u00e9fini comme <code>CONVERTTOUPPER</code>.</li> <li>D\u00e9claration d'entr\u00e9e pour le <code>processus</code> <code>CONVERTTOUPPER</code>.</li> <li>Indique au <code>processus</code> d'attendre un ou plusieurs fichiers <code>input</code> (<code>path</code> ; i.e. chunk_aa et chunk_ab), que nous assignons \u00e0 la variable 'y'.</li> <li>D\u00e9claration de sortie pour le processus <code>CONVERTTOUPPER</code>.</li> <li>Indique au processus de s'attendre \u00e0 une sortie standard (stdout) et envoie cette sortie en tant que canal.</li> <li>Trois guillemets doubles commencent et terminent le bloc de code pour ex\u00e9cuter ce <code>process</code>.     Le bloc contient un script qui lit les fichiers (cat) en utilisant la variable d'entr\u00e9e \"$y\", puis convertit les caract\u00e8res en majuscules et les envoie vers la sortie standard.</li> <li>Fin du deuxi\u00e8me bloc <code>process</code>.</li> <li>D\u00e9but de la port\u00e9e du workflow o\u00f9 chaque processus peut \u00eatre appel\u00e9.</li> <li>Ex\u00e9cutez le <code>processus</code> <code>SPLITLETTERS</code> sur le <code>greeting_ch</code> (aka greeting channel), et stockez la sortie dans le canal <code>letters_ch</code>.</li> <li>Ex\u00e9cutez le <code>processus</code> <code>CONVERTTOUPPER</code> sur le canal de lettres <code>letters_ch</code>, qui est aplati en utilisant l'op\u00e9rateur <code>.flatten()</code>. Cela transforme le canal d'entr\u00e9e de telle sorte que chaque \u00e9l\u00e9ment est un \u00e9l\u00e9ment s\u00e9par\u00e9. Nous stockons la sortie dans le canal <code>results_ch</code>.</li> <li>La sortie finale (dans le canal <code>results_ch</code>) est imprim\u00e9e \u00e0 l'\u00e9cran en utilisant l'op\u00e9rateur <code>view</code> (ajout\u00e9 au nom du canal).</li> <li>Fin de l'\u00e9tendue du workflow.</li> </ol> <p>L'utilisation de l'op\u00e9rateur <code>.flatten()</code> ici permet de diviser les deux fichiers en deux \u00e9l\u00e9ments distincts qui seront trait\u00e9s par le processus suivant (sinon, ils seraient trait\u00e9s comme un seul \u00e9l\u00e9ment).</p>"},{"location":"fr/archive/basic_training/intro/#dans-la-pratique","title":"Dans la pratique","text":"<p>Copiez maintenant l'exemple ci-dessus dans votre \u00e9diteur de texte favori et enregistrez-le dans un fichier nomm\u00e9 <code>hello.nf</code>.</p> <p>Warning</p> <p>Pour le tutoriel Gitpod, assurez-vous d'\u00eatre dans le dossier appel\u00e9 <code>nf-training</code></p> <p>Ex\u00e9cutez le script en entrant la commande suivante dans votre terminal :</p> <pre><code>nextflow run hello.nf\n</code></pre> <p>Le r\u00e9sultat ressemblera au texte ci-dessous :</p> <pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `hello.nf` [cheeky_keller] DSL2 - revision: 197a0e289a\nexecutor &gt;  local (3)\n[31/52c31e] process &gt; SPLITLETTERS (1)   [100%] 1 of 1 \u2714\n[37/b9332f] process &gt; CONVERTTOUPPER (2) [100%] 2 of 2 \u2714\nHELLO\nWORLD!\n</code></pre> <p>La sortie standard affiche (ligne par ligne) :</p> <ol> <li>La version de Nextflow qui a \u00e9t\u00e9 ex\u00e9cut\u00e9e.</li> <li>Les noms du script et de la version.</li> <li>L'ex\u00e9cuteur utilis\u00e9 (dans le cas ci-dessus : local).</li> <li>Le premier <code>processus</code> est ex\u00e9cut\u00e9 une fois, ce qui signifie qu'il y a une t\u00e2che. La ligne commence par une valeur hexad\u00e9cimale unique (voir TIP ci-dessous), et se termine par le pourcentage et d'autres informations sur l'ach\u00e8vement de la t\u00e2che.</li> <li>Le deuxi\u00e8me processus est ex\u00e9cut\u00e9 deux fois (une fois pour chunk_aa et une fois pour chunk_ab), ce qui signifie qu'il y a deux t\u00e2ches.</li> <li>La cha\u00eene de r\u00e9sultats de stdout est imprim\u00e9e.</li> </ol> <p>Info</p> <p>Les nombres hexad\u00e9cimaux, comme <code>31/52c31e</code>, identifient l'ex\u00e9cution unique du processus, que nous appelons une t\u00e2che. Ces nombres sont \u00e9galement le pr\u00e9fixe des r\u00e9pertoires o\u00f9 chaque t\u00e2che est ex\u00e9cut\u00e9e. Vous pouvez inspecter les fichiers produits en allant dans le r\u00e9pertoire <code>$PWD/work</code> et en utilisant ces num\u00e9ros pour trouver le chemin d'ex\u00e9cution sp\u00e9cifique \u00e0 la t\u00e2che.</p> <p>Tip</p> <p>Le second processus s'ex\u00e9cute deux fois, dans deux r\u00e9pertoires de travail diff\u00e9rents pour chaque fichier d'entr\u00e9e. La sortie du journal ANSI de Nextflow se rafra\u00eechit dynamiquement au fur et \u00e0 mesure que le workflow s'ex\u00e9cute ; dans l'exemple pr\u00e9c\u00e9dent, le r\u00e9pertoire de travail <code>[37/b9332f]</code> est le second des deux r\u00e9pertoires qui ont \u00e9t\u00e9 trait\u00e9s (en \u00e9crasant le journal avec le premier). Pour imprimer tous les chemins pertinents \u00e0 l'\u00e9cran, d\u00e9sactivez la sortie du journal ANSI en utilisant l'option <code>-ansi-log</code> (par exemple, <code>nextflow run hello.nf -ansi-log false</code>).</p> <p>Il faut noter que le processus <code>CONVERTTOUPPER</code> est ex\u00e9cut\u00e9 en parall\u00e8le, donc il n'y a aucune garantie que l'instance qui traite le premier split (le chunk Hello ') sera ex\u00e9cut\u00e9e avant celle qui traite le second split (le chunk 'world!).</p> <p>Il se peut donc que le r\u00e9sultat final soit imprim\u00e9 dans un ordre diff\u00e9rent :</p> <pre><code>WORLD!\nHELLO\n</code></pre>"},{"location":"fr/archive/basic_training/intro/#modifier-et-reprendre","title":"Modifier et reprendre","text":"<p>Nextflow garde la trace de tous les processus ex\u00e9cut\u00e9s dans votre workflow. Si vous modifiez certaines parties de votre script, seuls les processus modifi\u00e9s seront r\u00e9-ex\u00e9cut\u00e9s. L'ex\u00e9cution des processus qui n'ont pas \u00e9t\u00e9 modifi\u00e9s sera ignor\u00e9e et le r\u00e9sultat mis en cache sera utilis\u00e9 \u00e0 la place.</p> <p>Cela permet de tester ou de modifier une partie de votre flux de travail sans avoir \u00e0 le r\u00e9ex\u00e9cuter depuis le d\u00e9but.</p> <p>Pour les besoins de ce tutoriel, modifiez le processus <code>CONVERTTOUPPER</code> de l'exemple pr\u00e9c\u00e9dent, en rempla\u00e7ant le script du processus par la cha\u00eene <code>rev $y</code>, de fa\u00e7on \u00e0 ce que le processus ressemble \u00e0 ceci :</p> <pre><code>process CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    rev $y\n    \"\"\"\n}\n</code></pre> <p>Enregistrez ensuite le fichier sous le m\u00eame nom et ex\u00e9cutez-le en ajoutant l'option <code>-resume</code> \u00e0 la ligne de commande :</p> <pre><code>$ nextflow run hello.nf -resume\n\nN E X T F L O W  ~  version 23.04.1\nLaunching `hello.nf` [zen_colden] DSL2 - revision: 0676c711e8\nexecutor &gt;  local (2)\n[31/52c31e] process &gt; SPLITLETTERS (1)   [100%] 1 of 1, cached: 1 \u2714\n[0f/8175a7] process &gt; CONVERTTOUPPER (1) [100%] 2 of 2 \u2714\n!dlrow\n olleH\n</code></pre> <p>Vous verrez que l'ex\u00e9cution du processus <code>SPLITLETTERS</code> est ignor\u00e9e (l'ID de la t\u00e2che est le m\u00eame que dans la premi\u00e8re sortie) \u2014 ses r\u00e9sultats sont r\u00e9cup\u00e9r\u00e9s dans le cache. Le second processus est ex\u00e9cut\u00e9 comme pr\u00e9vu, imprimant les cha\u00eenes invers\u00e9es.</p> <p>Info</p> <p>Les r\u00e9sultats du workflow sont mis en cache par d\u00e9faut dans le r\u00e9pertoire <code>$PWD/work</code>. En fonction de votre script, ce dossier peut prendre beaucoup d'espace disque. Si vous \u00eates s\u00fbr de ne pas avoir besoin de reprendre l'ex\u00e9cution de votre workflow, nettoyez ce dossier p\u00e9riodiquement.</p>"},{"location":"fr/archive/basic_training/intro/#parametres-du-workflow","title":"Param\u00e8tres du workflow","text":"<p>Les param\u00e8tres de workflow sont simplement d\u00e9clar\u00e9s en ajoutant le pr\u00e9fixe <code>params</code> au nom d'une variable, s\u00e9par\u00e9 par un caract\u00e8re point. Leur valeur peut \u00eatre sp\u00e9cifi\u00e9e sur la ligne de commande en faisant pr\u00e9c\u00e9der le nom du param\u00e8tre d'un double tiret, c'est-\u00e0-dire <code>--paramName</code>.</p> <p>Essayons maintenant d'ex\u00e9cuter l'exemple pr\u00e9c\u00e9dent en sp\u00e9cifiant un param\u00e8tre de cha\u00eene d'entr\u00e9e diff\u00e9rent, comme indiqu\u00e9 ci-dessous :</p> <pre><code>nextflow run hello.nf --greeting 'Bonjour le monde!'\n</code></pre> <p>La cha\u00eene sp\u00e9cifi\u00e9e sur la ligne de commande remplacera la valeur par d\u00e9faut du param\u00e8tre. La sortie ressemblera \u00e0 ceci :</p> <pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `hello.nf` [goofy_kare] DSL2 - revision: 0676c711e8\nexecutor &gt;  local (4)\n[8b/7c7d13] process &gt; SPLITLETTERS (1)   [100%] 1 of 1 \u2714\n[58/3b2df0] process &gt; CONVERTTOUPPER (3) [100%] 3 of 3 \u2714\nuojnoB\nm el r\n!edno\n</code></pre>"},{"location":"fr/archive/basic_training/intro/#au-format-dag","title":"Au format DAG","text":"<p>Pour mieux comprendre comment Nextflow traite les donn\u00e9es dans ce workflow, voici une figure de type DAG pour visualiser toutes les <code>inputs</code>, <code>outputs</code>, <code>canaux</code> et <code>processes</code> :</p> <p></p>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/","title":"Rnaseq pipeline","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#workflow-simple-de-rna-seq","title":"Workflow simple de RNA-Seq","text":"<p>Pour d\u00e9montrer un sc\u00e9nario biom\u00e9dical r\u00e9el, nous mettrons en \u0153uvre une preuve de concept de flux de travail RNA-Seq qui :</p> <ol> <li>Indexer un fichier de transcriptome</li> <li>Effectuer des contr\u00f4les de qualit\u00e9</li> <li>Effectuer la quantification</li> <li>Cr\u00e9er un rapport MultiQC</li> </ol> <p>Pour ce faire, nous utiliserons une s\u00e9rie de sept scripts, chacun d'entre eux s'appuyant sur le pr\u00e9c\u00e9dent pour cr\u00e9er un flux de travail complet. Vous pouvez les trouver dans le dossier du didacticiel (<code>script1.nf</code> - <code>script7.nf</code>). Ces scripts utiliseront des outils tiers connus des bioinformaticiens mais qui peuvent \u00eatre nouveaux pour vous, nous les pr\u00e9senterons donc bri\u00e8vement ci-dessous.</p> <ol> <li>Salmon est un outil permettant de quantifier les mol\u00e9cules connues sous le nom de transcrits \u00e0 l'aide d'un type de donn\u00e9es appel\u00e9 donn\u00e9es RNA-seq.</li> <li>FastQC est un outil permettant d'effectuer un contr\u00f4le de la qualit\u00e9 des donn\u00e9es de s\u00e9quences \u00e0 haut d\u00e9bit. Vous pouvez le consid\u00e9rer comme un moyen d'\u00e9valuer la qualit\u00e9 de vos donn\u00e9es.</li> <li>MultiQC recherche les journaux d'analyse dans un r\u00e9pertoire donn\u00e9 et compile un rapport HTML. Il s'agit d'un outil d'usage g\u00e9n\u00e9ral, parfait pour r\u00e9sumer les r\u00e9sultats de nombreux outils bioinformatiques.</li> </ol> <p>M\u00eame si ces outils ne sont pas ceux que vous utiliserez dans votre pipeline, ils peuvent \u00eatre remplac\u00e9s par n'importe quel outil courant de votre secteur. C'est ca la puissance de Nextflow !</p>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#definir-les-parametres-du-workflow","title":"D\u00e9finir les param\u00e8tres du workflow","text":"<p>Les param\u00e8tres sont des entr\u00e9es et des options qui peuvent \u00eatre modifi\u00e9es lors de l'ex\u00e9cution du flux de travail.</p> <p>Le script <code>script1.nf</code> d\u00e9finit les param\u00e8tres d'entr\u00e9e du workflow.</p> <pre><code>params.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\n\nprintln \"reads: $params.reads\"\n</code></pre> <p>Ex\u00e9cutez-le en utilisant la commande suivante :</p> <pre><code>nextflow run script1.nf\n</code></pre> <p>Essayez de sp\u00e9cifier un param\u00e8tre d'entr\u00e9e diff\u00e9rent dans votre commande d'ex\u00e9cution, par exemple :</p> <pre><code>nextflow run script1.nf --reads '/workspaces/training/nf-training/data/ggal/lung_{1,2}.fq'\n</code></pre>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#exercices","title":"Exercices","text":"<p>Exercise</p> <p>Modifiez le fichier <code>script1.nf</code> en ajoutant un quatri\u00e8me param\u00e8tre nomm\u00e9 <code>outdir</code> et d\u00e9finissez-le comme chemin par d\u00e9faut qui sera utilis\u00e9 comme r\u00e9pertoire de sortie du workflow.</p> Solution <pre><code>params.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\nparams.outdir = \"results\"\n</code></pre> <p>Exercise</p> <p>Modifier <code>script1.nf</code> pour imprimer tous les param\u00e8tres du workflow en utilisant une seule commande <code>log.info</code> sous la forme d'une cha\u00eene multiligne.</p> <p> regarde l'example ici.</p> Solution <p>Ajoutez ce qui suit \u00e0 votre fichier script:</p> <pre><code>log.info \"\"\"\\\n    R N A S E Q - N F   P I P E L I N E\n    ===================================\n    transcriptome: ${params.transcriptome_file}\n    reads        : ${params.reads}\n    outdir       : ${params.outdir}\n    \"\"\"\n    .stripIndent(true)\n</code></pre>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#resume","title":"R\u00e9sum\u00e9","text":"<p>Au cours de cette \u00e9tape, vous avez appris:</p> <ol> <li>Comment d\u00e9finir les param\u00e8tres dans votre script de workflow</li> <li>Comment passer des param\u00e8tres en utilisant la ligne de commande</li> <li>L'utilisation des variables <code>$var</code> et <code>${var}</code>.</li> <li>Comment utiliser des cha\u00eenes de caract\u00e8res multilignes</li> <li>Comment utiliser <code>log.info</code> pour imprimer des informations et les sauvegarder dans le fichier d'ex\u00e9cution du journal.</li> </ol>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#creer-un-fichier-dindex-du-transcriptome","title":"Cr\u00e9er un fichier d'index du transcriptome","text":"<p>Nextflow permet l'ex\u00e9cution de n'importe quelle commande ou script en utilisant une d\u00e9finition <code>processus</code>.</p> <p>Un \"processus\" est d\u00e9fini par trois d\u00e9clarations principales : le processus <code>input</code>, <code>output</code> et la commande <code>script</code>.</p> <p>Pour ajouter une \u00e9tape de traitement <code>INDEX</code> du transcriptome, essayez d'ajouter les blocs de code suivants \u00e0 votre <code>script1.nf</code>. Alternativement, ces blocs de code ont d\u00e9j\u00e0 \u00e9t\u00e9 ajout\u00e9s \u00e0 <code>script2.nf</code>.</p> <pre><code>/*\n * d\u00e9finir le processus INDEX qui cr\u00e9e un index binaire\n * compte tenu du fichier de transcriptome\n */\nprocess INDEX {\n    input:\n    path transcriptome\n\n    output:\n    path 'salmon_index'\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_index\n    \"\"\"\n}\n</code></pre> <p>En outre, ajoutez un champ d'application de workflow contenant une d\u00e9finition de canal d'entr\u00e9e et le processus d'indexation :</p> <pre><code>workflow {\n    index_ch = INDEX(params.transcriptome_file)\n}\n</code></pre> <p>Ici, le param\u00e8tre <code>params.transcriptome_file</code> est utilis\u00e9 comme entr\u00e9e pour le processus <code>INDEX</code>. Le processus <code>INDEX</code> (utilisant l'outil <code>salmon</code>) cr\u00e9e <code>salmon_index</code>, un transcriptome index\u00e9 qui est transmis en sortie au canal <code>index_ch</code>.</p> <p>Info</p> <p>La d\u00e9claration <code>input</code> d\u00e9finit une variable de chemin <code>transcriptome</code> qui est utilis\u00e9e dans le <code>script</code> comme r\u00e9f\u00e9rence (en utilisant le symbole du dollar) dans la ligne de commande Salmon.</p> <p>Warning</p> <p>Les besoins en ressources tels que les CPUs et les limites de m\u00e9moire peuvent changer avec les diff\u00e9rentes ex\u00e9cutions de workflow et les plateformes. Nextflow peut utiliser <code>$task.cpus</code> comme variable pour le nombre de CPU. Voir process directives documentation pour plus de d\u00e9tails.</p> <p>Ex\u00e9cutez-le en utilisant la commande :</p> <pre><code>nextflow run script2.nf\n</code></pre> <p>L'ex\u00e9cution \u00e9chouera car <code>salmon</code> n'est pas install\u00e9 dans votre environnement.</p> <p>Ajoutez l'option de ligne de commande <code>-with-docker</code> pour lancer l'ex\u00e9cution \u00e0 travers un conteneur Docker, comme indiqu\u00e9 ci-dessous :</p> <pre><code>nextflow run script2.nf -with-docker\n</code></pre> <p>Cette fois l'ex\u00e9cution fonctionnera car elle utilise le conteneur Docker <code>nextflow/rnaseq-nf</code> qui est d\u00e9fini dans le fichier <code>nextflow.config</code> dans votre r\u00e9pertoire courant. Si vous ex\u00e9cutez ce script localement, vous devrez t\u00e9l\u00e9charger Docker sur votre machine, vous connecter et activer Docker, et autoriser le script \u00e0 t\u00e9l\u00e9charger le conteneur contenant les scripts d'ex\u00e9cution. Vous pouvez en savoir plus sur Docker ici.</p> <p>Pour \u00e9viter d'ajouter <code>-with-docker</code> \u00e0 chaque fois que vous ex\u00e9cutez le script, ajoutez la ligne suivante au fichier <code>nextflow.config</code> :</p> <pre><code>docker.enabled = true\n</code></pre>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#exercices_1","title":"Exercices","text":"<p>Exercise</p> <p>Activez l'ex\u00e9cution Docker par d\u00e9faut en ajoutant le param\u00e8tre ci-dessus dans le fichier <code>nextflow.config</code>.</p> <p>Exercise</p> <p>Imprimer la sortie du canal <code>index_ch</code> en utilisant l'op\u00e9rateur view.</p> Solution <p>Ajoutez ce qui suit \u00e0 la fin de votre bloc de workflow dans votre fichier script</p> <pre><code>index_ch.view()\n</code></pre> <p>Exercise</p> <p>Si vous disposez de plus d'unit\u00e9s centrales, essayez de modifier votre script pour demander plus de ressources pour ce processus. Par exemple, voir la directive docs. <code>$task.cpus</code> est d\u00e9j\u00e0 sp\u00e9cifi\u00e9 dans ce script, donc d\u00e9finir le nombre de CPUs comme une directive indiquera \u00e0 Nextflow comment ex\u00e9cuter ce processus, en termes de nombre de CPUs.</p> Solution <p>Ajouter <code>cpus 2</code> au d\u00e9but du processus d'indexation :</p> <pre><code>process INDEX {\n    cpus 2\n\n    input:\n    ...\n</code></pre> <p>V\u00e9rifiez ensuite qu'il a fonctionn\u00e9 en regardant le script ex\u00e9cut\u00e9 dans le r\u00e9pertoire work. Cherchez l'hexad\u00e9cimal (par exemple <code>work/7f/f285b80022d9f61e82cd7f90436aa4/</code>), puis <code>cat</code> le fichier <code>.command.sh</code>.</p> <p>Exercice Bonus</p> <p>Utilisez la commande <code>tree work</code> pour voir comment Nextflow organise le r\u00e9pertoire work du processus. V\u00e9rifiez ici si vous avez besoin de t\u00e9l\u00e9charger <code>tree</code>.</p> Solution <p>Il devrait ressembler \u00e0 ceci :</p> <pre><code>work\n\u251c\u2500\u2500 17\n\u2502   \u2514\u2500\u2500 263d3517b457de4525513ae5e34ea8\n\u2502       \u251c\u2500\u2500 index\n\u2502       \u2502   \u251c\u2500\u2500 complete_ref_lens.bin\n\u2502       \u2502   \u251c\u2500\u2500 ctable.bin\n\u2502       \u2502   \u251c\u2500\u2500 ctg_offsets.bin\n\u2502       \u2502   \u251c\u2500\u2500 duplicate_clusters.tsv\n\u2502       \u2502   \u251c\u2500\u2500 eqtable.bin\n\u2502       \u2502   \u251c\u2500\u2500 info.json\n\u2502       \u2502   \u251c\u2500\u2500 mphf.bin\n\u2502       \u2502   \u251c\u2500\u2500 pos.bin\n\u2502       \u2502   \u251c\u2500\u2500 pre_indexing.log\n\u2502       \u2502   \u251c\u2500\u2500 rank.bin\n\u2502       \u2502   \u251c\u2500\u2500 refAccumLengths.bin\n\u2502       \u2502   \u251c\u2500\u2500 ref_indexing.log\n\u2502       \u2502   \u251c\u2500\u2500 reflengths.bin\n\u2502       \u2502   \u251c\u2500\u2500 refseq.bin\n\u2502       \u2502   \u251c\u2500\u2500 seq.bin\n\u2502       \u2502   \u2514\u2500\u2500 versionInfo.json\n\u2502       \u2514\u2500\u2500 transcriptome.fa -&gt; /workspaces/training/data/ggal/transcriptome.fa\n\u251c\u2500\u2500 7f\n</code></pre>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#resume_1","title":"R\u00e9sum\u00e9","text":"<p>Dans cette \u00e9tape, vous avez appris</p> <ol> <li>Comment d\u00e9finir un processus ex\u00e9cutant une commande personnalis\u00e9e</li> <li>Comment d\u00e9clarer les entr\u00e9es d'un processus</li> <li>Comment d\u00e9clarer les sorties d'un processus</li> <li>Comment imprimer le contenu d'un canal</li> <li>Comment acc\u00e9der au nombre de CPU disponibles</li> </ol>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#collecter-les-fichiers-lus-par-paires","title":"Collecter les fichiers lus par paires","text":"<p>Cette \u00e9tape montre comment faire correspondre les fichiers read par paires, afin qu'ils puissent \u00eatre mis en correspondance par Salmon.</p> <p>Modifiez le script <code>script3.nf</code> en ajoutant la d\u00e9claration suivante \u00e0 la derni\u00e8re ligne du fichier :</p> <pre><code>read_pairs_ch.view()\n</code></pre> <p>Sauvegardez-le et ex\u00e9cutez-le avec la commande suivante :</p> <pre><code>nextflow run script3.nf\n</code></pre> <p>Il s'affichera quelque chose de similaire \u00e0 ceci :</p> <pre><code>[gut, [/.../data/ggal/gut_1.fq, /.../data/ggal/gut_2.fq]]\n</code></pre> <p>L'exemple ci-dessus montre comment le canal <code>read_pairs_ch</code> \u00e9met des tuples compos\u00e9s de deux \u00e9l\u00e9ments, le premier \u00e9tant le pr\u00e9fixe de la paire lue et le second une liste repr\u00e9sentant les fichiers actuels.</p> <p>Essayez \u00e0 nouveau en sp\u00e9cifiant diff\u00e9rents fichiers de lecture \u00e0 l'aide d'un motif global :</p> <pre><code>nextflow run script3.nf --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>Warning</p> <p>Les chemins d'acc\u00e8s aux fichiers qui incluent un ou plusieurs jokers, c'est-\u00e0-dire <code>*</code>, <code>?</code>, etc., DOIVENT \u00eatre entour\u00e9s de caract\u00e8res entre guillemets simples afin d'\u00e9viter que Bash n'\u00e9tende le glob.</p>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#exercices_2","title":"Exercices","text":"<p>Exercise</p> <p>Utilisez l'op\u00e9rateur set \u00e0 la place de l'affectation <code>=</code> pour d\u00e9finir le canal <code>read_pairs_ch</code>.</p> Solution <pre><code>Channel\n    .fromFilePairs(params.reads)\n    .set { read_pairs_ch }\n</code></pre> <p>Exercise</p> <p>Utilisez l'option <code>checkIfExists</code> pour la fabrique de canaux fromFilePairs pour v\u00e9rifier si le chemin sp\u00e9cifi\u00e9 contient des paires de fichiers.</p> Solution <pre><code>Channel\n    .fromFilePairs(params.reads, checkIfExists: true)\n    .set { read_pairs_ch }\n</code></pre>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#resume_2","title":"r\u00e9sum\u00e9","text":"<p>Au cours de cette \u00e9tape, vous avez appris:</p> <ol> <li>Comment utiliser <code>fromFilePairs</code> pour g\u00e9rer les fichiers de paires de lecture</li> <li>Comment utiliser l'option <code>checkIfExists</code> pour v\u00e9rifier l'existence des fichiers d'entr\u00e9e</li> <li>Comment utiliser l'op\u00e9rateur <code>set</code> pour d\u00e9finir une nouvelle variable de canal.</li> </ol> <p>Info</p> <p>La d\u00e9claration d'un canal peut se situer avant le champ d'application du workflow ou \u00e0 l'int\u00e9rieur de celui-ci. Tant qu'elle se situe en amont du processus qui requiert le canal sp\u00e9cifique.</p>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#quantification-de-lexpression","title":"Quantification de l'expression","text":"<p>Le fichier <code>script4.nf</code> ajoute un processus de <code>QUANTIFICATION</code> de l'expression des g\u00e8nes et un appel \u00e0 ce processus dans le champ d'application du flux de travail. La quantification n\u00e9cessite les fichiers fastq du transcriptome index\u00e9 et de la paire reads de RNA-Seq.</p> <p>Dans le workflow scope, notez comment le canal <code>index_ch</code> est assign\u00e9 comme sortie dans le processus <code>INDEX</code>.</p> <p>Ensuite, notez que le premier canal d'entr\u00e9e pour le processus <code>QUANTIFICATION</code> est le <code>index_ch</code> pr\u00e9c\u00e9demment d\u00e9clar\u00e9, qui contient le <code>chemin</code> vers le <code>salmon_index</code>.</p> <p>Notez \u00e9galement que le second canal d'entr\u00e9e pour le processus de <code>QUANTIFICATION</code> est le <code>read_pair_ch</code> que nous venons de cr\u00e9er. C'est un <code>tuple</code> compos\u00e9 de deux \u00e9l\u00e9ments (une valeur : <code>sample_id</code> et une liste de chemins vers les lectures fastq : <code>reads</code>) afin de correspondre \u00e0 la structure des \u00e9l\u00e9ments \u00e9mis par la fabrique de canaux <code>fromFilePairs</code>.</p> <p>Ex\u00e9cutez-la en utilisant la commande suivante :</p> <pre><code>nextflow run script4.nf -resume\n</code></pre> <p>Vous verrez l'ex\u00e9cution du processus <code>QUANTIFICATION</code>.</p> <p>Lors de l'utilisation de l'option <code>-resume</code>, toute \u00e9tape qui a d\u00e9j\u00e0 \u00e9t\u00e9 trait\u00e9e est saut\u00e9e.</p> <p>Essayez d'ex\u00e9cuter \u00e0 nouveau le m\u00eame script avec davantage de fichiers lus, comme indiqu\u00e9 ci-dessous :</p> <pre><code>nextflow run script4.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>Vous remarquerez que le processus <code>QUANTIFICATION</code> est ex\u00e9cut\u00e9 plusieurs fois.</p> <p>Nextflow parall\u00e9lise l'ex\u00e9cution de votre workflow en fournissant simplement plusieurs jeux de donn\u00e9es d'entr\u00e9e \u00e0 votre script.</p> <p>Tip</p> <p>Il peut \u00eatre utile d'appliquer des param\u00e8tres facultatifs \u00e0 un processus sp\u00e9cifique \u00e0 l'aide de directives en les sp\u00e9cifiant dans le corps du processus.</p>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#exercices_3","title":"Exercices","text":"<p>Exercise</p> <p>Ajout d'une directive tag au processus <code>QUANTIFICATION</code> pour fournir un journal d'ex\u00e9cution plus lisible.</p> Solution <p>Ajoutez ce qui suit avant la d\u00e9claration d'entr\u00e9e :</p> <pre><code>tag \"Salmon on $sample_id\"\n</code></pre> <p>Exercise</p> <p>Ajoutez une directive publishDir au processus <code>QUANTIFICATION</code> pour stocker les r\u00e9sultats du processus dans un r\u00e9pertoire de votre choix.</p> Solution <p>Ajoutez ce qui suit avant la d\u00e9claration <code>input</code> dans le processus <code>QUANTIFICATION</code> :</p> <pre><code>publishDir params.outdir, mode: 'copy'\n</code></pre>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#resume_3","title":"R\u00e9sum\u00e9","text":"<p>Dans cette \u00e9tape, vous avez appris</p> <ol> <li>Comment connecter deux processus ensemble en utilisant les d\u00e9clarations de canal</li> <li>Comment reprendre l'ex\u00e9cution du script et sauter les \u00e9tapes mises en cache</li> <li>Comment utiliser la directive <code>tag</code> pour fournir une sortie d'ex\u00e9cution plus lisible</li> <li>Comment utiliser la directive <code>publishDir</code> pour stocker les r\u00e9sultats d'un processus dans un chemin de votre choix.</li> </ol>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#controle-qualite","title":"Contr\u00f4le qualit\u00e9","text":"<p>Ensuite, nous impl\u00e9mentons une \u00e9tape de contr\u00f4le de qualit\u00e9 <code>FASTQC</code> pour vos reads d'entr\u00e9e (en utilisant le label <code>fastqc</code>). Les entr\u00e9es sont les m\u00eames que les paires de reads utilis\u00e9es dans l'\u00e9tape <code>QUANTIFICATION</code>.</p> <p>Vous pouvez l'ex\u00e9cuter en utilisant la commande suivante :</p> <pre><code>nextflow run script5.nf -resume\n</code></pre> <p>Nextflow DSL2 sait qu'il faut diviser les <code>reads_pair_ch</code> en deux canaux identiques car ils sont requis deux fois en tant qu'entr\u00e9e pour les processus <code>FASTQC</code> et <code>QUANTIFICATION</code>.</p>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#rapport-de-multiqc","title":"Rapport de MultiQC","text":"<p>Cette \u00e9tape rassemble les r\u00e9sultats des processus de <code>QUANTIFICATION</code> et de <code>FASTQC</code> pour cr\u00e9er un rapport final \u00e0 l'aide de l'outil MultiQC.</p> <p>Ex\u00e9cutez le script suivant avec la commande suivante :</p> <pre><code>nextflow run script6.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>Il cr\u00e9e le rapport final dans le dossier <code>results</code> du r\u00e9pertoire <code>work</code> actuel.</p> <p>Dans ce script, notez l'utilisation des op\u00e9rateurs mix et collect combin\u00e9s ensemble pour rassembler les sorties des processus <code>QUANTIFICATION</code> et <code>FASTQC</code> en une seule entr\u00e9e. Les op\u00e9rateurs Operators peuvent \u00eatre utilis\u00e9s pour combiner et transformer les canaux.</p> <pre><code>MULTIQC(quant_ch.mix(fastqc_ch).collect())\n</code></pre> <p>Nous voulons qu'une seule t\u00e2che de MultiQC soit ex\u00e9cut\u00e9e pour produire un rapport. Nous utilisons donc l'op\u00e9rateur de canal <code>mix</code> pour combiner les deux canaux, suivi de l'op\u00e9rateur <code>collect</code>, pour retourner le contenu complet du canal en un seul \u00e9l\u00e9ment.</p>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#resume_4","title":"R\u00e9sum\u00e9","text":"<p>Dans cette \u00e9tape, vous avez appris</p> <ol> <li>Comment collecter plusieurs sorties vers une seule entr\u00e9e avec l'op\u00e9rateur <code>collect</code>.</li> <li>Comment <code>mixer</code> deux canaux en un seul canal</li> <li>Comment encha\u00eener deux ou plusieurs op\u00e9rateurs ensemble</li> </ol>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#gerer-levenement-dachevement","title":"G\u00e9rer l'\u00e9v\u00e9nement d'ach\u00e8vement","text":"<p>Cette \u00e9tape montre comment ex\u00e9cuter une action lorsque le flux de travail a termin\u00e9 son ex\u00e9cution.</p> <p>Notez que les processus Nextflow d\u00e9finissent l'ex\u00e9cution de t\u00e2ches asynchrones, c'est-\u00e0-dire qu'elles ne sont pas ex\u00e9cut\u00e9es l'une apr\u00e8s l'autre comme si elles \u00e9taient \u00e9crites dans le script du workflow dans un langage de programmation imp\u00e9ratif commun.</p> <p>Le script utilise le gestionnaire d'\u00e9v\u00e9nement <code>workflow.onComplete</code> pour imprimer un message de confirmation lorsque le script est termin\u00e9.</p> <p>Essayez de l'ex\u00e9cuter en utilisant la commande suivante :</p> <pre><code>nextflow run script7.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#notifications-par-e-mail","title":"Notifications par e-mail","text":"<p>Envoyer un e-mail de notification lorsque l'ex\u00e9cution du flux de travail est termin\u00e9e en utilisant l'option de ligne de commande <code>-N &lt;adresse e-mail&gt;</code>.</p> <p>Note : ceci n\u00e9cessite la configuration d'un serveur SMTP dans le fichier de configuration de nextflow. Vous trouverez ci-dessous un exemple de fichier <code>nextflow.config</code> montrant les param\u00e8tres \u00e0 configurer :</p> <pre><code>mail {\n    from = 'info@nextflow.io'\n    smtp.host = 'email-smtp.eu-west-1.amazonaws.com'\n    smtp.port = 587\n    smtp.user = \"xxxxx\"\n    smtp.password = \"yyyyy\"\n    smtp.auth = true\n    smtp.starttls.enable = true\n    smtp.starttls.required = true\n}\n</code></pre> <p>Voir mail documentation pour plus de d\u00e9tails.</p>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#scripts-personnalises","title":"Scripts personnalis\u00e9s","text":"<p>Les workflows du monde r\u00e9el utilisent beaucoup de scripts utilisateurs personnalis\u00e9s (BASH, R, Python, etc.). Nextflow vous permet d'utiliser et de g\u00e9rer ces scripts de mani\u00e8re coh\u00e9rente. Il suffit de les placer dans un r\u00e9pertoire nomm\u00e9 <code>bin</code> \u00e0 la racine du projet de workflow. Ils seront automatiquement ajout\u00e9s au <code>PATH</code> d'ex\u00e9cution du workflow.</p> <p>Par exemple, cr\u00e9ez un fichier nomm\u00e9 <code>fastqc.sh</code> avec le contenu suivant :</p> <pre><code>#!/bin/bash\nset -e\nset -u\n\nsample_id=${1}\nreads=${2}\n\nmkdir fastqc_${sample_id}_logs\nfastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n</code></pre> <p>Sauvegardez-le, donnez-lui la permission d'ex\u00e9cuter et placez-le dans le r\u00e9pertoire <code>bin</code> comme indiqu\u00e9 ci-dessous :</p> <pre><code>chmod +x fastqc.sh\nmkdir -p bin\nmv fastqc.sh bin\n</code></pre> <p>Ensuite, ouvrez le fichier <code>script7.nf</code> et remplacez le script du processus <code>FASTQC</code> par le code suivant :</p> <pre><code>script:\n\"\"\"\nfastqc.sh \"$sample_id\" \"$reads\"\n\"\"\"\n</code></pre> <p>Ex\u00e9cutez-la comme auparavant :</p> <pre><code>nextflow run script7.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#resume_5","title":"R\u00e9sum\u00e9","text":"<p>Dans cette \u00e9tape, vous avez appris</p> <ol> <li>Comment \u00e9crire ou utiliser des scripts personnalis\u00e9s existants dans votre workflow Nextflow.</li> <li>Comment \u00e9viter l'utilisation de chemins absolus en ayant vos scripts dans le dossier <code>bin/</code>.</li> </ol>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#mesures-et-rapports","title":"Mesures et rapports","text":"<p>Nextflow peut produire de multiples rapports et graphiques fournissant plusieurs mesures de temps d'ex\u00e9cution et des informations sur l'ex\u00e9cution.</p> <p>Ex\u00e9cutez le workflow rnaseq-nf pr\u00e9c\u00e9demment introduit comme indiqu\u00e9 ci-dessous :</p> <pre><code>nextflow run rnaseq-nf -with-docker -with-report -with-trace -with-timeline -with-dag dag.png\n</code></pre> <p>L'option <code>-with-docker</code> lance chaque t\u00e2che de l'ex\u00e9cution comme une commande d'ex\u00e9cution d'un conteneur Docker.</p> <p>L'option <code>-with-report</code> permet la cr\u00e9ation du rapport d'ex\u00e9cution du workflow. Ouvrez le fichier <code>report.html</code> avec un navigateur pour voir le rapport cr\u00e9\u00e9 avec la commande ci-dessus.</p> <p>L'option <code>-with-trace</code> permet la cr\u00e9ation d'un fichier de valeurs s\u00e9par\u00e9es par des tabulations (TSV) contenant des informations d'ex\u00e9cution pour chaque t\u00e2che ex\u00e9cut\u00e9e. Consultez le fichier <code>trace.txt</code> pour un exemple.</p> <p>L'option <code>-with-timeline</code> permet la cr\u00e9ation d'un rapport sur la chronologie du flux de travail montrant comment les processus ont \u00e9t\u00e9 ex\u00e9cut\u00e9s au fil du temps. Cela peut \u00eatre utile pour identifier les t\u00e2ches qui prennent le plus de temps et les goulots d'\u00e9tranglement. Voir un exemple \u00e0 ce lien.</p> <p>Enfin, l'option <code>-with-dag</code> permet le rendu de la repr\u00e9sentation graphique acyclique directe de l'ex\u00e9cution du workflow. Note : Cette fonctionnalit\u00e9 n\u00e9cessite l'installation de Graphviz sur votre ordinateur. Voir ici pour plus de d\u00e9tails. Essayez ensuite d'ex\u00e9cuter :</p> <pre><code>open dag.png\n</code></pre> <p>Warning</p> <p>Les mesures de temps d'ex\u00e9cution peuvent \u00eatre incompl\u00e8tes pour les ex\u00e9cutions avec des t\u00e2ches courtes, comme dans le cas de ce tutoriel.</p> <p>Info</p> <p>Vous pouvez visualiser les fichiers HTML en cliquant avec le bouton droit de la souris sur le nom du fichier dans la barre lat\u00e9rale gauche et en choisissant l'option de menu Preview.</p>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#executer-un-projet-a-partir-de-github","title":"Ex\u00e9cuter un projet \u00e0 partir de GitHub","text":"<p>Nextflow permet l'ex\u00e9cution d'un projet de workflow directement \u00e0 partir d'un d\u00e9positoire GitHub (ou de services similaires, par exemple BitBucket et GitLab).</p> <p>Cela simplifie le partage et le d\u00e9ploiement de projets complexes et le suivi des modifications de mani\u00e8re coh\u00e9rente.</p> <p>Le d\u00e9positoire GitHub suivant h\u00e9berge une version compl\u00e8te du flux de travail pr\u00e9sent\u00e9 dans ce tutoriel : https://github.com/nextflow-io/rnaseq-nf</p> <p>Vous pouvez l'ex\u00e9cuter en sp\u00e9cifiant le nom du projet et en lan\u00e7ant chaque t\u00e2che de l'ex\u00e9cution comme une commande run d'un conteneur Docker :</p> <pre><code>nextflow run nextflow-io/rnaseq-nf -with-docker\n</code></pre> <p>Il t\u00e9l\u00e9charge automatiquement le conteneur et le stocke dans le dossier <code>$HOME/.nextflow</code>.</p> <p>Utilisez la commande <code>info</code> pour afficher les informations sur le projet :</p> <pre><code>nextflow info nextflow-io/rnaseq-nf\n</code></pre> <p>Nextflow permet l'ex\u00e9cution d'une r\u00e9vision sp\u00e9cifique de votre projet en utilisant l'option de ligne de commande <code>-r</code>. Par exemple, l'option de ligne de commande <code>-r</code> permet d'ex\u00e9cuter une r\u00e9vision sp\u00e9cifique de votre projet :</p> <pre><code>nextflow run nextflow-io/rnaseq-nf -r v2.1 -with-docker\n</code></pre> <p>Les r\u00e9visions sont d\u00e9finies \u00e0 l'aide de tags Git ou de branches d\u00e9finies dans le r\u00e9f\u00e9rentiel du projet.</p> <p>Les tags permettent un contr\u00f4le pr\u00e9cis des modifications apport\u00e9es \u00e0 vos fichiers de projet et \u00e0 vos d\u00e9pendances au fil du temps.</p>"},{"location":"fr/archive/basic_training/rnaseq_pipeline/#plus-de-ressources","title":"Plus de ressources","text":"<ul> <li>Documentation Nextflow - L'accueil de la documentation Nextflow.</li> <li>Mod\u00e8les Nextflow - Une collection de mod\u00e8les de mise en \u0153uvre de Nextflow.</li> <li>CalliNGS-NF - Un workflow d'appel de variants mettant en \u0153uvre les meilleures pratiques de GATK.</li> <li>nf-core - Une collection communautaire de workflows g\u00e9nomiques pr\u00eats \u00e0 la production.</li> </ul>"},{"location":"fr/archive/basic_training/seqera_platform/","title":"Seqera platform","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"fr/archive/basic_training/seqera_platform/#demarrer-avec-seqera-platform","title":"Demarrer avec Seqera Platform","text":""},{"location":"fr/archive/basic_training/seqera_platform/#concept-de-base","title":"Concept de Base","text":"<p>Seqera Platform, anciennement connu sous le nom Nextflow Tower, est le poste de commande centralis\u00e9 pour la gestion des donn\u00e9es et des workflows. Il apporte la surveillance, la journalisation et l'observabilit\u00e9 aux flux de travail distribu\u00e9s et simplifie le d\u00e9ploiement des flux de travail sur n'importe quel cloud, cluster ou ordinateur portable. Dans la terminologie de Seqera Platform, un flux de travail est ce sur quoi nous avons travaill\u00e9 jusqu'\u00e0 pr\u00e9sent, et les pipelines sont des workflows pr\u00e9configur\u00e9s qui peuvent \u00eatre utilis\u00e9s par tous les utilisateurs d'un espace de travail. Il est compos\u00e9 d'un repositoire de workflows, de param\u00e8tres de lancement et d'un environnement de calcul. Nous nous en tiendrons \u00e0 ces d\u00e9finitions dans cette section.</p> <p>Les principales caract\u00e9ristiques de Seqera Platform sont les suivantes</p> <ul> <li>Le lancement de pipelines pr\u00e9configur\u00e9s en toute simplicit\u00e9.</li> <li>L'int\u00e9gration programmatique pour r\u00e9pondre aux besoins d'une organisation.</li> <li>La publication de pipelines dans des espaces de travail partag\u00e9s.</li> <li>La gestion de l'infrastructure n\u00e9cessaire \u00e0 l'ex\u00e9cution d'analyses de donn\u00e9es \u00e0 grande \u00e9chelle.</li> </ul> <p>Conseil</p> <p>Inscrivez-vous pour essayer Seqera Platform gratuitement ou demander une demo pour des d\u00e9ploiements dans votre propre environnement sur site ou en could.</p>"},{"location":"fr/archive/basic_training/seqera_platform/#utilisation","title":"Utilisation","text":"<p>Vous pouvez utiliser Seqera Platform via l'option <code>-with-tower</code> lors de l'utilisation de la commande <code>nextflow run</code>, via l'interface graphique en ligne ou via l'interface utilisateur API.</p>"},{"location":"fr/archive/basic_training/seqera_platform/#via-la-commande-nextflow-run","title":"Via la commande <code>nextflow run</code>.","text":"<p>Cr\u00e9ez un compte et connectez-vous \u00e0 Seqera Platform.</p> <p>1. Cr\u00e9er un nouveau jeton</p> <p>Vous pouvez acc\u00e9der \u00e0 vos jetons \u00e0 partir du menu d\u00e9roulant R\u00e9glages :</p> <p></p> <p>2. Nommez votre jeton</p> <p></p> <p>3. Sauvegardez votre jeton en toute s\u00e9curit\u00e9</p> <p>Copiez et conservez votre nouveau jeton en lieu s\u00fbr.</p> <p></p> <p>4. Exporter votre jeton</p> <p>Une fois que votre jeton a \u00e9t\u00e9 cr\u00e9\u00e9, ouvrez un terminal et tapez :</p> <pre><code>export TOWER_ACCESS_TOKEN=eyxxxxxxxxxxxxxxxQ1ZTE=\n</code></pre> <p>O\u00f9 <code>eyxxxxxxxxxxxQ1ZTE=</code> est le jeton que vous venez de cr\u00e9er.</p> <p>Remarque</p> <p>V\u00e9rifiez votre <code>nextflow -version</code>. Les porteurs de jetons n\u00e9cessitent la version 20.10.0 ou plus r\u00e9cente de Nextflow et peuvent \u00eatre configur\u00e9s avec la seconde commande ci-dessus. Vous pouvez changer la version si n\u00e9cessaire.</p> <p>Pour soumettre un pipeline \u00e0 un espace de travail en utilisant l'outil de ligne de commande Nextflow, ajoutez l'ID de l'espace de travail \u00e0 votre environnement. Par exemple :</p> <pre><code>export TOWER_WORKSPACE_ID=000000000000000\n</code></pre> <p>L'identifiant de l'espace de travail se trouve sur la page de pr\u00e9sentation des espaces de travail de l'organisation.</p> <p>5. Ex\u00e9cuter Nextflow avec Seqera Platform</p> <p>Ex\u00e9cutez vos workflows Nextflow comme d'habitude avec l'ajout de la commande <code>-with-tower</code> :</p> <pre><code>nextflow run hello.nf -with-tower\n</code></pre> <p>Vous verrez et pourrez suivre vos Nextflow jobs dans Seqera Platform.</p> <p>Pour configurer et ex\u00e9cuter des jobs Nextflow dans des environnements Cloud, visitez la section Environnements de calcul.</p> <p>Exercise</p> <p>Ex\u00e9cutez le fichier RNA-Seq <code>script7.nf</code> en utilisant le flag <code>-with-tower</code>, apr\u00e8s avoir correctement compl\u00e9t\u00e9 les param\u00e8tres de jetons d\u00e9crits ci-dessus.</p> Conseil <p>Allez sur https://tower.nf/, connectez-vous, cliquez sur l'onglet run et s\u00e9lectionnez le run que vous venez de soumettre. Si vous ne le trouvez pas, v\u00e9rifiez que votre jeton a \u00e9t\u00e9 saisi correctement.</p>"},{"location":"fr/archive/basic_training/seqera_platform/#via-linterface-graphique-en-ligne","title":"Via l'interface graphique en ligne","text":"<p>L'ex\u00e9cution \u00e0 l'aide de l'interface graphique se fait en trois \u00e9tapes principales :</p> <ol> <li>Cr\u00e9er un compte et se connecter \u00e0 Seqera Platform, disponible gratuitement \u00e0 l'adresse tower.nf.</li> <li>Cr\u00e9er et configurer un nouvel environnement de calcul.</li> <li>Lancez le lancement des pipelines.</li> </ol>"},{"location":"fr/archive/basic_training/seqera_platform/#configurer-votre-environnement-informatique","title":"Configurer votre environnement informatique","text":"<p>ower utilise le concept de Compute Environments pour d\u00e9finir la plateforme d'ex\u00e9cution o\u00f9 un flux de travail sera ex\u00e9cut\u00e9.</p> <p>Il permet de lancer des flux de travail dans un nombre croissant d'infrastructures cloud et on-premise.</p> <p></p> <p>Chaque environnement de calcul doit \u00eatre pr\u00e9configur\u00e9 pour permettre \u00e0 Seqera Platform de soumettre des t\u00e2ches. Pour en savoir plus sur la configuration de chaque environnement, cliquez sur les liens ci-dessous.</p> <p>!!! conseil \"Les guides suivants d\u00e9crivent comment configurer chacun de ces environnements informatiques\".</p> <pre><code>* [AWS Batch](https://help.tower.nf/compute-envs/aws-batch/)\n* [Azure Batch](https://help.tower.nf/compute-envs/azure-batch/)\n* [Google Batch](https://help.tower.nf/compute-envs/google-cloud-batch/)\n* [Google Life Sciences](https://help.tower.nf/compute-envs/google-cloud-lifesciences/)\n* [IBM LSF](https://help.tower.nf/compute-envs/lsf/)\n* [Slurm](https://help.tower.nf/compute-envs/slurm/)\n* [Grid Engine](https://help.tower.nf/compute-envs/altair-grid-engine/)\n* [Altair PBS Pro](https://help.tower.nf/compute-envs/altair-pbs-pro/)\n* [Amazon Kubernetes (EKS)](https://help.tower.nf/compute-envs/eks/)\n* [Google Kubernetes (GKE)](https://help.tower.nf/compute-envs/gke/)\n* [Hosted Kubernetes](https://help.tower.nf/compute-envs/k8s/)\n</code></pre>"},{"location":"fr/archive/basic_training/seqera_platform/#selection-dun-environnement-informatique-par-defaut","title":"S\u00e9lection d'un environnement informatique par d\u00e9faut","text":"<p>Si vous avez plus d'un Environnement informatique, vous pouvez s\u00e9lectionner celui qui sera utilis\u00e9 par d\u00e9faut lors du lancement d'un pipeline.</p> <ol> <li>Naviguez vers vos environnements de informatique.</li> <li>Choisissez votre environnement par d\u00e9faut en s\u00e9lectionnant le bouton Make primary.</li> </ol> <p>Felicitations!</p> <p>Vous \u00eates maintenant pr\u00eat \u00e0 lancer des flux de travail avec votre environnement de calcul principal.</p>"},{"location":"fr/archive/basic_training/seqera_platform/#launchpad","title":"Launchpad","text":"<p>Launchpad permet \u00e0 tout utilisateur de l'espace de travail de lancer facilement un pipeline pr\u00e9configur\u00e9.</p> <p></p> <p>Un pipeline est un repositoire contenant un flux de travail Nextflow, un environnement de calcul et des param\u00e8tres de workflow.</p>"},{"location":"fr/archive/basic_training/seqera_platform/#formulaire-des-parametres-du-pipeline","title":"Formulaire des param\u00e8tres du pipeline","text":"<p>Launchpad d\u00e9tecte automatiquement la pr\u00e9sence d'un <code>nextflow_schema.json</code> dans la racine du r\u00e9f\u00e9rentiel et cr\u00e9e dynamiquement un formulaire o\u00f9 les utilisateurs peuvent facilement mettre \u00e0 jour les param\u00e8tres.</p> <p>Info</p> <p>La vue des formulaires de param\u00e8tres appara\u00eetra si le pipeline dispose d'un fichier de sch\u00e9ma Nextflow pour les param\u00e8tres. Veuillez vous r\u00e9f\u00e9rer au Guide du sch\u00e9ma Nextflow pour en savoir plus sur les cas d'utilisation des fichiers de sch\u00e9ma et sur la mani\u00e8re de les cr\u00e9er.</p> <p>Cela permet aux utilisateurs qui n'ont pas d'expertise en Nextflow de saisir les param\u00e8tres de leur workflow et de le lancer.</p> <p></p>"},{"location":"fr/archive/basic_training/seqera_platform/#ajouter-une-nouveau-pipeline","title":"Ajouter une nouveau pipeline","text":"<p>L'ajout d'un pipeline \u00e0 la zone de lancement de l'espace de travail pr\u00e9enregistr\u00e9 est d\u00e9crit en d\u00e9tail dans la documentation de la page web de la Seqera Platform.</p> <p>En bref, voici les \u00e9tapes \u00e0 suivre pour mettre en place une fili\u00e8re.</p> <ol> <li>S\u00e9lectionnez le bouton Launchpad dans la barre de navigation. Le Launch Form s'ouvre alors.</li> <li>S\u00e9lectionnez un environnement informatique.</li> <li>Saisissez le repositoire du workflow que vous souhaitez lancer, par exemple https://github.com/nf-core/rnaseq.git.</li> <li>S\u00e9lectionner un workflow Num\u00e9ro de r\u00e9vision. La branche par d\u00e9faut de Git (main/master) ou <code>manifest.defaultBranch</code> dans la configuration de Nextflow sera utilis\u00e9e par d\u00e9faut.</li> <li>D\u00e9finir l'emplacement R\u00e9pertoire de travail du r\u00e9pertoire de travail de Nextflow. L'emplacement associ\u00e9 \u00e0 l'environnement informatique sera s\u00e9lectionn\u00e9 par d\u00e9faut.</li> <li>Entrez le(s) nom(s) de chacun des profils de configuration de Nextflow suivi de la touche <code>Enter</code>. Voir la documentation Nextflow Configuration des profiles pour plus de d\u00e9tails.</li> <li>Saisissez les param\u00e8tres du workflow au format YAML ou JSON. Exemple YAML :</li> </ol> <pre><code>reads: \"s3://nf-bucket/exome-data/ERR013140_{1,2}.fastq.bz2\"\npaired_end: true\n</code></pre> <ol> <li>S\u00e9lectionnez Launch (Lancer) pour commencer l'ex\u00e9cution du pipeline.</li> </ol> <p>Info</p> <p>Les workflows Nextflow sont simplement des repositoires Git et peuvent \u00eatre chang\u00e9s pour n'importe quelle plateforme d'h\u00e9bergement Git publique ou priv\u00e9e. Voir Git Integration dans la documentation Seqera Platform et Partage de Pipeline  dans la documentation Nextflow pour plus de d\u00e9tails.</p> <p>Remarque</p> <p>Les informations d'identification associ\u00e9es \u00e0 l'environnement de informatique doivent pouvoir acc\u00e9der au r\u00e9pertoire de travail.</p> <p>Info</p> <p>Dans la configuration, le chemin d'acc\u00e8s complet \u00e0 un godet doit \u00eatre sp\u00e9cifi\u00e9 entre guillemets simples pour les cha\u00eenes de caract\u00e8res et sans guillemets pour les bool\u00e9ens ou les nombres.</p> <p>Conseil</p> <p>Pour cr\u00e9er votre propre sch\u00e9ma Nextflow personnalis\u00e9 pour votre workflow, consultez les exemples des workflows <code>nf-core</code> qui ont adopt\u00e9 cette approche. Par exemple, eager et rnaseq.</p> <p>Pour les options de param\u00e9trage avanc\u00e9es, consultez cette page.</p> <p>Un soutien communautaire est \u00e9galement disponible en cas de probl\u00e8me, rejoignez le Slack Nextflow en suivant ce lien.</p>"},{"location":"fr/archive/basic_training/seqera_platform/#api","title":"API","text":"<p>Pour en savoir plus sur l'utilisation de l'API Seqera Platform, consultez la section API de cette documentation.</p>"},{"location":"fr/archive/basic_training/seqera_platform/#espaces-de-travail-et-organisations","title":"Espaces de travail et organisations","text":"<p>Seqera Platform simplifie le d\u00e9veloppement et l'ex\u00e9cution des pipelines en fournissant une interface centralis\u00e9e pour les utilisateurs et les organisations.</p> <p>Chaque utilisateur dispose d'un espace de travail unique o\u00f9 il peut interagir et g\u00e9rer toutes les ressources telles que les flux de travail, les environnements informatiques et les credits. Les d\u00e9tails sont disponibles ici.</p> <p>Les organisations peuvent avoir plusieurs espaces de travail avec un acc\u00e8s personnalis\u00e9 pour les membres et collaborateurs de l'organisation.</p>"},{"location":"fr/archive/basic_training/seqera_platform/#ressources-de-lorganisation","title":"Ressources de l'organisation","text":"<p>Vous pouvez cr\u00e9er votre propre organisation et votre propre espace de travail pour les participants en suivant la documentation \u00e0 Seqera Platform.</p> <p>Seqera Platform permet la cr\u00e9ation de plusieurs organisations, chacune pouvant contenir plusieurs espaces de travail avec des utilisateurs et des ressources partag\u00e9s. Cela permet \u00e0 toute organisation de personnaliser et d'organiser l'utilisation des ressources tout en maintenant une couche de contr\u00f4le d'acc\u00e8s pour les utilisateurs associ\u00e9s \u00e0 un espace de travail.</p>"},{"location":"fr/archive/basic_training/seqera_platform/#organisation-utilisateurs","title":"Organisation utilisateurs","text":"<p>Tout utilisateur peut \u00eatre ajout\u00e9 ou supprim\u00e9 d'une organisation particuli\u00e8re ou d'un espace de travail et peut se voir attribuer un r\u00f4le d'acc\u00e8s sp\u00e9cifique au sein de cet espace de travail.</p> <p>La fonction Teams (\u00c9quipes) permet aux organisations de regrouper divers utilisateurs et participants en \u00e9quipes. Par exemple, <code>d\u00e9veloppeurs de workflow</code> ou <code>analystes</code>, et d'appliquer le contr\u00f4le d'acc\u00e8s \u00e0 tous les utilisateurs de cette \u00e9quipe collectivement.</p> <p>Pour plus d'informations, veuillez vous referez \u00e0 la section Gestion des utilisateurs.</p>"},{"location":"fr/archive/basic_training/seqera_platform/#mise-en-place-dune-nouvelle-organisation","title":"Mise en place d'une nouvelle organisation","text":"<p>Les organisations constituent la structure de premier niveau et contiennent des espaces de travail, des membres, des \u00e9quipes et des collaborateurs.</p> <p>Pour cr\u00e9er une nouvelle organisation :</p> <ol> <li>Cliquez sur le menu d\u00e9roulant \u00e0 c\u00f4t\u00e9 de votre nom et s\u00e9lectionnez Nouvelle organisation pour ouvrir la bo\u00eete de dialogue de cr\u00e9ation.</li> <li> <p>Dans la bo\u00eete de dialogue, remplissez les champs en fonction de votre organisation. Les champs Nom et Nom complet sont obligatoires.</p> <p>Remarque</p> <p>Un nom valide pour l'organisation doit suivre un mod\u00e8le sp\u00e9cifique. Veuillez vous r\u00e9f\u00e9rer \u00e0 l'interface utilisateur pour plus d'informations.</p> </li> <li> <p>Les autres champs, tels que la description, la localisation, l'URL du site web et l'URL du logo, sont facultatifs.</p> </li> <li> <p>Une fois les d\u00e9tails renseign\u00e9s, vous pouvez acc\u00e9der \u00e0 l'organisation nouvellement cr\u00e9\u00e9e en utilisant la page de l'organisation, qui r\u00e9pertorie toutes vos organisations.</p> <p>Remarque</p> <p>Il est possible de modifier les valeurs des champs facultatifs soit en utilisant l'option Modifier sur la page de l'organisation, soit en utilisant l'onglet Param\u00e8tres sur la page de l'organisation, \u00e0 condition d'\u00eatre le propri\u00e9taire de l'organisation.</p> <p>Conseil</p> <p>Une liste de tous les membres, \u00e9quipes et collaborateurs inclus se trouve sur la page de l'organisation.</p> </li> </ol>"},{"location":"fr/archive/basic_training/setup/","title":"Setup","text":"<p>Warning</p> <p>Some of the translations on the training portal are out of date. The translated material may be incomplete or incorrect. We plan to update the translations later this year. In the meantime, please try to work through the English-language material if you can.</p>"},{"location":"fr/archive/basic_training/setup/#installation-de-lenvironnement","title":"Installation de l'environnement","text":"<p>Il y a deux fa\u00e7ons principales de commencer avec le cours de formation communautaire de Nextflow.</p> <p>La premi\u00e8re consiste \u00e0 installer les exigences localement, ce qui est pr\u00e9f\u00e9rable si vous \u00eates d\u00e9j\u00e0 familier avec Git et Docker, ou si vous travaillez hors ligne.</p> <p>La seconde consiste \u00e0 utiliser Gitpod, ce qui est pr\u00e9f\u00e9rable pour les d\u00e9butants car cette plateforme contient tous les programmes et donn\u00e9es n\u00e9cessaires. Il suffit de cliquer sur le lien et de se connecter \u00e0 l'aide de son compte GitHub pour commencer le tutoriel :</p> <p></p>"},{"location":"fr/archive/basic_training/setup/#installation-locale","title":"Installation locale","text":"<p>Nextflow peut \u00eatre utilis\u00e9 sur n'importe quel syst\u00e8me compatible POSIX (Linux, macOS, Windows Subsystem for Linux, etc.).</p>"},{"location":"fr/archive/basic_training/setup/#exigences","title":"Exigences","text":"<ul> <li>Bash</li> <li>Java 11 (or later, up to 18)</li> <li>Git</li> <li>Docker</li> </ul>"},{"location":"fr/archive/basic_training/setup/#exigences-optionnelles-pour-ce-tutoriel","title":"Exigences optionnelles pour ce tutoriel","text":"<ul> <li>Singularity 2.5.x (ou plus)</li> <li>Conda 4.5 (ou plus)</li> <li>Graphviz</li> <li>AWS CLI</li> <li>Un environnement AWS Batch configur\u00e9</li> </ul>"},{"location":"fr/archive/basic_training/setup/#telecharger-nextflow","title":"T\u00e9l\u00e9charger Nextflow","text":"<p>Entrez cette commande dans votre terminal :</p> <pre><code>wget -qO- https://get.nextflow.io | bash\n</code></pre> <p>Ou, vous pr\u00e9f\u00e9rez <code>curl</code>:</p> <pre><code>curl -s https://get.nextflow.io | bash\n</code></pre> <p>Then ensure that the downloaded binary is executable:</p> <pre><code>chmod +x nextflow\n</code></pre> <p>Et mettez l'ex\u00e9cutable <code>nextflow</code> dans votre <code>$PATH</code> (par exemple <code>/usr/local/bin</code> ou <code>/bin/</code>)</p>"},{"location":"fr/archive/basic_training/setup/#docker","title":"Docker","text":"<p>Assurez-vous que Docker Desktop fonctionne sur votre machine. T\u00e9l\u00e9chargez Docker ici.</p>"},{"location":"fr/archive/basic_training/setup/#materiel-de-formation","title":"Mat\u00e9riel de formation","text":"<p>Vous pouvez consulter le mat\u00e9riel de formation ici : https://training.nextflow.io/</p> <p>Pour t\u00e9l\u00e9charger le mat\u00e9riel, utilisez la commande :</p> <pre><code>git clone https://github.com/nextflow-io/training.git\n</code></pre> <p>Puis <code>cd</code> dans le r\u00e9pertoire <code>nf-training</code>.</p>"},{"location":"fr/archive/basic_training/setup/#verifier-votre-installation","title":"V\u00e9rifier votre installation","text":"<p>V\u00e9rifiez l'installation correcte de <code>nextflow</code> en ex\u00e9cutant la commande suivante :</p> <pre><code>nextflow info\n</code></pre> <p>Cela devrait indiquer la version actuelle, le syst\u00e8me et la dur\u00e9e d'ex\u00e9cution.</p>"},{"location":"fr/archive/basic_training/setup/#gitpod","title":"Gitpod","text":"<p>Un environnement de d\u00e9veloppement Nextflow pr\u00e9configur\u00e9 est disponible via Gitpod.</p>"},{"location":"fr/archive/basic_training/setup/#exigences_1","title":"Exigences","text":"<ul> <li>Un compte GitHub</li> <li>Navigateur web (Google Chrome, Firefox)</li> <li>Une connexion internet</li> </ul>"},{"location":"fr/archive/basic_training/setup/#demarrage-rapide-de-gitpod","title":"D\u00e9marrage rapide de Gitpod","text":"<p>Pour ex\u00e9cuter Gitpod :</p> <ul> <li>Cliquez sur l'URL suivante : https://gitpod.io/#https://github.com/nextflow-io/training</li> <li>Il s'agit de l'URL de notre repositoire GitHub, pr\u00e9fix\u00e9e par <code>https://gitpod.io/#</code>.</li> <li>Connectez-vous \u00e0 votre compte GitHub (et autorisez l'acc\u00e8s).</li> </ul> <p>Une fois que vous vous \u00eates connect\u00e9, Gitpod devrait se charger (sautez le prebuild si on vous le demande).</p>"},{"location":"fr/archive/basic_training/setup/#explorez-votre-ide-gitpod","title":"Explorez votre IDE Gitpod","text":"<p>Vous devriez maintenant voir quelque chose de similaire \u00e0 ce qui suit :</p> <p></p> <ul> <li>La barre lat\u00e9rale vous permet de personnaliser votre environnement Gitpod et d'effectuer des t\u00e2ches de base (copier, coller, ouvrir des fichiers, rechercher, git, etc.) Cliquez sur le bouton Explorer pour voir quels fichiers se trouvent dans ce d\u00e9p\u00f4t.</li> <li>Le terminal vous permet d'ex\u00e9cuter tous les programmes du repositoire. Par exemple, <code>nextflow</code> et <code>docker</code> sont install\u00e9s et peuvent \u00eatre ex\u00e9cut\u00e9s</li> <li>La fen\u00eatre principale vous permet de visualiser et d'\u00e9diter des fichiers. En cliquant sur un fichier dans l'explorateur, vous l'ouvrez dans la fen\u00eatre principale. Vous devriez \u00e9galement voir le materiel du navigateur de formation nf-training (&lt;https://training.nextflow.io</li> </ul> <p>Pour v\u00e9rifier que l'environnement fonctionne correctement, tapez ce qui suit dans le terminal :</p> <pre><code>nextflow info\n</code></pre> <p>Vous devriez obtenir la version de Nextflow et des informations sur la dur\u00e9e d'ex\u00e9cution :</p> <pre><code>Version: 22.10.4 build 5836\nCreated: 09-12-2022 09:58 UTC\nSystem: Linux 5.15.0-47-generic\nRuntime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 17.0.3-internal+0-adhoc..src\nEncoding: UTF-8 (UTF-8)\n</code></pre>"},{"location":"fr/archive/basic_training/setup/#resources-de-gitpod","title":"Resources de Gitpod","text":"<ul> <li>Gitpod vous offre 500 cr\u00e9dits gratuits par mois, ce qui \u00e9quivaut \u00e0 50 heures d'utilisation gratuite de l'environnement en utilisant l'espace de travail standard (jusqu'\u00e0 4 c\u0153urs, 8 Go de RAM et 30 Go d'espace de stockage).</li> <li>Il existe \u00e9galement une option de grand espace de travail qui vous permet d'utiliser jusqu'\u00e0 8 c\u0153urs, 16 Go de RAM et 50 Go d'espace de stockage. Cependant, le grand espace de travail utilisera vos cr\u00e9dits gratuits plus rapidement et vous aurez moins d'heures d'acc\u00e8s \u00e0 cet espace.</li> <li>Gitpod s'arr\u00eate au bout de 30 minutes d'inactivit\u00e9 et conserve les modifications jusqu'\u00e0 deux semaines (voir la section suivante pour r\u00e9ouvrir une session interrompue).</li> </ul> <p>Voir gitpod.io pour plus de details.</p>"},{"location":"fr/archive/basic_training/setup/#reouvrir-une-session-gitpod","title":"R\u00e9ouvrir une session Gitpod","text":"<p>Vous pouvez reouvrir un environnement \u00e0 partir de https://gitpod.io/workspaces. Recherchez votre ancien environnement dans la liste, puis s\u00e9lectionnez l'ellipse (ic\u00f4ne \u00e0 trois points) et s\u00e9lectionnez Ouvrir.</p> <p>Si vous avez sauvegard\u00e9 l'URL de votre pr\u00e9c\u00e9dent environnement Gitpod, vous pouvez simplement l'ouvrir dans votre navigateur.</p> <p>Vous pouvez \u00e9galement d\u00e9marrer un nouvel espace de travail en suivant l'URL de Gitpod : https://gitpod.io/#https://github.com/nextflow-io/training</p> <p>Si vous avez perdu votre environnement, vous pouvez trouver les principaux scripts utilis\u00e9s dans ce tutoriel dans le r\u00e9pertoire <code>nf-training</code>.</p>"},{"location":"fr/archive/basic_training/setup/#sauvegarde-des-fichiers-de-gitpod-sur-votre-machine-locale","title":"Sauvegarde des fichiers de Gitpod sur votre machine locale","text":"<p>Pour enregistrer un fichier \u00e0 partir du panneau de l'explorateur, cliquez avec le bouton droit de la souris sur le fichier et s\u00e9lectionnez T\u00e9l\u00e9charger.</p>"},{"location":"fr/archive/basic_training/setup/#materiel-de-formation_1","title":"Mat\u00e9riel de formation","text":"<p>Le cours de formation est accessible dans votre navigateur \u00e0 l'adresse suivante : https://training.nextflow.io/</p>"},{"location":"fr/archive/basic_training/setup/#selection-dune-version-de-nextflow","title":"S\u00e9lection d'une version de Nextflow","text":"<p>Par d\u00e9faut, Nextflow int\u00e8gre la derni\u00e8re version stable dans votre environnement.</p> <p>Cependant, Nextflow est en constante \u00e9volution car nous apportons des am\u00e9liorations et corrigeons des bugs.</p> <p>Les derni\u00e8res versions peuvent \u00eatre consult\u00e9es sur GitHub ici.</p> <p>Si vous souhaitez utiliser une version sp\u00e9cifique de Nextflow, vous pouvez d\u00e9finir la variable <code>NXF_VER</code> comme indiqu\u00e9 ci-dessous :</p> <pre><code>export NXF_VER=23.10.0\n</code></pre> <p>Remarque</p> <p>Cet atelier tutoriel n\u00e9cessite <code>NXF_VER=23.10.0</code>, ou une version plus r\u00e9cente.</p> <p>Ex\u00e9cutez <code>nextflow -version</code> \u00e0 nouveau pour confirmer que le changement a pris effet.</p>"},{"location":"it/hello_nextflow/","title":"Hello Nextflow","text":"<p>Ciao! Ora sei sulla buona strada per scrivere flussi di lavoro scientifici riproducibili e scalabili utilizzando Nextflow.</p> <p>L'ascesa dei big data ha reso sempre pi\u00f9 necessaria la capacit\u00e0 di analizzare ed eseguire esperimenti su grandi set di dati in modo portabile e riproducibile. Parallelizzazione e calcolo distribuito sono le soluzioni migliori per affrontare questa sfida, ma gli strumenti comunemente disponibili per gli scienziati computazionali spesso non supportano adeguatamente queste tecniche o forniscono un modello che non soddisfa appieno le loro esigenze. Nextflow \u00e8 stato creato appositamente per affrontare queste sfide.</p> <p>Durante questa formazione, ti verr\u00e0 presentato Nextflow attraverso una serie di workshop pratici complementari.</p> <p>Iniziamo! Fai clic sul pulsante \"Apri in GitHub Codespaces\" qui sotto per avviare l'ambiente di training (preferibilmente in una scheda separata), quindi continua a leggere mentre si carica.</p> <p></p>   Segui i video   <p>La formazione Hello Nextflow contiene un video per ogni capitolo, incorporato nella parte superiore di ogni pagina.</p> <p>Puoi trovare l'intera playlist anche sul canale YouTube di Nextflow.</p>"},{"location":"it/hello_nextflow/#obiettivi-di-apprendimento","title":"Obiettivi di apprendimento","text":"<p>In questo workshop apprenderai i concetti fondamentali per la creazione di pipeline.</p> <p>Alla fine di questo workshop sarai in grado di:</p> <ul> <li>Descrivere e utilizzare i componenti principali di Nextflow sufficienti per creare un semplice flusso di lavoro multi-step</li> <li>Descrivere i concetti del passaggio successivo come operatori e fabbriche di canali</li> <li>Avvia un flusso di lavoro Nextflow in locale</li> <li>Trova e interpreta gli output (risultati) e i file di registro generati da Nextflow</li> <li>Risolvere i problemi di base</li> </ul>"},{"location":"it/hello_nextflow/#pubblico-e-prerequisiti","title":"Pubblico e prerequisiti","text":"<p>Questo workshop \u00e8 rivolto a chi non ha mai usato Nextflow. Si presuppone una certa familiarit\u00e0 con la riga di comando e con i formati di file pi\u00f9 comuni.</p> <p>Prerequisiti</p> <ul> <li>Un account GitHub OPPURE un'installazione locale come descritto qui.</li> <li>Esperienza con la riga di comando e la scrittura di script di base</li> </ul>"},{"location":"it/hello_nextflow/00_orientation/","title":"Orientation","text":"<p> Guarda tutta la playlist sul canale Youtube Nextflow.</p> <p> La trascrizione di questo video \u00e8 disponibile qui.</p>"},{"location":"it/hello_nextflow/00_orientation/#github-codespaces","title":"GitHub Codespaces","text":"<p>L'ambiente GitHub Codespaces contiene tutto il software, il codice e i dati necessari per lavorare attraverso questo corso di formazione, quindi non devi installare nulla da solo. Tuttavia, hai bisogno di un account GitHub (gratuito) per effettuare l'accesso e dovresti prenderti qualche minuto per familiarizzare con l'interfaccia.</p> <p>Se non l'hai ancora fatto, segui il mini-corso Environment Setup prima di proseguire.</p>"},{"location":"it/hello_nextflow/00_orientation/#working-directory","title":"Working directory","text":"<p>Durante questo corso di formazione, lavoreremo nella directory <code>hello-nextflow/</code>.</p> <p>Cambia directory ora eseguendo questo comando nel terminale:</p> <pre><code>cd hello-nextflow/\n</code></pre> <p>Tip</p> <p>Se per qualsiasi motivo esci da questa directory, puoi sempre utilizzare il percorso completo per tornarci, supponendo che tu stia eseguendo questo comando nell'ambiente di formazione Github Codespaces:</p> <pre><code>cd /workspaces/training/hello-nextflow\n</code></pre> <p>Diamo ora un'occhiata al contenuto di questa directory.</p>"},{"location":"it/hello_nextflow/00_orientation/#materiali-forniti","title":"Materiali forniti","text":"<p>Puoi esplorare il contenuto di questa directory utilizzando l'esploratore di file sul lato sinistro dell'area di lavoro di training. In alternativa, puoi utilizzare il comando <code>tree</code>.</p> <p>Durante il corso, utilizziamo l'output <code>tree</code> per rappresentare la struttura e il contenuto delle directory in un formato leggibile, a volte con piccole modifiche per chiarezza.</p> <p>Qui generiamo un indice fino al secondo livello in basso:</p> <pre><code>tree . -L 2\n</code></pre> <p>Se esegui questo comando all'interno di <code>hello-nextflow</code>, dovresti vedere il seguente output:</p> Directory contents<pre><code>.\n\u251c\u2500\u2500 greetings.csv\n\u251c\u2500\u2500 hello-channels.nf\n\u251c\u2500\u2500 hello-config.nf\n\u251c\u2500\u2500 hello-containers.nf\n\u251c\u2500\u2500 hello-modules.nf\n\u251c\u2500\u2500 hello-workflow.nf\n\u251c\u2500\u2500 hello-world.nf\n\u251c\u2500\u2500 nextflow.config\n\u251c\u2500\u2500 solutions\n\u2502   \u251c\u2500\u2500 1-hello-world\n\u2502   \u251c\u2500\u2500 2-hello-channels\n\u2502   \u251c\u2500\u2500 3-hello-workflow\n\u2502   \u251c\u2500\u2500 4-hello-modules\n\u2502   \u251c\u2500\u2500 5-hello-containers\n\u2502   \u2514\u2500\u2500 6-hello-config\n\u2514\u2500\u2500 test-params.json\n\n7 directories, 9 files\n</code></pre> <p>Ecco un riepilogo di ci\u00f2 che dovresti sapere per iniziare:</p> <ul> <li> <p>I file <code>.nf</code> sono script di workflow denominati in base alla parte del corso in cui vengono utilizzati.</p> </li> <li> <p>Il file <code>nextflow.config</code> \u00e8 un file di configurazione che imposta le propriet\u00e0 minime dell'ambiente.   Per ora puoi ignorarlo.</p> </li> <li> <p>Il file <code>greetings.csv</code> contiene i dati di input che utilizzeremo nella maggior parte del corso. \u00c8 descritto nella Parte 1, quando lo introduciamo per la prima volta.</p> </li> <li> <p>Il file <code>test-params.json</code> \u00e8 un file che utilizzeremo nella Parte 6. Per ora puoi ignorarlo.</p> </li> <li> <p>La directory <code>solutions</code> contiene gli script del workflow completati che risultano da ogni passaggio del corso.   Sono pensati per essere utilizzati come riferimento per controllare il tuo lavoro e risolvere eventuali problemi.   Il nome e il numero nel nome del file corrispondono al passaggio della parte pertinente del corso.   Ad esempio, il file <code>hello-world-4.nf</code> \u00e8 il risultato previsto del completamento dei passaggi da 1 a 4 della Parte 1: Hello World.</p> </li> </ul> <p>Ora, per iniziare il corso, clicca sulla freccia nell'angolo in basso a destra di questa pagina.</p>"},{"location":"it/hello_nextflow/01_hello_world/","title":"Parte 1: Hello World","text":"<p> Guarda tutta la playlist sul canale Youtube Nextflow.</p> <p> La trascrizione di questo video \u00e8 disponibile qui.</p> <p>In questa prima parte del corso di formazione Hello Nextflow, ci addentriamo nell'argomento con un esempio di Hello World molto elementare e indipendente dal dominio, che svilupperemo progressivamente per dimostrare l'uso della logica e dei componenti fondamentali di Nextflow.</p> <p>Note</p> <p>Un \u201cHello World!\u201d \u00e8 un esempio minimalista che ha lo scopo di dimostrare la sintassi e la struttura di base di un linguaggio di programmazione o di un framework software. L'esempio consiste tipicamente nello stampare la frase \u201cHello, World!\u201d su un dispositivo di output, come la console o il terminale, o nello scriverla su un file.</p>"},{"location":"it/hello_nextflow/01_hello_world/#0-riscaldamento-eseguire-direttamente-hello-world","title":"0. Riscaldamento: Eseguire direttamente Hello World","text":"<p>Possiamo farlo con un semplice comando eseguito direttamente nel terminale, per mostrare cosa fa prima di adentrarci in Nextflow.</p> <p>Tip</p> <p>Ricordate che ora dovreste trovarvi all'interno della cartella <code>hello-nextflow/</code>, come descritto nella sezione Orientamento.</p>"},{"location":"it/hello_nextflow/01_hello_world/#01-facciamo-dire-al-terminale-hello","title":"0.1. Facciamo dire al terminale \"Hello\"","text":"<pre><code>echo 'Hello World!'\n</code></pre> <p>Questo testo viene inviato al terminale con la scritta 'Hello World'.</p> Output<pre><code>Hello World!\n</code></pre>"},{"location":"it/hello_nextflow/01_hello_world/#02-ora-fate-in-modo-che-scriva-il-testo-output-in-un-file","title":"0.2. Ora fate in modo che scriva il testo output in un file","text":"<pre><code>echo 'Hello World!' &gt; output.txt\n</code></pre> <p>Questo non produce alcun output nel terminale.</p> Output<pre><code>\n</code></pre>"},{"location":"it/hello_nextflow/01_hello_world/#03-mostra-il-contenuto-del-file","title":"0.3. Mostra il contenuto del file","text":"<pre><code>cat output.txt\n</code></pre> <p>Adesso, il testo 'Hello World' si trova nel file output che abbiamo specificato.</p> output.txt<pre><code>Hello World!\n</code></pre> <p>Tip</p> <p>Nell'ambiente di addestramento, \u00e8 possibile trovare il file di output nell'esploratore di file e visualizzarne il contenuto facendo clic su di esso. In alternativa, si pu\u00f2 usare il comando <code>code</code> per aprire il file per visualizzarlo</p> <pre><code>code output.txt\n</code></pre>"},{"location":"it/hello_nextflow/01_hello_world/#takeaway","title":"Takeaway","text":"<p>Ora sapete come eseguire un semplice comando nel terminale che produce un testo e, facoltativamente, come far scrivere l'output in un file.</p>"},{"location":"it/hello_nextflow/01_hello_world/#cosa-ce-dopo","title":"Cosa c'\u00e8 dopo?","text":"<p>Scoprire come potrebbe essere scritto un flusso di lavoro Nextflow.</p>"},{"location":"it/hello_nextflow/01_hello_world/#1-esaminare-lo-script-di-avvio-del-flusso-di-lavoro-hello-world","title":"1. Esaminare lo script di avvio del flusso di lavoro Hello World","text":"<p>Come accennato nella guida, vi forniamo uno script di flusso di lavoro completamente funzionale, anche se minimalista, chiamato <code>hello-world.nf</code> che fa la stessa cosa di prima (scrivere \u201cHello World!\u201d) ma con Nextflow.</p> <p>Per iniziare, apriremo prima lo script del flusso di lavoro, in modo da avere un'idea di come \u00e8 strutturato.</p>"},{"location":"it/hello_nextflow/01_hello_world/#11-esaminare-la-struttura-complessiva-del-codice","title":"1.1. Esaminare la struttura complessiva del codice","text":"<p>Apriamo lo script <code>hello-world.nf</code> nel pannello dell'editor.</p> <p>Note</p> <p>Il file si trova nella cartella <code>hello-nextflow</code>, che dovrebbe essere la cartella di lavoro corrente. \u00c8 possibile fare clic sul file nell'esploratore di file, oppure digitare <code>ls</code> nel terminale e fare Cmd+clic (MacOS) o Ctrl+clic (PC) sul file per aprirlo.</p> hello-world.nf<pre><code>#!/usr/bin/env nextflow\n\n/*\n * Usa echo per stampare 'Hello World!' in un file\n */\nprocess sayHello {\n\n    output:\n        path 'output.txt'\n\n    script:\n    \"\"\"\n    echo 'Hello World!' &gt; output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // emette un saluto\n    sayHello()\n}\n</code></pre> <p>Come si pu\u00f2 vedere, uno script Nextflow comprende due tipi principali di componenti fondamentali: uno o pi\u00f9 process e il workflow stesso. Ogni process descrive le operazioni che il passo corrispondente della pipeline deve compiere, mentre il workflow descrive la logica del flusso di dati che collega i vari passi.</p> <p>Vediamo prima il blocco process e poi il blocco workflow.</p>"},{"location":"it/hello_nextflow/01_hello_world/#12-definzione-di-process","title":"1.2. Definzione di <code>process</code>","text":"<p>Il primo blocco di codice descrive un process. La definizione del process inizia con la parola chiave <code>process</code>, seguita dal nome del processo e infine dal corpo del processo delimitato da parentesi graffe. Il corpo del processo deve contenere un blocco di script che specifica il comando da eseguire, che pu\u00f2 essere qualsiasi cosa si possa eseguire in un terminale a riga di comando. Qui abbiamo un process chiamato <code>sayHello</code> che produce un output in un file chiamato <code>output.txt</code>.</p> hello-world.nf<pre><code>/*\n * Usa echo per stampare 'Hello World!' in un file\n */\nprocess sayHello {\n\n    output:\n        path 'output.txt'\n\n    script:\n    \"\"\"\n    echo 'Hello World!' &gt; output.txt\n    \"\"\"\n}\n</code></pre> <p>Si tratta di una definizione di processo molto minimale, che contiene solo una definizione di <code>output' e lo</code>script' da eseguire</p> <p>La definizione <code>output</code> include il qualificatore <code>path</code>, che indica a Nextflow che questo deve essere gestito come un percorso (include sia percorsi di directory che di file). Un altro qualificatore comune \u00e8 <code>val</code>.</p> <p>Note</p> <p>La definzione dell'output non determina quale output verr\u00e0 creato. Semplicemente dichiara qual \u00e8 l'output atteso, in modo che Nextflow possa cercarlo al termine dell'esecuzione. Questo \u00e8 necessario per verificare che il comando sia stato eseguito correttamente e per passare l'output ai processi a valle, se necessario. L'output prodotto che non corrisponde a quanto dichiarato nel blocco di output non verr\u00e0 passato ai processi a valle.</p> <p>Warning</p> <p>Questo esempio \u00e8 fragile perch\u00e9 abbiamo codificato il nome del file di output in due punti separati (lo script e i blocchi di output). Se ne cambiamo uno ma non l'altro, lo script si rompe. Pi\u00f9 avanti, si imparer\u00e0 a usare le variabili per evitare questo problema.</p> <p>In una pipeline reale, un processo di solito contiene blocchi aggiuntivi come le direttive e gli input, che introdurremo tra poco.</p>"},{"location":"it/hello_nextflow/01_hello_world/#13-la-definizione-di-workflow","title":"1.3. La definizione di <code>workflow</code>","text":"<p>Il secondo blocco di codice descrive il workflow stesso. La definizione di workflow comincia con la parola chiabe <code>workflow</code>, seguita da un nome opzionale, e dal corpo del workflow delimitato da parentesi graffe.</p> <p>Qui abbiamo un workflow che consiste in una chiamata al processo <code>sayHello</code>.</p> hello-world.nf<pre><code>workflow {\n\n    // emette un saluto\n    sayHello()\n}\n</code></pre> <p>Questa \u00e8 definizione minimale del workflow. In una pipeline reale, the workflow contiene tipicamente chiamate multiple a processes connesse da channels, e i processi prevedono una o pi\u00f9 variabili in input(s).</p> <p>Si apprender\u00e0 come aggiungere ingressi variabili pi\u00f9 avanti in questo modulo di formazione; e si apprender\u00e0 come aggiungere altri processi e collegarli tramite canali nella Parte 3 di questo corso.</p>"},{"location":"it/hello_nextflow/01_hello_world/#takeaway_1","title":"Takeaway","text":"<p>Adesso sai com'\u00e8 strutturato un semplice worflow di Nextflow.</p>"},{"location":"it/hello_nextflow/01_hello_world/#cosa-ce-dopo_1","title":"Cosa c'\u00e8 dopo?","text":"<p>Impara come lanciare un workflow e a monitorare l'esecuzione e a trovare i vostri output.</p>"},{"location":"it/hello_nextflow/01_hello_world/#2-esecuzione-del-workflow","title":"2. Esecuzione del workflow","text":"<p>Guarda il codice non \u00e8 cos\u00ec divertente come eseguirlo, quindi proviamao a metterlo in pratica.</p>"},{"location":"it/hello_nextflow/01_hello_world/#21-lancio-del-workflow-e-monitorare-lesecuzione","title":"2.1. Lancio del workflow e monitorare l'esecuzione","text":"<p>Nel terminale, esegui i seguenti comandi:</p> <pre><code>nextflow run hello-world.nf\n</code></pre> <p>L'output della console dovrebbe apparire simile a questo:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-world.nf` [goofy_torvalds] DSL2 - revision: c33d41f479\n\nexecutor &gt;  local (1)\n[a3/7be2fa] sayHello | 1 of 1 \u2714\n</code></pre> <p>Congratulazione, hai appena eseguito il tuo primo workflow i Nextflow.</p> <p>L'output pi\u00f9 importante \u00e8 l'ultima riga (riga 6):</p> Output<pre><code>[a3/7be2fa] sayHello | 1 of 1 \u2714\n</code></pre> <p>Questo ci dice che il processo <code>sayHello' \u00e8 stato eseguito con successo una volta (</code>1 di 1 \u2714`).</p> <p>\u00c8 importante che questa riga indichi anche dove trovare l'output della chiamata al processo <code>sayHello</code>. Vediamolo ora.</p>"},{"location":"it/hello_nextflow/01_hello_world/#22-trovare-loutput-e-i-registri-nella-directory-work","title":"2.2. Trovare l'output e i registri nella directory <code>work</code>","text":"<p>Quando si esegue Nextflow per la prima volta in una determinata directory, viene creata una directory chiamata <code>work</code> in cui verranno scritti tutti i file (e gli eventuali symlinks) generati nel corso dell'esecuzione.</p> <p>All'interno della directory <code>work</code>, Nextflow organizza gli output e i registri per ogni chiamata di processo. Per ogni chiamata di processo, Nextflow crea una sottodirectory nidificata, denominata con un hash per renderla unica, in cui inserisce tutti gli input necessari (usando i symlinks per impostazione predefinita), scrive i file di aiuto e scrive i log e gli output del processo.</p> <p>Il percorso della subdirectory viene mostrato in forma tronca tra parentesi quadre nell'output della console. Osservando i risultati dell'esecuzione mostrata sopra, la riga di log della console per il processo sayHello inizia con <code>[a3/7be2fa]</code>. Ci\u00f2 corrisponde al seguente percorso di directory: <code>work/a3/7be2fa7be2fad5e71e5f49998f795677fd68</code></p> <p>Diamo un occhiata a cosa c'\u00e8 dentro.</p> <p>Tip</p> <p>Se si sfoglia il contenuto della subdirectory task nel file explorer di VSCode, si vedranno subito tutti i file. Tuttavia, i file di log sono impostati per essere invisibili nel terminale, quindi se si vuole usare <code>ls</code> o <code>tree</code> per visualizzarli, \u00e8 necessario impostare l'opzione corrispondente per la visualizzazione dei file invisibili.</p> <p><code>bash tree -a work</code></p> <p>Si dovrebbe vedere qualcosa di simile, anche se i nomi esatti delle subdirectory saranno diversi sul vostro sistema:</p> Directory contents<pre><code>work\n\u2514\u2500\u2500 a3\n    \u2514\u2500\u2500 7be2fad5e71e5f49998f795677fd68\n        \u251c\u2500\u2500 .command.begin\n        \u251c\u2500\u2500 .command.err\n        \u251c\u2500\u2500 .command.log\n        \u251c\u2500\u2500 .command.out\n        \u251c\u2500\u2500 .command.run\n        \u251c\u2500\u2500 .command.sh\n        \u251c\u2500\u2500 .exitcode\n        \u2514\u2500\u2500 output.txt\n</code></pre> <p>Questi sono i file di aiuto e di registro:</p> <ul> <li><code>.command.begin</code>: Metadati relativi all'inizio dell'esecuzione della chiamata di processo.</li> <li><code>.command.err</code>: Messaggi di errore (<code>stderr</code>) emessi dalla chiamata al processo.</li> <li><code>.command.log</code>: Output di log completo emesso dalla chiamata al processo</li> <li><code>.command.out</code>: Output regolare (<code>stdout</code>) emesso dalla chiamata al processo</li> <li><code>.command.run</code>: Script completo eseguito da Nextflow per eseguire la chiamata al processo</li> <li><code>.command.sh</code>: Il comando che \u00e8 stato effettivamente eseguito dalla chiamata di processo</li> <li><code>.exitcode</code>: Il codice di uscita risultante dal comando</li> </ul> <p>Il file <code>.command.sh</code> \u00e8 particolarmente utile perch\u00e9 dice quale comando Nextflow ha effettivamente eseguito. In questo caso \u00e8 molto semplice, ma pi\u00f9 avanti nel corso si vedranno comandi che comportano un'interpolazione di variabili. Quando si ha a che fare con questi comandi, \u00e8 necessario essere in grado di controllare esattamente cosa \u00e8 stato eseguito, soprattutto quando si risolve un problema.</p> <p>L'output effettivo del processo <code>sayHello</code> \u00e8 <code>output.txt</code>. Aprendola, troverete il saluto <code>Hello World!</code>, che era il risultato atteso del nostro flusso di lavoro minimalista.</p> output.txt<pre><code>Hello World!\n</code></pre>"},{"location":"it/hello_nextflow/01_hello_world/#takeaway_2","title":"Takeaway","text":"<p>Sapete come decifrare un semplice script di Nextflow, eseguirlo e trovare l'output e i relativi file di log nella directory di lavoro.</p>"},{"location":"it/hello_nextflow/01_hello_world/#cosa-ce-dopo_2","title":"Cosa c'\u00e8 dopo?","text":"<p>Imparate a gestire comodamente le esecuzioni dei vostri workflow.</p>"},{"location":"it/hello_nextflow/01_hello_world/#3-gestire-le-esecuzioni-dei-workflow","title":"3. Gestire le esecuzioni dei workflow","text":"<p>Sapere come lanciare workflow e recuperarne i risultati \u00e8 ottimo, ma scoprirete subito che ci sono altri aspetti della gestione dei workflow che vi renderanno la vita pi\u00f9 facile, soprattutto se ne state sviluppando uno di personale.</p> <p>Qui mostriamo come usare la direttiva <code>publishDir</code> per memorizzare in una cartella di output tutti i risultati principali dell'esecuzione della pipeline, la funzione <code>resume</code> per quando \u00e8 necessario rilanciare lo stesso workflow e come cancellare le vecchie directory di lavoro con <code>nextflow clean</code>.</p>"},{"location":"it/hello_nextflow/01_hello_world/#31-pubblicare-i-risultati","title":"3.1. Pubblicare i risultati","text":"<p>Come si \u00e8 appena appreso, l'output prodotto dalla nostra pipeline \u00e8 sepolto in una directory di lavoro a diversi livelli di profondit\u00e0. Questo \u00e8 stato fatto di proposito; Nextflow ha il controllo di questa directory e noi non dobbiamo interagire con essa.</p> <p>Tuttavia, questo rende scomodo recuperare gli output che ci interessano.</p> <p>Fortunatamente, Nextflow offre un modo per gestire pi\u00f9 comodamente questo aspetto, chiamato direttiva <code>publishDir</code>, che agisce a livello di processo. Questa direttiva indica a Nextflow di pubblicare gli output del processo in una directory di output designata. Per impostazione predefinita, gli output sono pubblicati come collegamenti simbolici dalla directory <code>work</code>. Permette di recuperare il file di output desiderato senza dover scavare nella directory di lavoro.</p>"},{"location":"it/hello_nextflow/01_hello_world/#311-aggiungere-una-direttiva-publishdir-al-processo-sayhello","title":"3.1.1. Aggiungere una direttiva <code>publishDir</code> al processo <code>sayHello</code>","text":"<p>Nel file di script del flusso di lavoro <code>hello-world.nf</code>, apportare la seguente modifica al codice:</p> DopoPrima hello-world.nf<pre><code>process sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    output:\n        path 'output.txt'\n</code></pre> hello-world.nf<pre><code>process sayHello {\n\n    output:\n        path 'output.txt'\n</code></pre>"},{"location":"it/hello_nextflow/01_hello_world/#312-eseguire-nuovamente-il-workflow","title":"3.1.2. Eseguire nuovamente il workflow","text":"<p>Eseguite ora lo script del workflow modificato:</p> <pre><code>nextflow run hello-world.nf\n</code></pre> <p>L'output del log dovrebbe essere molto familiare</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-world.nf` [jovial_mayer] DSL2 - revision: 35bd3425e5\n\nexecutor &gt;  local (1)\n[62/49a1f8] sayHello | 1 of 1 \u2714\n</code></pre> <p>Questa volta, Nextflow ha creato una nuova directory chiamata <code>results/</code>. Il nostro file <code>output.txt</code> si trova in questa directory. Se si controlla il contenuto, dovrebbe corrispondere all'output della sottodirectory di lavoro. In questo modo si pubblicano i file dei risultati al di fuori delle directory di lavoro.</p> <p>Quando si ha a che fare con file molto grandi che non devono essere conservati a lungo, si pu\u00f2 preferire impostare la direttiva <code>publishDir</code> per creare un collegamento simbolico al file invece di copiarlo. Tuttavia, se si elimina la directory di lavoro come parte di un'operazione di pulizia, si perder\u00e0 l'accesso al file, quindi assicuratevi sempre di avere copie effettive di tutto ci\u00f2 che vi interessa prima di eliminare qualcosa.</p> <p>Note</p> <p>\u00c8 stata proposta una nuova opzione di sintassi documentata qui per rendere possibile la dichiarazione e la pubblicazione di output a livello di flusso di lavoro. Questo render\u00e0 l'uso di <code>publishDir</code>, a livello del processo, ridondante per le pipeline completate. Tuttavia, ci aspettiamo che <code>publishDir</code> rimanga molto utile durante lo sviluppo della pipeline.</p>"},{"location":"it/hello_nextflow/01_hello_world/#32-rilanciare-un-workflow-con-resume","title":"3.2. Rilanciare un workflow con <code>-resume</code>","text":"<p>A volte si desidera rieseguire una pipeline gi\u00e0 avviata in precedenza, senza ripetere le fasi gi\u00e0 completate con successo.</p> <p>Nextflow ha un'opzione chiamata <code>resume</code> che permette di farlo. In particolare, in questa modalit\u00e0 vengono saltati tutti i processi gi\u00e0 eseguiti con lo stesso codice, le stesse impostazioni e gli stessi input. Ci\u00f2 significa che Nextflow eseguir\u00e0 solo i processi aggiunti o modificati dall'ultima volta che sono stati eseguiti o per i quali sono state fornite nuove impostazioni o input.</p> <p>Ci sono due vantaggi principali nel fare ci\u00f2:</p> <ul> <li>Se siete nel bel mezzo dello sviluppo della vostra pipeline, potete iterare pi\u00f9 rapidamente, poich\u00e9 dovete eseguire solo i processi su cui state lavorando attivamente per testare le vostre modifiche.</li> <li>Se si sta eseguendo una pipeline in produzione e qualcosa va storto, in molti casi \u00e8 possibile risolvere il problema e rilanciare la pipeline, che riprender\u00e0 a funzionare dal punto di guasto, con un notevole risparmio di tempo e di calcolo.</li> </ul> <p>Per usarlo, aggiungete semplicemente <code>-resume</code> al vostro comando ed eseguitelo:</p> <pre><code>nextflow run hello-world.nf -resume\n</code></pre> <p>L'output della console dovresse essere simile.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-world.nf` [golden_cantor] DSL2 - revision: 35bd3425e5\n\n[62/49a1f8] sayHello | 1 of 1, cached: 1 \u2714\n</code></pre> <p>Cercate il bit <code>cached:</code> che \u00e8 stato aggiunto nella riga di stato del processo (riga 5), il che significa che Nextflow ha riconosciuto di aver gi\u00e0 svolto questo lavoro e ha semplicemente riutilizzato il risultato della precedente esecuzione andata a buon fine.</p> <p>Si pu\u00f2 anche notare che l'hash della sottodirectory di lavoro \u00e8 lo stesso dell'esecuzione precedente. Nextflow indica letteralmente l'esecuzione precedente e dice: \u201cL'ho gi\u00e0 fatto l\u00ec\u201d.</p> <p>Note</p> <p>Quando si riesegue una pipeline con <code>resume</code>, Nextflow non sovrascrive alcun file scritto in una directory <code>publishDir</code> da qualsiasi chiamata di processo precedentemente eseguita con successo.</p>"},{"location":"it/hello_nextflow/01_hello_world/#33-eliminare-vecchie-work-directories","title":"3.3. Eliminare vecchie work directories","text":"<p>Durante il processo di sviluppo, di solito si eseguono le pipeline in bozza un gran numero di volte, il che pu\u00f2 portare all'accumulo di moltissimi file in molte sottodirectory. Poich\u00e9 le sottodirectory sono denominate in modo casuale, \u00e8 difficile capire dai loro nomi quali sono le esecuzioni pi\u00f9 vecchie e quali quelle pi\u00f9 recenti.</p> <p>Nextflow include un utile sottocomando <code>clean</code> che pu\u00f2 cancellare automaticamente le sottodirectory di lavoro per le esecuzioni passate che non interessano pi\u00f9, con diverse opzioni per controllare cosa verr\u00e0 cancellato.</p> <p>Qui viene mostrato un esempio che cancella tutte le sottodirectory delle esecuzioni precedenti a una determinata esecuzione, specificata con il suo nome di esecuzione. Il nome dell'esecuzione \u00e8 la stringa in due parti generata dalla macchina e mostrata tra parentesi quadre nella riga di output della console <code>Launching (...)</code>.</p> <p>Per prima cosa usiamo il flag <code>-n</code> per verificare cosa verr\u00e0 cancellato con il comando:</p> <pre><code>nextflow clean -before golden_cantor -n\n</code></pre> <p>L'output dovrebbe risultare il seguente:</p> Output<pre><code>Would remove /workspaces/training/hello-nextflow/work/a3/7be2fad5e71e5f49998f795677fd68\n</code></pre> <p>Se non viene visualizzata alcuna riga, significa che non \u00e8 stato fornito un nome di sessione valido o che non ci sono sessioni precedenti da eliminare.</p> <p>Se l'output appare come previsto e si vuole procedere con la cancellazione, rieseguire il comando con il flag <code>-f</code> invece di <code>-n</code>:</p> <pre><code>nextflow clean -before golden_cantor -f\n</code></pre> <p>A questo punto si dovrebbe vedere quanto segue:</p> Output<pre><code>Removed /workspaces/training/hello-nextflow/work/a3/7be2fad5e71e5f49998f795677fd68\n</code></pre> <p>Warning</p> <p>L'eliminazione delle sottodirectory di lavoro delle esecuzioni precedenti le rimuove dalla cache di Nextflow e cancella tutti gli output memorizzati in tali directory. Ci\u00f2 significa che interrompe la capacit\u00e0 di Nextflow di riprendere l'esecuzione senza rieseguire i processi corrispondenti.</p> <p>L'utente \u00e8 responsabile del salvataggio di tutti gli output a cui tiene o su cui intende fare affidamento! Se si usa la direttiva <code>publishDir</code> a tale scopo, assicurarsi di usare la modalit\u00e0 <code>copy</code>, non la modalit\u00e0 <code>symlink</code>.</p>"},{"location":"it/hello_nextflow/01_hello_world/#takeaway_3","title":"Takeaway","text":"<p>Sapete come pubblicare gli output in una directory specifica, rilanciare una pipeline senza ripetere i passaggi gi\u00e0 eseguiti in modo identico e usare il comando <code>nextflow clean</code> per ripulire le vecchie directory di lavoro.</p>"},{"location":"it/hello_nextflow/01_hello_world/#cosa-ce-dopo_3","title":"Cosa c'\u00e8 dopo?","text":"<p>Imparare a fornire una variabile in ingresso tramite un parametro della riga di comando e a utilizzare efficacemente i valori predefiniti.</p>"},{"location":"it/hello_nextflow/01_hello_world/#4-usare-una-variabile-di-input-passata-alla-riga-di-comando","title":"4. Usare una variabile di input passata alla riga di comando","text":"<p>Nel suo stato attuale, il nostro flusso di lavoro utilizza un saluto codificato nel comando di processo. Vogliamo aggiungere un po' di flessibilit\u00e0 utilizzando una variabile di input, in modo da poter cambiare pi\u00f9 facilmente il saluto in fase di esecuzione.</p>"},{"location":"it/hello_nextflow/01_hello_world/#41-modificare-il-flusso-di-lavoro-per-accettare-e-utilizzare-un-input-variabile","title":"4.1. Modificare il flusso di lavoro per accettare e utilizzare un input variabile","text":"<p>Ci\u00f2 richiede di apportare tre modifiche al nostro script:</p> <ol> <li>Indicare al processo di aspettarsi un input variabile aggiungendo un blocco <code>input:</code></li> <li>Modificare il processo per utilizzare l'input</li> <li>Impostare un parametro della riga di comando e fornire il suo valore come input alla chiamata al processo</li> </ol> <p>Apportiamo queste modifiche una alla volta.</p>"},{"location":"it/hello_nextflow/01_hello_world/#411-aggiungere-un-blocco-di-input-alla-definizione-del-processo","title":"4.1.1. Aggiungere un blocco di input alla definizione del processo","text":"<p>Per prima cosa dobbiamo adattare la definizione del processo in modo che accetti un input chiamato <code>greeting</code>.</p> <p>Nel blocco del processo, apportare la seguente modifica al codice:</p> DopoPrima hello-world.nf<pre><code>process sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        val greeting\n\n    output:\n        path 'output.txt'\n</code></pre> hello-world.nf<pre><code>process sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    output:\n        path 'output.txt'\n</code></pre> <p>La variabile <code>greeting</code> \u00e8 preceduta da <code>val</code> per indicare a Nextflow che si tratta di un valore (non di un percorso).</p>"},{"location":"it/hello_nextflow/01_hello_world/#412-modificare-il-comando-di-processo-per-utilizzare-la-variabile-di-input","title":"4.1.2. Modificare il comando di processo per utilizzare la variabile di input","text":"<p>Ora scambiamo il valore originale codificato con il valore della variabile di ingresso che ci aspettiamo di ricevere.</p> <p>Nel blocco del processo, apportare la seguente modifica al codice:</p> DopoPrima hello-channels.nf<pre><code>script:\n\"\"\"\necho '$greeting' &gt; output.txt\n\"\"\"\n</code></pre> hello-channels.nf<pre><code>script:\n\"\"\"\necho 'Hello World!' &gt; output.txt\n\"\"\"\n</code></pre> <p>Assicuratevi di aggiungere il simbolo <code>$</code> per indicare a Nextflow che si tratta di un nome di variabile che deve essere sostituito con il valore effettivo (=interpolato).</p>"},{"location":"it/hello_nextflow/01_hello_world/#413-impostare-un-parametro-cli-e-fornirlo-come-input-alla-chiamata-al-processo","title":"4.1.3. Impostare un parametro CLI e fornirlo come input alla chiamata al processo","text":"<p>Ora dobbiamo impostare un modo per fornire un valore di input alla chiamata di processo <code>sayHello()</code>.</p> <p>Si potrebbe semplicemente codificare direttamente scrivendo <code>sayHello('Hello World!')</code>. Tuttavia, quando si lavora davvero con il flusso di lavoro, spesso si desidera poter controllare gli input dalla riga di comando.</p> <p>Buone notizie: Nextflow ha un sistema di parametri incorporato nel flusso di lavoro chiamato <code>params</code>, che rende facile dichiarare e utilizzare i parametri della CLI. La sintassi generale \u00e8 dichiarare <code>params.&lt;nome_parametro&gt;</code> per dire a Nextflow di aspettarsi un parametro <code>--&lt;nome_parametro&gt;</code> sulla riga di comando.</p> <p>In questo caso, vogliamo creare un parametro chiamato <code>--greeting</code>, quindi dobbiamo dichiarare <code>params.greeting</code> da qualche parte nel flusso di lavoro. In linea di principio, possiamo scriverla ovunque; ma poich\u00e9 vogliamo darla alla chiamata di processo <code>sayHello()</code>, possiamo inserirla direttamente scrivendo <code>sayHello(params.greeting)</code>.</p> <p>Note</p> <p>Il nome del parametro (a livello di flusso di lavoro) non deve necessariamente corrispondere al nome della variabile di input (a livello di processo). Usiamo la stessa parola perch\u00e9 \u00e8 quella che ha senso e mantiene il codice leggibile.</p> <p>Nel blocco del flusso di lavoro, apportare la seguente modifica al codice:</p> DopoPrima hello-world.nf<pre><code>// emit a greeting\nsayHello(params.greeting)\n</code></pre> hello-world.nf<pre><code>// emit a greeting\nsayHello()\n</code></pre> <p>Indica a Nextflow di eseguire il processo <code>sayHello</code> sul valore fornito attraverso il parametro <code>--greeting</code>.</p>"},{"location":"it/hello_nextflow/01_hello_world/#414-eseguite-nuovamente-il-comando-del-flusso-di-lavoro","title":"4.1.4. Eseguite nuovamente il comando del flusso di lavoro","text":"<p>Eseguiamolo!</p> <pre><code>nextflow run hello-world.nf --greeting 'Bonjour le monde!'\n</code></pre> <p>Se tutte e tre le modifiche sono state eseguite correttamente, si dovrebbe ottenere un'altra esecuzione di successo:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nAvvio di `hello-world.nf` [elated_lavoisier] DSL2 - revisione: 7c031b42ea\n\nexecutor &gt;  local (1)\n[4b/654319] sayHello | 1 of 1 \u2714\n</code></pre> <p>Assicurarsi di aprire il file di output per verificare che sia disponibile la nuova versione del saluto.</p> results/output.txt<pre><code>Bonjour le monde!\n</code></pre> <p>Voil\u00e0!</p> <p>Tip</p> <p>\u00c8 possibile distinguere facilmente i parametri a livello di Nextflow da quelli a livello di pipeline.</p> <ul> <li>I parametri che si applicano a una pipeline hanno sempre un doppio trattino (<code>--</code>).</li> <li>I parametri che modificano un'impostazione di Nextflow, ad esempio la funzione <code>riprendi' usata in precedenza, sono caratterizzati da un singolo trattino (</code>-`).</li> </ul>"},{"location":"it/hello_nextflow/01_hello_world/#42-utilizzare-i-valori-predefiniti-per-i-parametri-della-riga-di-comando","title":"4.2. Utilizzare i valori predefiniti per i parametri della riga di comando","text":"<p>In molti casi, ha senso fornire un valore predefinito per un determinato parametro, in modo da non doverlo specificare per ogni esecuzione.</p>"},{"location":"it/hello_nextflow/01_hello_world/#421-impostare-un-valore-predefinito-per-il-parametro-cli","title":"4.2.1. Impostare un valore predefinito per il parametro CLI","text":"<p>Diamo al parametro <code>greeting</code> un valore predefinito, dichiarandolo prima della definizione del flusso di lavoro.</p> hello-world.nf<pre><code>/*\n * Pipeline parameters\n */\nparams.greeting = 'Hol\u00e0 mundo!'\n</code></pre> <p>Tip</p> <p>Se si preferisce, si pu\u00f2 inserire la dichiarazione dei parametri all'interno del blocco del flusso di lavoro. Qualunque sia la scelta, si cerchi di raggruppare le cose simili nello stesso posto, in modo da non ritrovarsi con dichiarazioni sparse ovunque.</p>"},{"location":"it/hello_nextflow/01_hello_world/#422-eseguire-nuovamente-il-flusso-di-lavoro-senza-specificare-il-parametro","title":"4.2.2. Eseguire nuovamente il flusso di lavoro senza specificare il parametro","text":"<p>Ora che \u00e8 stato impostato un valore predefinito, \u00e8 possibile eseguire nuovamente il flusso di lavoro senza dover specificare un valore nella riga di comando.</p> <pre><code>nextflow run hello-world.nf\n</code></pre> <p>L'output della console dovrebbe essere lo stesso.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-world.nf` [determined_edison] DSL2 - revision: 3539118582\n\nexecutor &gt;  local (1)\n[72/394147] sayHello | 1 of 1 \u2714\n</code></pre> <p>Controllare l'output nella directory dei risultati:</p> results/output.txt<pre><code>Hol\u00e0 mundo!\n</code></pre> <p>Nextflow ha utilizzato il valore predefinito per assegnare un nome all'output</p>"},{"location":"it/hello_nextflow/01_hello_world/#423-eseguire-nuovamente-il-flusso-di-lavoro-con-il-parametro-per-sovrascrivere-il-valore-predefinito","title":"4.2.3. Eseguire nuovamente il flusso di lavoro con il parametro per sovrascrivere il valore predefinito","text":"<p>Se si fornisce il parametro sulla riga di comando, il valore della CLI sovrascrive il valore predefinito.</p> <p>Provate:</p> <pre><code>nextflow run hello-world.nf --greeting 'Konnichiwa!'\n</code></pre> <p>L'output della console dovrebbe essere lo stesso.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-world.nf` [elegant_faraday] DSL2 - revision: 3539118582\n\nexecutor &gt;  local (1)\n[6f/a12a91] sayHello | 1 of 1 \u2714\n</code></pre> <p>Ora si avr\u00e0 il nuovo output corrispondente nella cartella dei risultati.</p> results/output.txt<pre><code>Konnichiwa!\n</code></pre> <p>Note</p> <p>In Nextflow sono presenti pi\u00f9 punti in cui \u00e8 possibile specificare i valori dei parametri. Se lo stesso parametro \u00e8 impostato su valori diversi in pi\u00f9 punti, Nexflow determiner\u00e0 quale valore utilizzare in base all'ordine di precedenza descritto qui.</p>"},{"location":"it/hello_nextflow/01_hello_world/#takeaway_4","title":"Takeaway","text":"<p>Sapete come utilizzare una semplice variabile di input fornita in fase di esecuzione tramite un parametro della riga di comando, nonch\u00e9 come impostare, utilizzare e sovrascrivere i valori predefiniti.</p> <p>Pi\u00f9 in generale, sapete come interpretare un semplice flusso di lavoro Nextflow, gestirne l'esecuzione e recuperare i risultati.</p>"},{"location":"it/hello_nextflow/01_hello_world/#cosa-ce-dopo_4","title":"Cosa c'\u00e8 dopo?","text":"<p>Prendetevi una piccola pausa, ve la siete meritata!! Quando siete pronti, passate alla Parte 2 per imparare a usare i canali per alimentare gli input nel vostro flusso di lavoro, il che vi permetter\u00e0 di sfruttare il parallelismo dataflow integrato di Nextflow e altre potenti funzioni.</p>"},{"location":"it/hello_nextflow/02_hello_channels/","title":"Parte 2: Hello Channels","text":"<p> Guarda tutta la playlist sul canale Youtube Nextflow.</p> <p> La trascrizione di questo video \u00e8 disponibile qui.</p> <p>Nella parte 1 di questo corso (Hello World), ti abbiamo mostrato come fornire un input variabile a un processo fornendo direttamente l'input nella chiamata di processo: <code>sayHello(params.greet)</code>. Era un approccio volutamente semplificato. In pratica, questo approccio ha grandi limitazioni; vale a dire che funziona solo per casi molto semplici in cui vogliamo eseguire il processo solo una volta, su un singolo valore. Nella maggior parte dei casi d'uso del workflow realistici, vogliamo elaborare pi\u00f9 valori (dati sperimentali per pi\u00f9 campioni, ad esempio), quindi abbiamo bisogno di un modo pi\u00f9 sofisticato per gestire gli input.</p> <p>Ecco a cosa servono i canali di Nextflow. I canali sono code progettate per gestire gli input in modo efficiente e spostarli da un passaggio all'altro in workflows in pi\u00f9 fasi, fornendo parallelismo integrato e molti vantaggi aggiuntivi.</p> <p>In questa parte del corso, imparerai come utilizzare un canale per gestire pi\u00f9 input da una variet\u00e0 di fonti diverse. Imparerai anche a usare operatori per trasformare i contenuti del canale secondo necessit\u00e0.</p> <p>_Per la formazione sull'uso dei canali per collegare i passaggi in un workflow multi-step, vedere la parte 3 di questo corso. _</p>"},{"location":"it/hello_nextflow/02_hello_channels/#0-riscaldamento-esegui-hello-channelsnf","title":"0. Riscaldamento: esegui <code>hello-channels.nf</code>","text":"<p>Useremo lo script del workflow <code>hello-channels.nf</code> come punto di partenza. \u00c8 equivalente allo script prodotto lavorando attraverso la Parte 1 di questo corso di formazione.</p> <p>Solo per assicurarti che tutto funzioni, esegui lo script una volta prima di apportare qualsiasi modifica:</p> <pre><code>nextflow run hello-channels.nf --greeting 'Hello Channels!'\n</code></pre> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-channels.nf` [insane_lichterman] DSL2 - revision: c33d41f479\n\nexecutor &gt;  local (1)\n[86/9efa08] sayHello | 1 of 1 \u2714\n</code></pre> <p>Come in precedenza, troverai il file di output denominato <code>output.txt</code> nella directory <code>results</code> (specificata dalla direttiva <code>publishDir</code>).</p> results/output.txt<pre><code>Hello Channels!\n</code></pre> <p>Se ha funzionato per te, sei pronto a conoscere i canali.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#1-fornire-input-variabili-tramite-un-canale-esplicitamente","title":"1. Fornire input variabili tramite un canale esplicitamente","text":"<p>Creeremo un canale per passare l'input variabile al processo <code>sayHello()</code> invece di fare affidamento sulla gestione implicita, che ha alcune limitazioni.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#11-crea-un-canale-di-input","title":"1.1. Crea un canale di input","text":"<p>Ci sono una variet\u00e0 di fabbriche di canali che possiamo utilizzare per creare un canale. Per mantenere le cose semplici per ora, useremo la fabbrica di canale pi\u00f9 semplice, chiamata <code>Channel.of</code>, che creer\u00e0 un canale contenente un singolo valore. Funzionalmente questo sar\u00e0 simile a come lo avevamo impostato prima, ma invece di avere Nextflow a creare implicitamente un canale, lo stiamo facendo esplicitamente ora.</p> <p>Questa \u00e8 la riga di codice che useremo:</p> Syntax<pre><code>greeting_ch = Channel.of('Hello Channels!')\n</code></pre> <p>Questo crea un canale chiamato <code>greeting_ch</code> usando la fabbrica del canale <code>Channel.of()</code>, che imposta un semplice canale di coda e carica la stringa <code>'Hello Channels!'</code> da usare come valore di saluto.</p> <p>Note</p> <p>Stiamo temporaneamente tornando alle stringhe codificate invece di utilizzare un parametro CLI per motivi di leggibilit\u00e0. Torneremo a utilizzare i parametri CLI una volta che avremo coperto ci\u00f2 che sta accadendo a livello del canale.</p> <p>Nel blocco del workflow, aggiungi il codice di fabbrica del canale:</p> DopoPrima hello-channels.nf<pre><code>workflow {\n\n    // create a channel for inputs\n    greeting_ch = Channel.of('Hello Channels!')\n\n    // emit a greeting\n    sayHello(params.greeting)\n}\n</code></pre> hello-channels.nf<pre><code>workflow {\n\n    // emit a greeting\n    sayHello(params.greeting)\n}\n</code></pre> <p>Questo non \u00e8 ancora funzionale poich\u00e9 non abbiamo ancora cambiato l'input alla chiamata di processo.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#12-aggiungi-il-canale-come-input-alla-chiamata-di-processo","title":"1.2. Aggiungi il canale come input alla chiamata di processo","text":"<p>Ora dobbiamo effettivamente collegare il nostro canale appena creato alla chiamata di processo <code>sayHello()</code>, sostituendo il parametro CLI che stavamo fornendo direttamente prima.</p> <p>Nel blocco del workflow, apportare la seguente modifica del codice:</p> DopoPrima hello-channels.nf<pre><code>workflow {\n\n    // create a channel for inputs\n    greeting_ch = Channel.of('Hello Channels!')\n\n    // emit a greeting\n    sayHello(greeting_ch)\n}\n</code></pre> hello-channels.nf<pre><code>workflow {\n\n    // create a channel for inputs\n    greeting_ch = Channel.of('Hello Channels!')\n\n    // emit a greeting\n    sayHello(params.greeting)\n}\n</code></pre> <p>Questo dice a Nextflow di eseguire il processo <code>sayHello</code> sui contenuti del canale <code>greeting_ch</code>.</p> <p>Ora il nostro workflow \u00e8 correttamente funzionale; \u00e8 l'equivalente esplicito di scrivere <code>sayHello('Hello Channels!')</code>.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#13-esegui-di-nuovo-il-comando-del-workflow","title":"1.3. Esegui di nuovo il comando del workflow","text":"<p>Eseguiamolo!</p> <pre><code>nextflow run hello-channels.nf\n</code></pre> <p>Se hai apportato entrambe le modifiche correttamente, dovresti ottenere un'altra esecuzione riuscita:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-channels.nf` [nice_heisenberg] DSL2 - revision: 41b4aeb7e9\n\nexecutor &gt;  local (1)\n[3b/f2b109] sayHello (1) | 1 of 1 \u2714\n</code></pre> <p>Puoi controllare la directory dei risultati per assicurarti che il risultato sia ancora lo stesso di prima.</p> results/output.txt<pre><code>Hello Channels!\n</code></pre> <p>Finora stiamo solo modificando progressivamente il codice per aumentare la flessibilit\u00e0 del nostro workflow ottenendo allo stesso tempo lo stesso risultato finale.</p> <p>Note</p> <p>Pu\u00f2 sembrare che stiamo scrivendo pi\u00f9 codice senza alcun beneficio tangibile, ma il valore diventer\u00e0 chiaro non appena inizieremo a gestire pi\u00f9 input.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#conclusioni","title":"Conclusioni","text":"<p>Sai come utilizzare una fabbrica di canale di base per fornire un input a un processo.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#cosa-ce-dopo","title":"Cosa c'\u00e8 dopo?","text":"<p>Scopri come utilizzare i canali per far in modo che il workflow iteri su pi\u00f9 valori di input.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#2-modifica-il-workflow-per-eseguire-su-piu-valori-di-input","title":"2. Modifica il workflow per eseguire su pi\u00f9 valori di input","text":"<p>I workflows in genere vengono eseguiti su lotti di input che devono essere elaborati in blocco, quindi vogliamo aggiornare il workflow per accettare pi\u00f9 valori di input.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#21-carica-piu-saluti-nel-canale-di-input","title":"2.1. Carica pi\u00f9 saluti nel canale di input","text":"<p>Convenientemente, la fabbrica di canali <code>Channel.of()</code> che abbiamo utilizzato \u00e8 abbastanza felice di accettare pi\u00f9 di un valore, quindi non abbiamo affatto bisogno di modificarlo.</p> <p>Dobbiamo solo caricare pi\u00f9 valori nel canale.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#211-aggiungi-altri-saluti","title":"2.1.1. Aggiungi altri saluti","text":"<p>Nel blocco del workflow, apportare la seguente modifica del codice:</p> DopoPrima hello-channels.nf<pre><code>// create a channel for inputs\ngreeting_ch = Channel.of('Hello','Bonjour','Hol\u00e0')\n</code></pre> hello-channels.nf<pre><code>// create a channel for inputs\ngreeting_ch = Channel.of('Hello Channels')\n</code></pre> <p>La documentazione ci dice che questo dovrebbe funzionare. Pu\u00f2 davvero essere cos\u00ec semplice?</p>"},{"location":"it/hello_nextflow/02_hello_channels/#212-esegui-il-comando-e-guarda-loutput-del-registro","title":"2.1.2. Esegui il comando e guarda l'output del registro","text":"<p>Proviamolo.</p> <pre><code>nextflow run hello-channels.nf\n</code></pre> <p>Certamente sembra funzionare bene:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-channels.nf` [suspicious_lamport] DSL2 - revision: 778deadaea\n\nexecutor &gt;  local (3)\n[cd/77a81f] sayHello (3) | 3 of 3 \u2714\n</code></pre> <p>Tuttavia... Questo sembra indicare che sono state effettuate chiamate \"3 di 3\" per il processo, il che \u00e8 incoraggiante, ma questo ci mostra solo una singola esecuzione del processo, con un percorso di sottodirectory (<code>cd/77a81f</code>). Cosa sta succedendo?</p> <p>Per impostazione predefinita, il sistema di registrazione ANSI scrive la registrazione da pi\u00f9 chiamate allo stesso processo sulla stessa linea. Fortunatamente, possiamo disabilitare quel comportamento per vedere l'elenco completo delle chiamate di processo.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#213-esegui-di-nuovo-il-comando-con-lopzione-ansi-log-false","title":"2.1.3. Esegui di nuovo il comando con l'opzione <code>-ansi-log false</code>","text":"<p>Per espandere la registrazione per visualizzare una riga per chiamata di processo, aggiungi <code>-ansi-log false</code> al comando.</p> <pre><code>nextflow run hello-channels.nf -ansi-log false\n</code></pre> <p>Questa volta vediamo tutte e tre le esecuzioni di processo e le loro sottodirectory di lavoro associate elencate nell'output:</p> Output<pre><code>N E X T F L O W  ~  version 24.10.0\nLaunching `hello-channels.nf` [pensive_poitras] DSL2 - revision: 778deadaea\n[76/f61695] Submitted process &gt; sayHello (1)\n[6e/d12e35] Submitted process &gt; sayHello (3)\n[c1/097679] Submitted process &gt; sayHello (2)\n</code></pre> <p>\u00c8 molto meglio; almeno per un semplice workflow. Per un workflow complesso o un gran numero di input, avere l'output completo dell'elenco sul terminale potrebbe diventare un po' travolgente, quindi potresti non scegliere di usare <code>-ansi-log false</code> in quei casi.</p> <p>Note</p> <p>Il modo in cui viene riportato lo stato \u00e8 un po' diverso tra le due modalit\u00e0 di registrazione. In modalit\u00e0 condensata, Nextflow segnala se le chiamate sono state completate con successo o meno. In questa modalit\u00e0 espansa, riporta solo che sono stati inviati.</p> <p>Detto questo, abbiamo un altro problema. Se guardi nella directory <code>results</code>, c'\u00e8 solo un file: <code>output.txt</code>!</p> Directory contents<pre><code>results\n\u2514\u2500\u2500 output.txt\n</code></pre> <p>Che succede? Non dovremmo aspettarci un file separato per saluto di input, quindi tre file in tutto? Tutti e tre i saluti sono andati in un unico file?</p> <p>Puoi controllare il contenuto di <code>output.txt</code>; troverai solo uno dei tre, contenente uno dei tre saluti che abbiamo fornito.</p> output.txt<pre><code>Bonjour\n</code></pre> <p>Potresti ricordare che abbiamo codificato il nome del file di output per il processo <code>sayHello</code>, quindi tutte e tre le chiamate hanno prodotto un file chiamato <code>output.txt</code>. Puoi controllare le sottodirectory di lavoro per ciascuno dei tre processi; ognuno di essi contiene un file chiamato <code>output.txt</code> come previsto.</p> <p>Finch\u00e9 i file di output rimangono l\u00ec, isolati dagli altri processi, va bene. Ma quando la direttiva <code>publishDir</code> copia ciascuno di essi nella stessa directory <code>results</code>, qualunque sia stato copiato l\u00ec per primo viene sovrascritto da quello successivo, e cos\u00ec via.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#22-assicurati-che-i-nomi-dei-file-di-output-siano-univoci","title":"2.2. Assicurati che i nomi dei file di output siano univoci","text":"<p>Possiamo continuare a pubblicare tutti gli output nella stessa directory dei risultati, ma dobbiamo assicurarci che abbiano nomi univoci. In particolare, dobbiamo modificare il primo processo per generare un nome di file in modo dinamico in modo che i nomi dei file finali siano unici.</p> <p>Quindi, come facciamo a rendere unici i nomi dei file? Un modo comune per farlo \u00e8 utilizzare un pezzo unico di metadati dagli input (ricevuti dal canale di input) come parte del nome del file di output. Qui, per comodit\u00e0, useremo solo il saluto stesso poich\u00e9 \u00e8 solo una stringa breve e lo antemeremo al nome del file di output di base.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#221-costruisci-un-nome-di-file-di-output-dinamico","title":"2.2.1. Costruisci un nome di file di output dinamico","text":"<p>Nel blocco di processo, apporta le seguenti modifiche al codice:</p> DopoPrima hello-channels.nf<pre><code>process sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        val greeting\n\n    output:\n        path \"${greeting}-output.txt\"\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; '$greeting-output.txt'\n    \"\"\"\n}\n</code></pre> hello-channels.nf<pre><code>process sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        val greeting\n\n    output:\n        path 'output.txt'\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; output.txt\n    \"\"\"\n}\n</code></pre> <p>Assicurati di sostituire <code>output.txt</code> sia nella definizione di output che nel blocco di comando <code>script:</code>.</p> <p>Tip</p> <p>Nella definizione di output, DEVI usare virgolette doppie attorno all'espressione del nome del file di output (NON virgolette singole), altrimenti fallir\u00e0.</p> <p>Questo dovrebbe produrre un nome di file di output univoco ogni volta che viene chiamato il processo, in modo che possa essere distinto dagli output di altre iterazioni dello stesso processo nella directory di output.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#222-esegui-il-workflow","title":"2.2.2. Esegui il workflow","text":"<p>Eseguiamolo:</p> <pre><code>nextflow run hello-channels.nf\n</code></pre> <p>Tornando alla vista di riepilogo, l'output ha di nuovo questo aspetto:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-channels.nf` [astonishing_bell] DSL2 - revision: f57ff44a69\n\nexecutor &gt;  local (3)\n[2d/90a2e2] sayHello (1) | 3 of 3 \u2714\n</code></pre> <p>\u00c8 importante sottolineare che ora abbiamo tre nuovi file oltre a quello che avevamo gi\u00e0 nella directory <code>risultati</code>:</p> Directory contents<pre><code>results\n\u251c\u2500\u2500 Bonjour-output.txt\n\u251c\u2500\u2500 Hello-output.txt\n\u251c\u2500\u2500 Hol\u00e0-output.txt\n\u2514\u2500\u2500 output.txt\n</code></pre> <p>Ognuno di loro ha i contenuti previsti:</p> Bonjour-output.txt<pre><code>Bonjour\n</code></pre> Hello-output.txt<pre><code>Hello\n</code></pre> Hol\u00e0-output.txt<pre><code>Hol\u00e0\n</code></pre> <p>Successo! Ora possiamo aggiungere tutti i saluti che vorremmo senza preoccuparci che i file di output vengano sovrascritti.</p> <p>Note</p> <p>In pratica, nominare i file in base ai dati di input stessi \u00e8 quasi sempre impraticabile. Il modo migliore per generare nomi di file dinamici \u00e8 passare i metadati a un processo insieme ai file di input. I metadati sono in genere forniti tramite un \"foglio campione\" o equivalenti. Imparerai come farlo pi\u00f9 avanti nel tuo corso di formazione Nextflow.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#conclusioni_1","title":"Conclusioni","text":"<p>Sai come alimentare pi\u00f9 elementi di input attraverso un canale.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#cosa-ce-dopo_1","title":"Cosa c'\u00e8 dopo?","text":"<p>Impara a usare un operatore per trasformare i contenuti di un canale.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#3-usa-un-operatore-per-trasformare-il-contenuto-di-un-canale","title":"3. Usa un operatore per trasformare il contenuto di un canale","text":"<p>In Nextflow, operatori ci consentono di trasformare il contenuto di un canale.</p> <p>Ti abbiamo appena mostrato come gestire pi\u00f9 elementi di input che sono stati codificati direttamente nella fabbrica del canale. E se volessimo fornire quegli input multipli in una forma diversa?</p> <p>Ad esempio, immagina di impostare una variabile di input contenente una serie di elementi come questa:</p> <p><code>greetings_array = ['Ciao','Bonjour','Hol\u00e0']</code></p> <p>Possiamo caricarlo nel nostro canale di output e aspettarci che funzioni? Scopriamolo.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#31-fornire-una-serie-di-valori-come-input-al-canale","title":"3.1. Fornire una serie di valori come input al canale","text":"<p>Il buon senso suggerisce che dovremmo essere in grado di passare semplicemente una serie di valori invece di un singolo valore. Giusto?</p>"},{"location":"it/hello_nextflow/02_hello_channels/#311-imposta-la-variabile-di-input","title":"3.1.1. Imposta la variabile di input","text":"<p>Prendiamo la variabile <code>greetings_array</code> che abbiamo appena immaginato e rendiamola una realt\u00e0 aggiungendola al blocco del workflow:</p> DopoPrima hello-channels.nf<pre><code>workflow {\n\n    // declare an array of input greetings\n    greetings_array = ['Hello','Bonjour','Hol\u00e0']\n\n    // create a channel for inputs\n    greeting_ch = Channel.of('Hello','Bonjour','Hol\u00e0')\n</code></pre> hello-channels.nf<pre><code>workflow {\n\n    // create a channel for inputs\n    greeting_ch = Channel.of('Hello','Bonjour','Hol\u00e0')\n</code></pre>"},{"location":"it/hello_nextflow/02_hello_channels/#312-imposta-larray-di-saluti-come-input-alla-fabbrica-del-canale","title":"3.1.2. Imposta l'array di saluti come input alla fabbrica del canale","text":"<p>Sostituiremo i valori <code>'Hello','Bonjour', 'Hol\u00e0'</code> attualmente codificati nella fabbrica del canale con il <code>greetings_array</code> che abbiamo appena creato.</p> <p>Nel blocco del workflow, apportare la seguente modifica:</p> DopoPrima hello-channels.nf<pre><code>    // create a channel for inputs\n    greeting_ch = Channel.of(greetings_array)\n</code></pre> hello-channels.nf<pre><code>    // create a channel for inputs\n    greeting_ch = Channel.of('Hello','Bonjour','Hol\u00e0')\n</code></pre>"},{"location":"it/hello_nextflow/02_hello_channels/#313-esegui-il-workflow","title":"3.1.3. Esegui il workflow","text":"<p>Proviamo a eseguire questo:</p> <pre><code>nextflow run hello-channels.nf\n</code></pre> <p>Oh no! Nextflow genera un errore che inizia cos\u00ec:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-channels.nf` [friendly_koch] DSL2 - revision: 97256837a7\n\nexecutor &gt;  local (1)\n[22/57e015] sayHello (1) | 0 of 1\nERROR ~ Error executing process &gt; 'sayHello (1)'\n\nCaused by:\n  Missing output file(s) `[Hello, Bonjour, Hol\u00e0]-output.txt` expected by process `sayHello (1)`\n</code></pre> <p>Sembra che Nextflow abbia tentato di eseguire una singola chiamata di processo, usando <code>[Hello, Bonjour, Hol\u00e0]</code> come valore di stringa, invece di utilizzare le tre stringhe nell'array come valori separati.</p> <p>Come facciamo a ottenere Nextflow per decomprimere l'array e caricare le singole stringhe nel canale?</p>"},{"location":"it/hello_nextflow/02_hello_channels/#32-usa-un-operatore-per-trasformare-i-contenuti-del-canale","title":"3.2. Usa un operatore per trasformare i contenuti del canale","text":"<p>\u00c8 qui che entrano in gioco gli operatori.</p> <p>Se sfogli elenco di operatori nella documentazione di Nextflow, troverai <code>flatten()</code>, che fa esattamente ci\u00f2 di cui abbiamo bisogno: decomprimere il contenuto di un array e li emette come singoli elementi.</p> <p>Note</p> <p>\u00c8 tecnicamente possibile ottenere gli stessi risultati utilizzando una fabbrica di canali diversa, <code>Channel.fromList</code>, che include una fase di mappatura implicita nella sua operazione. Qui abbiamo scelto di non usarlo per dimostrare l'uso di un operatore su un caso d'uso abbastanza semplice.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#321-aggiungi-loperatore-flatten","title":"3.2.1. Aggiungi l'operatore <code>flatten()</code>","text":"<p>Per applicare l'operatore <code>flatten()</code> al nostro canale di input, lo apponiamo alla dichiarazione di fabbrica del canale.</p> <p>Nel blocco del workflow, apportare la seguente modifica del codice:</p> DopoPrima hello-channels.nf<pre><code>    // create a channel for inputs\n    greeting_ch = Channel.of(greetings_array)\n                         .flatten()\n</code></pre> hello-channels.nf<pre><code>    // create a channel for inputs\n    greeting_ch = Channel.of(greetings_array)\n</code></pre> <p>Qui abbiamo aggiunto l'operatore sulla riga successiva per la leggibilit\u00e0, ma puoi aggiungere operatori sulla stessa riga della fabbrica del canale se preferisci, come questo: <code>greeting_ch = Channel.of(greetings_array).flatten()</code></p>"},{"location":"it/hello_nextflow/02_hello_channels/#322-aggiungi-view-per-ispezionare-i-contenuti-del-canale","title":"3.2.2. Aggiungi <code>view()</code> per ispezionare i contenuti del canale","text":"<p>Potremmo eseguirlo subito per verificare se funziona, ma gi\u00e0 che ci siamo, aggiungeremo anche un paio di operatori <code>view()</code>, che ci consentono di ispezionare il contenuto di un canale. Puoi pensare a <code>view()</code> come a uno strumento di debug, come un'istruzione <code>print()</code> in Python, o il suo equivalente in altre lingue.</p> <p>Nel blocco del workflow, apportare la seguente modifica del codice:</p> DopoPrima hello-channels.nf<pre><code>    // create a channel for inputs\n    greeting_ch = Channel.of(greetings_array)\n                         .view { greeting -&gt; \"Before flatten: $greeting\" }\n                         .flatten()\n                         .view { greeting -&gt; \"After flatten: $greeting\" }\n</code></pre> hello-channels.nf<pre><code>    // create a channel for inputs\n    greeting_ch = Channel.of(greetings_array)\n                         .flatten()\n</code></pre> <p>Stiamo usando un operatore closure qui - le parentesi graffe. Questo codice viene eseguito per ogni elemento nel canale. Definiamo una variabile temporanea per il valore interno, qui chiamata <code>salunto</code> (potrebbe essere qualsiasi cosa). Questa variabile viene utilizzata solo nell'ambito di tale chiusura.</p> <p>In questo esempio, <code>$greeting</code> rappresenta ogni singolo elemento caricato in un canale.</p> <p>Nota su <code>$it</code></p> <p>In alcune pipeline potresti vedere una variabile speciale chiamata <code>$it</code> utilizzata all'interno delle chiusure dell'operatore. Questa \u00e8 una variabile implicita che consente un accesso abbreviato alla variabile interna, Senza bisogno di definirlo con un <code>-&gt;</code>.</p> <p>Preferiamo essere espliciti per aiutare la chiarezza del codice, in quanto tale la sintassi <code>$it</code> \u00e8 scoraggiata e verr\u00e0 lentamente eliminata dal linguaggio Nextflow.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#323-esegui-il-workflow","title":"3.2.3. Esegui il workflow","text":"<p>Infine, puoi provare a eseguire di nuovo il workflow!</p> <pre><code>nextflow run hello-channels.nf\n</code></pre> <p>Questa volta funziona E ci d\u00e0 una visione aggiuntiva di come appaiono i contenuti del canale prima e dopo aver eseguito l'operatore <code>flatten()</code>:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-channels.nf` [tiny_elion] DSL2 - revision: 1d834f23d2\n\nexecutor &gt;  local (3)\n[8e/bb08f3] sayHello (2) | 3 of 3 \u2714\nBefore flatten: [Hello, Bonjour, Hol\u00e0]\nAfter flatten: Hello\nAfter flatten: Bonjour\nAfter flatten: Hol\u00e0\n</code></pre> <p>Vedi che otteniamo una singola istruzione <code>Before flatten:</code> perch\u00e9 a quel punto il canale contiene un elemento, l'array originale. Quindi otteniamo tre istruzioni separate <code>Dopo appiattire:</code>, una per ogni saluto, che ora sono singoli elementi nel canale.</p> <p>\u00c8 importante sottolineare che questo significa che ogni elemento pu\u00f2 ora essere elaborato separatamente dal workflow.</p> <p>Tip</p> <p>Dovresti eliminare o commentare le istruzioni <code>view()</code> prima di andare avanti.</p> hello-channels.nf<pre><code>// crea un canale per gli input\ngreeting_ch = Channel.of(greetings_array)\n                     .flatten()\n</code></pre> <p>Li abbiamo lasciati nel file di soluzione <code>hello-channels-3.nf</code> a scopo di riferimento.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#conclusioni_2","title":"Conclusioni","text":"<p>Sai come usare un operatore come <code>flatten()</code> per trasformare il contenuto di un canale e come usare l'operatore <code>view()</code> per ispezionare il contenuto del canale prima e dopo aver applicato un operatore.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#cosa-ce-dopo_2","title":"Cosa c'\u00e8 dopo?","text":"<p>Scopri come fare in modo che il workflow prenda un file come fonte di valori di input.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#4-usa-un-operatore-per-analizzare-i-valori-di-input-da-un-file-csv","title":"4. Usa un operatore per analizzare i valori di input da un file CSV","text":"<p>Spesso accade che, quando vogliamo eseguire su pi\u00f9 ingressi, i valori di input sono contenuti in un file. Ad esempio, abbiamo preparato un file CSV chiamato <code>greetings.csv</code> contenente diversi saluti, uno su ogni riga (come una colonna di dati).</p> greetings.csv<pre><code>Hello\nBonjour\nHol\u00e0\n</code></pre> <p>Quindi ora dobbiamo modificare il nostro workflow per leggere i valori di un file del genere.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#41-modifica-lo-script-per-aspettarti-un-file-csv-come-fonte-di-saluti","title":"4.1. Modifica lo script per aspettarti un file CSV come fonte di saluti","text":"<p>Per iniziare, dovremo apportare due modifiche chiave allo script:</p> <ul> <li>Cambia il parametro di input per puntare al file CSV</li> <li>Passa a una fabbrica di canali progettata per gestire un file</li> </ul>"},{"location":"it/hello_nextflow/02_hello_channels/#411-cambia-il-parametro-di-input-per-puntare-al-file-csv","title":"4.1.1. Cambia il parametro di input per puntare al file CSV","text":"<p>Ricordi il parametro <code>params.greeting</code> che abbiamo impostato nella parte 1? Lo aggiorneremo per puntare al file CSV contenente i nostri saluti.</p> <p>Nel blocco del workflow, apportare la seguente modifica del codice:</p> DopoPrima hello-channels.nf<pre><code>/*\n * Pipeline parameters\n */\nparams.greeting = 'greetings.csv'\n</code></pre> hello-channels.nf<pre><code>/*\n * Pipeline parameters\n */\nparams.greeting = ['Hello','Bonjour','Hol\u00e0']\n</code></pre>"},{"location":"it/hello_nextflow/02_hello_channels/#412-passa-a-una-fabbrica-di-canali-progettata-per-gestire-un-file","title":"4.1.2. Passa a una fabbrica di canali progettata per gestire un file","text":"<p>Dal momento che ora vogliamo usare un file invece di semplici stringhe come input, non possiamo usare la fabbrica di canali <code>Channel.of()</code> di prima. Dobbiamo passare all'utilizzo di una nuova fabbrica di canali, <code>Channel.fromPath()</code>, che ha alcune funzionalit\u00e0 integrate per la gestione dei percorsi dei file.</p> <p>Nel blocco del flusso di lavoro, apportare la seguente modifica del codice:</p> DopoPrima hello-channels.nf<pre><code>    // create a channel for inputs from a CSV file\n    greeting_ch = Channel.fromPath(params.greeting)\n</code></pre> hello-channels.nf<pre><code>    // create a channel for inputs\n    greeting_ch = Channel.of(greetings_array)\n                         .flatten()\n</code></pre>"},{"location":"it/hello_nextflow/02_hello_channels/#413-esegui-il-workflow","title":"4.1.3. Esegui il workflow","text":"<p>Proviamo a eseguire il workflow con la nuova fabbrica di canali e il file di input.</p> <pre><code>nextflow run hello-channels.nf\n</code></pre> <p>Oh no, non funziona. Ecco l'inizio dell'output della console e il messaggio di errore:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-channels.nf` [adoring_bhabha] DSL2 - revision: 8ce25edc39\n\n[-        ] sayHello | 0 of 1\nERROR ~ Error executing process &gt; 'sayHello (1)'\n\nCaused by:\n  File `/workspaces/training/hello-nextflow/data/greetings.csv-output.txt` is outside the scope of the process work directory: /workspaces/training/hello-nextflow/work/e3/c459b3c8f4029094cc778c89a4393d\n\n\nCommand executed:\n\n  echo '/workspaces/training/hello-nextflow/data/greetings.csv' &gt; '/workspaces/training/hello-nextflow/data/greetings.\n</code></pre> <p>The <code>Command executed:</code> bit (lines 13-15) is especially helpful here.</p> <p>Questo pu\u00f2 sembrare un po' familiare. Sembra che Nextflow abbia tentato di eseguire una singola chiamata di processo utilizzando il percorso del file stesso come valore stringa. Quindi ha risolto correttamente il percorso del file, ma in realt\u00e0 non ha analizzato il suo contenuto, che \u00e8 quello che volevamo.</p> <p>Come facciamo a fare in modo che Nextflow apra il file e carichi i suoi contenuti nel canale?</p> <p>Sembra che abbiamo bisogno di un altro operatore!</p>"},{"location":"it/hello_nextflow/02_hello_channels/#42-usa-loperatore-splitcsv-per-analizzare-il-file","title":"4.2. Usa l'operatore <code>splitCsv()</code> per analizzare il file","text":"<p>Guardando di nuovo l'elenco degli operatori, troviamo <code>splitCsv()</code>, che \u00e8 progettato per analizzare e dividere il testo in formato CSV.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#421-applica-splitcsv-al-canale","title":"4.2.1. Applica <code>splitCsv()</code> al canale","text":"<p>Per applicare l'operatore, lo assediamo alla linea di fabbrica del canale come in precedenza.</p> <p>Nel blocco del workflow, apportare la seguente modifica del codice:</p> DopoPrima hello-channels.nf<pre><code>// create a channel for inputs from a CSV file\ngreeting_ch = Channel.fromPath(params.greeting)\n                     .view { csv -&gt; \"Before splitCsv: $csv\" }\n                     .splitCsv()\n                     .view { csv -&gt; \"After splitCsv: $csv\" }\n</code></pre> hello-channels.nf<pre><code>// create a channel for inputs from a CSV file\ngreeting_ch = Channel.fromPath(params.greeting)\n</code></pre> <p>Come puoi vedere, includiamo anche le dichiarazioni prima/dopo la vista mentre ci siamo.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#422-esegui-di-nuovo-il-workflow","title":"4.2.2. Esegui di nuovo il workflow","text":"<p>Proviamo a eseguire il workflow con la logica di parsing CSV aggiunta.</p> <pre><code>nextflow run hello-channels.nf\n</code></pre> <p>\u00c8 interessante notare che anche questo fallisce, ma con un errore diverso. L'output e l'errore della console iniziano cos\u00ec:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-channels.nf` [stoic_ride] DSL2 - revision: a0e5de507e\n\nexecutor &gt;  local (3)\n[42/8fea64] sayHello (1) | 0 of 3\nBefore splitCsv: /workspaces/training/hello-nextflow/greetings.csv\nAfter splitCsv: [Hello]\nAfter splitCsv: [Bonjour]\nAfter splitCsv: [Hol\u00e0]\nERROR ~ Error executing process &gt; 'sayHello (2)'\n\nCaused by:\n  Missing output file(s) `[Bonjour]-output.txt` expected by process `sayHello (2)`\n\n\nCommand executed:\n\n  echo '[Bonjour]' &gt; '[Bonjour]-output.txt'\n</code></pre> <p>Questa volta Nextflow ha analizzato il contenuto del file (eviva!) Ma sono state aggiunte parentesi intorno ai saluti.</p> <p>Per farla breve, <code>splitCsv()</code> legge ogni riga in un array e ogni valore separato da virgole nella riga diventa un elemento nell'array. Quindi qui ci d\u00e0 tre array contenenti un elemento ciascuno.</p> <p>Note</p> <p>Anche se questo comportamento sembra scomodo in questo momento, sar\u00e0 estremamente utile in seguito quando ci occuperemo di file di input con pi\u00f9 colonne di dati.</p> <p>We could solve this by using <code>flatten()</code>, which you already know. However, there's another operator called <code>map()</code> that's more appropriate to use here and is really useful to know; it pops up a lot in Nextflow pipelines.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#43-usa-loperatore-map-per-estrarre-i-saluti","title":"4.3. Usa l'operatore <code>map()</code> per estrarre i saluti","text":"<p>L'operatore <code>map()</code> \u00e8 un piccolo strumento molto pratico che ci consente di fare tutti i tipi di mappature ai contenuti di un canale.</p> <p>In questo caso, lo useremo per estrarre quell'elemento che vogliamo da ogni riga del nostro file. Ecco come appare la sintassi:</p> Syntax<pre><code>.map { item -&gt; item[0] }\n</code></pre> <p>Ci\u00f2 significa \"per ogni elemento nel canale, prendi il primo di qualsiasi elemento che contiene\".</p> <p>Quindi applichiamolo al nostro parsing CSV.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#431-applica-map-al-canale","title":"4.3.1. Applica <code>map()</code> al canale","text":"<p>Nel blocco del workflow, apportare la seguente modifica del codice:</p> DopoPrima hello-channels.nf<pre><code>// create a channel for inputs from a CSV file\ngreeting_ch = Channel.fromPath(params.greeting)\n                     .view { csv -&gt; \"Before splitCsv: $csv\" }\n                     .splitCsv()\n                     .view { csv -&gt; \"After splitCsv: $csv\" }\n                     .map { item -&gt; item[0] }\n                     .view { csv -&gt; \"After map: $csv\" }\n</code></pre> hello-channels.nf<pre><code>// create a channel for inputs from a CSV file\ngreeting_ch = Channel.fromPath(params.greeting)\n                     .view { csv -&gt; \"Before splitCsv: $csv\" }\n                     .splitCsv()\n                     .view { csv -&gt; \"After splitCsv: $csv\" }\n</code></pre> <p>Ancora una volta includiamo un'altra chiamata <code>view()</code> per confermare che l'operatore fa ci\u00f2 che ci aspettiamo.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#432-esegui-il-workflow-ancora-una-volta","title":"4.3.2. Esegui il workflow ancora una volta","text":"<p>Eseguiamolo ancora una volta:</p> <pre><code>nextflow run hello-channels.nf\n</code></pre> <p>Questa volta dovrebbe funzionare senza errori.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-channels.nf` [tiny_heisenberg] DSL2 - revision: 845b471427\n\nexecutor &gt;  local (3)\n[1a/1d19ab] sayHello (2) | 3 of 3 \u2714\nBefore splitCsv: /workspaces/training/hello-nextflow/greetings.csv\nAfter splitCsv: [Hello]\nAfter splitCsv: [Bonjour]\nAfter splitCsv: [Hol\u00e0]\nAfter map: Hello\nAfter map: Bonjour\nAfter map: Hol\u00e0\n</code></pre> <p>Guardando l'output delle istruzioni <code>view()</code>, vediamo quanto segue:</p> <ul> <li>Una singola istruzione <code>Before splitCsv:</code>: a quel punto il canale contiene un elemento, il percorso del file originale.</li> <li>Tre istruzioni separate <code>After splitCsv:</code>: una per ogni saluto, ma ognuna \u00e8 contenuta all'interno di un array che corrisponde a quella riga nel file.</li> <li>Tre istruzioni separate <code>After map:</code>: una per ogni saluto, che ora sono elementi individuali nel canale.</li> </ul> <p>Puoi anche guardare i file di output per verificare che ogni saluto sia stato correttamente estratto ed elaborato attraverso il workflow.</p> <p>Abbiamo ottenuto lo stesso risultato di prima, ma ora abbiamo molta pi\u00f9 flessibilit\u00e0 per aggiungere pi\u00f9 elementi al canale di saluti che vogliamo elaborare modificando un file di input, senza modificare alcun codice.</p> <p>Note</p> <p>Qui abbiamo avuto tutti i saluti su una riga nel file CSV. Puoi provare ad aggiungere pi\u00f9 colonne al file CSV e vedere cosa succede; ad esempio, prova quanto segue:</p> greetings.csv<pre><code>Hello,English\nBonjour,French\nHol\u00e0,Spanish\n</code></pre> <p>Puoi anche provare a sostituire <code>.map { item -&gt; item[0] }</code> con <code>.flatten()</code> e vedere cosa succede a seconda di quante righe e colonne hai nel file di input.</p> <p>Imparerai approcci pi\u00f9 avanzati per gestire input complessi in una formazione successiva.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#conclusioni_3","title":"Conclusioni","text":"<p>Sai come usare gli operatori <code>splitCsv()</code> e <code>map()</code> per leggere in un file di valori di input e gestirli in modo appropriato.</p> <p>Pi\u00f9 in generale, hai una comprensione di base di come Nextflow utilizza canali per gestire gli input dei processi e operatori per trasformare i loro contenuti.</p>"},{"location":"it/hello_nextflow/02_hello_channels/#cosa-ce-dopo_3","title":"Cosa c'\u00e8 dopo?","text":"<p>Fai una grande pausa, hai lavorato sodo in questo! Quando sei pronto, passa alla parte 3 per imparare come aggiungere altri passaggi e collegarli insieme in un workflow corretto.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/","title":"Parte 3: Hello Workflow","text":"<p> Guarda tutta la playlist sul canale Youtube Nextflow.</p> <p> La trascrizione di questo video \u00e8 disponibile qui.</p> <p>La maggior parte dei workflow nel mondo reale coinvolge pi\u00f9 di un passaggio. In questo modulo di formazione, imparerai come connettere i processi insieme in un workflow a pi\u00f9 fasi.</p> <p>Questo ti insegner\u00e0 il modo Nextflow per ottenere i seguenti obiettivi:</p> <ol> <li>Far fluire i dati da un processo al successivo</li> <li>Raccogliere gli output da pi\u00f9 chiamate di processo in una singola chiamata</li> <li>Passare pi\u00f9 di un input a un processo</li> <li>Gestire pi\u00f9 output provenienti da un processo</li> </ol> <p>Per dimostrare ci\u00f2, continueremo a costruire sull'esempio del dominio indipendente Hello World delle Parti 1 e 2. Questa volta, apporteremo le seguenti modifiche al nostro workflow per riflettere meglio su come le persone costruiscono flussi di lavoro reali:</p> <ol> <li>Aggiungere un secondo passaggio che converte il saluto in maiuscolo.</li> <li>Aggiungere un terzo passaggio che raccoglie tutti i saluti trasformati e li scrive in un unico file.</li> <li>Aggiungere un parametro per nominare il file di output finale e passarlo come input secondario al passaggio di raccolta.</li> <li>Far s\u00ec che il passaggio di raccolta produca anche una statistica semplice su ci\u00f2 che \u00e8 stato elaborato.</li> </ol>"},{"location":"it/hello_nextflow/03_hello_workflow/#0-warmup-run-hello-workflownf","title":"0. Warmup: Run <code>hello-workflow.nf</code>","text":"<p>Useremo lo script del workflow <code>hello-workflow.nf</code> come punto di partenza. Esso \u00e8 equivalente allo script prodotto lavorando attraverso la Parte 2 di questo corso di formazione.</p> <p>Per essere sicuri che tutto funzioni, esegui lo script una volta prima di apportare qualsiasi modifica</p> <pre><code>nextflow run hello-workflow.nf\n</code></pre> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-workflow.nf` [stupefied_sammet] DSL2 - revision: b9e466930b\n\nexecutor &gt;  local (3)\n[2a/324ce6] sayHello (3) | 3 of 3 \u2714\n</code></pre> <p>Come in precedenza, troverai i file di output nella directory <code>results</code> (specificata dalla direttiva <code>publishDir</code>).</p> Directory contents<pre><code>results\n\u251c\u2500\u2500 Bonjour-output.txt\n\u251c\u2500\u2500 Hello-output.txt\n\u2514\u2500\u2500 Hol\u00e0-output.txt\n</code></pre> <p>Note</p> <p>Potrebbe esserci anche un file chiamato <code>output.txt</code> se hai lavorato attraverso la Parte 2 nello stesso ambiente.</p> <p>Se tutto ha funzionato, sei pronto per imparare come assemblare un workflow a pi\u00f9 fasi.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#1-aggiungi-un-secondo-step-al-workflow","title":"1. Aggiungi un secondo step al workflow","text":"<p>Aggiungeremo un passaggio per convertire il saluto in maiuscolo. A tal fine, dobbiamo fare tre cose:</p> <ul> <li>Definire il comando che useremo per eseguire la conversione in maiuscolo.</li> <li>Scrivere un nuovo processo che racchiuda il comando per la conversione in maiuscolo.</li> <li>Chiamare il nuovo processo nel blocco del workflow e configurarlo per prendere l'output del processo <code>sayHello()</code> come input.</li> </ul>"},{"location":"it/hello_nextflow/03_hello_workflow/#11-definire-il-comando-per-la-conversione-in-maiuscolo-e-testarlo-nel-terminale","title":"1.1. Definire il comando per la conversione in maiuscolo e testarlo nel terminale","text":"<p>Per eseguire la conversione dei saluti in maiuscolo, useremo uno strumento UNIX classico chiamato <code>tr</code> per 'text replacement' (sostituzione del testo), con la seguente sintassi:</p> Syntax<pre><code>tr '[a-z]' '[A-Z]'\n</code></pre> <p>Questa \u00e8 una sostituzione di testo molto semplice che non tiene conto delle lettere accentate, quindi ad esempio 'Hol\u00e0' diventer\u00e0 'HOL\u00e0', ma andr\u00e0 bene lo stesso per dimostrare i concetti di Nextflow, e questo \u00e8 ci\u00f2 che conta.</p> <p>Per testarlo, possiamo eseguire il comando <code>echo 'Hello World'</code> e passare il suo output al comando <code>tr</code>:</p> <pre><code>echo 'Hello World' | tr '[a-z]' '[A-Z]' &gt; UPPER-output.txt\n</code></pre> <p>Il risultato \u00e8 un file di testo chiamato <code>UPPER-output.txt</code> che contiene la versione in maiuscolo della stringa <code>Hello World</code>:</p> UPPER-output.txt<pre><code>HELLO WORLD\n</code></pre> <p>Questo \u00e8 ci\u00f2 che proveremo a fare con il nostro workflow.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#12-scrivere-il-passaggio-di-conversione-in-maiuscolo-come-un-processo-nextflow","title":"1.2. Scrivere il passaggio di conversione in maiuscolo come un processo Nextflow","text":"<p>Possiamo modellare il nostro nuovo processo sul primo, poich\u00e9 vogliamo usare gli stessi componenti.</p> <p>Aggiungi la seguente definizione del processo allo script del workflow:</p> hello-workflow.nf<pre><code>/*\n * Usa uno strumento di sostituzione del testo per convertire il saluto in maiuscolo.\n */\nprocess convertToUpper {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        path input_file\n\n    output:\n        path \"UPPER-${input_file}\"\n\n    script:\n    \"\"\"\n    cat '$input_file' | tr '[a-z]' '[A-Z]' &gt; 'UPPER-${input_file}'\n    \"\"\"\n}\n</code></pre> <p>Qui, componiamo il secondo nome del file di output in base al nome del file di input, in modo simile a quanto abbiamo fatto inizialmente per l'output del primo processo.</p> <p>Note</p> <p>Nextflow determiner\u00e0 l'ordine delle operazioni in base alla concatenazione degli input e degli output, quindi l'ordine delle definizioni dei processi nello script del flusso di lavoro non \u00e8 importante. Tuttavia, ti consigliamo di essere gentile con i tuoi collaboratori e con il futuro te stesso, e cercare di scriverle in un ordine logico per motivi di leggibilit\u00e0.\"</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#13-aggiungi-una-chiamata-al-nuovo-processo-nel-blocco-del-workflow","title":"1.3. Aggiungi una chiamata al nuovo processo nel blocco del workflow","text":"<p>Ora dobbiamo dire a Nextflow di chiamare effettivamente il processo che abbiamo appena definito.</p> <p>Nel blocco del workflow, apporta la seguente modifica al codice:</p> DopoPrima hello-workflow.nf<pre><code>    // emit a greeting\n    sayHello(greeting_ch)\n\n    // convert the greeting to uppercase\n    convertToUpper()\n}\n</code></pre> hello-workflow.nf<pre><code>    // emit a greeting\n    sayHello(greeting_ch)\n}\n</code></pre> <p>Questo non \u00e8 ancora funzionante perch\u00e9 non abbiamo specificato cosa deve essere l'input per il processo <code>convertToUpper()</code>.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#14-passare-loutput-del-primo-processo-al-secondo-processo","title":"1.4. Passare l'output del primo processo al secondo processo","text":"<p>Ora dobbiamo fare in modo che l'output del processo <code>sayHello()</code> fluisca nel processo <code>convertToUpper()</code>.</p> <p>Comodamente, Nextflow impacchetta automaticamente l'output di un processo in un canale chiamato <code>&lt;process&gt;.out</code>. Quindi, l'output del processo <code>sayHello</code> \u00e8 un canale chiamato <code>sayHello.out</code>, che possiamo collegare direttamente alla chiamata a <code>convertToUpper()</code>.</p> <p>Nel blocco del workflow, apporta la seguente modifica al codice:</p> DopoPrima hello-workflow.nf<pre><code>    // convert the greeting to uppercase\n    convertToUpper(sayHello.out)\n}\n</code></pre> hello-workflow.nf<pre><code>    // convert the greeting to uppercase\n    convertToUpper()\n}\n</code></pre> <p>Per un caso semplice come questo (un output a un input), \u00e8 tutto ci\u00f2 che dobbiamo fare per connettere due processi!</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#15-esegui-di-nuovo-il-flusso-di-lavoro-con-resume","title":"1.5. Esegui di nuovo il flusso di lavoro con <code>-resume</code>","text":"<p>Eseguiamo di nuovo il flusso di lavoro utilizzando il flag <code>-resume</code>, poich\u00e9 abbiamo gi\u00e0 eseguito con successo il primo passaggio del workflow.</p> <pre><code>nextflow run hello-workflow.nf -resume\n</code></pre> <p>Dovresti vedere il seguete output:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-workflow.nf` [disturbed_darwin] DSL2 - revision: 4e252c048f\n\nexecutor &gt;  local (3)\n[79/33b2f0] sayHello (2)       | 3 of 3, cached: 3 \u2714\n[b3/d52708] convertToUpper (3) | 3 of 3 \u2714\n</code></pre> <p>Ora c'\u00e8 una riga extra nell'output della console (riga 7), che corrisponde al nuovo processo che abbiamo appena aggiunto.</p> <p>Diamo un'occhiata all'interno della directory di lavoro di una delle chiamate al secondo processo.</p> Directory contents<pre><code>work/b3/d52708edba8b864024589285cb3445/\n\u251c\u2500\u2500 Bonjour-output.txt -&gt; /workspaces/training/hello-nextflow/work/79/33b2f0af8438486258d200045bd9e8/Bonjour-output.txt\n\u2514\u2500\u2500 UPPER-Bonjour-output.txt\n</code></pre> <p>Troviamo due file di output: l'output del primo processo e l'output del secondo.</p> <p>L'output del primo processo \u00e8 l\u00ec perch\u00e9 Nextflow lo ha messo in quella directory per avere tutto il necessario per l'esecuzione all'interno della stessa sottodirectory. Tuttavia, in realt\u00e0 si tratta di un collegamento simbolico che punta al file originale nella sottodirectory della prima chiamata al processo. Per impostazione predefinita, quando si esegue su una singola macchina, come stiamo facendo qui, Nextflow utilizza collegamenti simbolici anzich\u00e9 copie per mettere in scena i file di input e i file intermedi.</p> <p>Troverai anche i file di output finali nella directory <code>results</code>, poich\u00e9 abbiamo usato la direttiva <code>publishDir</code> anche nel secondo processo.</p> Directory contents<pre><code>results\n\u251c\u2500\u2500 Bonjour-output.txt\n\u251c\u2500\u2500 Hello-output.txt\n\u251c\u2500\u2500 Hol\u00e0-output.txt\n\u251c\u2500\u2500 UPPER-Bonjour-output.txt\n\u251c\u2500\u2500 UPPER-Hello-output.txt\n\u2514\u2500\u2500 UPPER-Hol\u00e0-output.txt\n</code></pre> <p>Pensiamo a come tutto ci\u00f2 che abbiamo fatto \u00e8 stato connettere l'output di <code>sayHello</code> all'input di <code>convertToUpper</code> e i due processi potrebbero essere eseguiti in serie. Nextflow ha fatto il lavoro difficile di gestire i singoli file di input e output e passarli tra i due comandi per noi.</p> <p>Questa \u00e8 una delle ragioni per cui i canali di Nextflow sono cos\u00ec potenti: si occupano del lavoro noioso coinvolto nel connettere i passaggi del workflow.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#conclusioni","title":"Conclusioni","text":"<p>Ora sai come aggiungere un secondo passaggio che prende l'output del primo passaggio come input.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#cosa-ce-dopo","title":"Cosa c'\u00e8 dopo?","text":"<p>Impara come raccogliere gli output da chiamate di processo in batch e passarli a un singolo processo.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#2-aggiungi-un-terzo-passo-per-raccogliere-tutti-i-saluti","title":"2. Aggiungi un terzo passo per raccogliere tutti i saluti","text":"<p>Quando utilizziamo un processo per applicare una trasformazione a ciascuno degli elementi di un canale, come stiamo facendo qui con i saluti multipli, a volte vogliamo raccogliere gli elementi dal canale di output di quel processo e passarli a un altro processo che esegue una sorta di analisi o somma.</p> <p>Nel prossimo passo scriveremo semplicemente tutti gli elementi di un canale in un singolo file, utilizzando il comando UNIX <code>cat</code>.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#21-definisci-il-comando-di-raccolta-e-testalo-nel-terminale","title":"2.1. Definisci il comando di raccolta e testalo nel terminale","text":"<p>Il passo di raccolta che vogliamo aggiungere al nostro flusso di lavoro utilizzer\u00e0 il comando <code>cat</code> per concatenare i saluti in maiuscolo in un unico file.</p> <p>Eseguiamo il comando da solo nel terminale per verificare che funzioni come previsto, proprio come abbiamo fatto in precedenza.</p> <p>Esegui il seguente comando nel tuo terminale:</p> <pre><code>echo 'Hello' | tr '[a-z]' '[A-Z]' &gt; UPPER-Hello-output.txt\necho 'Bonjour' | tr '[a-z]' '[A-Z]' &gt; UPPER-Bonjour-output.txt\necho 'Hol\u00e0' | tr '[a-z]' '[A-Z]' &gt; UPPER-Hol\u00e0-output.txt\ncat UPPER-Hello-output.txt UPPER-Bonjour-output.txt UPPER-Hol\u00e0-output.txt &gt; COLLECTED-output.txt\n</code></pre> <p>L'output \u00e8 un file di testo chiamato <code>COLLECTED-output.txt</code> che contiene le versioni in maiuscolo dei saluti originali.</p> COLLECTED-output.txt<pre><code>HELLO\nBONJOUR\nHOL\u00e0\n</code></pre> <p>Questo \u00e8 il risultato che vogliamo ottenere con il nostro workflow.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#22-crea-un-nuovo-processo-per-eseguire-il-passo-di-raccolta","title":"2.2. Crea un nuovo processo per eseguire il passo di raccolta","text":"<p>Creiamo un nuovo processo e chiamamolo <code>collectGreetings()</code>. Possiamo iniziare a scriverlo basandoci su quello precedente.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#221-scrivi-le-parti-ovvie-del-processo","title":"2.2.1. Scrivi le parti 'ovvie' del processo","text":"<p>Aggiungi la seguente definizione del processo allo script del workflow:</p> hello-workflow.nf<pre><code>/*\n * Raccogli i saluti in maiuscolo in un unico file di output.\n */\nprocess collectGreetings {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        ???\n\n    output:\n        path \"COLLECTED-output.txt\"\n\n    script:\n    \"\"\"\n    ??? &gt; 'COLLECTED-output.txt'\n    \"\"\"\n}\n</code></pre> <p>Questo \u00e8 ci\u00f2 che possiamo scrivere con sicurezza in base a quanto appreso finora. Ma non \u00e8 funzionale! Manca la definizione dell'input e la prima met\u00e0 del comando dello script perch\u00e9 dobbiamo capire come scrivere quella parte.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#222-definire-gli-input-per-collectgreetings","title":"2.2.2. Definire gli input per <code>collectGreetings()</code>","text":"<p>Dobbiamo raccogliere i saluti da tutte le chiamate al processo <code>convertToUpper()</code>. Cosa sappiamo che possiamo ottenere dal passo precedente nel workflow?</p> <p>Il canale di output di <code>convertToUpper()</code> conterr\u00e0 i percorsi dei singoli file che contengono i saluti in maiuscolo. Questo corrisponde a uno slot di input; chiamiamolo <code>input_files</code> per semplicit\u00e0.</p> <p>Nel blocco del processo, apporta la seguente modifica al codice:</p> DopoPrima hello-workflow.nf<pre><code>        input:\n            path input_files\n</code></pre> hello-workflow.nf<pre><code>        input:\n            ???\n</code></pre> <p>Nota che usiamo il prefisso <code>path</code> anche se ci aspettiamo che questo contenga pi\u00f9 file. Nextflow non ha problemi con questo, quindi non importa.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#223-comporre-il-comando-di-concatenazione","title":"2.2.3. Comporre il comando di concatenazione","text":"<p>Qui le cose potrebbero diventare un po' complicate, perch\u00e9 dobbiamo essere in grado di gestire un numero arbitrario di file di input. In particolare, non possiamo scrivere il comando in anticipo, quindi dobbiamo dire a Nextflow come comporlo in fase di esecuzione in base agli input che fluiscono nel processo.</p> <p>In altre parole, se abbiamo un canale di input che contiene l'elemento <code>[file1.txt, file2.txt, file3.txt]</code>, dobbiamo fare in modo che Nextflow lo trasformi in <code>cat file1.txt file2.txt file3.txt</code>.</p> <p>Fortunatamente, Nextflow \u00e8 abbastanza felice di farlo per noi se scriviamo semplicemente <code>cat ${input_files}</code> nel comando dello script.</p> <p>Nel blocco del processo, apporta la seguente modifica al codice:</p> DopoPrima hello-workflow.nf<pre><code>    script:\n    \"\"\"\n    cat ${input_files} &gt; 'COLLECTED-output.txt'\n    \"\"\"\n</code></pre> hello-workflow.nf<pre><code>    script:\n    \"\"\"\n    ??? &gt; 'COLLECTED-output.txt'\n    \"\"\"\n</code></pre> <p>In teoria, questo dovrebbe gestire qualsiasi numero arbitrario di file di input.</p> <p>Tip</p> <p>Alcuni strumenti da riga di comando richiedono di fornire un argomento (come <code>-input</code>) per ogni file di input. In tal caso, dovremmo fare un po' di lavoro extra per comporre il comando. Puoi vedere un esempio di questo nel corso di formazione Nextflow for Genomics.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#23-aggiungi-il-passo-di-raccolta-al-workflow","title":"2.3. Aggiungi il passo di raccolta al workflow","text":"<p>Ora dovremmo semplicemente chiamare il processo di raccolta sull'output del passo di trasformazione in maiuscolo.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#231-collega-le-chiamate-ai-processi","title":"2.3.1. Collega le chiamate ai processi","text":"<p>Nel blocco del workflow, apporta la seguente modifica al codice:</p> DopoPrima hello-workflow.nf<pre><code>    // convert the greeting to uppercase\n    convertToUpper(sayHello.out)\n\n    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out)\n}\n</code></pre> hello-workflow.nf<pre><code>    // convert the greeting to uppercase\n    convertToUpper(sayHello.out)\n}\n</code></pre> <p>Questo collega l'output di <code>convertToUpper()</code> all'input di <code>collectGreetings()</code>.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#232-esegui-il-workflow-con-resume","title":"2.3.2. Esegui il workflow con <code>-resume</code>","text":"<p>Proviamolo.</p> <pre><code>nextflow run hello-workflow.nf -resume\n</code></pre> <p>Viene eseguito con successo, compreso il terzo passo:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-workflow.nf` [mad_gilbert] DSL2 - revision: 6acfd5e28d\n\nexecutor &gt;  local (3)\n[79/33b2f0] sayHello (2)         | 3 of 3, cached: 3 \u2714\n[99/79394f] convertToUpper (3)   | 3 of 3, cached: 3 \u2714\n[47/50fe4a] collectGreetings (1) | 3 of 3 \u2714\n</code></pre> <p>Tuttavia, guarda il numero di chiamate per collectGreetings() alla riga 8. Ce ne aspettavamo solo una, ma ce ne sono tre.</p> <p>E dai un'occhiata anche ai contenuti del file di output finale:</p> results/COLLECTED-output.txt<pre><code>Hol\u00e0\n</code></pre> <p>Oh no. Il passo di raccolta \u00e8 stato eseguito singolarmente su ogni saluto, il che NON \u00e8 quello che volevamo.</p> <p>Dobbiamo fare qualcosa per dire esplicitamente a Nextflow che vogliamo che quel terzo passo venga eseguito su tutti gli elementi nel canale di output di <code>convertToUpper()</code>.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#24-usa-un-operatore-per-raccogliere-i-saluti-in-un-unico-input","title":"2.4. Usa un operatore per raccogliere i saluti in un unico input","text":"<p>S\u00ec, ancora una volta la risposta al nostro problema \u00e8 un operatore.</p> <p>In particolare, utilizzeremo l'operatore opportunamente chiamato [<code>collect()</code>] (https://www.nextflow.io/docs/latest/reference/operator.html#collect).</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#241-aggiungi-loperatore-collect","title":"2.4.1. Aggiungi l'operatore <code>collect()</code>","text":"<p>Questa volta sar\u00e0 un po' diverso perch\u00e9 non stiamo aggiungendo l'operatore nel contesto di una fabbrica di canali, ma a un canale di output.</p> <p>Prendiamo <code>convertToUpper.out</code> e aggiungiamo l'operatore <code>collect()</code>, che diventa <code>convertToUpper.out.collect()</code>. Possiamo collegarlo direttamente alla chiamata del processo <code>collectGreetings()</code>.</p> <p>Nel blocco del workflow, apporta la seguente modifica al codice:</p> DopoPrima hello-workflow.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect())\n}\n</code></pre> hello-workflow.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out)\n}\n</code></pre>"},{"location":"it/hello_nextflow/03_hello_workflow/#242-aggiungi-alcune-dichiarazioni-view","title":"2.4.2. Aggiungi alcune dichiarazioni <code>view()</code>","text":"<p>Includiamo anche un paio di dichiarazioni <code>view()</code> per visualizzare lo stato prima e dopo dei contenuti del canale.</p> DopoPrima hello-workflow.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect())\n\n    // optional view statements\n    convertToUpper.out.view { greeting -&gt; \"Before collect: $greeting\" }\n    convertToUpper.out.collect().view { greeting -&gt; \"After collect: $greeting\" }\n}\n</code></pre> hello-workflow.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect())\n}\n</code></pre> <p>Le dichiarazioni <code>view()</code> possono essere posizionate dove vuoi; noi le abbiamo messe dopo la chiamata per migliorare la leggibilit\u00e0.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#243-esegui-di-nuovo-il-workflow-con-resume","title":"2.4.3. Esegui di nuovo il workflow con <code>-resume</code>","text":"<p>Proviamolo:</p> <pre><code>nextflow run hello-workflow.nf -resume\n</code></pre> <p>Viene eseguito con successo, anche se l'output del log potrebbe sembrare un po' pi\u00f9 disordinato di cos\u00ec (l'abbiamo ripulito per migliorare la leggibilit\u00e0).</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-workflow.nf` [soggy_franklin] DSL2 - revision: bc8e1b2726\n\n[d6/cdf466] sayHello (1)       | 3 of 3, cached: 3 \u2714\n[99/79394f] convertToUpper (2) | 3 of 3, cached: 3 \u2714\n[1e/83586c] collectGreetings   | 1 of 1 \u2714\nBefore collect: /workspaces/training/hello-nextflow/work/b3/d52708edba8b864024589285cb3445/UPPER-Bonjour-output.txt\nBefore collect: /workspaces/training/hello-nextflow/work/99/79394f549e3040dfc2440f69ede1fc/UPPER-Hello-output.txt\nBefore collect: /workspaces/training/hello-nextflow/work/aa/56bfe7cf00239dc5badc1d04b60ac4/UPPER-Hol\u00e0-output.txt\nAfter collect: [/workspaces/training/hello-nextflow/work/b3/d52708edba8b864024589285cb3445/UPPER-Bonjour-output.txt, /workspaces/training/hello-nextflow/work/99/79394f549e3040dfc2440f69ede1fc/UPPER-Hello-output.txt, /workspaces/training/hello-nextflow/work/aa/56bfe7cf00239dc5badc1d04b60ac4/UPPER-Hol\u00e0-output.txt]\n</code></pre> <p>Questa volta il terzo passo \u00e8 stato chiamato solo una volta!</p> <p>Guardando l'output delle dichiarazioni <code>view()</code>, vediamo quanto segue:</p> <ul> <li>Tre dichiarazioni <code>Before collect:</code>, una per ciascun saluto: a quel punto i percorsi dei file sono elementi individuali nel canale.</li> <li>Una sola dichiarazione <code>After collect:</code>: i tre percorsi dei file sono ora raggruppati in un singolo elemento.</li> </ul> <p>Dai un'occhiata anche ai contenuti del file di output finale:</p> results/COLLECTED-output.txt<pre><code>BONJOUR\nHELLO\nHOL\u00e0\n</code></pre> <p>Questa volta abbiamo tutti e tre i saluti nel file di output finale. Successo! Rimuovi le chiamate <code>view</code> opzionali per rendere gli output successivi meno verbosi.</p> <p>Note</p> <p>Se esegui questo processo pi\u00f9 volte senza <code>-resume</code>, vedrai che l'ordine dei saluti cambia da un'esecuzione all'altra. Questo ti mostra che l'ordine in cui gli elementi fluiscono attraverso le chiamate ai processi non \u00e8 garantito essere consistente.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#conclusione","title":"Conclusione","text":"<p>Ora sai come raccogliere gli output da un gruppo di chiamate ai processi e passarli a un passo di analisi o somma congiunta.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#cosa-ce-dopo_1","title":"Cosa c'\u00e8 dopo?","text":"<p>Impara come passare pi\u00f9 di un input a un processo.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#3-passare-piu-di-un-input-a-un-processo-per-nominare-in-modo-univoco-il-file-di-output-finale","title":"3. Passare pi\u00f9 di un input a un processo per nominare in modo univoco il file di output finale","text":"<p>Vogliamo essere in grado di dare al file di output finale un nome specifico, in modo da poter elaborare successivi lotti di saluti senza sovrascrivere i risultati finali.</p> <p>A tal fine, apporteremo le seguenti modifiche al workflow:</p> <ul> <li>Modificare il processo di raccolta per accettare un nome definito dall'utente per il file di output</li> <li>Aggiungere un parametro da riga di comando al workflow e passarne il valore al processo di raccolta</li> </ul>"},{"location":"it/hello_nextflow/03_hello_workflow/#31-modificare-il-processo-di-raccolta-per-accettare-un-nome-definito-dallutente-per-il-file-di-output","title":"3.1. Modificare il processo di raccolta per accettare un nome definito dall'utente per il file di output","text":"<p>Dobbiamo dichiarare l'input aggiuntivo e integrarlo nel nome del file di output.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#311-dichiarare-linput-aggiuntivo-nella-definizione-del-processo","title":"3.1.1. Dichiarare l'input aggiuntivo nella definizione del processo","text":"<p>Buone notizie: possiamo dichiarare tutte le variabili di input che vogliamo. Chiamiamo questa <code>batch_name</code>.</p> <p>Nel blocco del processo, apportiamo la seguente modifica al codice:</p> DopoPrima hello-workflow.nf<pre><code>    input:\n        path input_files\n        val batch_name\n</code></pre> hello-workflow.nf<pre><code>    input:\n        path input_files\n</code></pre> <p>Puoi configurare i tuoi processi per aspettarsi quanti input desideri. Pi\u00f9 avanti, imparerai come gestire gli input obbligatori e opzionali.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#312-utilizzare-la-variabile-batch_name-nel-nome-del-file-di-output","title":"3.1.2. Utilizzare la variabile <code>batch_name</code> nel nome del file di output","text":"<p>Nel blocco del processo, apporta la seguente modifica al codice:</p> DopoPrima hello-workflow.nf<pre><code>    output:\n        path \"COLLECTED-${batch_name}-output.txt\"\n\n    script:\n    \"\"\"\n    cat ${input_files} &gt; 'COLLECTED-${batch_name}-output.txt'\n    \"\"\"\n</code></pre> hello-workflow.nf<pre><code>    output:\n        path \"COLLECTED-output.txt\"\n\n    script:\n    \"\"\"\n    cat ${input_files} &gt; 'COLLECTED-output.txt'\n    \"\"\"\n</code></pre> <p>Questo configura il processo per utilizzare il valore di <code>batch_name</code> per generare un nome file specifico per il file di output finale del workflow.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#32-aggiungere-un-parametro-batch-da-riga-di-comando","title":"3.2. Aggiungere un parametro <code>batch</code> da riga di comando","text":"<p>Ora abbiamo bisogno di un modo per fornire il valore di <code>batch_name</code> e passarne il valore alla chiamata del processo.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#321-utilizzare-params-per-configurare-il-parametro","title":"3.2.1. Utilizzare <code>params</code> per configurare il parametro","text":"<p>Sai gi\u00e0 come utilizzare il sistema <code>params</code> per dichiarare i parametri CLI. Usiamo questo sistema per dichiarare un parametro <code>batch</code> (con un valore predefinito, perch\u00e9 siamo pigri).</p> <p>Nella sezione dei parametri della pipeline, apporta le seguenti modifiche al codice:</p> DopoPrima hello-workflow.nf<pre><code>/*\n * Pipeline parameters\n */\nparams.greeting = 'greetings.csv'\nparams.batch = 'test-batch'\n</code></pre> hello-workflow.nf<pre><code>/*\n * Pipeline parameters\n */\nparams.greeting = 'greetings.csv'\n</code></pre> <p>Ricorda che puoi sovrascrivere il valore predefinito specificando un valore con <code>--batch</code> sulla riga di comando.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#322-passare-il-parametro-batch-al-processo","title":"3.2.2. Passare il parametro <code>batch</code> al processo","text":"<p>Per fornire il valore del parametro al processo, dobbiamo aggiungerlo nella chiamata del processo.</p> <p>Nel blocco del workflow, apporta la seguente modifica al codice:</p> DopoPrima hello-workflow.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect(), params.batch)\n</code></pre> hello-workflow.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect())\n</code></pre> <p>Warning</p> <p>Devi fornire gli input a un processo NELLO STESSO ORDINE ESATTO in cui sono elencati nel blocco di definizione degli input del processo.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#33-eseguire-il-workflow","title":"3.3. Eseguire il workflow","text":"<p>Proviamo a eseguire questo con un nome di batch sulla riga di comando.</p> <pre><code>nextflow run hello-workflow.nf -resume --batch trio\n</code></pre> <p>Viene eseguito con successo:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-workflow.nf` [confident_rutherford] DSL2 - revision: bc58af409c\n\nexecutor &gt;  local (1)\n[79/33b2f0] sayHello (2)       | 3 of 3, cached: 3 \u2714\n[99/79394f] convertToUpper (2) | 3 of 3, cached: 3 \u2714\n[b5/f19efe] collectGreetings   | 1 of 1 \u2714\n</code></pre> <p>E produce l'output desiderato:</p> bash<pre><code>cat results/COLLECTED-trio-output.txt\n</code></pre> Output<pre><code>HELLO\nBONJOUR\nHOL\u00e0\n</code></pre> <p>Ora, le esecuzioni successive su altri lotti di input non sovrascriveranno i risultati precedenti (purch\u00e9 specifichiamo correttamente il parametro).</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#cosa-devi-ricordare","title":"Cosa devi ricordare","text":"<p>Ora sai come passare pi\u00f9 di un input a un processo.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#cosa-succede-dopo","title":"Cosa succede dopo?","text":"<p>Impara come emettere pi\u00f9 output e gestirli comodamente.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#4-aggiungere-un-output-al-passo-di-raccolta","title":"4. Aggiungere un output al passo di raccolta","text":"<p>Quando un processo produce un solo output, \u00e8 facile accedervi (nel blocco del workflow) utilizzando la sintassi <code>&lt;process&gt;.out</code>. Quando ci sono due o pi\u00f9 output, il metodo predefinito per selezionare un output specifico \u00e8 usare l'indice corrispondente (basato su zero); per esempio, useresti <code>&lt;process&gt;.out[0]</code> per ottenere il primo output. Questo per\u00f2 non \u00e8 particolarmente comodo, perch\u00e9 \u00e8 facile selezionare l'indice sbagliato.</p> <p>Vediamo come possiamo selezionare e utilizzare un output specifico di un processo quando ce ne sono pi\u00f9 di uno.</p> <p>Per scopi dimostrativi, supponiamo che vogliamo contare e segnalare il numero di saluti che vengono raccolti per un dato lotto di input.</p> <p>A tal fine, apporteremo le seguenti modifiche al workflow:</p> <ul> <li>Modificare il processo per contare e produrre il numero di saluti</li> <li>Una volta che il processo \u00e8 stato eseguito, selezionare il conteggio e riportarlo utilizzando <code>view</code> (nel blocco del workflow)</li> </ul>"},{"location":"it/hello_nextflow/03_hello_workflow/#41-modificare-il-processo-per-contare-e-produrre-il-numero-di-saluti","title":"4.1. Modificare il processo per contare e produrre il numero di saluti","text":"<p>Questo richieder\u00e0 due modifiche principali alla definizione del processo: dobbiamo trovare un modo per contare i saluti, poi dobbiamo aggiungere quel conteggio al blocco <code>output</code> del processo.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#411-contare-il-numero-di-saluti-raccolti","title":"4.1.1. Contare il numero di saluti raccolti","text":"<p>Fortunatamente, Nextflow ci consente di aggiungere codice arbitrario nel blocco <code>script:</code> della definizione del processo, che risulta molto utile per fare cose come questa.</p> <p>Ci\u00f2 significa che possiamo utilizzare la funzione incorporata <code>size()</code> per ottenere il numero di file nell'array <code>input_files</code>.</p> <p>Nel blocco del processo <code>collectGreetings</code>, apporta la seguente modifica al codice:</p> DopoPrima hello-workflow.nf<pre><code>    script:\n        count_greetings = input_files.size()\n    \"\"\"\n    cat ${input_files} &gt; 'COLLECTED-${batch_name}-output.txt'\n    \"\"\"\n</code></pre> hello-workflow.nf<pre><code>    script:\n    \"\"\"\n    cat ${input_files} &gt; 'COLLECTED-${batch_name}-output.txt'\n    \"\"\"\n</code></pre> <p>La variabile <code>count_greetings</code> verr\u00e0 calcolata durante l'esecuzione.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#412-emissione-del-conteggio-come-output-con-nome","title":"4.1.2. Emissione del conteggio come output con nome","text":"<p>In linea di principio, tutto ci\u00f2 che dobbiamo fare \u00e8 aggiungere la variabile <code>count_greetings</code> al blocco <code>output:</code>.</p> <p>Tuttavia, mentre ci siamo, aggiungeremo anche alcune etichette <code>emit:</code> alle nostre dichiarazioni di output. Queste ci permetteranno di selezionare gli output per nome, invece di dover usare indici posizionali.</p> <p>Nel blocco del processo, apporta la seguente modifica al codice:</p> DopoPrima hello-workflow.nf<pre><code>    output:\n        path \"COLLECTED-${batch_name}-output.txt\" , emit: outfile\n        val count_greetings , emit: count\n</code></pre> hello-workflow.nf<pre><code>    output:\n        path \"COLLECTED-${batch_name}-output.txt\"\n</code></pre> <p>Le etichette <code>emit:</code> sono opzionali, e avremmo potuto aggiungere un'etichetta solo a uno degli output. Ma, come si suol dire, perch\u00e9 non entrambi?</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#42-segnalare-loutput-alla-fine-del-workflow","title":"4.2. Segnalare l'output alla fine del workflow","text":"<p>Ora che abbiamo due output provenienti dal processo <code>collectGreetings</code>, l'output <code>collectGreetings.out</code> contiene due canali:</p> <ul> <li><code>collectGreetings.out.outfile</code> contiene il file di output finale</li> <li><code>collectGreetings.out.count</code> contiene il conteggio dei saluti</li> </ul> <p>Potremmo inviare uno o entrambi questi output a un altro processo per ulteriori elaborazioni. Tuttavia, per concludere il lavoro, useremo semplicemente <code>view()</code> per dimostrare che possiamo accedere e segnalare il conteggio dei saluti.</p> <p>Nel blocco del workflow, apporta la seguente modifica al codice:</p> DopoPrima hello-workflow.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect(), params.batch)\n\n    // emit a message about the size of the batch\n    collectGreetings.out.count.view { num_greetings -&gt; \"There were $num_greetings greetings in this batch\" }\n</code></pre> hello-workflow.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect(), params.batch)\n</code></pre> <p>Note</p> <p>Esistono altri modi per ottenere un risultato simile, inclusi alcuni pi\u00f9 eleganti, come l'operatore <code>count()</code>, ma questo ci permette di mostrare come gestire pi\u00f9 output, che \u00e8 ci\u00f2 che ci interessa.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#43-eseguire-il-workflow","title":"4.3. Eseguire il workflow","text":"<p>Proviamo a eseguire questo con l'attuale lotto di saluti.</p> <pre><code>nextflow run hello-workflow.nf -resume --batch trio\n</code></pre> <p>Viene eseguito con successo:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-workflow.nf` [evil_sinoussi] DSL2 - revision: eeca64cdb1\n\n[d6/cdf466] sayHello (1)       | 3 of 3, cached: 3 \u2714\n[99/79394f] convertToUpper (2) | 3 of 3, cached: 3 \u2714\n[9e/1dfda7] collectGreetings   | 1 of 1, cached: 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>L'ultima riga (riga 8) mostra che abbiamo correttamente recuperato il conteggio dei saluti elaborati. Sentiti libero di aggiungere pi\u00f9 saluti al CSV e vedere cosa succede.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#cosa-devi-ricordare_1","title":"Cosa devi ricordare","text":"<p>Ora sai come far emettere a un processo un output con nome e come accedervi dal blocco del workflow.</p> <p>Pi\u00f9 in generale, comprendi i principi chiave per connettere i processi in modi comuni.</p>"},{"location":"it/hello_nextflow/03_hello_workflow/#cosa-succede-dopo_1","title":"Cosa succede dopo?","text":"<p>Fai una lunga pausa, te la sei guadagnata. Quando sei pronto, passa alla Parte 4 per imparare a modularizzare il tuo codice per una migliore manutenibilit\u00e0 e efficienza del codice.</p>"},{"location":"it/hello_nextflow/04_hello_modules/","title":"Parte 4: Hello Modules","text":"<p> Guarda tutta la playlist sul canale Youtube Nextflow.</p> <p> La trascrizione di questo video \u00e8 disponibile qui.</p> <p>Questa sezione spiega come organizzare il codice del workflow per rendere pi\u00f9 efficiente e sostenibile lo sviluppo e la manutenzione della pipeline. In particolare, dimostreremo come utilizzare i moduli.</p> <p>In Nextflow, un modulo \u00e8 una singola definizione di processo incapsulata in un file di codice indipendente. Per utilizzare un modulo in un workflow, basta aggiungere una singola riga di dichiarazione di importazione al file di codice del workflow; quindi \u00e8 possibile integrare il processo nel workflow come si farebbe normalmente.</p> <p>Quando abbiamo iniziato a sviluppare il nostro workflow, abbiamo inserito tutto in un unico file di codice.</p> <p>La suddivisione dei processi in singoli moduli consente di riutilizzare le definizioni dei processi in pi\u00f9 workflow senza produrre pi\u00f9 copie del codice. Questo rende il codice pi\u00f9 condivisibile, flessibile e manutenibile.</p> <p>Note</p> <p>\u00c8 anche possibile incapsulare una sezione di un workflow come un \"sottoflusso\" che pu\u00f2 essere importato in una pipeline pi\u00f9 ampia, ma ci\u00f2 esula dallo scopo di questo corso.</p>"},{"location":"it/hello_nextflow/04_hello_modules/#0-riscaldamento-eseguire-hello-modulesnf","title":"0. Riscaldamento: Eseguire <code>hello-modules.nf</code>","text":"<p>Utilizzeremo lo script del workflow <code>hello-modules.nf</code> come punto di partenza. \u00c8 equivalente allo script prodotto lavorando alla Parte 3 di questo corso di formazione.</p> <p>Per assicurarsi che tutto funzioni, eseguire lo script una volta prima di apportare qualsiasi modifica:</p> <pre><code>nextflow run hello-modules.nf\n</code></pre> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-modules.nf` [festering_nobel] DSL2 - revision: eeca64cdb1\n\nexecutor &gt;  local (7)\n[25/648bdd] sayHello (2)       | 3 of 3 \u2714\n[60/bc6831] convertToUpper (1) | 3 of 3 \u2714\n[1a/bc5901] collectGreetings   | 1 of 1 \u2714\nthere were 3 greetings in this batch\n</code></pre> <p>Come in precedenza, i file di output si trovano nella directory <code>results</code> (specificata dalla direttiva <code>publishDir</code>).</p> Directory contents<pre><code>results\n\u251c\u2500\u2500 Bonjour-output.txt\n\u251c\u2500\u2500 COLLECTED-output.txt\n\u251c\u2500\u2500 COLLECTED-test-batch-output.txt\n\u251c\u2500\u2500 COLLECTED-trio-output.txt\n\u251c\u2500\u2500 Hello-output.txt\n\u251c\u2500\u2500 Hol\u00e0-output.txt\n\u251c\u2500\u2500 UPPER-Bonjour-output.txt\n\u251c\u2500\u2500 UPPER-Hello-output.txt\n\u2514\u2500\u2500 UPPER-Hol\u00e0-output.txt\n</code></pre> <p>Note</p> <p>Potrebbe anche essere rimasto un file chiamato <code>output.txt</code> se si \u00e8 lavorato alla Parte 2 nello stesso ambiente.</p> <p>Se questo ha funzionato, siete pronti per imparare a modularizzare il codice del workflow.</p>"},{"location":"it/hello_nextflow/04_hello_modules/#1-creare-una-cartella-per-memorizzare-i-moduli","title":"1. Creare una cartella per memorizzare i moduli","text":"<p>\u00c8 buona norma memorizzare i moduli in una directory specifica. Si pu\u00f2 chiamare questa cartella come si vuole, ma la convenzione \u00e8 di chiamarla <code>modules/</code>.</p> <pre><code>mkdir modules\n</code></pre> <p>Note</p> <p>Qui mostriamo come usare i moduli locali, cio\u00e8 i moduli memorizzati localmente nello stesso repository del resto del codice del workflow, in contrasto con i moduli remoti, che sono memorizzati in altri repository (remoti). Per maggiori informazioni sui moduli remoti, si veda la documentazione.</p>"},{"location":"it/hello_nextflow/04_hello_modules/#2-creare-un-modulo-per-sayhello","title":"2. Creare un modulo per <code>sayHello()</code>","text":"<p>Nella sua forma pi\u00f9 semplice, trasformare un processo esistente in un modulo \u00e8 poco pi\u00f9 di un'operazione di copia-incolla. Creeremo un file stub per il modulo, copieremo il codice pertinente e lo cancelleremo dal file principale del workflow.</p> <p>A questo punto, baster\u00e0 aggiungere una dichiarazione di importazione, in modo che Nextflow sappia che deve inserire il codice in questione in fase di esecuzione.</p>"},{"location":"it/hello_nextflow/04_hello_modules/#21-creare-un-file-stub-per-il-nuovo-modulo","title":"2.1. Creare un file stub per il nuovo modulo","text":"<p>Creiamo un file vuoto per il modulo, chiamato <code>sayHello.nf</code>.</p> <pre><code>touch modules/sayHello.nf\n</code></pre> <p>Questo ci d\u00e0 un posto dove mettere il codice del processo.</p>"},{"location":"it/hello_nextflow/04_hello_modules/#22-spostare-il-codice-del-processo-sayhello-nel-file-del-modulo","title":"2.2. Spostare il codice del processo `sayHello' nel file del modulo","text":"<p>Copiare l'intera definizione del processo dal file del workflow al file del modulo, assicurandosi di copiare anche lo shebang <code>#!/usr/bin/env nextflow</code>.</p> modules/sayHello.nf<pre><code>#!/usr/bin/env nextflow\n\n/*\n * Usa echo per stampare 'Hello World!' A un file.\n */\nprocess sayHello {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        val greeting\n\n    output:\n        path \"${greeting}-output.txt\"\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; '$greeting-output.txt'\n    \"\"\"\n}\n</code></pre> <p>Una volta fatto ci\u00f2, eliminate la definizione del processo dal file del workflow, ma assicuratevi di lasciare lo shebang al suo posto.</p>"},{"location":"it/hello_nextflow/04_hello_modules/#23-aggiungere-una-dichiarazione-di-importazione-prima-del-blocco-del-workflow","title":"2.3. Aggiungere una dichiarazione di importazione prima del blocco del workflow","text":"<p>La sintassi per importare un modulo locale \u00e8 abbastanza semplice:</p> Syntax: Import declaration<pre><code>include { &lt;MODULE_NAME&gt; } from '&lt;path_to_module&gt;'\n</code></pre> <p>Inseriamo questo blocco sopra il blocco del workflow e compiliamolo in modo appropriato.</p> DopoPrima hello-modules.nf<pre><code>// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\n\nworkflow {\n</code></pre> hello-modules.nf<pre><code>workflow {\n</code></pre>"},{"location":"it/hello_nextflow/04_hello_modules/#24-eseguite-il-workflow-per-verificare-che-faccia-la-stessa-cosa-di-prima","title":"2.4. Eseguite il workflow per verificare che faccia la stessa cosa di prima","text":"<p>Stiamo eseguendo il workflow essenzialmente con lo stesso codice e gli stessi input di prima, quindi eseguiamolo con il flag <code>resume</code> e vediamo cosa succede.</p> <pre><code>nextflow run hello-modules.nf -resume\n</code></pre> <p>L'esecuzione \u00e8 molto rapida perch\u00e9 tutto viene memorizzato nella cache.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-modules.nf` [romantic_poisson] DSL2 - revision: 96edfa9ad3\n\n[f6/cc0107] sayHello (1)       | 3 of 3, cached: 3 \u2714\n[3c/4058ba] convertToUpper (2) | 3 of 3, cached: 3 \u2714\n[1a/bc5901] collectGreetings   | 1 of 1, cached: 1 \u2714\nthere were 3 greetings in this batch\n</code></pre> <p>Nextflow ha riconosciuto che il lavoro da fare \u00e8 sempre lo stesso, anche se il codice \u00e8 suddiviso in pi\u00f9 file.</p>"},{"location":"it/hello_nextflow/04_hello_modules/#conclusione","title":"Conclusione","text":"<p>Sapete come estrarre un processo in un modulo locale e sapete che questo non rompe la riprendibilit\u00e0 del workflow.</p>"},{"location":"it/hello_nextflow/04_hello_modules/#cosa-ce-dopo","title":"Cosa c'\u00e8 dopo?","text":"<p>Esercitatevi a creare altri moduli. Una volta che ne avete fatto uno, potete farne un milione... Ma per ora facciamone solo altri due.</p>"},{"location":"it/hello_nextflow/04_hello_modules/#3-modularizzare-il-processo-converttoupper","title":"3. Modularizzare il processo <code>convertToUpper()</code>","text":""},{"location":"it/hello_nextflow/04_hello_modules/#31-creare-un-file-stub-per-il-nuovo-modulo","title":"3.1. Creare un file stub per il nuovo modulo","text":"<p>Creare un file vuoto per il modulo, chiamato <code>convertToUpper.nf</code>.</p> <pre><code>touch modules/convertToUpper.nf\n</code></pre>"},{"location":"it/hello_nextflow/04_hello_modules/#32-spostare-il-codice-del-processo-converttoupper-nel-file-del-modulo","title":"3.2. Spostare il codice del processo <code>convertToUpper</code> nel file del modulo","text":"<p>Copiare l'intera definizione del processo dal file del workflow al file del modulo, assicurandosi di copiare anche lo shebang <code>#!/usr/bin/env nextflow</code>.</p> modules/convertToUpper.nf<pre><code>#!/usr/bin/env nextflow\n\n/*\n * Utilizzare uno strumento di sostituzione del testo per convertire il saluto in maiuscolo.\n */\nprocess convertToUpper {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        path input_file\n\n    output:\n        path \"UPPER-${input_file}\"\n\n    script:\n    \"\"\"\n    cat '$input_file' | tr '[a-z]' '[A-Z]' &gt; 'UPPER-${input_file}'\n    \"\"\"\n}\n</code></pre> <p>Una volta fatto ci\u00f2, eliminate la definizione del processo dal file del workflow, ma assicuratevi di lasciare lo shebang al suo posto.</p>"},{"location":"it/hello_nextflow/04_hello_modules/#33-aggiungere-una-dichiarazione-di-importazione-prima-del-blocco-del-workflow","title":"3.3. Aggiungere una dichiarazione di importazione prima del blocco del workflow","text":"<p>Inserite la dichiarazione di importazione sopra il blocco del workflow e compilatela in modo appropriato.</p> DopoPrima hello-modules.nf<pre><code>// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\ninclude { convertToUpper } from './modules/convertToUpper.nf'\n\nworkflow {\n</code></pre> hello-modules.nf<pre><code>// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\n\nworkflow {\n</code></pre>"},{"location":"it/hello_nextflow/04_hello_modules/#34-eseguite-il-workflow-per-verificare-che-faccia-la-stessa-cosa-di-prima","title":"3.4. Eseguite il workflow per verificare che faccia la stessa cosa di prima","text":"<p>Eseguire questa operazione con il flag <code>-resume</code>.</p> <pre><code>nextflow run hello-modules.nf -resume\n</code></pre> <p>Il risultato dovrebbe essere lo stesso di quello precedente.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-modules.nf` [nauseous_heisenberg] DSL2 - revision: a04a9f2da0\n\n[c9/763d42] sayHello (3)       | 3 of 3, cached: 3 \u2714\n[60/bc6831] convertToUpper (3) | 3 of 3, cached: 3 \u2714\n[1a/bc5901] collectGreetings   | 1 of 1, cached: 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>Due fatti, uno ancora da fare!</p>"},{"location":"it/hello_nextflow/04_hello_modules/#4-modularizzare-il-processo-collectgreetings","title":"4. Modularizzare il processo <code>collectGreetings()</code>","text":""},{"location":"it/hello_nextflow/04_hello_modules/#41-creare-un-file-stub-per-il-nuovo-modulo","title":"4.1. Creare un file stub per il nuovo modulo","text":"<p>Creare un file vuoto per il modulo, chiamato <code>collectGreetings.nf</code>.</p> <pre><code>touch modules/collectGreetings.nf\n</code></pre>"},{"location":"it/hello_nextflow/04_hello_modules/#42-spostare-il-codice-del-processo-collectgreetings-nel-file-del-modulo","title":"4.2. Spostare il codice del processo <code>collectGreetings</code> nel file del modulo","text":"<p>Copiare l'intera definizione del processo dal file del workflow al file del modulo, assicurandosi di copiare anche lo shebang <code>#!/usr/bin/env nextflow</code>.</p> modules/collectGreetings.nf<pre><code>#!/usr/bin/env nextflow\n\n/*\n * Raccogli i saluti maiuscoli in un singolo file di output\n */\nprocess collectGreetings {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        path input_files\n        val batch_name\n\n    output:\n        path \"COLLECTED-${batch_name}-output.txt\" , emit: outfile\n        val count_greetings , emit: count\n\n    script:\n        count_greetings = input_files.size()\n    \"\"\"\n    cat ${input_files} &gt; 'COLLECTED-${batch_name}-output.txt'\n    \"\"\"\n}\n</code></pre> <p>Una volta fatto ci\u00f2, eliminate la definizione del processo dal file del workflow, ma assicuratevi di lasciare lo shebang al suo posto.</p>"},{"location":"it/hello_nextflow/04_hello_modules/#43-aggiungere-una-dichiarazione-di-importazione-prima-del-blocco-del-workflow","title":"4.3. Aggiungere una dichiarazione di importazione prima del blocco del workflow","text":"<p>Inserite la dichiarazione di importazione sopra il blocco del workflow e compilatela in modo appropriato.</p> DopoPrima hello-modules.nf<pre><code>// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\ninclude { convertToUpper } from './modules/convertToUpper.nf'\ninclude { collectGreetings } from './modules/collectGreetings.nf'\n\nworkflow {\n</code></pre> hello-modules.nf<pre><code>// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\ninclude { convertToUpper } from './modules/convertToUpper.nf'\n\nworkflow {\n</code></pre>"},{"location":"it/hello_nextflow/04_hello_modules/#44-eseguite-il-workflow-per-verificare-che-faccia-la-stessa-cosa-di-prima","title":"4.4. Eseguite il workflow per verificare che faccia la stessa cosa di prima","text":"<p>Eseguire questa operazione con il flag <code>-resume</code>.</p> <pre><code>nextflow run hello-modules.nf -resume\n</code></pre> <p>Il risultato dovrebbe essere lo stesso di quello precedente.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-modules.nf` [friendly_coulomb] DSL2 - revision: 7aa2b9bc0f\n\n[f6/cc0107] sayHello (1)       | 3 of 3, cached: 3 \u2714\n[3c/4058ba] convertToUpper (2) | 3 of 3, cached: 3 \u2714\n[1a/bc5901] collectGreetings   | 1 of 1, cached: 1 \u2714\nThere were 3 greetings in this batch\n</code></pre>"},{"location":"it/hello_nextflow/04_hello_modules/#conclusione_1","title":"Conclusione","text":"<p>Sapete come modulare pi\u00f9 processi in un workflow.</p> <p>Congratulazioni, avete fatto tutto questo lavoro e non \u00e8 cambiato assolutamente nulla nel funzionamento della pipeline!</p> <p>A parte gli scherzi, ora il codice \u00e8 pi\u00f9 modulare e se si decide di scrivere un'altra pipeline che richiama uno di questi processi, \u00e8 sufficiente digitare una breve dichiarazione di importazione per utilizzare il modulo corrispondente. \u00c8 meglio che copiare il codice, perch\u00e9 se in seguito si decide di migliorare il modulo, tutte le pipeline erediteranno i miglioramenti.</p>"},{"location":"it/hello_nextflow/04_hello_modules/#cosa-ce-dopo_1","title":"Cosa c'\u00e8 dopo?","text":"<p>Fate una breve pausa se ne avete voglia. Quando siete pronti, passate alla Parte 5 per imparare a usare i container per gestire le dipendenze del software in modo pi\u00f9 pratico e riproducibile.</p>"},{"location":"it/hello_nextflow/05_hello_containers/","title":"Parte 5: Hello Containers","text":"<p> Guarda tutta la playlist sul canale Youtube Nextflow.</p> <p> La trascrizione di questo video \u00e8 disponibile qui.</p> <p>Nella parte 1-4 del corso di training, hai imparato come usare gli elementi di base di Nextflow per assemblare un semplice workflow capace di processare alcuni testi, a fare esecuzioni parallele se ci sono pi\u00f9 input e collezionare i risultati per esecuzioni future.</p> <p>Tuttavia, si era limitati agli strumenti unix di base disponibili nel proprio ambiente. Le attivit\u00e0 del mondo reale spesso richiedono vari strumenti e pacchetti non inclusi default. In genere, \u00e8 necessario installare questi strumenti, gestire le loro dipendenze e risolvere eventuali conflitti.</p> <p>Tutto ci\u00f2 \u00e8 molto noiso e fastidioso, quindi vi mostreremo come usare i container per risolvere questo problema in modo molto pi\u00f9 comodo.</p> <p>Un container \u00e8 un'unit\u00e0 di software leggera, autonoma ed eseguibile creata da un'immagine di container che incluede tutto ci\u00f2 che serve per eseguitre un'applicazione, compresi codici librerie di sistema e impostazioni.</p> <p>Note</p> <p>We'll be teaching this using the technology Docker, but Nextflow supports several other container technologies as well.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#0-warmup-eseguire-hello-containersnf","title":"0. Warmup: Eseguire <code>hello-containers.nf</code>","text":"<p>Utilizzeremo lo script del workflow hello-containers.nf come punto di paretnza per la seconda sezione.</p> <p>Just to make sure everything is working, run the script once before making any changes: \u00c8 equivalente allo script prodotto lavorando alla Parte 4 di questo corso di formazione.</p> <pre><code>nextflow run hello-containers.nf\n</code></pre> <p>Questo dovrebbe produrre il seguente risultato:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-containers.nf` [tender_becquerel] DSL2 - revision: f7cat8e223\n\nexecutor &gt;  local (7)\n[bd/4bb541] sayHello (1)         [100%] 3 of 3 \u2714\n[85/b627e8] convertToUpper (3)   [100%] 3 of 3 \u2714\n[7d/f7961c] collectGreetings     [100%] 1 of 1 \u2714\n</code></pre> <p>Come in precendenza, i file output si trovano nella directory <code>results</code> (specificata dalla direttiva <code>publishDir</code>).</p> Directory contents<pre><code>results\n\u251c\u2500\u2500 Bonjour-output.txt\n\u251c\u2500\u2500 COLLECTED-output.txt\n\u251c\u2500\u2500 COLLECTED-test-batch-output.txt\n\u251c\u2500\u2500 COLLECTED-trio-output.txt\n\u251c\u2500\u2500 Hello-output.txt\n\u251c\u2500\u2500 Hol\u00e0-output.txt\n\u251c\u2500\u2500 UPPER-Bonjour-output.txt\n\u251c\u2500\u2500 UPPER-Hello-output.txt\n\u2514\u2500\u2500 UPPER-Hol\u00e0-output.txt\n</code></pre> <p>Note</p> <p>There may also be a file named <code>output.txt</code> left over if you worked through Part 2 in the same environment.</p> <p>Se questo ha funzionato, siete pronti per imparare a usare i container.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#1-utilizzare-un-container-manualmente","title":"1. Utilizzare un container 'manualmente'","text":"<p>Quello che vogliamo fare \u00e8 aggiungere un passo al nostro workflow che utilizzer\u00e0 un container per l'esecuzione.</p> <p>Tuttavia, prima di iniziare a usarli in Nextflow, esamineremo alcuni concetti e operazioni di base, per consolidare la comprensione di cosa sono i container.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#11-estrarre-limmagine-del-container","title":"1.1. Estrarre l'immagine del container","text":"<p>Per utilizzare un container, di solito si scarica o si 'pull' un'immagine del container da un reggistro dei container e poi si eseguel'immagien del container per creare un'istanza del container.</p> <p>La sintassi generale \u00e8 la seguente:</p> Syntax<pre><code>docker pull '&lt;container&gt;'\n</code></pre> <p>La parte <code>docker pull</code> \u00e8 l'istruzione al sistema di container di prelevare un'immagine di container da un repository.</p> <p>La parte <code>'&lt;container&gt;'</code> \u00e8 l'indirizzo URI dell'imagine del conatiner.</p> <p>a titolo d'esempio estraiamo un'immagine ocntainer che contenga cowpy, un'implementazione di Python di uno strumento chiamato cowsay che genera arte ASCII per visualizzare in modo divertente input di testo arbitrari.</p> <p>Esistono vari repository dove \u00e8 possibile trovare i container pubblicati. Noi abbiamo utilizzato il servizio di Seqera Containers per generare quest'immagine Docker container dal pacchetto cowpy Conda: <code>'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'</code>.</p> <p>Eseguire il comando di pull richiesto:</p> <pre><code>docker pull 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n</code></pre> <p>In questo modo si ottiene il seguente output di console mebntro il sistema scarica le immagini:</p> Output<pre><code>Unable to find image 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273' locally\n131d6a1b707a8e65: Pulling from library/cowpy\ndafa2b0c44d2: Pull complete\ndec6b097362e: Pull complete\nf88da01cff0b: Pull complete\n4f4fb700ef54: Pull complete\n92dc97a3ef36: Pull complete\n403f74b0f85e: Pull complete\n10b8c00c10a5: Pull complete\n17dc7ea432cc: Pull complete\nbb36d6c3110d: Pull complete\n0ea1a16bbe82: Pull complete\n030a47592a0a: Pull complete\n622dd7f15040: Pull complete\n895fb5d0f4df: Pull complete\nDigest: sha256:fa50498b32534d83e0a89bb21fec0c47cc03933ac95c6b6587df82aaa9d68db3\nStatus: Downloaded newer image for community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273\ncommunity.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273\n</code></pre> <p>Una volta completato il download, si ha una copia locale dell'immagine del container.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#12-utilizzare-il-container-per-eseguire-cowpy-come-comando-singolo","title":"1.2. Utilizzare il container per eseguire cowpy come comando singolo","text":"<p>Un modo molto comune di utilizzare i container \u00e8 quello di eseguirli direttamente, cio\u00e8 in modo non interattivo. Questo \u00e8 ottimo per eseguire comandi una tantum.</p> <p>La sintassi generale \u00e8 la seguente:</p> Syntax<pre><code>docker run --rm '&lt;container&gt;' [tool command]\n</code></pre> <p>La parte <code>docker run --rm '&lt;container&gt;'</code> \u00e8 l'istruzione al sistema di container di creare un'istanza da un'immagine di container ed eseguire un comando in essa. Il flag <code>--rm</code> indica al sistema di chiudere l'istanza del container al termine del comando.</p> <p>La sintassi di <code>[tool command]</code> dipende dallo strumento che si sta usando e da come \u00e8 impostato il conatiner. Cominciamo con <code>cowpy</code>.</p> <p>Completamente assemblato, il comadno di esecuzione del container si presenta come segue:</p> <pre><code>docker run --rm 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273' cowpy\n</code></pre> <p>Esegui per produrre il seguente output:</p> Output<pre><code> ______________________________________________________\n&lt; Cowacter, eyes:default, tongue:False, thoughts:False &gt;\n ------------------------------------------------------\n     \\   ^__^\n      \\  (oo)\\_______\n         (__)\\       )\\/\\\n           ||----w |\n           ||     ||\n</code></pre> <p>Il sistema avvia il container, esegue il comando cowpy con i suoi parametri, invia l'output alla console e infine chiude l'istanza del container.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#13-ustilizzare-il-container-per-eseguire-cowpy-in-modo-interattivo","title":"1.3. Ustilizzare il container per eseguire cowpy in modo interattivo","text":"<p>E anche possibile eseguire un conatiner in modo interattivo, in modo da avere un prompt di shell all'interno del container e poter giocare con i comandi.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#131-avviare-il-container","title":"1.3.1. Avviare il container","text":"<p>Per eseguire il container in modo interattivo, basta aggiungere <code>-it</code> al comando <code>docker run'. Facoltativamente, possiamo specificare la shell che vogliamo usare all'inetrno del conatiner, aggiungendo ad esempio</code>/bin/bash` al comando.</p> <pre><code>docker run --rm -it 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273' /bin/bash\n</code></pre> <p>Notate che il prompt cambia in qualcosa di simile a<code>(base) root@b645838b3314:/tmp#</code>, che indica che ora siete all'interno del conatienr.</p> <p>E' possibile verificarlo eseguendo 'ls'per elencare il contenuto della directory:</p> <pre><code>ls /\n</code></pre> Output<pre><code>bin    dev    etc    home   lib    media  mnt    opt    proc   root   run    sbin   srv    sys    tmp    usr    var\n</code></pre> <p>Si pu\u00f2 notare che il filesystem all'interno del conatiner \u00e8 diverso da quello del sistema host. .</p> <p>Note</p> <p>Quando esegui un container, questo \u00e8 isolato dal sistema host per impostazione predefinita. Ci\u00f2 significa che il container non pu\u00f2 accedere ad alcun file sul sistema host a meno che tu non glielo consenta esplicitamente.</p> <p>Imparerai come farlo in un minuto.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#132-eseguire-i-comandi-dello-strumento-desiderato","title":"1.3.2. Eseguire i comandi dello strumento desiderato","text":"<p>Ora che sei all'interno del container, puoi eseguire direttamente il comando <code>cowpy</code> e fornirgli alcuni parametri. Ad esempio, la documentazione dello strumento afferma che possiamo modificare il carattere ('cowacter') con <code>-c</code>.</p> <pre><code>cowpy \"Hello Containers\" -c tux\n</code></pre> <p>Ora l'output mostra il pinguino di Linux, Tux, invece della mucca predefinita, perch\u00e9 abbiamo specificato il parametro <code>-c tux</code>.</p> Output<pre><code> __________________\n&lt; Hello Containers &gt;\n ------------------\n   \\\n    \\\n        .--.\n       |o_o |\n       |:_/ |\n      //   \\ \\\n     (|     | )\n    /'\\_   _/`\\\n    \\___)=(___/\n</code></pre> <p>Poich\u00e9 ti trovi all'interno del container, puoi eseguire il comando cowpy tutte le volte che vuoi, variando i parametri di input, senza dover utilizzare i comandi Docker.</p> <p>Tip</p> <p>Utilizzare il flag '-c' per selezionare un carattere diverso, incluso: <code>beavis</code>, <code>cheese</code>, <code>daemon</code>, <code>dragonandcow</code>, <code>ghostbusters</code>, <code>kitty</code>, <code>moose</code>, <code>milk</code>, <code>stegosaurus</code>, <code>turkey</code>, <code>turtle</code>, <code>tux</code></p> <p>Questo \u00e8 carino. Sarebbe ancora pi\u00f9 carino se potessimo alimentare il nostro <code>greetings.csv</code> come input in questo. Ma poich\u00e9 non abbiamo accesso al file system, non possiamo farlo.</p> <p>Risolviamolo.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#133-uscire-dal-container","title":"1.3.3. Uscire dal container","text":"<p>Per uscire dal container, puoi digitare <code>exit</code> al prompt oppure usare la scorciatoia da tastiera Ctrl+D.</p> <pre><code>exit\n</code></pre> <p>Il prompt dovrebbe ora tornare a essere quello che era prima dell'avvio del container.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#134-montare-i-dati-nel-container","title":"1.3.4. Montare i dati nel container","text":"<p>Quando si esegue un container, questo \u00e8 isolato dal sistema host per impostazione predefinita. Ci\u00f2 significa che il container non pu\u00f2 accedere ad alcun file sul sistema host a meno che non gli venga esplicitamente consentito di farlo.</p> <p>Un modo per farlo \u00e8 montare un volume dal sistema host nel container utilizzando la seguente sintassi:</p> Syntax<pre><code>-v &lt;outside_path&gt;:&lt;inside_path&gt;\n</code></pre> <p>Nel nostro caso <code>&lt;outside_path&gt;</code> sar\u00e0 la directory di lavoro corrente, quindi possiamo semplicemente usare un punto (<code>.</code>), e <code>&lt;inside_path&gt;</code> \u00e8 solo un nome che inventiamo; Chiamiamolo <code>/data</code>.</p> <p>Per montare un volume, sostituiamo i percorsi e aggiungiamo l'argomento di montaggio del volume al comando docker run come segue:</p> <pre><code>docker run --rm -it -v .:/data 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273' /bin/bash\n</code></pre> <p>Questo monta la directory di lavoro corrente come un volume che sar\u00e0 accessibile sotto <code>/data</code> all'interno del container.</p> <p>Puoi verificare che funzioni elencando il contenuto di <code>/data</code>:</p> <pre><code>ls /data\n</code></pre> <p>A seconda della parte di questa formazione che hai gi\u00e0 svolto in precedenza, il risultato riportato di seguito potrebbe essere leggermente diverso.</p> Output<pre><code>demo-params.json  hello-channels.nf  hello-workflow.nf  modules          results\ngreetings.csv     hello-modules.nf   hello-world.nf     nextflow.config  work\n</code></pre> <p>Ora puoi vedere il contenuto della directory <code>data</code> dall'interno del container, incluso il file <code>greetings.csv</code>.</p> <p>Questo ha effettivamente creato un tunnel attraverso il muro del container che puoi usare per accedere a quella parte del tuo file system.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#135-utilizza-i-dati-montati","title":"1.3.5. Utilizza i dati montati","text":"<p>Ora che abbiamo montato la directory <code>data</code> nel container, possiamo usare il comando <code>cowpy</code> per visualizzare il contenuto del file <code>greetings.csv</code>.</p> <p>Per fare questo, useremo<code>cat /data/greetings.csv |</code> per inviare il contenuto del file CSV al comando <code>cowpy</code>.</p> <pre><code>cat /data/greetings.csv | cowpy -c turkey\n</code></pre> <p>Questo produce l'ASCII art desiderata di un tacchino che snocciola i nostri saluti di esempio:</p> Output<pre><code> _________\n/ Hello   \\\n| Bonjour |\n\\ Hol\u00e0    /\n ---------\n  \\                                  ,+*^^*+___+++_\n   \\                           ,*^^^^              )\n    \\                       _+*                     ^**+_\n     \\                    +^       _ _++*+_+++_,         )\n              _+^^*+_    (     ,+*^ ^          \\+_        )\n             {       )  (    ,(    ,_+--+--,      ^)      ^\\\n            { (\\@)    } f   ,(  ,+-^ __*_*_  ^^\\_   ^\\       )\n           {:;-/    (_+*-+^^^^^+*+*&lt;_ _++_)_    )    )      /\n          ( /  (    (        ,___    ^*+_+* )   &lt;    &lt;      \\\n           U _/     )    *--&lt;  ) ^\\-----++__)   )    )       )\n            (      )  _(^)^^))  )  )\\^^^^^))^*+/    /       /\n          (      /  (_))_^)) )  )  ))^^^^^))^^^)__/     +^^\n         (     ,/    (^))^))  )  ) ))^^^^^^^))^^)       _)\n          *+__+*       (_))^)  ) ) ))^^^^^^))^^^^^)____*^\n          \\             \\_)^)_)) ))^^^^^^^^^^))^^^^)\n           (_             ^\\__^^^^^^^^^^^^))^^^^^^^)\n             ^\\___            ^\\__^^^^^^))^^^^^^^^)\\\\\n                  ^^^^^\\uuu/^^\\uuu/^^^^\\^\\^\\^\\^\\^\\^\\^\\\n                     ___) &gt;____) &gt;___   ^\\_\\_\\_\\_\\_\\_\\)\n                    ^^^//\\\\_^^//\\\\_^       ^(\\_\\_\\_\\)\n                      ^^^ ^^ ^^^ ^\n</code></pre> <p>Sentiti libero di giocare con questo comando. Quando hai finito, esci dal container come in precedenza:</p> <pre><code>exit\n</code></pre> <p>Ti ritroverai nel tuo guscio normale.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#takeaway","title":"Takeaway","text":"<p>Sai come estrarre uncontainer ed eseguirlo singolarmente o in modo interattivo. Sai anche come rendere accessibili i tuoi dati dall'interno del container, il che ti consente di provare qualsiasi strumento a cui sei interessato su dati reali senza dover installare alcun software sul tuo sistema.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#prossimo-passo","title":"Prossimo passo?","text":"<p>Scopri come utilizzare i contenitori per l'esecuzione dei processi Nextflow.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#2-utilizzare-i-container-in-nextflow","title":"2. Utilizzare i container in Nextflow","text":"<p>Nextflow ha un supporto integrato per l'esecuzione di processi all'interno di contenitori per consentirti di eseguire strumenti che non hai installato nel tuo ambiente di elaborazione. Ci\u00f2 significa che puoi utilizzare qualsiasi immagine di container desideri per eseguire i tuoi processi e Nextflow si occuper\u00e0 di estrarre l'immagine, montare i dati ed eseguire il processo al suo interno.</p> <p>Per dimostrarlo, aggiungeremo un passaggio <code>cowpy</code> alla pipeline che stiamo sviluppando, dopo il passaggio <code>collectGreetings</code>.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#21-scrivi-un-modulo-cowpy","title":"2.1. Scrivi un modulo <code>cowpy</code>","text":""},{"location":"it/hello_nextflow/05_hello_containers/#211-crea-uno-stub-di-file-per-il-nuovo-modulo","title":"2.1.1. Crea uno stub di file per il nuovo modulo","text":"<p>Crea un file vuoto per il modulo chiamato <code>cowpy.nf</code>.</p> <pre><code>touch modules/cowpy.nf\n</code></pre> <p>Questo ci fornisce un posto dove mettere il codice del processo.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#212-copia-il-codice-del-processo-cowpy-nel-file-del-modulo","title":"2.1.2. Copia il codice del processo <code>cowpy</code> nel file del modulo","text":"<p>Possiamo modellare il nostro processo <code>cowpy</code> sugli altri processi che abbiamo scritto in precedenza.</p> modules/cowpy.nf<pre><code>#!/usr/bin/env nextflow\n\n// Generate ASCII art with cowpy\nprocess cowpy {\n\n    publishDir 'results', mode: 'copy'\n\n    input:\n        path input_file\n        val character\n\n    output:\n        path \"cowpy-${input_file}\"\n\n    script:\n    \"\"\"\n    cat $input_file | cowpy -c \"$character\" &gt; cowpy-${input_file}\n    \"\"\"\n\n}\n</code></pre> <p>L'output sar\u00e0 un nuovo file di testo contenente l'ASCII art generata dallo strumento <code>cowpy</code>.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#22-aggiungi-cowpy-al-flusso-di-lavoro","title":"2.2. Aggiungi cowpy al flusso di lavoro","text":"<p>Ora dobbiamo importare il modulo e chiamare il processo.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#221-importa-il-processo-cowpy-in-hello-containersnf","title":"2.2.1. Importa il processo <code>cowpy</code> in <code>hello-containers.nf</code>","text":"<p>Inserire la dichiarazione di importazione sopra il blocco del workflow e compilarla in modo appropriato.</p> DopoPrima hello-containers.nf<pre><code>// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\ninclude { convertToUpper } from './modules/convertToUpper.nf'\ninclude { collectGreetings } from './modules/collectGreetings.nf'\ninclude { cowpy } from './modules/cowpy.nf'\n\nworkflow {\n</code></pre> hello-containers.nf<pre><code>// Include modules\ninclude { sayHello } from './modules/sayHello.nf'\ninclude { convertToUpper } from './modules/convertToUpper.nf'\ninclude { collectGreetings } from './modules/collectGreetings.nf'\n\nworkflow {\n</code></pre>"},{"location":"it/hello_nextflow/05_hello_containers/#222-aggiungere-una-chiamata-al-processo-cowpy-nel-flusso-di-lavoro","title":"2.2.2. Aggiungere una chiamata al processo <code>cowpy</code> nel flusso di lavoro","text":"<p>Colleghiamo il processo <code>cowpy()</code> all'output del processo <code>collectGreetings()</code>, che come ricorderai produce due output:</p> <ul> <li><code>collectGreetings.out.outfile</code> contiene gli otput dei file</li> <li><code>collectGreetings.out.count</code>contiene il conteggio dei saluti per batch</li> </ul> <p>Nel blocco del flusso di lavoro, apportare la seguente modifica al codice:</p> DopoPrima hello-containers.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect(), params.batch)\n\n    // emit a message about the size of the batch\n    collectGreetings.out.count.view{ num_greetings -&gt; \"There were $num_greetings greetings in this batch\" }\n\n    // generate ASCII art of the greetings with cowpy\n    cowpy(collectGreetings.out.outfile, params.character)\n</code></pre> hello-containers.nf<pre><code>    // collect all the greetings into one file\n    collectGreetings(convertToUpper.out.collect(), params.batch)\n\n    // emit a message about the size of the batch\n    collectGreetings.out.count.view{ num_greetings -&gt; \"There were $num_greetings greetings in this batch\" }\n</code></pre> <p>Si noti che includiamo un nuovo parametro CLI, <code>params.character</code>, per specificare quale carattere vogliamo che dica i saluti.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#223-imposta-un-valore-predefinito-per-paramscharacter","title":"2.2.3. Imposta un valore predefinito per <code>params.character</code>","text":"<p>Ci piace essere pigri e saltare la digitazione dei parametri nelle nostre righe di comando.</p> DopoPrima hello-containers.nf<pre><code>/*\n * Pipeline parameters\n */\nparams.greeting = 'greetings.csv'\nparams.batch = 'test-batch'\nparams.character = 'turkey'\n</code></pre> hello-containers.nf<pre><code>/*\n * Pipeline parameters\n */\nparams.greeting = 'greetings.csv'\nparams.batch = 'test-batch'\n</code></pre> <p>Dovrebbe essere tutto ci\u00f2 di cui abbiamo bisogno per far funzionare tutto.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#224-eseguite-il-workflow-per-verificarne-il-funzionamento","title":"2.2.4. Eseguite il workflow per verificarne il funzionamento","text":"<p>Eseguire questa operazione con il flag <code>-resume</code>.</p> <pre><code>nextflow run hello-containers.nf -resume\n</code></pre> <p>Oh no, c'\u00e8 un errore!</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-containers.nf` [special_lovelace] DSL2 - revision: 028a841db1\n\nexecutor &gt;  local (1)\n[f6/cc0107] sayHello (1)       | 3 of 3, cached: 3 \u2714\n[2c/67a06b] convertToUpper (3) | 3 of 3, cached: 3 \u2714\n[1a/bc5901] collectGreetings   | 1 of 1, cached: 1 \u2714\n[b2/488871] cowpy             | 0 of 1\nThere were 3 greetings in this batch\nERROR ~ Error executing process &gt; 'cowpy'\n\nCaused by:\n  Process `cowpy` terminated with an error exit status (127)\n\nCommand executed:\n\n  cat COLLECTED-test-batch-output.txt | cowpy -c \"turkey\" &gt; cowpy-COLLECTED-test-batch-output.txt\n\nCommand exit status:\n  127\n\nCommand output:\n  (empty)\n\nCommand error:\n  .command.sh: line 2: cowpy: command not found\n\n(trimmed output)\n</code></pre> <p>Questo codice di errore, <code>error exit status (127)</code>, significa che l'eseguibile richiesto non \u00e8 stato trovato. Naturalmente, dato che stiamo chiamando lo strumento <code>cowpy</code> ma non abbiamo ancora specificato un container.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#23-utilizzare-un-container-per-lesecuzione","title":"2.3. Utilizzare un container per l'esecuzione","text":"<p>Dobbiamo specificare un container e dire a Nextflow di usarlo per il processo <code>cowpy()</code>.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#231-specificare-un-container-per-il-processo-cowpy-da-utilizzare","title":"2.3.1. Specificare un container per il processo <code>cowpy</code> da utilizzare","text":"<p>Modificare il modulo <code>cowpy.nf</code> per aggiungere la direttiva <code>container</code> alla definizione del processo come segue:</p> DopoPrima modules/cowpy.nf<pre><code>process cowpy {\n\n    publishDir 'containers/results', mode: 'copy'\n    container 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n</code></pre> modules/cowpy.nf<pre><code>process cowpy {\n\n    publishDir 'containers/results', mode: 'copy'\n</code></pre> <p>indica a Nextflow che, se l'uso di Docker \u00e8 abilitato, deve usare l'immagine del container specificata qui per eseguire il processo.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#232-abilitare-luso-di-docker-tramite-il-file-nextflowconfig","title":"2.3.2. Abilitare l'uso di Docker tramite il file `nextflow.config","text":"<p>Qui anticipiamo leggermente l'argomento della prossima e ultima parte di questo corso (Parte 6), che riguarda la configurazione.</p> <p>Uno dei modi principali che Nextflow offre per configurare l'esecuzione del workflow \u00e8 l'uso di un file <code>nextflow.config</code>. Quando un file di questo tipo \u00e8 presente nella directory corrente, Nextflow lo caricher\u00e0 automaticamente e applicher\u00e0 la configurazione in esso contenuta. Abbiamo fornito un file <code>nextflow.config</code> con una singola riga di codice che disabilita Docker: <code>docker.enabled = false</code>.</p> <p>Ora, passiamo a <code>true</code> per abilitare Docker:</p> DopoPrima nextflow.config<pre><code>docker.enabled = true\n</code></pre> nextflow.config<pre><code>docker.enabled = false\n</code></pre> <p>Note</p> <p>It is possible to enable Docker execution from the command-line, on a per-run basis, using the <code>-with-docker &lt;container&gt;</code> parameter. However, that only allows us to specify one container for the entire workflow, whereas the approach we just showed you allows us to specify a different container per process. This is better for modularity, code maintenance and reproducibility.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#233-eseguire-il-workflow-con-docker-abilitato","title":"2.3.3. Eseguire il workflow con Docker abilitato","text":"<p>Eseguire il workflow con il flag <code>resume</code>:</p> <pre><code>nextflow run hello-containers.nf -resume\n</code></pre> <p>Questa volta funziona davvero.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-containers.nf` [elegant_brattain] DSL2 - revision: 028a841db1\n\nexecutor &gt;  local (1)\n[95/fa0bac] sayHello (3)       | 3 of 3, cached: 3 \u2714\n[92/32533f] convertToUpper (3) | 3 of 3, cached: 3 \u2714\n[aa/e697a2] collectGreetings   | 1 of 1, cached: 1 \u2714\n[7f/caf718] cowpy              | 1 of 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>\u00c8 possibile trovare l'output di cowpy nella directory <code>results</code>.</p> results/cowpy-COLLECTED-test-batch-output.txt<pre><code> _________\n/ HOL\u00e0    \\\n| HELLO   |\n\\ BONJOUR /\n ---------\n  \\                                  ,+*^^*+___+++_\n   \\                           ,*^^^^              )\n    \\                       _+*                     ^**+_\n     \\                    +^       _ _++*+_+++_,         )\n              _+^^*+_    (     ,+*^ ^          \\+_        )\n             {       )  (    ,(    ,_+--+--,      ^)      ^\\\n            { (\\@)    } f   ,(  ,+-^ __*_*_  ^^\\_   ^\\       )\n           {:;-/    (_+*-+^^^^^+*+*&lt;_ _++_)_    )    )      /\n          ( /  (    (        ,___    ^*+_+* )   &lt;    &lt;      \\\n           U _/     )    *--&lt;  ) ^\\-----++__)   )    )       )\n            (      )  _(^)^^))  )  )\\^^^^^))^*+/    /       /\n          (      /  (_))_^)) )  )  ))^^^^^))^^^)__/     +^^\n         (     ,/    (^))^))  )  ) ))^^^^^^^))^^)       _)\n          *+__+*       (_))^)  ) ) ))^^^^^^))^^^^^)____*^\n          \\             \\_)^)_)) ))^^^^^^^^^^))^^^^)\n           (_             ^\\__^^^^^^^^^^^^))^^^^^^^)\n             ^\\___            ^\\__^^^^^^))^^^^^^^^)\\\\\n                  ^^^^^\\uuu/^^\\uuu/^^^^\\^\\^\\^\\^\\^\\^\\^\\\n                     ___) &gt;____) &gt;___   ^\\_\\_\\_\\_\\_\\_\\)\n                    ^^^//\\\\_^^//\\\\_^       ^(\\_\\_\\_\\)\n                      ^^^ ^^ ^^^ ^\n</code></pre> <p>Si vede che il personaggio sta pronunciando tutti i saluti, proprio come ha fatto quando abbiamo eseguito il comando <code>cowpy</code> sul file <code>greetings.csv</code> dall'interno del container.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#234-controllare-come-nextflow-ha-lanciato-il-task-containerizzato","title":"2.3.4. Controllare come Nextflow ha lanciato il task containerizzato","text":"<p>Diamo un'occhiata alla sottodirectory di lavoro di una delle chiamate al processo cowpy per capire meglio come Nextflow lavora con i container.</p> <p>Controllare l'output del comando nextflow run per trovare l'ID della chiamata al processo cowpy. Quindi navigare nella sottodirectory work. Al suo interno si trova il file .command.run che contiene tutti i comandi eseguiti da Nextflow per conto dell'utente durante l'esecuzione della pipeline. Aprite il file .command.run e cercate nxf_launch; dovreste vedere qualcosa di simile:</p> <pre><code>nxf_launch() {\n    docker run -i --cpu-shares 1024 -e \"NXF_TASK_WORKDIR\" -v /workspaces/training/hello-nextflow/work:/workspaces/training/hello-nextflow/work -w \"$NXF_TASK_WORKDIR\" --name $NXF_BOXID community.wave.seqera.io/library/pip_cowpy:131d6a1b707a8e65 /bin/bash -ue /workspaces/training/hello-nextflow/work/7f/caf7189fca6c56ba627b75749edcb3/.command.sh\n}\n</code></pre> <p>Come si pu\u00f2 vedere, Nextflow utilizza il comando docker run per lanciare la chiamata di processo. Inoltre, monta la corrispondente sottodirectory di lavoro nel container, imposta di conseguenza la directory di lavoro all'interno del container ed esegue il nostro script bash templato nel file .command.sh. Tutto il duro lavoro che abbiamo dovuto fare manualmente nella sezione precedente viene svolto da Nextflow!</p>"},{"location":"it/hello_nextflow/05_hello_containers/#takeaway_1","title":"Takeaway","text":"<p>Sapete come usare i container in Nextflow per eseguire i processi.</p>"},{"location":"it/hello_nextflow/05_hello_containers/#e-ora","title":"E ora?","text":"<p>Fate una pausa! Quando sarete pronti, passate alla Parte 6 per imparare a configurare l'esecuzione della pipeline in base alla vostra infrastruttura e a gestire la configurazione di input e parametri. \u00c8 l'ultima parte e il gioco \u00e8 fatto!</p>"},{"location":"it/hello_nextflow/06_hello_config/","title":"Parte 6: Hello Config","text":"<p> Guarda tutta la playlist sul canale Youtube Nextflow.</p> <p> La trascrizione di questo video \u00e8 disponibile qui.</p> <p>Questa sezione esplorer\u00e0 come impostare e gestire la configurazione della pipeline Nextflow in modo da poterne personalizzare il comportamento, adattarla a diversi ambienti e ottimizzare l'utilizzo delle risorse senza modificare una sola riga del codice del flusso di lavoro stesso.</p> <p>Ci sono diversi modi per farlo; qui useremo il meccanismo di file di configurazione pi\u00f9 semplice e comune, il file <code>nextflow.config</code>. Ogni volta che c'\u00e8 un file chiamato <code>nextflow.config</code> nella directory corrente, Nextflow caricher\u00e0 automaticamente la configurazione da esso.</p> <p>Note</p> <p>Tutto ci\u00f2 che inserisci in <code>nextflow.config</code> pu\u00f2 essere sovrascritto in fase di esecuzione fornendo le direttive di processo o i parametri e i valori pertinenti sulla riga di comando, oppure importando un altro file di configurazione, secondo l'ordine di precedenza descritto qui.</p> <p>In questa parte della formazione, utilizzeremo il file <code>nextflow.config</code> per illustrare i componenti essenziali della configurazione di Nextflow, quali direttive di processo, esecutori, profili e file di parametri.</p> <p>Imparando a utilizzare efficacemente queste opzioni di configurazione, puoi migliorare la flessibilit\u00e0, la scalabilit\u00e0 e le prestazioni delle tue pipeline.</p>"},{"location":"it/hello_nextflow/06_hello_config/#0-warmup-verifica-che-docker-sia-abilitato-ed-esegui-il-hello-config-workflow","title":"0. Warmup: verifica che Docker sia abilitato ed esegui il Hello Config workflow","text":"<p>Innanzitutto, un rapido controllo. C'\u00e8 un file <code>nextflow.config</code> nella directory corrente che contiene la riga <code>docker.enabled = &lt;setting&gt;</code>, dove <code>&lt;setting&gt;</code> \u00e8 <code>true</code> o <code>false</code> a seconda che tu abbia lavorato o meno alla Parte 5 di questo corso nello stesso ambiente.</p> <p>Se \u00e8 impostato su <code>true</code>, non \u00e8 necessario fare nulla.</p> <p>Se \u00e8 impostato su <code>false</code>, impostalo ora su <code>true</code>.</p> nextflow.config<pre><code>docker.enabled = true\n</code></pre> <p>Una volta fatto questo, verifica che il workflow iniziale funzioni correttamente:</p> <pre><code>nextflow run hello-config.nf\n</code></pre> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-config.nf` [reverent_heisenberg] DSL2 - revision: 028a841db1\n\nexecutor &gt;  local (8)\n[7f/0da515] sayHello (1)       | 3 of 3 \u2714\n[f3/42f5a5] convertToUpper (3) | 3 of 3 \u2714\n[04/fe90e4] collectGreetings   | 1 of 1 \u2714\n[81/4f5fa9] cowpy              | 1 of 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>Se tutto funziona, sei pronto per imparare come modificare le propriet\u00e0 di configurazione di base per adattarle ai requisiti del tuo ambiente di elaborazione.</p>"},{"location":"it/hello_nextflow/06_hello_config/#1-determinare-quale-tecnologia-di-confezionamento-software-utilizzare","title":"1. Determinare quale tecnologia di confezionamento software utilizzare","text":"<p>Il primo passo per adattare la configurazione del workflow al tuo ambiente di calcolo \u00e8 specificare da dove proverranno i pacchetti software che verranno eseguiti in ogni fase. Sono gi\u00e0 installati nell'ambiente di calcolo locale? Dobbiamo recuperare le immagini ed eseguirle tramite un sistema a container? Oppure dobbiamo recuperare i pacchetti Conda e creare un ambiente Conda locale?</p> <p>Nella primissima parte di questo corso di formazione (Parti 1-4) abbiamo utilizzato solo il software installato localmente nel nostro workflow. Nella Parte 5 abbiamo poi introdotto i container Docker e il file <code>nextflow.config</code>, che abbiamo utilizzato per abilitare l'uso dei container Docker.</p> <p>Nel warmup di questa sezione, hai verificato che Docker fosse abilitato nel file <code>nextflow.config</code> ed eseguito il workflow, che ha utilizzato un container Docker per eseguire il processo <code>cowpy()</code>.</p> <p>Note</p> <p>Se questo non ti suona familiare, probabilmente dovresti tornare indietro e completare la Parte 5 prima di continuare.</p> <p>Vediamo ora come possiamo configurare un'opzione alternativa di software packaging tramite il file <code>nextflow.config</code>.</p>"},{"location":"it/hello_nextflow/06_hello_config/#11-disabilitare-docker-e-abilitare-conda-nel-file-di-configurazione","title":"1.1. Disabilitare Docker e abilitare Conda nel file di configurazione","text":"<p>Supponiamo di lavorare su un cluster HPC e che l'amministratore non consenta l'uso di Docker per motivi di sicurezza.</p> <p>Fortunatamente per noi, Nextflow supporta molte altre tecnologie di container, tra cui Singularity (pi\u00f9 ampiamente utilizzata su HPC) e gestori di pacchetti software come Conda.</p> <p>Possiamo modificare il nostro file di configurazione per usare Conda invece di Docker. Per farlo, cambiamo il valore di <code>docker.enabled</code> in <code>false</code> e \u200b\u200baggiungiamo una direttiva che abilita l'uso di Conda:</p> DopoPrima nextflow.config<pre><code>docker.enabled = false\nconda.enabled = true\n</code></pre> nextflow.config<pre><code>docker.enabled = true\n</code></pre> <p>Ci\u00f2 consentir\u00e0 a Nextflow di creare e utilizzare ambienti Conda per i processi che hanno pacchetti Conda specificati. Ci\u00f2 significa che ora dobbiamo aggiungerne uno al nostro processo <code>cowpy</code>!</p>"},{"location":"it/hello_nextflow/06_hello_config/#12-specificare-un-pacchetto-conda-nella-definizione-del-processo","title":"1.2. Specificare un pacchetto Conda nella definizione del processo","text":"<p>Abbiamo gi\u00e0 recuperato l'URI per un pacchetto Conda contenente lo strumento <code>cowpy</code>: <code>conda-forge::cowpy==1.1.5</code></p> <p>Note</p> <p>Esistono diversi modi per ottenere l'URI per un determinato pacchetto conda. Consigliamo di usare la ricerca su Seqera Containers, che ti fornir\u00e0 un URI che puoi copiare e incollare, anche se non hai intenzione di creare un container da esso.</p> <p>Ora aggiungiamo l'URI alla definizione del processo <code>cowpy</code> utilizzando la direttiva <code>conda</code>:</p> DopoPrima modules/cowpy.nf<pre><code>process cowpy {\n\n    container 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n    conda 'conda-forge::cowpy==1.1.5'\n\n    publishDir 'results', mode: 'copy'\n</code></pre> modules/cowpy.nf<pre><code>process cowpy {\n\n    container 'community.wave.seqera.io/library/cowpy:1.1.5--3db457ae1977a273'\n\n    publishDir 'results', mode: 'copy'\n</code></pre> <p>Per essere chiari, non stiamo sostituendo la direttiva <code>docker</code>, stiamo aggiungendo un'opzione alternativa.</p>"},{"location":"it/hello_nextflow/06_hello_config/#13-eseguire-il-workflow-per-verificare-luso-di-conda","title":"1.3. Eseguire il workflow per verificare l'uso di Conda","text":"<p>Proviamolo.</p> <pre><code>nextflow run hello-config.nf\n</code></pre> <p>Dovrebbe funzionare senza problemi.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-config.nf` [trusting_lovelace] DSL2 - revision: 028a841db1\n\nexecutor &gt;  local (8)\n[ee/4ca1f2] sayHello (3)       | 3 of 3 \u2714\n[20/2596a7] convertToUpper (1) | 3 of 3 \u2714\n[b3/e15de5] collectGreetings   | 1 of 1 \u2714\n[c5/af5f88] cowpy              | 1 of 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>Dietro le quinte, Nextflow ha recuperato i pacchetti Conda e creato l'ambiente, il che normalmente richiede un po' di lavoro; \u00e8 bello quindi che non dobbiamo farlo manualmente!</p> <p>Note</p> <p>Questa operazione \u00e8 veloce perch\u00e8 il pacchetto <code>cowpy</code> \u00e8 piuttosto piccolo. Tuttavia,se si lavora con pacchetti di grandi dimensioni, il processo potrebbe richiedere pi\u00f9 tempo al primo utilizzo, perch\u00e8 si potrebbe vedere l'output della console rimanere \"bloccato\" per circa un minuto prima di completarsi. Ci\u00f2 \u00e8 normale ed \u00e8 dovuto al lavoro extra che Nextflow esegue la prima volta che si utilizza un nuovo pacchetto.</p> <p>Dal nostro punto di vista, sembra che funzioni esattamente come con Docker, anche se nel backend i meccanismi sono leggermente diversi.</p> <p>Ci\u00f2 significa che siamo pronti per l'esecuzione con gli ambienti Conda, se necessario.</p> <p>Note</p> <p>Poich\u00e9 queste direttive vengono assegnate per processo, \u00e8 possibile \"mescolare e abbinare\", ovvero configurare alcuni processi nel workflow in modo che vengano eseguiti con Docker e altri con Conda, ad esempio se l'infrastruttura di elaborazione utilizzata supporta entrambi. In tal caso, dovresti abilitare sia Docker che Conda nel tuo file di configurazione. Se entrambi sono disponibili per un dato processo, Nextflow dar\u00e0 priorit\u00e0 ai container.</p> <p>Come accennato in precedenza, Nextflow supporta molte altre tecnologie di packaging e container software, quindi non sei limitato solo a queste due.</p>"},{"location":"it/hello_nextflow/06_hello_config/#conclusione","title":"Conclusione","text":"<p>Sai come configurare quale pacchetto software deve utilizzare ogni processo e come passare da una tecnologia all'altra.</p>"},{"location":"it/hello_nextflow/06_hello_config/#prossimi-passi","title":"Prossimi passi","text":"<p>Scopri come modificare l'esecutore utilizzato da Nextflow per eseguire il lavoro.</p>"},{"location":"it/hello_nextflow/06_hello_config/#2-assegnare-risorse-di-elaborazione-con-direttive-di-processo","title":"2. Assegnare risorse di elaborazione con direttive di processo","text":"<p>La maggior parte delle piattaforme di elaborazione ad alte prestazioni consente (e talvolta richiede) di specificare determinati parametri di allocazione delle risorse, come il numero di CPU e la memoria.</p> <p>Di default, Nextflow utilizzer\u00e0 una singola CPU e 2 GB di memoria per ogni processo. Le direttive di processo corrispondenti sono chiamate <code>cpus</code> e <code>memory</code>, quindi \u00e8 implicita la seguente configurazione:</p> Built-in configuration<pre><code>process {\n    cpus = 1\n    memory = 2.GB\n}\n</code></pre> <p>Puoi modificare questi valori, sia per tutti i processi che per specifici processi denominati, utilizzando direttive di processo aggiuntive nel tuo file di configurazione. Nextflow le tradurr\u00e0 nelle istruzioni appropriate per l'esecutore scelto.</p> <p>Ma come fai a sapere quali valori utilizzare?</p>"},{"location":"it/hello_nextflow/06_hello_config/#21-eseguire-il-workflow-per-generare-un-report-sullutilizzo-delle-risorse","title":"2.1. Eseguire il workflow per generare un report sull'utilizzo delle risorse","text":"<p>Se non si sa in anticipo quanta CPU e memoria saranno probabilmente necessarie ai propri processi, \u00e8 possibile effettuare una profilazione delle risorse, ovvero eseguire il workflow con alcune allocazioni predefinite, registrare la quantit\u00e0 utilizzata da ciascun processo e, da l\u00ec, stimare come modificare le allocazioni di base.</p> <p>Nextflow include strumenti integrati per fare questo e sar\u00e0 felice di generare un report per te su richiesta.</p> <p>Per farlo, aggiungi <code>-with-report &lt;nomefile&gt;.html</code> alla riga di comando.</p> <pre><code>nextflow run hello-config.nf -with-report report-config-1.html\n</code></pre> <p>Il report \u00e8 un file html, che puoi scaricare e aprire nel tuo browser. Puoi anche fare clic destro su di esso nell'esploratore file a sinistra e cliccare su <code>Mostra anteprima</code> per visualizzarlo nell'ambiente di training.</p> <p>Prenditi qualche minuto per esaminare il report e vedere se riesci a identificare alcune opportunit\u00e0 per regolare le risorse. Assicurati di cliccare sulle schede che mostrano i risultati di utilizzo come percentuale di quanto \u00e8 stato assegnato. C'\u00e8 della documentazione che descrive tutte le funzionalit\u00e0 disponibili.</p>"},{"location":"it/hello_nextflow/06_hello_config/#22-imposta-le-allocazioni-delle-risorse-per-tutti-i-processi","title":"2.2. Imposta le allocazioni delle risorse per tutti i processi","text":"<p>La profilazione mostra che i processi nel nostro workflow di formazione sono molto leggeri, quindi riduciamo l'allocazione di memoria predefinita a 1 GB per processo.</p> <p>Aggiungere quanto segue al file <code>nextflow.config</code>:</p> nextflow.config<pre><code>process {\n    memory = 1.GB\n}\n</code></pre>"},{"location":"it/hello_nextflow/06_hello_config/#23-imposta-le-allocazioni-delle-risorse-per-un-singolo-processo","title":"2.3. Imposta le allocazioni delle risorse per un singolo processo","text":"<p>Allo stesso tempo, faremo finta che il processo <code>cowpy</code> richieda pi\u00f9 risorse degli altri, cos\u00ec da poter dimostrare come adattare le allocazioni per un singolo processo.</p> DopoPrima nextflow.config<pre><code>process {\n    memory = 1.GB\n    withName: 'cowpy' {\n        memory = 2.GB\n        cpus = 2\n    }\n}\n</code></pre> nextflow.config<pre><code>process {\n    memory = 1.GB\n}\n</code></pre> <p>Con questa configurazione, tutti i processi richiederanno 1 GB di memoria e una singola CPU (impostazione predefinita), tranne il processo <code>cowpy</code>, che richieder\u00e0 2 GB e 2 CPU.</p> <p>Note</p> <p>Se hai una macchina con poche CPU e ne assegni un numero elevato per processo, potresti vedere le chiamate di processo accodarsi una dietro l'altra. Questo perch\u00e9 Nextflow assicura che non richiediamo pi\u00f9 CPU di quelle disponibili.</p>"},{"location":"it/hello_nextflow/06_hello_config/#24-esegui-il-workflow-con-la-configurazione-modificata","title":"2.4. Esegui il workflow con la configurazione modificata","text":"<p>Proviamo a fare una prova specificando un nome file diverso per il report di profilazione, in modo da poter confrontare le prestazioni prima e dopo le modifiche alla configurazione.</p> <pre><code>nextflow run hello-config.nf -with-report report-config-2.html\n</code></pre> <p>Probabilmente non noterai alcuna differenza reale, poich\u00e9 si tratta di un carico di lavoro molto ridotto, ma questo \u00e8 l'approccio che utilizzeresti per analizzare i requisiti di prestazioni e risorse di un workflow reale.</p> <p>\u00c8 molto utile quando i tuoi processi hanno requisiti di risorse diversi. Ti consente di dimensionare correttamente le allocazioni di risorse che imposti per ogni processo in base a dati effettivi, non a supposizioni.</p> <p>Note</p> <p>Questo \u00e8 solo un piccolo assaggio di ci\u00f2 che puoi fare per ottimizzare l'uso delle risorse. Nextflow stesso ha una logica di retry dinamica davvero interessante, utilizzata per riprovare i lavori che falliscono a causa di limitazioni di risorse. Inoltre, la piattaforma Seqera offre anche strumenti basati sull'intelligenza artificiale per ottimizzare automaticamente le allocazioni delle risorse.</p> <p>Parleremo di entrambi gli approcci in una prossima parte di questo corso di formazione.</p>"},{"location":"it/hello_nextflow/06_hello_config/#25-aggiungere-limiti-alle-risorse","title":"2.5. Aggiungere limiti alle risorse","text":"<p>A seconda dell'esecutore di elaborazione e dell'infrastruttura di elaborazione che stai utilizzando, potrebbero esserci dei vincoli su ci\u00f2 che puoi (o devi) allocare. Ad esempio, il tuo cluster potrebbe richiedere di rimanere entro determinati limiti.</p> <p>Puoi utilizzare la direttiva <code>resourceLimits</code> per impostare le limitazioni pertinenti. La sintassi appare cos\u00ec quando \u00e8 da sola in un blocco di processo:</p> Syntax example<pre><code>process {\n    resourceLimits = [\n        memory: 750.GB,\n        cpus: 200,\n        time: 30.d\n    ]\n}\n</code></pre> <p>Nextflow tradurr\u00e0 questi valori nelle istruzioni appropriate a seconda dell'esecutore specificato.</p> <p>Non lo eseguiremo, poich\u00e9 non abbiamo accesso all'infrastruttura pertinente nell'ambiente di formazione. Tuttavia, se provassi a eseguire il workflow con allocazioni di risorse che superano questi limiti, quindi cercassi il comando <code>sbatch</code> nel file di script <code>.command.run</code>, vedresti che le richieste che vengono effettivamente inviate all'esecutore sono limitate ai valori specificati da <code>resourceLimits</code>.</p> <p>Note</p> <p>Il progetto nf-core ha compilato una raccolta di file di configurazione condivisa da varie istituzioni in tutto il mondo, che copre un'ampia gamma di esecutori HPC e cloud.</p> <p>Tali configurazioni condivise sono preziose sia per le persone che lavorano l\u00ec e che possono quindi utilizzare la configurazione della propria istituzione cos\u00ec com'\u00e8, sia come modello per coloro che desiderano sviluppare una configurazione per la propria infrastruttura.</p>"},{"location":"it/hello_nextflow/06_hello_config/#conclusione_1","title":"Conclusione","text":"<p>Sai come generare un report di profilazione per valutare l'utilizzo delle risorse e come modificare le allocazioni delle risorse per tutti i processi e/o per singoli processi, nonch\u00e9 come impostare limitazioni delle risorse per l'esecuzione su HPC.</p>"},{"location":"it/hello_nextflow/06_hello_config/#prossimi-passi_1","title":"Prossimi passi","text":"<p>Impara a usare un file di parametri per memorizzare i parametri del workflow.</p>"},{"location":"it/hello_nextflow/06_hello_config/#3-utilizzare-un-file-di-parametri-per-memorizzare-i-parametri-del-workflow","title":"3. Utilizzare un file di parametri per memorizzare i parametri del workflow","text":"<p>Finora abbiamo esaminato la configurazione dal punto di vista tecnico dell'infrastruttura di elaborazione. Ora prendiamo in considerazione un altro aspetto della configurazione del workflow che \u00e8 molto importante per la riproducibilit\u00e0: la configurazione dei parametri del workflow.</p> <p>Attualmente, il nostro workflow \u00e8 impostato per accettare diversi valori di parametri tramite la riga di comando, con valori predefiniti impostati nello script del workflow stesso. Questo va bene per un semplice workflow con pochissimi parametri che devono essere impostati per una determinata esecuzione. Tuttavia, molti workflows del mondo reale avranno molti pi\u00f9 parametri che potrebbero essere specifici dell'esecuzione e inserirli tutti nella riga di comando sarebbe noioso e soggetto a errori.</p> <p>Nextflow ci consente di specificare i parametri tramite un file di parametri in formato JSON, il che rende molto comodo gestire e distribuire set alternativi di valori predefiniti, ad esempio, nonch\u00e9 valori di parametri specifici dell'esecuzione.</p> <p>Forniamo un file di parametri di esempio nella directory corrente, denominato <code>test-params.json</code>:</p> test-params.json<pre><code>{\n  \"greeting\": \"greetings.csv\",\n  \"batch\": \"Trio\",\n  \"character\": \"turkey\"\n}\n</code></pre> <p>Questo file di parametri contiene una coppia chiave-valore per ciascuno degli input previsti dal nostro workflow.</p>"},{"location":"it/hello_nextflow/06_hello_config/#31-eseguire-il-workflow-utilizzando-un-file-di-parametri","title":"3.1. Eseguire il workflow utilizzando un file di parametri","text":"<p>Per eseguire il workflow con questo file di parametri, \u00e8 sufficiente aggiungere <code>-params-file &lt;nomefile&gt;</code> al comando di base.</p> <pre><code>nextflow run hello-config.nf -params-file test-params.json\n</code></pre> <p>Funziona! E come previsto, produce gli stessi output di prima.</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-config.nf` [disturbed_sammet] DSL2 - revision: ede9037d02\n\nexecutor &gt;  local (8)\n[f0/35723c] sayHello (2)       | 3 of 3 \u2714\n[40/3efd1a] convertToUpper (3) | 3 of 3 \u2714\n[17/e97d32] collectGreetings   | 1 of 1 \u2714\n[98/c6b57b] cowpy              | 1 of 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>Questo potrebbe sembrare eccessivo quando hai solo pochi parametri da specificare, ma alcune pipeline si aspettano decine di parametri. In quei casi, usare un file di parametri ci consentir\u00e0 di fornire valori di parametri in fase di esecuzione senza dover digitare lunghe righe di comando e senza modificare lo script del workflow.</p>"},{"location":"it/hello_nextflow/06_hello_config/#conclusione_2","title":"Conclusione","text":"<p>Sai come gestire i parametri predefiniti e sovrascriverli in fase di esecuzione utilizzando un file di parametri.</p>"},{"location":"it/hello_nextflow/06_hello_config/#prossimi-passi_2","title":"Prossimi passi","text":"<p>Scopri come utilizzare i profili per passare comodamente da una configurazione alternativa all'altra.</p>"},{"location":"it/hello_nextflow/06_hello_config/#4-determinare-quale-esecutore-dovrebbe-essere-utilizzato-per-svolgere-il-lavoro","title":"4. Determinare quale esecutore dovrebbe essere utilizzato per svolgere il lavoro","text":"<p>Finora abbiamo eseguito la nostra pipeline con l'esecutore locale. Questo esegue ogni task sulla macchina su cui \u00e8 in esecuzione Nextflow. Quando Nextflow inizia, esamina le CPU e la memoria disponibili. Se le risorse dei task pronti per l'esecuzione superano le risorse disponibili, Nextflow tratterr\u00e0 gli ultimi task dall'esecuzione fino al completamento di uno o pi\u00f9 task precedenti, liberando le risorse necessarie.</p> <p>Per carichi di lavoro molto grandi, potresti scoprire che la tua macchina locale \u00e8 un collo di bottiglia, sia perch\u00e9 hai un singolo task che richiede pi\u00f9 risorse di quelle disponibili, sia perch\u00e9 hai cos\u00ec tanti task che aspettare che un singolo computer li esegua richiederebbe troppo tempo. L'esecutore locale \u00e8 comodo ed efficiente, ma \u00e8 limitato a quel singolo computer. Nextflow supporta molti backend di esecuzione diversi, inclusi gli scheduler HPC (Slurm, LSF, SGE, PBS, Moab, OAR, Bridge, HTCondor e altri) cos\u00ec come i backend di esecuzione cloud come (AWS Batch, Google Cloud Batch, Azure Batch, Kubernetes e altri).</p> <p>Ognuno di questi sistemi utilizza tecnologie, sintassi e configurazioni diverse per definire come dovrebbe essere definito un job. Ad esempio, /se non avessimo Nextflow/, un job che richiede 8 CPU e 4 GB di RAM per essere eseguito sulla coda \"my-science-work\" dovrebbe includere la seguente configurazione su SLURM e inviare il job tramite <code>sbatch</code>:</p> <pre><code>#SBATCH -o /path/to/my/task/directory/my-task-1.log\n#SBATCH --no-requeue\n#SBATCH -c 8\n#SBATCH --mem 4096M\n#SBATCH -p my-science-work\n</code></pre> <p>Se volessi rendere il workflow disponibile a un collega che utilizza PBS, dovrei ricordarmi di utilizzare un programma di invio diverso, <code>qsub</code>, e dovrei modificare i miei script per utilizzare una nuova sintassi per le risorse:</p> <pre><code>#PBS -o /path/to/my/task/directory/my-task-1.log\n#PBS -j oe\n#PBS -q my-science-work\n#PBS -l nodes=1:ppn=5\n#PBS -l mem=4gb\n</code></pre> <p>Se volessi usare SGE, la configurazione sarebbe leggermente diversa:</p> <pre><code>#$ -o /path/to/my/task/directory/my-task-1.log\n#$ -j y\n#$ -terse\n#$ -notify\n#$ -q my-science-work\n#$ -l slots=5\n#$ -l h_rss=4096M,mem_free=4096M\n</code></pre> <p>L'esecuzione su un singolo motore di esecuzione cloud richiederebbe di nuovo un nuovo approccio, probabilmente utilizzando un SDK che utilizza le API della piattaforma cloud.</p> <p>Nextflow semplifica la scrittura di un singolo flusso di lavoro che pu\u00f2 essere eseguito su ciascuna di queste diverse infrastrutture e sistemi, senza dover modificare il workflow L'esecutore \u00e8 soggetto a una direttiva di processo denominata <code>executor</code>. Per impostazione predefinita \u00e8 impostato su <code>local</code>, quindi \u00e8 implicita la seguente configurazione:</p> Built-in configuration<pre><code>process {\n    executor = 'local'\n}\n</code></pre>"},{"location":"it/hello_nextflow/06_hello_config/#41-targeting-di-un-backend-diverso","title":"4.1. Targeting di un backend diverso","text":"<p>Per impostazione predefinita, questo ambiente di formazione non include uno scheduler HPC in esecuzione, ma se si esegue su un sistema con SLURM installato, ad esempio, \u00e8 possibile far s\u00ec che Nextflow converta <code>cpus</code>, <code>memory</code>, <code>queue</code> e altre direttive di processo nella sintassi corretta in fase di esecuzione aggiungendo le seguenti righe al file <code>nextflow.config</code>:</p> nextflow.config<pre><code>process {\n    executor = 'slurm'\n}\n</code></pre> <p>E... questo \u00e8 tutto! Come detto prima, questo presuppone che Slurm stesso sia gi\u00e0 impostato per te, ma questo \u00e8 tutto ci\u00f2 che Nextflow stesso deve sapere.</p> <p>In pratica stiamo dicendo a Nextflow di generare uno script di invio Slurm e di inviarlo usando un comando <code>sbatch</code>.</p>"},{"location":"it/hello_nextflow/06_hello_config/#conclusione_3","title":"Conclusione","text":"<p>Ora sai come modificare l'esecutore per utilizzare diversi tipi di infrastrutture informatiche.</p>"},{"location":"it/hello_nextflow/06_hello_config/#prossimi-passi_3","title":"Prossimi passi?","text":"<p>Scopri come controllare le risorse assegnate per l'esecuzione dei processi.</p>"},{"location":"it/hello_nextflow/06_hello_config/#5-utilizza-i-profili-per-selezionare-configurazioni-preimpostate","title":"5. Utilizza i profili per selezionare configurazioni preimpostate","text":"<p>Potresti voler passare da un'impostazione alternativa all'altra a seconda dell'infrastruttura informatica che stai utilizzando. Ad esempio, potresti voler sviluppare ed eseguire test su piccola scala localmente sul tuo laptop, quindi eseguire carichi di lavoro su larga scala su HPC o cloud.</p> <p>Nextflow ti consente di impostare profili che descrivono diverse configurazioni, che puoi quindi selezionare in fase di esecuzione utilizzando un argomento della riga di comando, anzich\u00e9 dover modificare il file di configurazione stesso.</p>"},{"location":"it/hello_nextflow/06_hello_config/#51-creare-profili-per-passare-dallo-sviluppo-locale-allesecuzione-su-hpc","title":"5.1. Creare profili per passare dallo sviluppo locale all'esecuzione su HPC","text":"<p>Impostiamo due profili alternativi: uno per l'esecuzione di carichi su piccola scala su un computer normale, dove utilizzeremo contenitori Docker, e uno per l'esecuzione su un HPC universitario con uno scheduler Slurm, dove utilizzeremo pacchetti Conda.</p> <p>Aggiungere quanto segue al file <code>nextflow.config</code>:</p> nextflow.config<pre><code>profiles {\n    my_laptop {\n        process.executor = 'local'\n        docker.enabled = true\n    }\n    univ_hpc {\n        process.executor = 'slurm'\n        conda.enabled = true\n        process.resourceLimits = [\n            memory: 750.GB,\n            cpus: 200,\n            time: 30.d\n        ]\n    }\n}\n</code></pre> <p>Come puoi vedere, per l'HPC universitario stiamo anche specificando le limitazioni delle risorse.</p>"},{"location":"it/hello_nextflow/06_hello_config/#52-esegui-workflow-con-un-profilo","title":"5.2. Esegui workflow con un profilo.","text":"<p>Per specificare un profilo nella nostra riga di comando Nextflow, utilizziamo l'argomento <code>-profile</code>.</p> <p>Proviamo a eseguire il workflow con la configurazione <code>my_laptop</code>.</p> <pre><code>nextflow run hello-config.nf -profile my_laptop\n</code></pre> <p>Questo produce ancora il seguente output:</p> <pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-config.nf` [gigantic_brazil] DSL2 - revision: ede9037d02\n\nexecutor &gt;  local (8)\n[58/da9437] sayHello (3)       | 3 of 3 \u2714\n[35/9cbe77] convertToUpper (2) | 3 of 3 \u2714\n[67/857d05] collectGreetings   | 1 of 1 \u2714\n[37/7b51b5] cowpy              | 1 of 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>Come potete vedere, questo ci consente di passare da una configurazione all'altra in modo molto pratico durante l'esecuzione.</p> <p>Warning</p> <p>Il profilo <code>univ_hpc</code> non verr\u00e0 eseguito correttamente nell'ambiente di training poich\u00e9 non abbiamo accesso a uno scheduler Slurm.</p> <p>Se in futuro dovessimo trovare altri elementi di configurazione che si verificano sempre contemporaneamente a questi, potremmo semplicemente aggiungerli al profilo/i corrispondente/i. Possiamo anche creare profili aggiuntivi se ci sono altri elementi di configurazione che vogliamo raggruppare.</p>"},{"location":"it/hello_nextflow/06_hello_config/#53-crea-un-profilo-di-prova","title":"5.3. Crea un profilo di prova","text":"<p>I profili non servono solo per la configurazione dell'infrastruttura. Possiamo anche usarli per impostare valori predefiniti per i parametri del workflow, per rendere pi\u00f9 facile per altri provare il workflow senza dover raccogliere autonomamente i valori di input appropriati. Questo \u00e8 inteso come alternativa all'uso di un file di parametri.</p> <p>La sintassi per esprimere i valori predefiniti \u00e8 la stessa di quando li scriviamo nel file del workflow stesso, tranne per il fatto che li racchiudiamo in un blocco denominato <code>test</code>:</p> Syntax example<pre><code>    test {\n        params.&lt;parameter1&gt;\n        params.&lt;parameter2&gt;\n        ...\n    }\n</code></pre> <p>Se aggiungiamo un profilo di prova per il nostro workflow, il blocco <code>profili</code> diventa:</p> nextflow.config<pre><code>profiles {\n    my_laptop {\n        process.executor = 'local'\n        docker.enabled = true\n    }\n    univ_hpc {\n        process.executor = 'slurm'\n        conda.enabled = true\n        process.resourceLimits = [\n            memory: 750.GB,\n            cpus: 200,\n            time: 30.d\n        ]\n    }\n    test {\n        params.greeting = 'greetings.csv'\n        params.batch = 'test-batch'\n        params.character = 'turkey'\n    }\n}\n</code></pre> <p>Proprio come per i profili di configurazione tecnica, \u00e8 possibile impostare pi\u00f9 profili diversi specificando i parametri con qualsiasi nome arbitrario si desideri.</p>"},{"location":"it/hello_nextflow/06_hello_config/#54-esegui-il-workflow-localmente-con-il-profilo-di-prova","title":"5.4. Esegui il workflow localmente con il profilo di prova","text":"<p>Per comodit\u00e0, i profili non si escludono a vicenda, quindi possiamo specificare pi\u00f9 profili nella nostra riga di comando utilizzando la seguente sintassi <code>-profile &lt;profile1&gt;,&lt;profile2&gt;</code> (per qualsiasi numero di profili).</p> <p>Note</p> <p>Se si combinano profili che impostano valori per gli stessi elementi di configurazione e sono descritti nello stesso file di configurazione, Nextflow risolver\u00e0 il conflitto utilizzando il valore letto per ultimo (ovvero, qualsiasi cosa si trovi dopo nel file). Se le impostazioni in conflitto sono impostate in diverse origini di configurazione, si applica l'ordine di precedenza predefinito.</p> <p>Proviamo ad aggiungere il profilo di prova al nostro comando precedente:</p> <pre><code>nextflow run hello-config.nf -profile my_laptop,test\n</code></pre> <p>Ci\u00f2 dovrebbe produrre quanto segue:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `hello-config.nf` [gigantic_brazil] DSL2 - revision: ede9037d02\n\nexecutor &gt;  local (8)\n[58/da9437] sayHello (3)       | 3 of 3 \u2714\n[35/9cbe77] convertToUpper (2) | 3 of 3 \u2714\n[67/857d05] collectGreetings   | 1 of 1 \u2714\n[37/7b51b5] cowpy              | 1 of 1 \u2714\nThere were 3 greetings in this batch\n</code></pre> <p>Ci\u00f2 significa che finch\u00e9 distribuiamo file di dati di prova con il codice del workflow, chiunque pu\u00f2 provare rapidamente il workflow senza dover fornire i propri input tramite la riga di comando o un file di parametri.</p> <p>Note</p> <p>Possiamo anche puntare agli URL per file pi\u00f9 grandi che sono archiviati esternamente. Nextflow li scaricher\u00e0 automaticamente finch\u00e9 c'\u00e8 una connessione aperta.</p>"},{"location":"it/hello_nextflow/06_hello_config/#conclusione_4","title":"Conclusione","text":"<p>Sai come usare i profili per selezionare una configurazione preimpostata in fase di esecuzione con il minimo sforzo. Pi\u00f9 in generale, sai come configurare le esecuzioni del tuo workflow per adattarle a diverse piattaforme di elaborazione e migliorare la riproducibilit\u00e0 delle tue analisi.</p>"},{"location":"it/hello_nextflow/06_hello_config/#prossimi-passi_4","title":"Prossimi passi","text":"<p>Festeggia e datti una bella pacca sulla spalla! Hai completato il tuo primo corso per sviluppatori Nextflow.</p> <p>Successivamente, ti chiederemo di compilare un brevissimo sondaggio sulla tua esperienza con questo corso di formazione, dopodich\u00e9 ti indirizzeremo a una pagina con link ad ulteriori risorse di formazione e link utili.</p>"},{"location":"ko/","title":"Nextflow Training","text":"<p>Nextflow \ucee4\ubba4\ub2c8\ud2f0 \uad50\uc721 \ud3ec\ud138\uc5d0 \uc624\uc2e0 \uac83\uc744 \ud658\uc601\ud569\ub2c8\ub2e4!</p> <p>\uc774 \uc6f9\uc0ac\uc774\ud2b8\uc5d0\ub294 \uc5ec\ub7ec \uac00\uc9c0 \ub2e4\uc591\ud55c \uad50\uc721 \uacfc\uc815\uc774 \uc900\ube44\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc544\ub798\ub85c \uc2a4\ud06c\ub864\ud558\uc5ec \uc5ec\ub7ec\ubd84\uc5d0\uac8c \ub9de\ub294 \uacfc\uc815\uc744 \ucc3e\uc544\ubcf4\uc138\uc694!</p> <p>\uc544\ub798 \ub098\uc5f4\ub41c \uad50\uc721 \uacfc\uc815\ub4e4\uc740 \uc140\ud504 \uc11c\ube44\uc2a4 \ub9ac\uc18c\uc2a4\ub85c \ud65c\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc73c\uba70, \uc5b8\uc81c\ub4e0\uc9c0 \ud63c\uc790\uc11c \ud559\uc2b5\uc744 \uc9c4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4 (\uc790\uc138\ud55c \ub0b4\uc6a9\uc740 Environment Setup\uc744 \ucc38\uace0\ud558\uc138\uc694). \ud558\uc9c0\ub9cc \uadf8\ub8f9 \uad50\uc721 \ud589\uc0ac\uc5d0 \ucc38\uc5ec\ud558\uba74 \ub354\uc6b1 \ud6a8\uacfc\uc801\uc73c\ub85c \ud559\uc2b5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <ul> <li>nf-core \ucee4\ubba4\ub2c8\ud2f0\uc5d0\uc11c \uc815\uae30\uc801\uc73c\ub85c \ubb34\ub8cc \uc628\ub77c\uc778 \ud589\uc0ac\ub97c \uc9c4\ud589\ud569\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 nf-core \ud589\uc0ac \ud398\uc774\uc9c0\ub97c \ud655\uc778\ud558\uc138\uc694.</li> <li>Nextflow\ub97c \uac1c\ubc1c\ud55c \ud68c\uc0ac\uc778 Seqera\uc5d0\uc11c\ub294 \ub2e4\uc591\ud55c \uad50\uc721 \ud589\uc0ac\ub97c \uc6b4\uc601\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. Seqera \ud589\uc0ac \ud398\uc774\uc9c0\ub97c \ubc29\ubb38\ud558\uc5ec 'Seqera Sessions' \ubc0f 'Nextflow Summit'\uc744 \ud655\uc778\ud574\ubcf4\uc138\uc694.</li> <li>\uc800\ud76c \ucee4\ubba4\ub2c8\ud2f0 \ud300\uc740 \uc81c3\uc790 \uae30\uad00\uc774 \uc8fc\ucd5c\ud558\ub294 \uad50\uc721\ub3c4 \uc815\uae30\uc801\uc73c\ub85c \uc9c4\ud589\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ud574\ub2f9 \uad50\uc721\uc740 \uc77c\ubc18\uc801\uc73c\ub85c \uc8fc\ucd5c \uce21\uc5d0\uc11c \uacf5\uc9c0 \ubc0f \uc2e0\uccad\uc744 \uad00\ub9ac\ud569\ub2c8\ub2e4.</li> </ul> <p>\uc900\ube44\uac00 \ub418\uc168\ub2e4\uba74, \uc774 \ud398\uc774\uc9c0\ub098 \uc120\ud0dd\ud55c \uacfc\uc815\uc758 \uc778\ub371\uc2a4 \ud398\uc774\uc9c0\uc5d0\uc11c 'Open in GitHub Codespaces' \ubc84\ud2bc\uc744 \ud074\ub9ad\ud558\uc138\uc694. \uc6f9 \uae30\ubc18\uc758 \uad50\uc721 \ud658\uacbd\uc774 \uc5f4\ub9bd\ub2c8\ub2e4 (GitHub \ubb34\ub8cc \uacc4\uc815 \ud544\uc694).</p> <p></p>"},{"location":"ko/#_1","title":"\uad50\uc721 \ud658\uacbd \uc124\uc815","text":"<p>\ud658\uacbd \uc124\uc815</p> <p> \ucc98\uc74c \ud658\uacbd\uc744 \uc124\uc815\ud574\ubcf4\uc138\uc694.</p> <p>\ubaa8\ub4e0 \uad50\uc721 \uacfc\uc815\uc5d0\uc11c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \ud559\uc2b5 \ud658\uacbd\uc744 \uc124\uc815\ud558\ub294 \ubc29\ubc95\uc5d0 \ub300\ud55c \uc548\ub0b4\uc785\ub2c8\ub2e4. GitHub Codespaces\uc5d0 \ub300\ud55c \uae30\ubcf8 \uc18c\uac1c\uc640 \ub85c\uceec \ucef4\ud4e8\ud130\uc5d0\uc11c \uc9c1\uc811 \uc791\uc5c5\ud560 \uc218 \uc788\ub294 \ub300\uccb4 \uc124\uce58 \ubc29\ubc95\ub3c4 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\ud658\uacbd \uc124\uc815 \uad50\uc721 \uc2dc\uc791\ud558\uae30 </p>"},{"location":"ko/#nextflow","title":"Nextflow \uc785\ubb38\uc790\ub97c \uc704\ud55c \uacfc\uc815","text":"<p>\uc774 \uacfc\uc815\ub4e4\uc740 Nextflow\ub97c \uc644\uc804\ud788 \ucc98\uc74c \uc811\ud558\ub294 \ubd84\ub4e4\uc744 \uc704\ud55c \uae30\ucd08 \uc218\uc5c5\uc73c\ub85c, \ud2b9\uc815 \ubd84\uc57c\uc758 \uc804\ubb38 \uc9c0\uc2dd \uc5c6\uc774\ub3c4 \ud559\uc2b5\ud560 \uc218 \uc788\ub3c4\ub85d \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \uacfc\uc815\uc740 \ub2e8\uacc4\ubcc4\ub85c \uc2e4\ub825\uc744 \uc313\uc744 \uc218 \uc788\ub3c4\ub85d \uc5ec\ub7ec \uac1c\uc758 \uad50\uc721 \ubaa8\ub4c8\ub85c \uc774\ub8e8\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>Hello Nextflow</p> <p> Nextflow\ub85c \ud30c\uc774\ud504\ub77c\uc778 \uac1c\ubc1c \ubc30\uc6b0\uae30</p> <p> \ub3d9\uc601\uc0c1 \uc790\ub8cc \uc81c\uacf5</p> <p>\uc774 \uacfc\uc815\uc740 \uc790\uc2e0\ub9cc\uc758 \ud30c\uc774\ud504\ub77c\uc778\uc744 \uac1c\ubc1c\ud558\uace0\uc790 \ud558\ub294 \uc785\ubb38\uc790\ub97c \uc704\ud55c \uad50\uc721\uc785\ub2c8\ub2e4. Nextflow \uc5b8\uc5b4\uc758 \ud575\uc2ec \uad6c\uc131 \uc694\uc18c\ub4e4\uc744 \ucda9\ubd84\ud788 \uc0c1\uc138\ud558\uac8c \ub2e4\ub8e8\uba70, \ub2e8\uc21c\ud558\uc9c0\ub9cc \uc644\uc804\ud55c \uae30\ub2a5\uc744 \uac16\ucd98 \ud30c\uc774\ud504\ub77c\uc778\uc744 \uac1c\ubc1c\ud560 \uc218 \uc788\ub3c4\ub85d \ub3c4\uc640\uc90d\ub2c8\ub2e4. \ub610\ud55c \ud30c\uc774\ud504\ub77c\uc778 \uc124\uacc4, \uac1c\ubc1c, \uc124\uc815\uc5d0 \ud544\uc694\ud55c \uc8fc\uc694 \uac1c\ub150\ub4e4\ub3c4 \ud568\uaed8 \ub2e4\ub8f9\ub2c8\ub2e4.</p> <p>Hello Nextflow \uad50\uc721 \uc2dc\uc791\ud558\uae30 </p> <p>\uace7 \ucd9c\uc2dc \uc608\uc815: \"Nextflow Run\" \u2014 Nextflow \ud30c\uc774\ud504\ub77c\uc778 \uc2e4\ud589 \ubc30\uc6b0\uae30 (\ucf54\ub4dc \uac1c\ubc1c \uc5c6\uc774 \uc2e4\ud589\ub9cc \ub2e4\ub8f8)</p>"},{"location":"ko/#nextflow_1","title":"\uacfc\ud559 \ubd84\uc57c\ub97c \uc704\ud55c Nextflow","text":"<p>\uc774 \uacfc\uc815\ub4e4\uc740 \uc704\uc758 'Hello Nextflow'\uc5d0\uc11c \uc18c\uac1c\ud55c \uac1c\ub150\uacfc \uad6c\uc131 \uc694\uc18c\ub4e4\uc744 \uc2e4\uc81c \uacfc\ud559 \ubd84\uc57c\uc758 \ud65c\uc6a9 \uc0ac\ub840\uc5d0 \uc801\uc6a9\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uacfc\uc815\uc740 \ub2e8\uacc4\uc801\uc73c\ub85c \uc2e4\ub825\uc744 \uc313\uc544\uac08 \uc218 \uc788\ub3c4\ub85d \uc5ec\ub7ec \uac1c\uc758 \uad50\uc721 \ubaa8\ub4c8\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc720\uc804\uccb4\ud559\uc744 \uc704\ud55c Nextflow</p> <p> Nextflow\ub85c \uc720\uc804\uccb4\ud559 \ud30c\uc774\ud504\ub77c\uc778 \uac1c\ubc1c \ubc30\uc6b0\uae30</p> <p>\uc774 \uacfc\uc815\uc740 \uc790\uc2e0\ub9cc\uc758 \uc720\uc804\uccb4\ud559 \ud30c\uc774\ud504\ub77c\uc778\uc744 \uac1c\ubc1c\ud558\uace0\uc790 \ud558\ub294 \uc5f0\uad6c\uc790\ub97c \uc704\ud55c \uad50\uc721\uc785\ub2c8\ub2e4. \ubcc0\uc774 \ubd84\uc11d(variant calling) \uc0ac\ub840\ub97c \ud1b5\ud574 \uac04\ub2e8\ud558\uba74\uc11c\ub3c4 \uae30\ub2a5\uc801\uc778 \uc720\uc804\uccb4\ud559 \ud30c\uc774\ud504\ub77c\uc778\uc744 \uac1c\ubc1c\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.</p> <p>\uc720\uc804\uccb4\ud559 \uad50\uc721 \uc2dc\uc791\ud558\uae30 </p> <p>RNAseq\ub97c \uc704\ud55c Nextflow</p> <p> Nextflow\ub85c RNAseq \ub370\uc774\ud130 \ucc98\ub9ac \ud30c\uc774\ud504\ub77c\uc778 \uac1c\ubc1c \ubc30\uc6b0\uae30</p> <p>\uc774 \uacfc\uc815\uc740 \uc790\uc2e0\ub9cc\uc758 RNAseq \ud30c\uc774\ud504\ub77c\uc778\uc744 \uac1c\ubc1c\ud558\uace0\uc790 \ud558\ub294 \uc5f0\uad6c\uc790\ub97c \uc704\ud55c \uad50\uc721\uc785\ub2c8\ub2e4. \ubc8c\ud06c RNAseq \ucc98\ub9ac \uc0ac\ub840\ub97c \ud1b5\ud574 \uac04\ub2e8\ud558\uba74\uc11c\ub3c4 \uae30\ub2a5\uc801\uc778 RNAseq \ud30c\uc774\ud504\ub77c\uc778\uc744 \uac1c\ubc1c\ud558\ub294 \ubc29\ubc95\uc744 \uc18c\uac1c\ud569\ub2c8\ub2e4.</p> <p>RNAseq \uad50\uc721 \uc2dc\uc791\ud558\uae30 </p> <p>\uc5b4\ub5a4 \ubd84\uc57c\ub098 \uc0ac\uc6a9 \uc0ac\ub840\uac00 \ucd94\uac00\ub418\uc5c8\uc73c\uba74 \ud558\ub294\uc9c0 \ucee4\ubba4\ub2c8\ud2f0 \ud3ec\ub7fc\uc758 \uad50\uc721 \uc139\uc158\uc5d0 \uae00\uc744 \ub0a8\uaca8 \uc54c\ub824\uc8fc\uc138\uc694.</p>"},{"location":"ko/#nextflow_2","title":"Nextflow \uc2ec\ud654 \uad50\uc721","text":"<p>\uc774 \uacfc\uc815\ub4e4\uc740 Nextflow\uc758 \uae30\ub2a5\uc744 \ub354 \uc790\uc138\ud788, \ub610\ub294 \ub354 \uace0\uae09 \uc218\uc900\uc5d0\uc11c \ud65c\uc6a9\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uacfc\uc815\uc740 \uad00\ub828 \uc8fc\uc81c\uc5d0 \ub300\ud55c \uc2e4\ub825\uc744 \ub354\uc6b1 \uae4a\uc774 \uc788\uac8c \ub2e4\ub4ec\uc744 \uc218 \uc788\ub3c4\ub85d \ud558\ub098 \uc774\uc0c1\uc758 \uad50\uc721 \ubaa8\ub4c8\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4</p> <p>\uc0ac\uc774\ub4dc \ud018\uc2a4\ud2b8</p> <p> \ub2e4\uc591\ud55c \uc8fc\uc81c\ub97c \ub2e4\ub8e8\ub294 \uad50\uc721 \ubaa8\ub4c8</p> <p>\uc774 \uacfc\uc815\uc740 Nextflow \uac1c\ubc1c\uc790\uac00 \uae30\uc220\uc758 \ud3ed\uc744 \ub113\ud788\uac70\ub098 \uae4a\uc774\ub97c \ub354\ud558\uace0\uc790 \ud560 \ub54c \uc801\ud569\ud569\ub2c8\ub2e4. \ubaa8\ub4c8\uc740 \uc21c\uc11c\ub300\ub85c \uc81c\uacf5\ub418\uc9c0\ub9cc, \ud559\uc2b5\uc790\ub294 \uc790\uc720\ub86d\uac8c \uc6d0\ud558\ub294 \uc8fc\uc81c\ubd80\ud130 \uc120\ud0dd\ud558\uc5ec \ud559\uc2b5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \u2018Hello Nextflow\u2019 \uacfc\uc815\uc744 \ub118\uc5b4\uc11c\ub294 \ub0b4\uc6a9\uc774 \ud544\uc694\ud55c \uacbd\uc6b0, \uac01 \ubaa8\ub4c8 \uac1c\uc694\uc5d0 \uadf8\uc5d0 \ub300\ud55c \uc758\uc874\uc131\uc774 \uba85\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc0ac\uc774\ub4dc \ud018\uc2a4\ud2b8 \uad50\uc721 \uc2dc\uc791\ud558\uae30 </p> <p>\uae30\ucd08 \uad50\uc721</p> <p> Nextflow\uc758 \ubaa8\ub4e0 \uae30\ub2a5\uc744 \uc544\uc6b0\ub974\ub294 \ud3ec\uad04\uc801\uc778 \uad50\uc721 \uc790\ub8cc</p> <p>\uc774 \uae30\ucd08 \uad50\uc721 \uc790\ub8cc\ub294 Nextflow\uc758 \uc804\ubc18\uc801\uc778 \ub0b4\uc6a9\uc744 \ub2e4\ub8e8\uba70, \ubcf5\uc7a1\ud55c \uc6cc\ud06c\ud50c\ub85c\uc6b0\ub97c \uad6c\ucd95\ud558\ub824\ub294 \ubd84\ub4e4\uc744 \uc704\ud55c \ucc38\uace0 \uc790\ub8cc\ub85c \ud65c\uc6a9\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uae30\ucd08 \uad50\uc721 \uc2dc\uc791\ud558\uae30 </p> <p>\uace0\uae09 \uad50\uc721</p> <p> Nextflow \ub9c8\uc2a4\ud130\ub97c \uc704\ud55c \uace0\uae09 \uad50\uc721 \uc790\ub8cc</p> <p>Nextflow \uc5b8\uc5b4\uc640 \ub7f0\ud0c0\uc784\uc758 \uace0\uae09 \uae30\ub2a5\uc744 \ud0d0\uc0c9\ud558\uba70, \uc774\ub97c \ud1b5\ud574 \ub370\uc774\ud130 \uc9d1\uc57d\uc801\uc778 \uc6cc\ud06c\ud50c\ub85c\uc6b0\ub97c \ud6a8\uc728\uc801\uc774\uace0 \ud655\uc7a5 \uac00\ub2a5\ud558\uac8c \uc791\uc131\ud558\ub294 \ubc29\ubc95\uc744 \ubc30\uc6b8 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uace0\uae09 \uad50\uc721 \uc2dc\uc791\ud558\uae30 </p>"},{"location":"ko/#_2","title":"\uae30\ud0c0 / \uc2e4\ud5d8\uc801 \uacfc\uc815","text":"<p>\uc774 \uad50\uc721 \uacfc\uc815\ub4e4\uc740 \ud604\uc7ac \uc801\uadf9\uc801\uc73c\ub85c \uc720\uc9c0\ub418\uac70\ub098 \uc9c4\ud589\ub418\uc9c0 \uc54a\uace0 \uc788\uc73c\uba70, \ud5a5\ud6c4 \ub2e4\ub978 \uc6a9\ub3c4\ub85c \uc7ac\uad6c\uc131\ub418\uac70\ub098 \uc0ad\uc81c\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud574\ub2f9 \uc790\ub8cc\ub4e4\uc740 \uad50\uc721 \ud658\uacbd\uc5d0\uc11c\ub294 \uc81c\uacf5\ub418\uc9c0 \uc54a\uc73c\uba70, GitHub \uc800\uc7a5\uc18c\uc5d0\uc11c \uc9c1\uc811 \ub2e4\uc6b4\ub85c\ub4dc\ud558\uc5ec \ub85c\uceec\uc5d0\uc11c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <ul> <li>nf-customize \u2014 nf-core \ud30c\uc774\ud504\ub77c\uc778 \uc124\uc815 (\ubb38\uc11c / \ucf54\ub4dc)</li> <li>troubleshoot \u2014 \ubb38\uc81c \ud574\uacb0 \uc5f0\uc2b5 (\ubb38\uc11c / \ucf54\ub4dc)</li> <li>hands-on (rnaseq) \u2014 \ubc8c\ud06c RNAseq \ud30c\uc774\ud504\ub77c\uc778 \uac1c\ubc1c (\uc0ac\uc6a9 \uc911\ub2e8\ub428) (\ubb38\uc11c / \ucf54\ub4dc)</li> </ul>"},{"location":"ko/#_3","title":"\uc790\ub8cc \ubaa8\uc74c","text":"<p>\uc720\uc6a9\ud55c \ucc38\uace0 \ub9c1\ud06c\ub4e4\uc744 \ube60\ub974\uac8c \ud655\uc778\ud574\ubcf4\uc138\uc694:</p> \ucc38\uace0 \uc790\ub8cc \ucee4\ubba4\ub2c8\ud2f0 Nextflow \ubb38\uc11c Nextflow Slack Nextflow \ud648\ud398\uc774\uc9c0 nf-core Seqera Seqera \ucee4\ubba4\ub2c8\ud2f0 \ud3ec\ub7fc <p>\uc5b4\ub514\uc11c\ubd80\ud130 \uc2dc\uc791\ud560\uc9c0 \ubaa8\ub974\uaca0\ub2e4\uba74 \ub3c4\uc6c0 \ubc1b\uae30 \ud398\uc774\uc9c0\ub97c \ucc38\uace0\ud558\uc138\uc694.</p>"},{"location":"ko/#_4","title":"\ud06c\ub808\ub527 \ubc0f \uae30\uc5ec","text":"<p>\uc774 \uad50\uc721 \uc790\ub8cc\ub294 Seqera\uc5d0\uc11c \uac1c\ubc1c \ubc0f \uc720\uc9c0 \uad00\ub9ac\ud558\uace0 \uc788\uc73c\uba70, \ucee4\ubba4\ub2c8\ud2f0\uc758 \uc774\uc775\uc744 \uc704\ud574 \uc624\ud508\uc18c\uc2a4 \ub77c\uc774\uc120\uc2a4(CC BY-NC-ND) \ud558\uc5d0 \ubc30\ud3ec\ub429\ub2c8\ub2e4. \ub77c\uc774\uc120\uc2a4 \uc870\uac74\uc5d0 \ub530\ub77c \uc790\uc720\ub86d\uac8c \uc7ac\uc0ac\uc6a9\ud558\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uad50\uc721\uc744 \uc9c4\ud589\ud558\ub294 \uac15\uc0ac\ubd84\uc774\ub77c\uba74, \uc0ac\uc6a9 \uacbd\ud5d8\uacfc \uac1c\uc120\uc810\uc744 \uc800\ud76c\uc5d0\uac8c \uc54c\ub824\uc8fc\uc2dc\uba74 \uac10\uc0ac\ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>\ucee4\ubba4\ub2c8\ud2f0\uc758 \uc218\uc815 \ubc0f \uac1c\uc120 \uc81c\uc548\uc744 \uc5b8\uc81c\ub4e0\uc9c0 \ud658\uc601\ud569\ub2c8\ub2e4. \uac01 \ud398\uc774\uc9c0 \uc624\ub978\ucabd \uc0c1\ub2e8\uc758  \uc544\uc774\ucf58\uc744 \ud074\ub9ad\ud558\uba74 GitHub\uc5d0\uc11c Pull Request\ub97c \ud1b5\ud574 \uc81c\uc548\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p></p> <p></p>"},{"location":"ko/envsetup/","title":"\ud658\uacbd \uc124\uc815","text":"<p>Nextflow \ucee4\ubba4\ub2c8\ud2f0 \uad50\uc721 \ud3ec\ud138\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 \uad50\uc721 \uacfc\uc815\ub4e4\uc740 GitHub Codespaces \ud658\uacbd\uc5d0\uc11c \uc0ac\uc6a9\ud558\uae30\uc5d0 \ucd5c\uc801\ud654\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>GitHub Codespaces\ub294 \uc6f9 \ube0c\ub77c\uc6b0\uc800 \ub610\ub294 \ucf54\ub4dc \ud3b8\uc9d1\uae30(\uc608: VSCode)\uc5d0\uc11c \uc811\uadfc \uac00\ub2a5\ud55c \uac00\uc0c1 \uba38\uc2e0\uc744 \uc81c\uacf5\ud558\uba70, \ud544\uc694\ud55c \ubaa8\ub4e0 \uac83\uc774 \uc0ac\uc804 \uad6c\uc131\ub418\uc5b4 \uc788\uc5b4 \ubc14\ub85c \uc0ac\uc6a9\ud558\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc774\ubbf8 GitHub Codespaces \uacc4\uc815\uc744 \uac00\uc9c0\uace0 \uacc4\uc2dc\ub2e4\uba74 \uc544\ub798 \ubc84\ud2bc\uc744 \ud074\ub9ad\ud558\uc138\uc694. \ucc98\uc74c \uc124\uc815\ud558\ub294 \uacbd\uc6b0\uc5d0\ub294 \uc774 \ubaa8\ub4c8\uc744 \uacc4\uc18d \uc9c4\ud589\ud558\uc5ec \uacc4\uc815\uc744 \uc124\uc815\ud574 \uc8fc\uc138\uc694.</p> <p>\uadf8\ub7fc \uc2dc\uc791\ud574\ubcfc\uae4c\uc694?</p> <p></p> <p>GitHub Codespaces\uc5d0 \ub300\ud55c \ub354 \uc790\uc138\ud55c \uc124\uc815 \ubc29\ubc95\uc740 GitHub Codespaces env-setup docs\ub97c \ucc38\uace0\ud558\uc138\uc694. GitHub Codespaces\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc5c6\uace0 \ub85c\uceec \uac1c\ubc1c \ud658\uacbd\uc744 \uc0ac\uc6a9\ud558\uace0\uc790 \ud558\uc2dc\ub294 \uacbd\uc6b0, documentation for local installation\ub97c \ucc38\uace0\ud558\uc138\uc694.</p> <p>Gitpod \uc0ac\uc6a9 \uc911\ub2e8 \uc548\ub0b4</p> <p>Nextflow \uad50\uc721\uc740 2025\ub144 2\uc6d4\uae4c\uc9c0 Gitpod\ub97c \uc0ac\uc6a9\ud574\uc654\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 Gitpod \uac1c\ubc1c\uc0ac\uc5d0\uc11c\ub294 \uae30\uc874\uc758 \ubb34\ub8cc \uae30\ub2a5\uc744 \uc911\ub2e8\ud558\uace0, \uc0c8\ub85c\uc6b4Gitpod Flex \uc2dc\uc2a4\ud15c\uc73c\ub85c \uc804\ud658\ud558\uae30\ub85c \uacb0\uc815\ud588\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ud55c \uc774\uc720\ub85c,\uc800\ud76c\ub294 Gitpod\uacfc \uc720\uc0ac\ud558\uac8c \ubcc4\ub3c4\uc758 \uc124\uc815 \uc5c6\uc774 \ud074\ub9ad \ud55c \ubc88\uc73c\ub85c \uac1c\ubc1c \ud658\uacbd\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 GitHub Codespaces\ub97c \ub3c4\uc785\ud558\uac8c \ub418\uc5c8\uc2b5\ub2c8\ub2e4.</p> <p>Gitpod \uac00\uc785 \uc2dc\uae30\uc640 \uc11c\ube44\uc2a4 \uc885\ub8cc \uc2dc\uc810\uc5d0 \ub530\ub77c, \uc774\uc804 \ud074\ub77c\uc6b0\ub4dc IDE\ub97c \ud1b5\ud574 \uad50\uc721 \ud658\uacbd\uc744 \uc2e4\ud589\ud560 \uc218 \uc788\uc744 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub9cc, \uc55e\uc73c\ub85c\ub294 \uc548\uc815\uc801\uc778 \uc811\uadfc\uc774 \ubcf4\uc7a5\ub418\uc9c0 \uc54a\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Open in Gitpod.</p>"},{"location":"ko/envsetup/01_setup/","title":"GitHub Codespaces","text":"<p>GitHub Codespaces\ub294 \ud300\uc774 \ud6a8\uc728\uc801\uc774\uace0 \uc548\uc804\ud558\uac8c \uc18c\ud504\ud2b8\uc6e8\uc5b4\ub97c \uac1c\ubc1c\ud560 \uc218 \uc788\ub3c4\ub85d \ub3c4\uc640\uc8fc\ub294 \ud074\ub77c\uc6b0\ub4dc \uac1c\ubc1c \ud658\uacbd\uc785\ub2c8\ub2e4. \ud559\uc2b5\uc790\uc5d0\uac8c \uc77c\uad00\ub418\uace0 \ucca0\uc800\ud788 \uac80\uc99d\ub41c \ud658\uacbd\uc744 \uc81c\uacf5\ud560 \uc218 \uc788\uae30 \ub54c\ubb38\uc5d0, \uc800\ud76c\ub294 \uc774\ub97c \uad50\uc721\uc6a9 \ud658\uacbd\uc73c\ub85c \uc0ac\uc6a9\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"ko/envsetup/01_setup/#github","title":"GitHub \uacc4\uc815 \ub9cc\ub4e4\uae30","text":"<p>GitHub \ud648\ud398\uc774\uc9c0\uc5d0\uc11c \ubb34\ub8cc GitHub \uacc4\uc815\uc744 \ub9cc\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"ko/envsetup/01_setup/#github-codespaces_1","title":"GitHub Codespaces \uc2e4\ud589\ud558\uae30","text":"<p>GitHub\uc5d0 \ub85c\uadf8\uc778\ud55c \ud6c4, \uc544\ub798 \ub9c1\ud06c\ub97c \ube0c\ub77c\uc6b0\uc800\uc5d0\uc11c \uc5f4\uba74 \uad50\uc721\uc6a9 \ud658\uacbd\uc774 \uc2e4\ud589\ub429\ub2c8\ub2e4: https://codespaces.new/nextflow-io/training?quickstart=1&amp;ref=master</p> <p>\ub610\ud55c, \uad50\uc721 \ud3ec\ud138 \ub0b4 \uc5ec\ub7ec \ud398\uc774\uc9c0\uc5d0 \ud45c\uc2dc\ub41c \uc544\ub798 \ubc84\ud2bc\uc744 \ud1b5\ud574\uc11c\ub3c4 \uc811\uadfc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p></p> <p>\ub9c1\ud06c\ub97c \uc5f4\uba74 \uc0c8\ub85c\uc6b4 GitHub Codespace\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\ub294 \ud654\uba74\uc774 \ud45c\uc2dc\ub429\ub2c8\ub2e4:</p> <p></p> <p>\"Change options\" \ubc84\ud2bc\uc744 \ud074\ub9ad\ud558\uba74 \uc0ac\uc6a9\ud560 \uba38\uc2e0\uc758 \uc0ac\uc591\uc744 \uc124\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ucf54\uc5b4 \uc218\uac00 \ub354 \ub9ce\uc740 \uba38\uc2e0\uc744 \uc0ac\uc6a9\ud558\uba74 Nextflow\uc758 \uc6cc\ud06c\ud50c\ub85c\uc6b0 \ubcd1\ub82c \ucc98\ub9ac \uae30\ub2a5\uc744 \ub354 \ud6a8\uacfc\uc801\uc73c\ub85c \ud65c\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>Hello Nextflow, Nextflow for Science, nf-core \uad50\uc721 \uacfc\uc815\uc758 \uacbd\uc6b0 4\ucf54\uc5b4 \uba38\uc2e0 \uc0ac\uc6a9\uc744 \uad8c\uc7a5\ud569\ub2c8\ub2e4.</p> <p>GitHub\uc758 \ubb34\ub8cc \ud50c\ub79c\uc740 \ub9e4\uc6d4 120\ucf54\uc5b4-\uc2dc\uac04\uc758 Codespaces \uc0ac\uc6a9\ub7c9\uc744 \uc81c\uacf5\ud558\uba70, \uc774\ub294 4\ucf54\uc5b4 \uba38\uc2e0 \uae30\uc900 \uc57d 30\uc2dc\uac04\uc5d0 \ud574\ub2f9\ud569\ub2c8\ub2e4. (\uc790\uc138\ud55c \uc0ac\uc6a9\ub7c9 \uc815\ubcf4\ub294 \uc544\ub798 \ucc38\uace0)</p> <p>Warning</p> <p>GitHub Codespaces \ud658\uacbd\uc744 \ucc98\uc74c \uc5f4 \ub54c\ub294 \uba87 \ubd84 \uc815\ub3c4 \uc2dc\uac04\uc774 \uac78\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub3d9\uc548 \ucc28 \ud55c \uc794\uc744 \uc900\ube44\ud558\uac70\ub098 \uc774\uba54\uc77c\uc744 \ud655\uc778\ud574 \ubcf4\uc138\uc694. \uadf8\ub8f9 \uad50\uc721 \uc911\uc774\ub77c\uba74 \uac04\ub2e8\ud788 \uc18c\uac1c \uc790\ub8cc\ub97c \uc0b4\ud3b4\ubcf4\ub294 \uac83\ub3c4 \uad1c\ucc2e\uaca0\uc8e0.</p>"},{"location":"ko/envsetup/01_setup/#github-codespaces-ide","title":"GitHub Codespaces IDE \ud0d0\uc0c9\ud558\uae30","text":"<p>GitHub Codespaces\uac00 \ub85c\ub529\ub418\uba74, \ub2e4\uc74c\uacfc \uc720\uc0ac\ud55c \ud654\uba74\uc774 \ub098\ud0c0\ub0a9\ub2c8\ub2e4 (\uacc4\uc815 \uc124\uc815\uc5d0 \ub530\ub77c \ubc1d\uc740 \ud14c\ub9c8\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4):</p> <p></p> <p>\uc774\uac83\uc740 VSCode IDE\uc758 \uc778\ud130\ud398\uc774\uc2a4\uc774\uba70, Nextflow \uac1c\ubc1c\uc5d0 \uc801\ud569\ud55c \uc778\uae30 \uc788\ub294 \ucf54\ub4dc \ud3b8\uc9d1\uae30\uc785\ub2c8\ub2e4.</p> <ul> <li>\uc0ac\uc774\ub4dc\ubc14(Sidebar) \ud658\uacbd\uc744 \ucee4\uc2a4\ud130\ub9c8\uc774\uc988\ud558\uac70\ub098 \ud30c\uc77c \uc5f4\uae30, \uac80\uc0c9, Git \uc791\uc5c5 \ub4f1\uc744 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud0d0\uc0c9\uae30 \uc544\uc774\ucf58\uc744 \ud074\ub9ad\ud558\uba74 \ud604\uc7ac \uc800\uc7a5\uc18c\uc5d0 \ud3ec\ud568\ub41c \ud30c\uc77c\ub4e4\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\ud130\ubbf8\ub110(Terminal) \uc800\uc7a5\uc18c \ub0b4 \uc124\uce58\ub41c \ud504\ub85c\uadf8\ub7a8\uc744 \uc2e4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc608: <code>nextflow</code>, <code>docker</code>.</li> <li>\ud30c\uc77c \ud0d0\uc0c9\uae30(File Explorer) \ud30c\uc77c\uc744 \uc5f4\uace0 \ud3b8\uc9d1\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud074\ub9ad \uc2dc \uba54\uc778 \ud3b8\uc9d1\uae30\uc5d0 \uc5f4\ub9bd\ub2c8\ub2e4.</li> <li>\uba54\uc778 \ud3b8\uc9d1\uae30(Main Editor) \uc608\uc2dc\ub85c <code>README.md</code> \ud30c\uc77c\uc774 \ubbf8\ub9ac \uc5f4\ub824 \uc788\uc73c\uba70, \ucf54\ub4dc\ub098 \ub370\uc774\ud130 \ud30c\uc77c\uc744 \uc5f4\uba74 \uc774\uacf3\uc5d0 \ud45c\uc2dc\ub429\ub2c8\ub2e4.</li> </ul>"},{"location":"ko/envsetup/01_setup/#github-codespaces_2","title":"GitHub Codespaces \uc138\uc158 \ub2e4\uc2dc \uc5f4\uae30","text":"<p>\ud55c \ubc88 \uc0dd\uc131\ud55c \ud658\uacbd\uc740 \uc27d\uac8c \uc7ac\uac1c\ud558\uac70\ub098 \ub2e4\uc2dc \uc2dc\uc791\ud560 \uc218 \uc788\uc73c\uba70, \ub9c8\uc9c0\ub9c9 \uc0c1\ud0dc\ubd80\ud130 \uc774\uc5b4\uc11c \uc791\uc5c5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. 30\ubd84 \uc774\uc0c1 \uc544\ubb34 \ud65c\ub3d9\uc774 \uc5c6\uc73c\uba74 \ud658\uacbd\uc774 \uc790\ub3d9 \uc885\ub8cc\ub418\uba70, \ubcc0\uacbd \ub0b4\uc6a9\uc740 \ucd5c\ub300 2\uc8fc \ub3d9\uc548 \uc800\uc7a5\ub429\ub2c8\ub2e4.</p> <p>https://github.com/codespaces/ \uc5d0\uc11c \uc774\uc804 \uc138\uc158\uc744 \ud655\uc778\ud558\uace0 \uc7ac\uac1c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub9ac\uc2a4\ud2b8\uc5d0\uc11c \uc6d0\ud558\ub294 \uc138\uc158\uc744 \ud074\ub9ad\ud558\uba74 \ub2e4\uc2dc \uc5f4\ub9bd\ub2c8\ub2e4.</p> <p></p> <p>\uc774\uc804 \uc138\uc158\uc758 URL\uc744 \uc800\uc7a5\ud574 \ub450\uc5c8\ub2e4\uba74, \ube0c\ub77c\uc6b0\uc800\uc5d0\uc11c \ub2e4\uc2dc \uc5f4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud639\uc740 \ucc98\uc74c \uc0dd\uc131\ud560 \ub54c \ud074\ub9ad\ud588\ub358 \ubc84\ud2bc\uc744 \ub2e4\uc2dc \ud074\ub9ad\ud574\ub3c4 \ub429\ub2c8\ub2e4:</p> <p></p> <p>\uc774\uc804 \uc138\uc158\uc774 \uc788\ub2e4\uba74 \uae30\ubcf8\uc801\uc73c\ub85c \uc774\uc5b4\uc11c \uc5f4\ub3c4\ub85d \uc124\uc815\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4:</p> <p></p>"},{"location":"ko/envsetup/01_setup/#github-codespaces_3","title":"GitHub Codespaces\uc5d0\uc11c \ud30c\uc77c\uc744 \ub85c\uceec\ub85c \uc800\uc7a5\ud558\uae30","text":"<p>\ud30c\uc77c \ud0d0\uc0c9\uae30\uc5d0\uc11c \uc6d0\ud558\ub294 \ud30c\uc77c\uc744 \ub9c8\uc6b0\uc2a4 \uc624\ub978\ucabd \ud074\ub9ad\ud55c \ub4a4 <code>Download</code>\ub97c \uc120\ud0dd\ud558\uba74 \ub85c\uceec\ub85c \uc800\uc7a5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"ko/envsetup/01_setup/#github-codespaces_4","title":"GitHub Codespaces \uc0ac\uc6a9\ub7c9","text":"<p>GitHub Codespaces\ub294 \ub9e4\uc6d4 \ucd5c\ub300 15GB \uc800\uc7a5 \uacf5\uac04\uacfc 120 \ucf54\uc5b4-\uc2dc\uac04\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774\ub294 \uae30\ubcf8 \ud658\uacbd \uae30\uc900\uc73c\ub85c \uc57d 60\uc2dc\uac04 \uc815\ub3c4 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \ubd84\ub7c9\uc785\ub2c8\ub2e4 (2\ucf54\uc5b4, 8GB RAM, 32GB \uc800\uc7a5\uc18c \uae30\uc900).</p> <p>GitHub Codespaces \ud658\uacbd\uc740 \uc0ac\uc6a9\uc790\uc758 \ud544\uc694\uc5d0 \ub530\ub77c \uad6c\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub354 \ub192\uc740 \uc0ac\uc591\uc73c\ub85c \ud658\uacbd\uc744 \uc0dd\uc131\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uc774 \uacbd\uc6b0 \ubb34\ub8cc \uc0ac\uc6a9\ub7c9\uc774 \ub354 \ube68\ub9ac \uc18c\ubaa8\ub418\uc5b4 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \uc2dc\uac04\uc774 \uc904\uc5b4\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ucd94\uac00 \ub9ac\uc18c\uc2a4\uac00 \ud544\uc694\ud55c \uacbd\uc6b0, \uc720\ub8cc\ub85c \uad6c\ub9e4\ud558\uc5ec \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc790\uc138\ud55c \ub0b4\uc6a9\uc740 GitHub \uacf5\uc2dd \ubb38\uc11c\ub97c \ucc38\uace0\ud558\uc138\uc694: GitHub Codespaces \uc694\uae08\uc81c \uad00\ub828 \ubb38\uc11c</p>"},{"location":"ko/envsetup/02_local/","title":"\ub85c\uceec \uc124\uce58","text":"<p>\uc5b4\ub5a0\ud55c \uc774\uc720\ub85c\ub4e0 GitHub Codespaces \uc138\uc158\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc5c6\ub294 \uacbd\uc6b0, \ubaa8\ub4e0 \ud658\uacbd\uc744 \ub85c\uceec\uc5d0 \uc124\uce58\ud558\uc5ec \uc0ac\uc6a9\ud560\uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\ub85c\uceec \uba38\uc2e0\uc758 \uc6b4\uc601\uccb4\uc81c\ub098 \uc124\uc815\uc5d0 \ub530\ub77c \uc694\uad6c \uc0ac\ud56d\uc774 \ub2e4\ub97c\uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"ko/envsetup/02_local/#_2","title":"\uc694\uad6c \uc0ac\ud56d","text":"<p>Nextflow\ub294 POSIX \ud638\ud658 \uc2dc\uc2a4\ud15c(Linux, macOS, Windows Subsystem for Linux \ub4f1)\uc5d0\uc11c \uc0ac\uc6a9 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p> <p>\ud544\uc218 \uad6c\uc131 \uc694\uc18c</p> <ul> <li>Bash</li> <li>Java 11 \uc774\uc0c1 (\ucd5c\ub300 21\uae4c\uc9c0 \uc9c0\uc6d0)</li> <li>Git</li> <li>Docker</li> </ul> <p>\uc120\ud0dd \uad6c\uc131 \uc694\uc18c</p> <ul> <li>Singularity 2.5.x (\uc774\uc0c1)</li> <li>Conda 4.5 (\uc774\uc0c1)</li> <li>Graphviz</li> <li>AWS CLI</li> <li>\uc0ac\uc804\uc5d0 \uc124\uc815\ub41c AWS Batch \uc791\uc5c5 \uc2e4\ud589 \ud658\uacbd</li> </ul>"},{"location":"ko/envsetup/02_local/#nextflow","title":"Nextflow \ub2e4\uc6b4\ub85c\ub4dc","text":"<p>\ud130\ubbf8\ub110\uc5d0\uc11c \ub2e4\uc74c \uba85\ub839\uc5b4\ub97c \uc2e4\ud589\ud558\uc138\uc694:</p> <pre><code>wget -qO- https://get.nextflow.io | bash\n</code></pre> <p>\ub610\ub294 <code>curl</code> \uba85\ub839\uc5b4\ub97c \uc0ac\uc6a9\ud560\uc218 \uc788\uc2b5\ub2c8\ub2e4:</p> <pre><code>curl -s https://get.nextflow.io | bash\n</code></pre> <p>\ub2e4\uc74c\uc73c\ub85c, \ub2e4\uc6b4\ub85c\ub4dc\ub41c \uc2e4\ud589 \ud30c\uc77c\uc5d0 \uc2e4\ud589 \uad8c\ud55c\uc744 \ubd80\uc5ec\ud558\uc138\uc694:</p> <pre><code>chmod +x nextflow\n</code></pre> <p>\ub9c8\uc9c0\ub9c9\uc73c\ub85c, <code>nextflow</code> \uc2e4\ud589 \ud30c\uc77c\uc774 <code>$PATH</code>\uc5d0 \ud3ec\ud568\ub418\uc5b4 \uc788\ub294\uc9c0 \ud655\uc778\ud558\uc138\uc694. \uc2e4\ud589 \ud30c\uc77c\uc740 <code>/usr/local/bin</code>, <code>/bin/</code> \ub4f1\uc758 \uacbd\ub85c\uc5d0 \uc704\uce58\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"ko/envsetup/02_local/#docker","title":"Docker","text":"<p>Docker Desktop\uc774 \ucef4\ud4e8\ud130\uc5d0\uc11c \uc2e4\ud589\ub418\uace0 \uc788\ub294\uc9c0 \ud655\uc778\ud558\uc138\uc694. \uc124\uce58\ud558\uc9c0 \uc54a\uc558\ub2e4\uba74 \uc774 \ub9c1\ud06c\ub97c \ud1b5\ud574 \ub2e4\uc6b4\ub85c\ub4dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"ko/envsetup/02_local/#_3","title":"\uad50\uc721 \uc790\ub8cc","text":"<p>\uc774 \ub9c1\ud06c\uc5d0\uc11c \uad50\uc721 \uc790\ub8cc\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc790\ub8cc\ub97c \ub2e4\uc6b4\ub85c\ub4dc\ud558\ub824\uba74 \ub2e4\uc74c \uba85\ub839\uc5b4\ub97c \uc2e4\ud589\ud558\uc138\uc694\uc694:</p> <pre><code>git clone https://github.com/nextflow-io/training.git\n</code></pre> <p>\uc774\ud6c4, \ud130\ubbf8\ub110\uc5d0\uc11c <code>cd</code> \uba85\ub839\uc5b4\ub97c \uc0ac\uc6a9\ud574 \ud574\ub2f9 \ub514\ub809\ud1a0\ub9ac\ub85c \uc774\ub3d9\ud558\uc138\uc694. \uae30\ubcf8\uc801\uc73c\ub85c <code>hello-nextflow</code> \ub514\ub809\ud1a0\ub9ac\ub85c \uc774\ub3d9\ud558\uba74 \ub429\ub2c8\ub2e4.</p>"},{"location":"ko/envsetup/02_local/#_4","title":"\uc124\uce58 \ud655\uc778","text":"<p>\ub2e4\uc74c \uba85\ub839\uc5b4\ub97c \uc2e4\ud589\ud558\uc5ec <code>nextflow</code>\uac00 \uc62c\ubc14\ub974\uac8c \uc124\uce58\ub418\uc5c8\ub294\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4:</p> <pre><code>nextflow info\n</code></pre> <p>\ud574\ub2f9 \uba85\ub839\uc5b4\ub97c \uc2e4\ud589\ud558\uba74 \ud604\uc7ac \ubc84\uc804, \uc2dc\uc2a4\ud15c \uc815\ubcf4, \ub7f0\ud0c0\uc784 \uc815\ubcf4\uac00 \ucd9c\ub825\ub429\ub2c8\ub2e4.</p> <p>\uc5f0\uc2b5 \ubb38\uc81c</p> <p>\ud658\uacbd\uc774 \uc81c\ub300\ub85c \uc791\ub3d9\ud558\ub294\uc9c0 \ud655\uc778\ud558\ub824\uba74 \ub2e4\uc74c \uba85\ub839\uc5b4\ub97c \uc2e4\ud589\ud574 \ubcf4\uc138\uc694:</p> <pre><code>nextflow info\n</code></pre> <p>\uc2e4\ud589 \uacb0\uacfc\uc5d0\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 Nextflow \ubc84\uc804 \ubc0f \ub7f0\ud0c0\uc784 \uc815\ubcf4\uac00 \ucd9c\ub825\ub429\ub2c8\ub2e4 (\uc2e4\uc81c \ubc84\uc804\uc740 \ub2e4\ub97c \uc218 \uc788\uc2b5\ub2c8\ub2e4):</p> <pre><code>Version: 23.10.1 build 5891\nCreated: 12-01-2024 22:01 UTC\nSystem: Linux 6.1.75-060175-generic\nRuntime: Groovy 3.0.19 on OpenJDK 64-Bit Server VM 11.0.1-internal+0-adhoc..src\nEncoding: UTF-8 (UTF-8)\n</code></pre>"}]}